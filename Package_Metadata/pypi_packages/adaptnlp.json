{"info": {"author": "Andrew Chang", "author_email": "achang@novetta.com", "bugtrack_url": null, "classifiers": [], "description": "<p align=\"center\">\n    <a href=\"https://github.com/Novetta/adaptnlp\"> <img src=\"https://raw.githubusercontent.com/novetta/adaptnlp/master/docs/img/NovettaAdaptNLPlogo-400px.png\" width=\"400\"/></a>\n</p>\n\n<p align=\"center\">\n<strong> A high level framework and library for running, training, and deploying state-of-the-art Natural Language Processing (NLP) models for end to end tasks.</strong>\n</p>\n<p align=\"center\">\n    <a href=\"https://circleci.com/gh/Novetta/adaptnlp\">\n        <img src=\"https://img.shields.io/circleci/build/github/Novetta/adaptnlp/master\">\n    </a>\n    <a href=\"https://badge.fury.io/py/adaptnlp\">\n        <img src=\"https://badge.fury.io/py/adaptnlp.svg\">\n    </a>\n    <a href=\"https://github.com/Novetta/adaptnlp/blob/master/LICENSE\">\n        <img src=\"https://img.shields.io/github/license/novetta/adaptnlp\">\n    </a>\n</p>\n\n\nAdaptNLP allows users ranging from beginner python coders to experienced machine learning engineers to leverage\nstate-of-the-art NLP models and training techniques in one easy-to-use python package.\n\nBuilt atop Zalando Research's Flair and Hugging Face's Transformers library, AdaptNLP provides Machine\nLearning Researchers and Scientists a modular and **adaptive** approach to a variety of NLP tasks with an\n**Easy** API for training, inference, and deploying NLP-based microservices.\n\n**Key Features**\n\n  - **[Full Guides and API Documentation](https://novetta.github.io/adaptnlp)**\n  - [Tutorial](https://github.com/Novetta/adaptnlp/tree/master/tutorials) Jupyter/Google Colab Notebooks\n  - Unified API for NLP Tasks with SOTA Pretrained Models (Adaptable with Flair and Transformer's Models)\n    - Token Tagging \n    - Sequence Classification\n    - Embeddings\n    - Question Answering\n    - Summarization\n    - Translation\n    - <em> More in development </em>\n  - Training and Fine-tuning Interface\n    - Jeremy's **[ULM-FIT](https://arxiv.org/abs/1801.06146)** approach for transfer learning in NLP\n    -  Fine-tuning Transformer's language models and task-specific predictive heads like Flair's `SequenceClassifier`\n  - [Rapid NLP Model Deployment](https://github.com/Novetta/adaptnlp/tree/master/rest) with Sebasti\u00e1n's [FastAPI](https://github.com/tiangolo/fastapi) Framework\n    - Containerized FastAPI app\n    - Immediately deploy any custom trained Flair or AdaptNLP model\n  - [Dockerizing AdaptNLP with GPUs](https://hub.docker.com/r/achangnovetta/adaptnlp)\n    - Easily build and run AdaptNLP containers leveraging NVIDIA GPUs with Docker\n\n\n## Quick Start\n\n#### Requirements and Installation\n\n##### Virtual Environment\nTo avoid dependency clustering and issues, it would be wise to install AdaptNLP in a virtual environment.\nTo create a new python 3.6+ virtual environment, run this command and then activate it however your operating\nsystem specifies:\n\n```\npython -m venv venv-adaptnlp\n```\n\n##### AdaptNLP Install\n\nInstall using pip in your virtual environment:\n```\npip install adaptnlp\n```\n\nIf you want to work on AdaptNLP, `pip install adaptnlp[dev]` will install its development tools.\n\n\n#### Examples and General Use\n\nOnce you have installed AdaptNLP, here are a few examples of what you can run with AdaptNLP modules:\n\n##### Named Entity Recognition with `EasyTokenTagger`\n\n```python\nfrom adaptnlp import EasyTokenTagger\n\n## Example Text\nexample_text = \"Novetta's headquarters is located in Mclean, Virginia.\"\n\n## Load the token tagger module and tag text with the NER model \ntagger = EasyTokenTagger()\nsentences = tagger.tag_text(text=example_text, model_name_or_path=\"ner\")\n\n## Output tagged token span results in Flair's Sentence object model\nfor sentence in sentences:\n    for entity in sentence.get_spans(\"ner\"):\n        print(entity)\n\n```\n\n##### English Sentiment Classifier `EasySequenceClassifier`\n\n```python\nfrom adaptnlp import EasySequenceClassifier \n\n## Example Text\nexample_text = \"Novetta is a great company that was chosen as one of top 50 great places to work!\"\n\n## Load the sequence classifier module and classify sequence of text with the english sentiment model \nclassifier = EasySequenceClassifier()\nsentences = classifier.tag_text(text=example_text, mini_batch_size=1, model_name_or_path=\"en-sentiment\")\n\n## Output labeled text results in Flair's Sentence object model\nfor sentence in sentences:\n    print(sentence.labels)\n\n```\n\n##### Span-based Question Answering `EasyQuestionAnswering`\n\n```python\nfrom adaptnlp import EasyQuestionAnswering \n\n## Example Query and Context \nquery = \"What is the meaning of life?\"\ncontext = \"Machine Learning is the meaning of life.\"\ntop_n = 5\n\n## Load the QA module and run inference on results \nqa = EasyQuestionAnswering()\nbest_answer, best_n_answers = qa.predict_qa(query=query, context=context, n_best_size=top_n, mini_batch_size=1, model_name_or_path=\"distilbert-base-uncased-distilled-squad\")\n\n## Output top answer as well as top 5 answers\nprint(best_answer)\nprint(best_n_answers)\n```\n\n##### Summarization `EasySummarizer`\n\n```python\nfrom adaptnlp import EasySummarizer\n\n# Text from encyclopedia Britannica on Einstein\ntext = \"\"\"Einstein would write that two \u201cwonders\u201d deeply affected his early years. The first was his encounter with a compass at age five. \n          He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. \n          The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry \n          book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on \n          the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge \n          to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized \n          by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would \n          never amount to anything.\"\"\"\n\nsummarizer = EasySummarizer()\n\n# Summarize\nsummaries = summarizer.summarize(text = text, model_name_or_path=\"t5-small\", mini_batch_size=1, num_beams = 4, min_length=0, max_length=100, early_stopping=True)\n\nprint(\"Summaries:\\n\")\nfor s in summaries:\n    print(s, \"\\n\")\n```\n\n##### Translation `EasyTranslator`\n```python\nfrom adaptnlp import EasyTranslator\n\ntext = [\"Machine learning will take over the world very soon.\",\n        \"Machines can speak in many languages.\",]\n\ntranslator = EasyTranslator()\n\n# Translate\ntranslations = translator.translate(text = text, t5_prefix=\"translate English to German\", model_name_or_path=\"t5-small\", mini_batch_size=1, min_length=0, max_length=100, early_stopping=True)\n\nprint(\"Translations:\\n\")\nfor t in translations:\n    print(t, \"\\n\")\n```\n\n##### Sequence Classification Training `SequenceClassifier`\n```python\nfrom adaptnlp import EasyDocumentEmbeddings, SequenceClassifierTrainer \n\n# Specify corpus data directory and model output directory\ncorpus = \"Path/to/data/directory\" \nOUTPUT_DIR = \"Path/to/output/directory\" \n\n# Instantiate AdaptNLP easy document embeddings module, which can take in a variable number of embeddings to make `Stacked Embeddings`.  \n# You may also use custom Transformers LM models by specifying the path the the language model\ndoc_embeddings = EasyDocumentEmbeddings(model_name_or_path=\"bert-base-cased\", methods = [\"rnn\"])\n\n# Instantiate Sequence Classifier Trainer by loading in the data, data column map, and embeddings as an encoder\nsc_trainer = SequenceClassifierTrainer(corpus=corpus, encoder=doc_embeddings, column_name_map={0: \"text\", 1:\"label\"})\n\n# Find Learning Rate\nlearning_rate = sc_trainer.find_learning_rate(output_dir=OUTPUT_DIR)\n\n# Train Using Flair's Sequence Classification Head\nsc_trainer.train(output_dir=OUTPUT_DIR, learning_rate=learning_rate, max_epochs=150)\n\n\n# Predict text labels with the trained model using `EasySequenceClassifier`\nfrom adaptnlp import EasySequenceClassifier\nexample_text = '''Where was the Queen's wedding held? '''\nclassifier = EasySequenceClassifier()\nsentences = classifier.tag_text(example_text, model_name_or_path=OUTPUT_DIR / \"final-model.pt\")\nprint(\"Label output:\\n\")\nfor sentence in sentences:\n    print(sentence.labels)\n```\n\n##### Transformers Language Model Fine Tuning `LMFineTuner`\n\n```python\nfrom adaptnlp import LMFineTuner\n\n# Specify Text Data File Paths\ntrain_data_file = \"Path/to/train.csv\"\neval_data_file = \"Path/to/test.csv\"\n\n# Instantiate Finetuner with Desired Language Model\nfinetuner = LMFineTuner(train_data_file=train_data_file, eval_data_file=eval_data_file, model_type=\"bert\", model_name_or_path=\"bert-base-cased\")\nfinetuner.freeze()\n\n# Find Optimal Learning Rate\nlearning_rate = finetuner.find_learning_rate(base_path=\"Path/to/base/directory\")\nfinetuner.freeze()\n\n# Train and Save Fine Tuned Language Models\nfinetuner.train_one_cycle(output_dir=\"Path/to/output/directory\", learning_rate=learning_rate)\n\n```\n\n## Tutorials\n\nLook in the [Tutorials](tutorials) directory for a quick introduction to the library and its very simple\nand straight forward use cases:\n\n**NLP Tasks**\n\n  1. [Token Classification: NER, POS, Chunk, and Frame Tagging](tutorials/1.%20Token%20Classification)\n      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/1.%20Token%20Classification/token_tagging.ipynb)\n  2. [Sequence Classification: Sentiment](tutorials/2.%20Sequence%20Classification)\n      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/2.%20Sequence%20Classification/sequence_classification.ipynb)\n  3. [Embeddings: Transformer Embeddings e.g. BERT, XLM, GPT2, XLNet, roBERTa, ALBERT](tutorials/3.%20Embeddings)\n      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/3.%20Embeddings/embeddings.ipynb)\n  4. [Question Answering: Span-based Question Answering Model](tutorials/4.%20Question%20Answering)\n      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/4.%20Question%20Answering/question_answering.ipynb)\n  5. [Summarization: Abstractive and Extractive](tutorials/5.%20Summarization)\n      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/5.%20Summarization/summarization.ipynb)\n  6. [Translation: Seq2Seq](tutorials/6.%20Translation)\n      - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/6.%20Translation/translation.ipynb)\n\n**[Custom Fine-Tuning and Training with Transformer Models](tutorials/Finetuning%20and%20Training%20(Advanced))**\n\n - Training a Sequence Classifier\n   - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/Finetuning%20and%20Training%20(Advanced)/sequence_classification_training.ipynb)\n - Fine-tuning a Transformers Language Model\n   - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/Finetuning%20and%20Training%20(Advanced)/fine_tuning.ipynb)\n\nCheckout the [documentation](https://novetta.github.io/adaptnlp) for more information.\n\n## REST Service \n\nWe use FastAPI for standing up endpoints for serving state-of-the-art NLP models with AdaptNLP.\n\n![Swagger Example](https://raw.githubusercontent.com/novetta/adaptnlp/master/docs/img/fastapi-docs.png)\n\nThe [REST](https://github.com/Novetta/adaptnlp/tree/master/rest) directory contains more detail on deploying a REST API locally or with docker in a very easy and\nfast way.\n\n## Docker\n\nAdaptNLP official docker images are up on [Docker Hub](https://hub.docker.com/r/achangnovetta/adaptnlp).\n\nImages have AdaptNLP installed from source in developer mode with tutorial notebooks available.\n\nImages can build with GPU support if NVIDA-Docker is correctly installed.\n\n#### Pull and Run AdaptNLP Immediately\nSimply run an image with AdaptNLP installed from source in developer mode by running:\n```\ndocker run -it --rm achangnovetta/adaptnlp:latest\n```\nRun an image with AdaptNLP running on GPUs if you have nvidia drivers and nvidia-docker 19.03+ installed:\n```\ndocker run -it --rm --gpus all achangnovetta/adaptnlp:latest\n```\n\n#### Build\n\nBuild docker image and run container with the following commands in the directory of the Dockerfile\nto create a container with adaptnlp installed and ready to go\n\nNote: A container with GPUs enabled requires Docker version 19.03+ and nvida-docker installed\n```\ndocker build -t achangnovetta/adaptnlp:latest .\ndocker run -it --rm achangnovetta/adaptnlp:latest\n```\nIf you want to use CUDA compatible GPUs \n```\ndocker run -it --rm --gpus all achangnovetta/adaptnlp:latest\n```\n\n## Contact\n\nPlease contact the author Andrew Chang at achang@novetta.com with questions or comments regarding AdaptNLP.\n\nFollow  us on Twitter at [@achang1618](https://twitter.com/achang1618) and [@AdaptNLP](https://twitter.com/AdaptNLP) for\nupdates and NLP dialogue.\n\n## License\n\nThis project is licensed under the terms of the Apache 2.0 license.\n\n\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "NLP,flair,Natural Language Processing,Machine Learning,ML,torch,pytorch,NER", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "adaptnlp", "package_url": "https://pypi.org/project/adaptnlp/", "platform": "", "project_url": "https://pypi.org/project/adaptnlp/", "project_urls": null, "release_url": "https://pypi.org/project/adaptnlp/0.1.6/", "requires_dist": ["click", "jupyter", "jupyterlab", "torch (==1.4.0)", "transformers (==2.8.0)", "flair (==0.4.5)", "html-text", "flake8 ; extra == 'dev'", "black ; extra == 'dev'"], "requires_python": "", "summary": "AdaptNLP: A Natural Language Processing Library and Framework", "version": "0.1.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p align=\"center\">\n    <a href=\"https://github.com/Novetta/adaptnlp\" rel=\"nofollow\"> <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0ed67a79db91b1816c98d21b269798d129be2619/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e6f76657474612f61646170746e6c702f6d61737465722f646f63732f696d672f4e6f766574746141646170744e4c506c6f676f2d34303070782e706e67\" width=\"400\"></a>\n</p>\n<p align=\"center\">\n<strong> A high level framework and library for running, training, and deploying state-of-the-art Natural Language Processing (NLP) models for end to end tasks.</strong>\n</p>\n<p align=\"center\">\n    <a href=\"https://circleci.com/gh/Novetta/adaptnlp\" rel=\"nofollow\">\n        <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/18b353e2f735b5d54cda68281b86339b44763315/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f6275696c642f6769746875622f4e6f76657474612f61646170746e6c702f6d6173746572\">\n    </a>\n    <a href=\"https://badge.fury.io/py/adaptnlp\" rel=\"nofollow\">\n        <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d67e30b43039edc14e00479babc299c743c91efe/68747470733a2f2f62616467652e667572792e696f2f70792f61646170746e6c702e737667\">\n    </a>\n    <a href=\"https://github.com/Novetta/adaptnlp/blob/master/LICENSE\" rel=\"nofollow\">\n        <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d9f73c8a30cbb4d19344c623890628ca4f3092ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6e6f76657474612f61646170746e6c70\">\n    </a>\n</p>\n<p>AdaptNLP allows users ranging from beginner python coders to experienced machine learning engineers to leverage\nstate-of-the-art NLP models and training techniques in one easy-to-use python package.</p>\n<p>Built atop Zalando Research's Flair and Hugging Face's Transformers library, AdaptNLP provides Machine\nLearning Researchers and Scientists a modular and <strong>adaptive</strong> approach to a variety of NLP tasks with an\n<strong>Easy</strong> API for training, inference, and deploying NLP-based microservices.</p>\n<p><strong>Key Features</strong></p>\n<ul>\n<li><strong><a href=\"https://novetta.github.io/adaptnlp\" rel=\"nofollow\">Full Guides and API Documentation</a></strong></li>\n<li><a href=\"https://github.com/Novetta/adaptnlp/tree/master/tutorials\" rel=\"nofollow\">Tutorial</a> Jupyter/Google Colab Notebooks</li>\n<li>Unified API for NLP Tasks with SOTA Pretrained Models (Adaptable with Flair and Transformer's Models)\n<ul>\n<li>Token Tagging</li>\n<li>Sequence Classification</li>\n<li>Embeddings</li>\n<li>Question Answering</li>\n<li>Summarization</li>\n<li>Translation</li>\n<li><em> More in development </em></li>\n</ul>\n</li>\n<li>Training and Fine-tuning Interface\n<ul>\n<li>Jeremy's <strong><a href=\"https://arxiv.org/abs/1801.06146\" rel=\"nofollow\">ULM-FIT</a></strong> approach for transfer learning in NLP</li>\n<li>Fine-tuning Transformer's language models and task-specific predictive heads like Flair's <code>SequenceClassifier</code></li>\n</ul>\n</li>\n<li><a href=\"https://github.com/Novetta/adaptnlp/tree/master/rest\" rel=\"nofollow\">Rapid NLP Model Deployment</a> with Sebasti\u00e1n's <a href=\"https://github.com/tiangolo/fastapi\" rel=\"nofollow\">FastAPI</a> Framework\n<ul>\n<li>Containerized FastAPI app</li>\n<li>Immediately deploy any custom trained Flair or AdaptNLP model</li>\n</ul>\n</li>\n<li><a href=\"https://hub.docker.com/r/achangnovetta/adaptnlp\" rel=\"nofollow\">Dockerizing AdaptNLP with GPUs</a>\n<ul>\n<li>Easily build and run AdaptNLP containers leveraging NVIDIA GPUs with Docker</li>\n</ul>\n</li>\n</ul>\n<h2>Quick Start</h2>\n<h4>Requirements and Installation</h4>\n<h5>Virtual Environment</h5>\n<p>To avoid dependency clustering and issues, it would be wise to install AdaptNLP in a virtual environment.\nTo create a new python 3.6+ virtual environment, run this command and then activate it however your operating\nsystem specifies:</p>\n<pre><code>python -m venv venv-adaptnlp\n</code></pre>\n<h5>AdaptNLP Install</h5>\n<p>Install using pip in your virtual environment:</p>\n<pre><code>pip install adaptnlp\n</code></pre>\n<p>If you want to work on AdaptNLP, <code>pip install adaptnlp[dev]</code> will install its development tools.</p>\n<h4>Examples and General Use</h4>\n<p>Once you have installed AdaptNLP, here are a few examples of what you can run with AdaptNLP modules:</p>\n<h5>Named Entity Recognition with <code>EasyTokenTagger</code></h5>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adaptnlp</span> <span class=\"kn\">import</span> <span class=\"n\">EasyTokenTagger</span>\n\n<span class=\"c1\">## Example Text</span>\n<span class=\"n\">example_text</span> <span class=\"o\">=</span> <span class=\"s2\">\"Novetta's headquarters is located in Mclean, Virginia.\"</span>\n\n<span class=\"c1\">## Load the token tagger module and tag text with the NER model </span>\n<span class=\"n\">tagger</span> <span class=\"o\">=</span> <span class=\"n\">EasyTokenTagger</span><span class=\"p\">()</span>\n<span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"n\">tagger</span><span class=\"o\">.</span><span class=\"n\">tag_text</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">example_text</span><span class=\"p\">,</span> <span class=\"n\">model_name_or_path</span><span class=\"o\">=</span><span class=\"s2\">\"ner\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\">## Output tagged token span results in Flair's Sentence object model</span>\n<span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">sentences</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">entity</span> <span class=\"ow\">in</span> <span class=\"n\">sentence</span><span class=\"o\">.</span><span class=\"n\">get_spans</span><span class=\"p\">(</span><span class=\"s2\">\"ner\"</span><span class=\"p\">):</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">entity</span><span class=\"p\">)</span>\n</pre>\n<h5>English Sentiment Classifier <code>EasySequenceClassifier</code></h5>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adaptnlp</span> <span class=\"kn\">import</span> <span class=\"n\">EasySequenceClassifier</span> \n\n<span class=\"c1\">## Example Text</span>\n<span class=\"n\">example_text</span> <span class=\"o\">=</span> <span class=\"s2\">\"Novetta is a great company that was chosen as one of top 50 great places to work!\"</span>\n\n<span class=\"c1\">## Load the sequence classifier module and classify sequence of text with the english sentiment model </span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"n\">EasySequenceClassifier</span><span class=\"p\">()</span>\n<span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">tag_text</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">example_text</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">model_name_or_path</span><span class=\"o\">=</span><span class=\"s2\">\"en-sentiment\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\">## Output labeled text results in Flair's Sentence object model</span>\n<span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">sentences</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"o\">.</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<h5>Span-based Question Answering <code>EasyQuestionAnswering</code></h5>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adaptnlp</span> <span class=\"kn\">import</span> <span class=\"n\">EasyQuestionAnswering</span> \n\n<span class=\"c1\">## Example Query and Context </span>\n<span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"s2\">\"What is the meaning of life?\"</span>\n<span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"s2\">\"Machine Learning is the meaning of life.\"</span>\n<span class=\"n\">top_n</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n\n<span class=\"c1\">## Load the QA module and run inference on results </span>\n<span class=\"n\">qa</span> <span class=\"o\">=</span> <span class=\"n\">EasyQuestionAnswering</span><span class=\"p\">()</span>\n<span class=\"n\">best_answer</span><span class=\"p\">,</span> <span class=\"n\">best_n_answers</span> <span class=\"o\">=</span> <span class=\"n\">qa</span><span class=\"o\">.</span><span class=\"n\">predict_qa</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"o\">=</span><span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"o\">=</span><span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">n_best_size</span><span class=\"o\">=</span><span class=\"n\">top_n</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">model_name_or_path</span><span class=\"o\">=</span><span class=\"s2\">\"distilbert-base-uncased-distilled-squad\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\">## Output top answer as well as top 5 answers</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">best_answer</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">best_n_answers</span><span class=\"p\">)</span>\n</pre>\n<h5>Summarization <code>EasySummarizer</code></h5>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adaptnlp</span> <span class=\"kn\">import</span> <span class=\"n\">EasySummarizer</span>\n\n<span class=\"c1\"># Text from encyclopedia Britannica on Einstein</span>\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s2\">\"\"\"Einstein would write that two \u201cwonders\u201d deeply affected his early years. The first was his encounter with a compass at age five. </span>\n<span class=\"s2\">          He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. </span>\n<span class=\"s2\">          The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry </span>\n<span class=\"s2\">          book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on </span>\n<span class=\"s2\">          the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge </span>\n<span class=\"s2\">          to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized </span>\n<span class=\"s2\">          by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would </span>\n<span class=\"s2\">          never amount to anything.\"\"\"</span>\n\n<span class=\"n\">summarizer</span> <span class=\"o\">=</span> <span class=\"n\">EasySummarizer</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Summarize</span>\n<span class=\"n\">summaries</span> <span class=\"o\">=</span> <span class=\"n\">summarizer</span><span class=\"o\">.</span><span class=\"n\">summarize</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">model_name_or_path</span><span class=\"o\">=</span><span class=\"s2\">\"t5-small\"</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">num_beams</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">min_length</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">max_length</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">early_stopping</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Summaries:</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">summaries</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"s2\">\"</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n</pre>\n<h5>Translation <code>EasyTranslator</code></h5>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adaptnlp</span> <span class=\"kn\">import</span> <span class=\"n\">EasyTranslator</span>\n\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"Machine learning will take over the world very soon.\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Machines can speak in many languages.\"</span><span class=\"p\">,]</span>\n\n<span class=\"n\">translator</span> <span class=\"o\">=</span> <span class=\"n\">EasyTranslator</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Translate</span>\n<span class=\"n\">translations</span> <span class=\"o\">=</span> <span class=\"n\">translator</span><span class=\"o\">.</span><span class=\"n\">translate</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">t5_prefix</span><span class=\"o\">=</span><span class=\"s2\">\"translate English to German\"</span><span class=\"p\">,</span> <span class=\"n\">model_name_or_path</span><span class=\"o\">=</span><span class=\"s2\">\"t5-small\"</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">min_length</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">max_length</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">early_stopping</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Translations:</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">translations</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"s2\">\"</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n</pre>\n<h5>Sequence Classification Training <code>SequenceClassifier</code></h5>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adaptnlp</span> <span class=\"kn\">import</span> <span class=\"n\">EasyDocumentEmbeddings</span><span class=\"p\">,</span> <span class=\"n\">SequenceClassifierTrainer</span> \n\n<span class=\"c1\"># Specify corpus data directory and model output directory</span>\n<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"s2\">\"Path/to/data/directory\"</span> \n<span class=\"n\">OUTPUT_DIR</span> <span class=\"o\">=</span> <span class=\"s2\">\"Path/to/output/directory\"</span> \n\n<span class=\"c1\"># Instantiate AdaptNLP easy document embeddings module, which can take in a variable number of embeddings to make `Stacked Embeddings`.  </span>\n<span class=\"c1\"># You may also use custom Transformers LM models by specifying the path the the language model</span>\n<span class=\"n\">doc_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">EasyDocumentEmbeddings</span><span class=\"p\">(</span><span class=\"n\">model_name_or_path</span><span class=\"o\">=</span><span class=\"s2\">\"bert-base-cased\"</span><span class=\"p\">,</span> <span class=\"n\">methods</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"rnn\"</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Instantiate Sequence Classifier Trainer by loading in the data, data column map, and embeddings as an encoder</span>\n<span class=\"n\">sc_trainer</span> <span class=\"o\">=</span> <span class=\"n\">SequenceClassifierTrainer</span><span class=\"p\">(</span><span class=\"n\">corpus</span><span class=\"o\">=</span><span class=\"n\">corpus</span><span class=\"p\">,</span> <span class=\"n\">encoder</span><span class=\"o\">=</span><span class=\"n\">doc_embeddings</span><span class=\"p\">,</span> <span class=\"n\">column_name_map</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"mi\">0</span><span class=\"p\">:</span> <span class=\"s2\">\"text\"</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"s2\">\"label\"</span><span class=\"p\">})</span>\n\n<span class=\"c1\"># Find Learning Rate</span>\n<span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"n\">sc_trainer</span><span class=\"o\">.</span><span class=\"n\">find_learning_rate</span><span class=\"p\">(</span><span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"n\">OUTPUT_DIR</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train Using Flair's Sequence Classification Head</span>\n<span class=\"n\">sc_trainer</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"n\">OUTPUT_DIR</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">learning_rate</span><span class=\"p\">,</span> <span class=\"n\">max_epochs</span><span class=\"o\">=</span><span class=\"mi\">150</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\"># Predict text labels with the trained model using `EasySequenceClassifier`</span>\n<span class=\"kn\">from</span> <span class=\"nn\">adaptnlp</span> <span class=\"kn\">import</span> <span class=\"n\">EasySequenceClassifier</span>\n<span class=\"n\">example_text</span> <span class=\"o\">=</span> <span class=\"s1\">'''Where was the Queen's wedding held? '''</span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"n\">EasySequenceClassifier</span><span class=\"p\">()</span>\n<span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">tag_text</span><span class=\"p\">(</span><span class=\"n\">example_text</span><span class=\"p\">,</span> <span class=\"n\">model_name_or_path</span><span class=\"o\">=</span><span class=\"n\">OUTPUT_DIR</span> <span class=\"o\">/</span> <span class=\"s2\">\"final-model.pt\"</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Label output:</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">sentences</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"o\">.</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<h5>Transformers Language Model Fine Tuning <code>LMFineTuner</code></h5>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adaptnlp</span> <span class=\"kn\">import</span> <span class=\"n\">LMFineTuner</span>\n\n<span class=\"c1\"># Specify Text Data File Paths</span>\n<span class=\"n\">train_data_file</span> <span class=\"o\">=</span> <span class=\"s2\">\"Path/to/train.csv\"</span>\n<span class=\"n\">eval_data_file</span> <span class=\"o\">=</span> <span class=\"s2\">\"Path/to/test.csv\"</span>\n\n<span class=\"c1\"># Instantiate Finetuner with Desired Language Model</span>\n<span class=\"n\">finetuner</span> <span class=\"o\">=</span> <span class=\"n\">LMFineTuner</span><span class=\"p\">(</span><span class=\"n\">train_data_file</span><span class=\"o\">=</span><span class=\"n\">train_data_file</span><span class=\"p\">,</span> <span class=\"n\">eval_data_file</span><span class=\"o\">=</span><span class=\"n\">eval_data_file</span><span class=\"p\">,</span> <span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"s2\">\"bert\"</span><span class=\"p\">,</span> <span class=\"n\">model_name_or_path</span><span class=\"o\">=</span><span class=\"s2\">\"bert-base-cased\"</span><span class=\"p\">)</span>\n<span class=\"n\">finetuner</span><span class=\"o\">.</span><span class=\"n\">freeze</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Find Optimal Learning Rate</span>\n<span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"n\">finetuner</span><span class=\"o\">.</span><span class=\"n\">find_learning_rate</span><span class=\"p\">(</span><span class=\"n\">base_path</span><span class=\"o\">=</span><span class=\"s2\">\"Path/to/base/directory\"</span><span class=\"p\">)</span>\n<span class=\"n\">finetuner</span><span class=\"o\">.</span><span class=\"n\">freeze</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Train and Save Fine Tuned Language Models</span>\n<span class=\"n\">finetuner</span><span class=\"o\">.</span><span class=\"n\">train_one_cycle</span><span class=\"p\">(</span><span class=\"n\">output_dir</span><span class=\"o\">=</span><span class=\"s2\">\"Path/to/output/directory\"</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n</pre>\n<h2>Tutorials</h2>\n<p>Look in the <a href=\"tutorials\" rel=\"nofollow\">Tutorials</a> directory for a quick introduction to the library and its very simple\nand straight forward use cases:</p>\n<p><strong>NLP Tasks</strong></p>\n<ol>\n<li><a href=\"tutorials/1.%20Token%20Classification\" rel=\"nofollow\">Token Classification: NER, POS, Chunk, and Frame Tagging</a>\n<ul>\n<li><a href=\"https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/1.%20Token%20Classification/token_tagging.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></li>\n</ul>\n</li>\n<li><a href=\"tutorials/2.%20Sequence%20Classification\" rel=\"nofollow\">Sequence Classification: Sentiment</a>\n<ul>\n<li><a href=\"https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/2.%20Sequence%20Classification/sequence_classification.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></li>\n</ul>\n</li>\n<li><a href=\"tutorials/3.%20Embeddings\" rel=\"nofollow\">Embeddings: Transformer Embeddings e.g. BERT, XLM, GPT2, XLNet, roBERTa, ALBERT</a>\n<ul>\n<li><a href=\"https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/3.%20Embeddings/embeddings.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></li>\n</ul>\n</li>\n<li><a href=\"tutorials/4.%20Question%20Answering\" rel=\"nofollow\">Question Answering: Span-based Question Answering Model</a>\n<ul>\n<li><a href=\"https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/4.%20Question%20Answering/question_answering.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></li>\n</ul>\n</li>\n<li><a href=\"tutorials/5.%20Summarization\" rel=\"nofollow\">Summarization: Abstractive and Extractive</a>\n<ul>\n<li><a href=\"https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/5.%20Summarization/summarization.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></li>\n</ul>\n</li>\n<li><a href=\"tutorials/6.%20Translation\" rel=\"nofollow\">Translation: Seq2Seq</a>\n<ul>\n<li><a href=\"https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/6.%20Translation/translation.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></li>\n</ul>\n</li>\n</ol>\n<p><strong><a href=\"tutorials/Finetuning%20and%20Training%20(Advanced)\" rel=\"nofollow\">Custom Fine-Tuning and Training with Transformer Models</a></strong></p>\n<ul>\n<li>Training a Sequence Classifier\n<ul>\n<li><a href=\"https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/Finetuning%20and%20Training%20(Advanced)/sequence_classification_training.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></li>\n</ul>\n</li>\n<li>Fine-tuning a Transformers Language Model\n<ul>\n<li><a href=\"https://colab.research.google.com/github/Novetta/adaptnlp/blob/master/tutorials/Finetuning%20and%20Training%20(Advanced)/fine_tuning.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></li>\n</ul>\n</li>\n</ul>\n<p>Checkout the <a href=\"https://novetta.github.io/adaptnlp\" rel=\"nofollow\">documentation</a> for more information.</p>\n<h2>REST Service</h2>\n<p>We use FastAPI for standing up endpoints for serving state-of-the-art NLP models with AdaptNLP.</p>\n<p><img alt=\"Swagger Example\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/887958bff2487bc698fd70d197570c645cf59cac/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e6f76657474612f61646170746e6c702f6d61737465722f646f63732f696d672f666173746170692d646f63732e706e67\"></p>\n<p>The <a href=\"https://github.com/Novetta/adaptnlp/tree/master/rest\" rel=\"nofollow\">REST</a> directory contains more detail on deploying a REST API locally or with docker in a very easy and\nfast way.</p>\n<h2>Docker</h2>\n<p>AdaptNLP official docker images are up on <a href=\"https://hub.docker.com/r/achangnovetta/adaptnlp\" rel=\"nofollow\">Docker Hub</a>.</p>\n<p>Images have AdaptNLP installed from source in developer mode with tutorial notebooks available.</p>\n<p>Images can build with GPU support if NVIDA-Docker is correctly installed.</p>\n<h4>Pull and Run AdaptNLP Immediately</h4>\n<p>Simply run an image with AdaptNLP installed from source in developer mode by running:</p>\n<pre><code>docker run -it --rm achangnovetta/adaptnlp:latest\n</code></pre>\n<p>Run an image with AdaptNLP running on GPUs if you have nvidia drivers and nvidia-docker 19.03+ installed:</p>\n<pre><code>docker run -it --rm --gpus all achangnovetta/adaptnlp:latest\n</code></pre>\n<h4>Build</h4>\n<p>Build docker image and run container with the following commands in the directory of the Dockerfile\nto create a container with adaptnlp installed and ready to go</p>\n<p>Note: A container with GPUs enabled requires Docker version 19.03+ and nvida-docker installed</p>\n<pre><code>docker build -t achangnovetta/adaptnlp:latest .\ndocker run -it --rm achangnovetta/adaptnlp:latest\n</code></pre>\n<p>If you want to use CUDA compatible GPUs</p>\n<pre><code>docker run -it --rm --gpus all achangnovetta/adaptnlp:latest\n</code></pre>\n<h2>Contact</h2>\n<p>Please contact the author Andrew Chang at <a href=\"mailto:achang@novetta.com\">achang@novetta.com</a> with questions or comments regarding AdaptNLP.</p>\n<p>Follow  us on Twitter at <a href=\"https://twitter.com/achang1618\" rel=\"nofollow\">@achang1618</a> and <a href=\"https://twitter.com/AdaptNLP\" rel=\"nofollow\">@AdaptNLP</a> for\nupdates and NLP dialogue.</p>\n<h2>License</h2>\n<p>This project is licensed under the terms of the Apache 2.0 license.</p>\n\n          </div>"}, "last_serial": 7145257, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "4b6a9175ea635f7325d9627cd7976c23", "sha256": "4cace90b652afa2dde707ff1715b9617993da70206c7ff0e8b68de01943e7636"}, "downloads": -1, "filename": "adaptnlp-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "4b6a9175ea635f7325d9627cd7976c23", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 47292, "upload_time": "2020-02-06T22:26:22", "upload_time_iso_8601": "2020-02-06T22:26:22.317034Z", "url": "https://files.pythonhosted.org/packages/38/5f/2ea9b205b095b4a1719f1bbcd100ec21cc7bfde2a620bdf38ca1166740d4/adaptnlp-0.0.1-py3-none-any.whl", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "1182013eb00aac7f64148d6796bc6908", "sha256": "99178dbd5162c1249b514d58f4e40c183c8d9f2e0de87c107eea3afbf1a92d2b"}, "downloads": -1, "filename": "adaptnlp-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "1182013eb00aac7f64148d6796bc6908", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 47295, "upload_time": "2020-02-06T22:31:11", "upload_time_iso_8601": "2020-02-06T22:31:11.802876Z", "url": "https://files.pythonhosted.org/packages/b1/71/4429c11954a4924c6da53945889efeffa169c671c20b64f6f7da8a3db05f/adaptnlp-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fa72236c47ee13f5dfde05dc5ea3ccfc", "sha256": "2c36f82c9828deed020a6cf32f351ea318d951224dccc6c93fc31aea1054ed98"}, "downloads": -1, "filename": "adaptnlp-0.0.2.tar.gz", "has_sig": false, "md5_digest": "fa72236c47ee13f5dfde05dc5ea3ccfc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 38372, "upload_time": "2020-02-06T22:31:13", "upload_time_iso_8601": "2020-02-06T22:31:13.294379Z", "url": "https://files.pythonhosted.org/packages/7f/42/ae894120ea6c6f663849a054ba32a4a66923cfd5607cbb38d4769d8ff2c0/adaptnlp-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "7c9c6c41f4f2022553a886f4e4fa1bf4", "sha256": "0b48de48c1f14eceeb2a07ee1ed858994a46cd6a67aa511332eecc95d046ec54"}, "downloads": -1, "filename": "adaptnlp-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "7c9c6c41f4f2022553a886f4e4fa1bf4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 47723, "upload_time": "2020-02-07T06:47:30", "upload_time_iso_8601": "2020-02-07T06:47:30.151158Z", "url": "https://files.pythonhosted.org/packages/9d/53/af2ebd43959867ae9b46d8cd54ab890fcab02f24817f8816552b75de0a31/adaptnlp-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0594961d6d7fc6b218dca87df44bbdba", "sha256": "d6357f6e4c903c2f715a9bab3bcd09c74ea4cf2281da22406d831a4d46267763"}, "downloads": -1, "filename": "adaptnlp-0.0.3.tar.gz", "has_sig": false, "md5_digest": "0594961d6d7fc6b218dca87df44bbdba", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39601, "upload_time": "2020-02-07T06:47:31", "upload_time_iso_8601": "2020-02-07T06:47:31.413714Z", "url": "https://files.pythonhosted.org/packages/27/11/4c95c8f411e62ea137b2d1e67af537eb23a3195b5ad961b24d18da76a5a8/adaptnlp-0.0.3.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "b075bf50aa119dc40a8a11e3c3c83117", "sha256": "30a6375cb9f7aa511d2f2e9e649127b2a838dfb060d79b4a06a02b6209674b87"}, "downloads": -1, "filename": "adaptnlp-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "b075bf50aa119dc40a8a11e3c3c83117", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 47128, "upload_time": "2020-02-06T08:59:07", "upload_time_iso_8601": "2020-02-06T08:59:07.967214Z", "url": "https://files.pythonhosted.org/packages/c4/d1/9c29dbda60ad69563401191463dba5cd4d301b74d82a4893264d6780831f/adaptnlp-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0ccf25fc36ab74d627a43faf63c86ea5", "sha256": "2173da749198df55a5bd71e79f7d7016ad9a1be3727ed18289ef45365bb4f7c3"}, "downloads": -1, "filename": "adaptnlp-0.1.0.tar.gz", "has_sig": false, "md5_digest": "0ccf25fc36ab74d627a43faf63c86ea5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 38263, "upload_time": "2020-02-06T08:59:10", "upload_time_iso_8601": "2020-02-06T08:59:10.753195Z", "url": "https://files.pythonhosted.org/packages/77/38/b120850b9fa155a4986cd4e0187161485927721877f5130f313b315a695e/adaptnlp-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "0ea7cdcf138fd5763799ec09bffc3eec", "sha256": "039dd366136fbbb73dd30d3f5784e1ca5272cfacc6bef05b50f624f6a12e99d0"}, "downloads": -1, "filename": "adaptnlp-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "0ea7cdcf138fd5763799ec09bffc3eec", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 47790, "upload_time": "2020-02-07T22:33:48", "upload_time_iso_8601": "2020-02-07T22:33:48.249182Z", "url": "https://files.pythonhosted.org/packages/c4/a8/072a114e5daeb6518da4a9d849646adec0e1935e92ce1a6b5eb51d93de1b/adaptnlp-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "acece60e236d35a6813bd6a8171a45ee", "sha256": "4f0556dbabd09bb562cb96f7e964ed6d923805eb8ccf89266317c81370d8cb6a"}, "downloads": -1, "filename": "adaptnlp-0.1.1.tar.gz", "has_sig": false, "md5_digest": "acece60e236d35a6813bd6a8171a45ee", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39801, "upload_time": "2020-02-07T22:33:49", "upload_time_iso_8601": "2020-02-07T22:33:49.771449Z", "url": "https://files.pythonhosted.org/packages/3a/a9/8849c213618fcdce7cb6c0de98b1d5fc06ccd65b30128633b19b7242a13d/adaptnlp-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "0119cec1a9f446b29556253c1c136458", "sha256": "1d2569335ea92d9792bb2382db2464df7eac670ab553fa565f1ab1b9cb9aa3f8"}, "downloads": -1, "filename": "adaptnlp-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0119cec1a9f446b29556253c1c136458", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 54643, "upload_time": "2020-02-19T09:50:19", "upload_time_iso_8601": "2020-02-19T09:50:19.904341Z", "url": "https://files.pythonhosted.org/packages/ea/f9/a4e00c1624c631fdc374985f765e64fe66b484ca0db839da7cb4270f36b4/adaptnlp-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3859ab5763ff149579c0f72cde69783d", "sha256": "730cd48d7c9c4c7165f2b84b1684029fc6ebe64cadeb675d05f11a9efeb5a75f"}, "downloads": -1, "filename": "adaptnlp-0.1.2.tar.gz", "has_sig": false, "md5_digest": "3859ab5763ff149579c0f72cde69783d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 49164, "upload_time": "2020-02-19T09:50:21", "upload_time_iso_8601": "2020-02-19T09:50:21.425142Z", "url": "https://files.pythonhosted.org/packages/35/f8/d3593797a69d1581e6b00da3e904bd92eaaf650c4f8a787019a4b2c783bf/adaptnlp-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "cd9e2de9e30975ad2d4c03efb97f218f", "sha256": "63e1f965212c9be8e6b51aa55b03b207f23bd15f76ae930259b1d4d8f6fb5683"}, "downloads": -1, "filename": "adaptnlp-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "cd9e2de9e30975ad2d4c03efb97f218f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 66846, "upload_time": "2020-03-06T08:09:47", "upload_time_iso_8601": "2020-03-06T08:09:47.193052Z", "url": "https://files.pythonhosted.org/packages/d8/08/520400bd952bb0d74e5d58525920d89705f3739cf3ce097ff63e5cbbd729/adaptnlp-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7f0685d6a7c3c13c5203cfdea111fa51", "sha256": "4cb94125fbcc0f5d87c2655c243fe196f2da6b2f6b05d7754624334d0de6d915"}, "downloads": -1, "filename": "adaptnlp-0.1.3.tar.gz", "has_sig": false, "md5_digest": "7f0685d6a7c3c13c5203cfdea111fa51", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 53997, "upload_time": "2020-03-06T08:09:48", "upload_time_iso_8601": "2020-03-06T08:09:48.305432Z", "url": "https://files.pythonhosted.org/packages/9f/20/4c903c2ba04e713f720dda2fff9b08ae74f73c7ff1061cac993046b81e1f/adaptnlp-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "5a389bc2bf1f659978569474f5456a2a", "sha256": "d3793ef2bd847f95617f92c040bab7cbc35be6cbd15dfb9e14bbf6b242ff5078"}, "downloads": -1, "filename": "adaptnlp-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "5a389bc2bf1f659978569474f5456a2a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 72334, "upload_time": "2020-04-02T15:15:43", "upload_time_iso_8601": "2020-04-02T15:15:43.953566Z", "url": "https://files.pythonhosted.org/packages/8c/db/c7eec111a0c110406d9407b80842cd8a14d2369ba8709561907a9d9aa06c/adaptnlp-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dbb6a7fd28d8e7f7a6479a4e0854b6d4", "sha256": "4ca09b570b79e2c6e6aaa1d2c38e46e0bcf96dff985e2c09274808b6bd2320aa"}, "downloads": -1, "filename": "adaptnlp-0.1.4.tar.gz", "has_sig": false, "md5_digest": "dbb6a7fd28d8e7f7a6479a4e0854b6d4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62703, "upload_time": "2020-04-02T15:15:45", "upload_time_iso_8601": "2020-04-02T15:15:45.514251Z", "url": "https://files.pythonhosted.org/packages/a3/8b/5bab3b85ed1d8a607c17fcdaa25e93a9b2e146a1813bdd65375c9e2e4d23/adaptnlp-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "dc8cb16700aad17d160ba08962d5be66", "sha256": "b122954a88a4c3f19479dc219c30dd1ef3ceb5a7f42003bdaca3f63d51e7f4cb"}, "downloads": -1, "filename": "adaptnlp-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "dc8cb16700aad17d160ba08962d5be66", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 72527, "upload_time": "2020-04-17T12:27:21", "upload_time_iso_8601": "2020-04-17T12:27:21.398236Z", "url": "https://files.pythonhosted.org/packages/f7/18/2e2a34bdf7546c1af80231f4ea1951b18b822248b63558a48f62cc351fd6/adaptnlp-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7a0de101ba5e4d10db0a77ec79df8805", "sha256": "bd63c2c813d6bc925c911ce4baf6223600808b397c9d7f94c0621fce37fb9988"}, "downloads": -1, "filename": "adaptnlp-0.1.5.tar.gz", "has_sig": false, "md5_digest": "7a0de101ba5e4d10db0a77ec79df8805", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 61321, "upload_time": "2020-04-17T12:27:22", "upload_time_iso_8601": "2020-04-17T12:27:22.592109Z", "url": "https://files.pythonhosted.org/packages/8e/b4/57fad722606246e57bf432dbe033c64d452ed5b3ec4ae6b8c98b197dd5ca/adaptnlp-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "126379ff949f0bd7d74d3b00fb3d5f5f", "sha256": "b455ebab5773ebaeb590a2b67b63a5507b35302f7bbbede56e6b490616bdf8d8"}, "downloads": -1, "filename": "adaptnlp-0.1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "126379ff949f0bd7d74d3b00fb3d5f5f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 72591, "upload_time": "2020-05-01T14:17:10", "upload_time_iso_8601": "2020-05-01T14:17:10.583826Z", "url": "https://files.pythonhosted.org/packages/f4/eb/9bdafff0b245cd6ffaa06550ac64fc85a558fed1bbf8b821862e6044da1f/adaptnlp-0.1.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7cc30974d0091efce264f421bbc7cade", "sha256": "7a968392672df20156e37ba6d4feb3fdbe298e702086e2bf1dcae13b282fe9ca"}, "downloads": -1, "filename": "adaptnlp-0.1.6.tar.gz", "has_sig": false, "md5_digest": "7cc30974d0091efce264f421bbc7cade", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 61457, "upload_time": "2020-05-01T14:17:12", "upload_time_iso_8601": "2020-05-01T14:17:12.084192Z", "url": "https://files.pythonhosted.org/packages/80/92/6b45d1dc88aaeb1ec771593c7f5b0af99435f69db3cb6cc9220703f83685/adaptnlp-0.1.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "126379ff949f0bd7d74d3b00fb3d5f5f", "sha256": "b455ebab5773ebaeb590a2b67b63a5507b35302f7bbbede56e6b490616bdf8d8"}, "downloads": -1, "filename": "adaptnlp-0.1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "126379ff949f0bd7d74d3b00fb3d5f5f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 72591, "upload_time": "2020-05-01T14:17:10", "upload_time_iso_8601": "2020-05-01T14:17:10.583826Z", "url": "https://files.pythonhosted.org/packages/f4/eb/9bdafff0b245cd6ffaa06550ac64fc85a558fed1bbf8b821862e6044da1f/adaptnlp-0.1.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7cc30974d0091efce264f421bbc7cade", "sha256": "7a968392672df20156e37ba6d4feb3fdbe298e702086e2bf1dcae13b282fe9ca"}, "downloads": -1, "filename": "adaptnlp-0.1.6.tar.gz", "has_sig": false, "md5_digest": "7cc30974d0091efce264f421bbc7cade", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 61457, "upload_time": "2020-05-01T14:17:12", "upload_time_iso_8601": "2020-05-01T14:17:12.084192Z", "url": "https://files.pythonhosted.org/packages/80/92/6b45d1dc88aaeb1ec771593c7f5b0af99435f69db3cb6cc9220703f83685/adaptnlp-0.1.6.tar.gz", "yanked": false}], "timestamp": "Thu May  7 16:23:36 2020"}