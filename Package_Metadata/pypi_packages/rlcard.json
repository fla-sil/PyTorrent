{"info": {"author": "Data Analytics at Texas A&M (DATA) Lab", "author_email": "khlai037@tamu.edu", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6"], "description": "# RLCard: A Toolkit for Reinforcement Learning in Card Games\n<img width=\"500\" src=\"./docs/imgs/logo.jpg\" alt=\"Logo\" />\n\n[![Build Status](https://travis-ci.org/datamllab/RLCard.svg?branch=master)](https://travis-ci.org/datamllab/RLCard)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/248eb15c086748a4bcc830755f1bd798)](https://www.codacy.com/manual/daochenzha/rlcard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=datamllab/rlcard&amp;utm_campaign=Badge_Grade)\n[![Coverage Status](https://coveralls.io/repos/github/datamllab/rlcard/badge.svg)](https://coveralls.io/github/datamllab/rlcard?branch=master)\n\nRLCard is a toolkit for Reinforcement Learning (RL) in card games. It supports multiple card environments with easy-to-use interfaces. The goal of RLCard is to bridge reinforcement learning and imperfect information games, and push forward the research of reinforcement learning in domains with multiple agents, large state and action space, and sparse reward. RLCard is developed by [DATA Lab](http://faculty.cs.tamu.edu/xiahu/) at Texas A&M University.\n\n*   Official Website: [http://www.rlcard.org](http://www.rlcard.org)\n*   Paper: [https://arxiv.org/abs/1910.04376](https://arxiv.org/abs/1910.04376)\n\n**News:**\n*   Now RLCard supports environment local seeding and multiprocessing. Thanks for the testing scripts provided by [@weepingwillowben](https://github.com/weepingwillowben).\n*   Human interface of NoLimit Holdem available. The action space of NoLimit Holdem has been abstracted. Thanks for the contribution of [@AdrianP-](https://github.com/AdrianP-).\n*   New game Gin Rummy and human GUI available. Thanks for the contribution of [@billh0420](https://github.com/billh0420).\n*   PyTorch implementation available. Thanks for the contribution of [@mjudell](https://github.com/mjudell).\n*   We have just initialized a list of [Awesome-Game-AI resources](https://github.com/datamllab/awesome-game-ai). Check it out!\n\n## Cite this work\nIf you find this repo useful, you may cite:\n```\n@article{zha2019rlcard,\n  title={RLCard: A Toolkit for Reinforcement Learning in Card Games},\n  author={Zha, Daochen and Lai, Kwei-Herng and Cao, Yuanpu and Huang, Songyi and Wei, Ruzhe and Guo, Junyu and Hu, Xia},\n  journal={arXiv preprint arXiv:1910.04376},\n  year={2019}\n}\n```\n\n## Installation\nMake sure that you have **Python 3.5+** and **pip** installed. We recommend installing `rlcard` with `pip` as follow:\n\n```\ngit clone https://github.com/datamllab/rlcard.git\ncd rlcard\npip install -e .\n```\nor use PyPI with:\n```\npip install rlcard\n```\nTo use tensorflow implementation, run:\n```\npip install rlcard[tensorflow]\n```\nTo try out PyTorch implementation for DQN and NFSP, please run: \n```\npip install rlcard[torch]\n```\nIf you meet any problems when installing PyTorch with the command above, you may follow the instructions on [PyTorch official website](https://pytorch.org/get-started/locally/) to manually install PyTorch.\n\n## Examples\nPlease refer to [examples/](examples). A **short example** is as below.\n\n```python\nimport rlcard\nfrom rlcard.agents import RandomAgent\n\nenv = rlcard.make('blackjack')\nenv.set_agents([RandomAgent(action_num=env.action_num)])\n\ntrajectories, payoffs = env.run()\n```\n\nWe also recommend the following **toy examples**.\n\n*   [Playing with random agents](docs/toy-examples.md#playing-with-random-agents)\n*   [Deep-Q learning on Blackjack](docs/toy-examples.md#deep-q-learning-on-blackjack)\n*   [Running multiple processes](docs/toy-examples.md#running-multiple-processes)\n*   [Training CFR on Leduc Hold'em](docs/toy-examples.md#training-cfr-on-leduc-holdem)\n*   [Having fun with pretrained Leduc model](docs/toy-examples.md#having-fun-with-pretrained-leduc-model)\n*   [Leduc Hold'em as single-agent environment](docs/toy-examples.md#leduc-holdem-as-single-agent-environment)\n\n## Demo\nRun `examples/leduc_holdem_human.py` to play with the pre-trained Leduc Hold'em model. Leduc Hold'em is a simplified version of Texas Hold'em. Rules can be found [here](docs/games.md#leduc-holdem).\n\n```\n>> Leduc Hold'em pre-trained model\n\n>> Start a new game!\n>> Agent 1 chooses raise\n\n=============== Community Card ===============\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n===============   Your Hand    ===============\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502J        \u2502\n\u2502         \u2502\n\u2502         \u2502\n\u2502    \u2665    \u2502\n\u2502         \u2502\n\u2502         \u2502\n\u2502        J\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n===============     Chips      ===============\nYours:   +\nAgent 1: +++\n=========== Actions You Can Choose ===========\n0: call, 1: raise, 2: fold\n\n>> You choose action (integer):\n```\n\n## Available Environments\nWe provide a complexity estimation for the games on several aspects. **InfoSet Number:** the number of information sets; **InfoSet Size:** the average number of states in a single information set; **Action Size:** the size of the action space. **Name:** the name that should be passed to `rlcard.make` to create the game environment. We also provide the link to the documentation and the random example.\n\n| Game                                                                                                                                                                                           | InfoSet Number  | InfoSet Size      | Action Size | Name            | Usage                                                                                       |\n| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------: | :---------------: | :---------: | :-------------: | :-----------------------------------------------------------------------------------------: |\n| Blackjack ([wiki](https://en.wikipedia.org/wiki/Blackjack), [baike](https://baike.baidu.com/item/21%E7%82%B9/5481683?fr=aladdin))                                                              | 10^3            | 10^1              | 10^0        | blackjack       | [doc](docs/games.md#blackjack), [example](examples/blackjack_random.py)                     |\n| Leduc Hold\u2019em ([paper](http://poker.cs.ualberta.ca/publications/UAI05.pdf))                                                                                                                    | 10^2            | 10^2              | 10^0        | leduc-holdem    | [doc](docs/games.md#leduc-holdem), [example](examples/leduc_holdem_random.py)               |\n| Limit Texas Hold'em ([wiki](https://en.wikipedia.org/wiki/Texas_hold_%27em), [baike](https://baike.baidu.com/item/%E5%BE%B7%E5%85%8B%E8%90%A8%E6%96%AF%E6%89%91%E5%85%8B/83440?fr=aladdin))    | 10^14           | 10^3              | 10^0        | limit-holdem    | [doc](docs/games.md#limit-texas-holdem), [example](examples/limit_holdem_random.py)         |\n| Dou Dizhu ([wiki](https://en.wikipedia.org/wiki/Dou_dizhu), [baike](https://baike.baidu.com/item/%E6%96%97%E5%9C%B0%E4%B8%BB/177997?fr=aladdin))                                               | 10^53 ~ 10^83   | 10^23             | 10^4        | doudizhu        | [doc](docs/games.md#dou-dizhu), [example](examples/doudizhu_random.py)                      |\n| Simple Dou Dizhu ([wiki](https://en.wikipedia.org/wiki/Dou_dizhu), [baike](https://baike.baidu.com/item/%E6%96%97%E5%9C%B0%E4%B8%BB/177997?fr=aladdin))                                        | -               | -                 | -           | simple-doudizhu | [doc](docs/games.md#simple-dou-dizhu), [example](examples/simple_doudizhu_random.py)        |\n| Mahjong ([wiki](https://en.wikipedia.org/wiki/Competition_Mahjong_scoring_rules), [baike](https://baike.baidu.com/item/%E9%BA%BB%E5%B0%86/215))                                                | 10^121          | 10^48             | 10^2        | mahjong         | [doc](docs/games.md#mahjong), [example](examples/mahjong_random.py)                         | \n| No-limit Texas Hold'em ([wiki](https://en.wikipedia.org/wiki/Texas_hold_%27em), [baike](https://baike.baidu.com/item/%E5%BE%B7%E5%85%8B%E8%90%A8%E6%96%AF%E6%89%91%E5%85%8B/83440?fr=aladdin)) | 10^162          | 10^3              | 10^4        | no-limit-holdem | [doc](docs/games.md#no-limit-texas-holdem), [example](examples/nolimit_holdem_random.py)    |\n| UNO ([wiki](https://en.wikipedia.org/wiki/Uno_\\(card_game\\)), [baike](https://baike.baidu.com/item/UNO%E7%89%8C/2249587))                                                                      |  10^163         | 10^10             | 10^1        | uno             | [doc](docs/games.md#uno), [example](examples/uno_random.py)                                 |\n| Gin Rummy ([wiki](https://en.wikipedia.org/wiki/Gin_rummy), [baike](https://baike.baidu.com/item/%E9%87%91%E6%8B%89%E7%B1%B3/3471710))                                                         | 10^52           | -                 | -           | gin-rummy       | [doc](docs/games.md#gin-rummy), [example](examples/gin_rummy_random.py)                     |\n\n## API Cheat Sheet\n### How to create an environment\nYou can use the the following interface to make an environment. You can specify some configurations with a dictionary.\n*   **env = rlcard.make(env_id, config={})**: Make an environment. `env_id` is a string of a environment; `config` is a dictionary specifying some environment configurations, which are as follows.\n\t*   `seed`: Default `None`. Set a environment local random seed for reproducing the results.\n\t*   `env_num`: Default `1`. It specifies how many environments running in parallel. If the number is larger than 1, then the tasks will be assigned to multiple processes for acceleration.\n\t*   `allow_step_back`: Defualt `False`. `True` if allowing `step_back` function to traverse backward in the tree.\n\t*   `allow_raw_data`: Default `False`. `True` if allowing raw data in the `state`.\n\t*   `single_agent_mode`: Default `False`. `True` if using single agent mode, i.e., Gym style interface with other players as pretrained/rule models.\n\t*   `active_player`: Defualt `0`. If `single_agent_mode` is `True`, `active_player` will specify operating on which player in single agent mode.\n\t*   `record_action`: Default `False`. If `True`, a field of `action_record` will be in the `state` to record the historical actions. This may be used for human-agent play.\n\nOnce the environemnt is made, we can access some information of the game.\n*   **env.action_num**: The number of actions.\n*   **env.player_num**: The number of players.\n*   **env.state_space**: Ther state space of the observations.\n*   **env.timestep**: The number of timesteps stepped by the environment.\n\n### What is state in RLCard\nState is a Python dictionary. It will always have observation `state['obs']` and legal actions `state['legal_actions']`. If `allow_raw_data` is `True`, state will have raw observation `state['raw_obs']` and raw legal actions `state['raw_legal_actions']`.\n\n### Basic interfaces\nThe following interfaces provide a basic usage. It is easy to use but it has assumtions on the agent. The agent must follow [agent template](docs/developping-algorithms.md). \n*   **env.set_agents(agents)**: `agents` is a list of `Agent` object. The length of the the list should equal to the number of the player in the game.\n*   **env.run(is_training=False)**: Run a complete game and return trajectories and payoffs. The function can be used after the `set_agents` is called. If `is_training` is `True`, the function will use `step` function in the agent to play the game. If `is_training` is `False`, `eval_step` will be called instead.\n\n### Advanced interfaces\nFor advanced usage, the following interfaces allow flexible operations on the game tree. These interfaces do not make any assumtions on the agent.\n*   **env.reset()**: Initialize a game. Return the state and the first player ID.\n*   **env.step(action, raw_action=False)**: Take one step in the environment. `action` can be raw action or integer; `raw_action` should be `True` if the action is raw action (string).\n*   **env.step_back()**: Available only when `allow_step_back` is `True`. Take one step backward. This can be used for algorithms that operate on the game tree, such as CFR.\n*   **env.is_over()**: Return `True` if the current game is over/ Return `False` otherwise.\n*   **env.get_player_id()**: Return the Player ID of the current player.\n*   **env.get_state(player_id)**: Return the state corresponds to `player_id`.\n*   **env.get_payoffs()**: In the end of the game, return a list of payoffs for all the players.\n*   **env.get_perfect_information()**: (Currently only support some of the games) Obtain the perfect information at the current state.\n\n### Running with multiple processes\nRLCard now supports acceleration with multiple processes. Simply change `env_num` when making the environment to indicate how many processes would be used. Currenly we only support `run()` function with multiple processes. An example is [DQN on blackjack](docs/toy-examples.md#running-multiple-processes)  \n\n## Library Structure\nThe purposes of the main modules are listed as below:\n\n*   [/examples](examples): Examples of using RLCard.\n*   [/docs](docs): Documentation of RLCard.\n*   [/tests](tests): Testing scripts for RLCard.\n*   [/rlcard/agents](rlcard/agents): Reinforcement learning algorithms and human agents.\n*   [/rlcard/envs](rlcard/envs): Environment wrappers (state representation, action encoding etc.)\n*   [/rlcard/games](rlcard/games): Various game engines.\n*   [/rlcard/models](rlcard/models): Model zoo including pre-trained models and rule models.\n\n## Evaluation\nThe perfomance is measured by winning rates through tournaments. Example outputs are as follows:\n![Learning Curves](http://rlcard.org/imgs/curves.png \"Learning Curves\")\n\n## More Documents\nFor more documentation, please refer to the [Documents](docs/README.md) for general introductions. API documents are available at our [website](http://www.rlcard.org).\n\n## Contributing\nContribution to this project is greatly appreciated! Please create an issue for feedbacks/bugs. If you want to contribute codes, please refer to [Contributing Guide](./CONTRIBUTING.md).\n\n## Acknowledgements\nWe would like to thank JJ World Network Technology Co.,LTD for the generous support and all the contributors in the community.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/datamllab/rlcard", "keywords": "Reinforcement Learning,game,RL,AI", "license": "", "maintainer": "", "maintainer_email": "", "name": "rlcard", "package_url": "https://pypi.org/project/rlcard/", "platform": "", "project_url": "https://pypi.org/project/rlcard/", "project_urls": {"Homepage": "https://github.com/datamllab/rlcard"}, "release_url": "https://pypi.org/project/rlcard/0.2.1/", "requires_dist": null, "requires_python": "", "summary": "A Toolkit for Reinforcement Learning in Card Games", "version": "0.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>RLCard: A Toolkit for Reinforcement Learning in Card Games</h1>\n<img alt=\"Logo\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/23c35c4930727f30a5836b1fde1b05055df10ddb/2e2f646f63732f696d67732f6c6f676f2e6a7067\" width=\"500\">\n<p><a href=\"https://travis-ci.org/datamllab/RLCard\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/be16eb9e8d3557c0d79666a1d77278fc3253e983/68747470733a2f2f7472617669732d63692e6f72672f646174616d6c6c61622f524c436172642e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://www.codacy.com/manual/daochenzha/rlcard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=datamllab/rlcard&amp;utm_campaign=Badge_Grade\" rel=\"nofollow\"><img alt=\"Codacy Badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fef6bf048e2cc5f9ec0ec147542d67dced297b8b/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3234386562313563303836373438613462636338333037353566316264373938\"></a>\n<a href=\"https://coveralls.io/github/datamllab/rlcard?branch=master\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/add4e1f00a17e2c2916e1dce5d947e2e3163c283/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f646174616d6c6c61622f726c636172642f62616467652e737667\"></a></p>\n<p>RLCard is a toolkit for Reinforcement Learning (RL) in card games. It supports multiple card environments with easy-to-use interfaces. The goal of RLCard is to bridge reinforcement learning and imperfect information games, and push forward the research of reinforcement learning in domains with multiple agents, large state and action space, and sparse reward. RLCard is developed by <a href=\"http://faculty.cs.tamu.edu/xiahu/\" rel=\"nofollow\">DATA Lab</a> at Texas A&amp;M University.</p>\n<ul>\n<li>Official Website: <a href=\"http://www.rlcard.org\" rel=\"nofollow\">http://www.rlcard.org</a></li>\n<li>Paper: <a href=\"https://arxiv.org/abs/1910.04376\" rel=\"nofollow\">https://arxiv.org/abs/1910.04376</a></li>\n</ul>\n<p><strong>News:</strong></p>\n<ul>\n<li>Now RLCard supports environment local seeding and multiprocessing. Thanks for the testing scripts provided by <a href=\"https://github.com/weepingwillowben\" rel=\"nofollow\">@weepingwillowben</a>.</li>\n<li>Human interface of NoLimit Holdem available. The action space of NoLimit Holdem has been abstracted. Thanks for the contribution of <a href=\"https://github.com/AdrianP-\" rel=\"nofollow\">@AdrianP-</a>.</li>\n<li>New game Gin Rummy and human GUI available. Thanks for the contribution of <a href=\"https://github.com/billh0420\" rel=\"nofollow\">@billh0420</a>.</li>\n<li>PyTorch implementation available. Thanks for the contribution of <a href=\"https://github.com/mjudell\" rel=\"nofollow\">@mjudell</a>.</li>\n<li>We have just initialized a list of <a href=\"https://github.com/datamllab/awesome-game-ai\" rel=\"nofollow\">Awesome-Game-AI resources</a>. Check it out!</li>\n</ul>\n<h2>Cite this work</h2>\n<p>If you find this repo useful, you may cite:</p>\n<pre><code>@article{zha2019rlcard,\n  title={RLCard: A Toolkit for Reinforcement Learning in Card Games},\n  author={Zha, Daochen and Lai, Kwei-Herng and Cao, Yuanpu and Huang, Songyi and Wei, Ruzhe and Guo, Junyu and Hu, Xia},\n  journal={arXiv preprint arXiv:1910.04376},\n  year={2019}\n}\n</code></pre>\n<h2>Installation</h2>\n<p>Make sure that you have <strong>Python 3.5+</strong> and <strong>pip</strong> installed. We recommend installing <code>rlcard</code> with <code>pip</code> as follow:</p>\n<pre><code>git clone https://github.com/datamllab/rlcard.git\ncd rlcard\npip install -e .\n</code></pre>\n<p>or use PyPI with:</p>\n<pre><code>pip install rlcard\n</code></pre>\n<p>To use tensorflow implementation, run:</p>\n<pre><code>pip install rlcard[tensorflow]\n</code></pre>\n<p>To try out PyTorch implementation for DQN and NFSP, please run:</p>\n<pre><code>pip install rlcard[torch]\n</code></pre>\n<p>If you meet any problems when installing PyTorch with the command above, you may follow the instructions on <a href=\"https://pytorch.org/get-started/locally/\" rel=\"nofollow\">PyTorch official website</a> to manually install PyTorch.</p>\n<h2>Examples</h2>\n<p>Please refer to <a href=\"examples\" rel=\"nofollow\">examples/</a>. A <strong>short example</strong> is as below.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">rlcard</span>\n<span class=\"kn\">from</span> <span class=\"nn\">rlcard.agents</span> <span class=\"kn\">import</span> <span class=\"n\">RandomAgent</span>\n\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">rlcard</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">'blackjack'</span><span class=\"p\">)</span>\n<span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">set_agents</span><span class=\"p\">([</span><span class=\"n\">RandomAgent</span><span class=\"p\">(</span><span class=\"n\">action_num</span><span class=\"o\">=</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">action_num</span><span class=\"p\">)])</span>\n\n<span class=\"n\">trajectories</span><span class=\"p\">,</span> <span class=\"n\">payoffs</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n<p>We also recommend the following <strong>toy examples</strong>.</p>\n<ul>\n<li><a href=\"docs/toy-examples.md#playing-with-random-agents\" rel=\"nofollow\">Playing with random agents</a></li>\n<li><a href=\"docs/toy-examples.md#deep-q-learning-on-blackjack\" rel=\"nofollow\">Deep-Q learning on Blackjack</a></li>\n<li><a href=\"docs/toy-examples.md#running-multiple-processes\" rel=\"nofollow\">Running multiple processes</a></li>\n<li><a href=\"docs/toy-examples.md#training-cfr-on-leduc-holdem\" rel=\"nofollow\">Training CFR on Leduc Hold'em</a></li>\n<li><a href=\"docs/toy-examples.md#having-fun-with-pretrained-leduc-model\" rel=\"nofollow\">Having fun with pretrained Leduc model</a></li>\n<li><a href=\"docs/toy-examples.md#leduc-holdem-as-single-agent-environment\" rel=\"nofollow\">Leduc Hold'em as single-agent environment</a></li>\n</ul>\n<h2>Demo</h2>\n<p>Run <code>examples/leduc_holdem_human.py</code> to play with the pre-trained Leduc Hold'em model. Leduc Hold'em is a simplified version of Texas Hold'em. Rules can be found <a href=\"docs/games.md#leduc-holdem\" rel=\"nofollow\">here</a>.</p>\n<pre><code>&gt;&gt; Leduc Hold'em pre-trained model\n\n&gt;&gt; Start a new game!\n&gt;&gt; Agent 1 chooses raise\n\n=============== Community Card ===============\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n===============   Your Hand    ===============\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502J        \u2502\n\u2502         \u2502\n\u2502         \u2502\n\u2502    \u2665    \u2502\n\u2502         \u2502\n\u2502         \u2502\n\u2502        J\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n===============     Chips      ===============\nYours:   +\nAgent 1: +++\n=========== Actions You Can Choose ===========\n0: call, 1: raise, 2: fold\n\n&gt;&gt; You choose action (integer):\n</code></pre>\n<h2>Available Environments</h2>\n<p>We provide a complexity estimation for the games on several aspects. <strong>InfoSet Number:</strong> the number of information sets; <strong>InfoSet Size:</strong> the average number of states in a single information set; <strong>Action Size:</strong> the size of the action space. <strong>Name:</strong> the name that should be passed to <code>rlcard.make</code> to create the game environment. We also provide the link to the documentation and the random example.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Game</th>\n<th align=\"center\">InfoSet Number</th>\n<th align=\"center\">InfoSet Size</th>\n<th align=\"center\">Action Size</th>\n<th align=\"center\">Name</th>\n<th align=\"center\">Usage</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">Blackjack (<a href=\"https://en.wikipedia.org/wiki/Blackjack\" rel=\"nofollow\">wiki</a>, <a href=\"https://baike.baidu.com/item/21%E7%82%B9/5481683?fr=aladdin\" rel=\"nofollow\">baike</a>)</td>\n<td align=\"center\">10^3</td>\n<td align=\"center\">10^1</td>\n<td align=\"center\">10^0</td>\n<td align=\"center\">blackjack</td>\n<td align=\"center\"><a href=\"docs/games.md#blackjack\" rel=\"nofollow\">doc</a>, <a href=\"examples/blackjack_random.py\" rel=\"nofollow\">example</a></td>\n</tr>\n<tr>\n<td align=\"center\">Leduc Hold\u2019em (<a href=\"http://poker.cs.ualberta.ca/publications/UAI05.pdf\" rel=\"nofollow\">paper</a>)</td>\n<td align=\"center\">10^2</td>\n<td align=\"center\">10^2</td>\n<td align=\"center\">10^0</td>\n<td align=\"center\">leduc-holdem</td>\n<td align=\"center\"><a href=\"docs/games.md#leduc-holdem\" rel=\"nofollow\">doc</a>, <a href=\"examples/leduc_holdem_random.py\" rel=\"nofollow\">example</a></td>\n</tr>\n<tr>\n<td align=\"center\">Limit Texas Hold'em (<a href=\"https://en.wikipedia.org/wiki/Texas_hold_%27em\" rel=\"nofollow\">wiki</a>, <a href=\"https://baike.baidu.com/item/%E5%BE%B7%E5%85%8B%E8%90%A8%E6%96%AF%E6%89%91%E5%85%8B/83440?fr=aladdin\" rel=\"nofollow\">baike</a>)</td>\n<td align=\"center\">10^14</td>\n<td align=\"center\">10^3</td>\n<td align=\"center\">10^0</td>\n<td align=\"center\">limit-holdem</td>\n<td align=\"center\"><a href=\"docs/games.md#limit-texas-holdem\" rel=\"nofollow\">doc</a>, <a href=\"examples/limit_holdem_random.py\" rel=\"nofollow\">example</a></td>\n</tr>\n<tr>\n<td align=\"center\">Dou Dizhu (<a href=\"https://en.wikipedia.org/wiki/Dou_dizhu\" rel=\"nofollow\">wiki</a>, <a href=\"https://baike.baidu.com/item/%E6%96%97%E5%9C%B0%E4%B8%BB/177997?fr=aladdin\" rel=\"nofollow\">baike</a>)</td>\n<td align=\"center\">10^53 ~ 10^83</td>\n<td align=\"center\">10^23</td>\n<td align=\"center\">10^4</td>\n<td align=\"center\">doudizhu</td>\n<td align=\"center\"><a href=\"docs/games.md#dou-dizhu\" rel=\"nofollow\">doc</a>, <a href=\"examples/doudizhu_random.py\" rel=\"nofollow\">example</a></td>\n</tr>\n<tr>\n<td align=\"center\">Simple Dou Dizhu (<a href=\"https://en.wikipedia.org/wiki/Dou_dizhu\" rel=\"nofollow\">wiki</a>, <a href=\"https://baike.baidu.com/item/%E6%96%97%E5%9C%B0%E4%B8%BB/177997?fr=aladdin\" rel=\"nofollow\">baike</a>)</td>\n<td align=\"center\">-</td>\n<td align=\"center\">-</td>\n<td align=\"center\">-</td>\n<td align=\"center\">simple-doudizhu</td>\n<td align=\"center\"><a href=\"docs/games.md#simple-dou-dizhu\" rel=\"nofollow\">doc</a>, <a href=\"examples/simple_doudizhu_random.py\" rel=\"nofollow\">example</a></td>\n</tr>\n<tr>\n<td align=\"center\">Mahjong (<a href=\"https://en.wikipedia.org/wiki/Competition_Mahjong_scoring_rules\" rel=\"nofollow\">wiki</a>, <a href=\"https://baike.baidu.com/item/%E9%BA%BB%E5%B0%86/215\" rel=\"nofollow\">baike</a>)</td>\n<td align=\"center\">10^121</td>\n<td align=\"center\">10^48</td>\n<td align=\"center\">10^2</td>\n<td align=\"center\">mahjong</td>\n<td align=\"center\"><a href=\"docs/games.md#mahjong\" rel=\"nofollow\">doc</a>, <a href=\"examples/mahjong_random.py\" rel=\"nofollow\">example</a></td>\n</tr>\n<tr>\n<td align=\"center\">No-limit Texas Hold'em (<a href=\"https://en.wikipedia.org/wiki/Texas_hold_%27em\" rel=\"nofollow\">wiki</a>, <a href=\"https://baike.baidu.com/item/%E5%BE%B7%E5%85%8B%E8%90%A8%E6%96%AF%E6%89%91%E5%85%8B/83440?fr=aladdin\" rel=\"nofollow\">baike</a>)</td>\n<td align=\"center\">10^162</td>\n<td align=\"center\">10^3</td>\n<td align=\"center\">10^4</td>\n<td align=\"center\">no-limit-holdem</td>\n<td align=\"center\"><a href=\"docs/games.md#no-limit-texas-holdem\" rel=\"nofollow\">doc</a>, <a href=\"examples/nolimit_holdem_random.py\" rel=\"nofollow\">example</a></td>\n</tr>\n<tr>\n<td align=\"center\">UNO (<a href=\"https://en.wikipedia.org/wiki/Uno_(card_game)\" rel=\"nofollow\">wiki</a>, <a href=\"https://baike.baidu.com/item/UNO%E7%89%8C/2249587\" rel=\"nofollow\">baike</a>)</td>\n<td align=\"center\">10^163</td>\n<td align=\"center\">10^10</td>\n<td align=\"center\">10^1</td>\n<td align=\"center\">uno</td>\n<td align=\"center\"><a href=\"docs/games.md#uno\" rel=\"nofollow\">doc</a>, <a href=\"examples/uno_random.py\" rel=\"nofollow\">example</a></td>\n</tr>\n<tr>\n<td align=\"center\">Gin Rummy (<a href=\"https://en.wikipedia.org/wiki/Gin_rummy\" rel=\"nofollow\">wiki</a>, <a href=\"https://baike.baidu.com/item/%E9%87%91%E6%8B%89%E7%B1%B3/3471710\" rel=\"nofollow\">baike</a>)</td>\n<td align=\"center\">10^52</td>\n<td align=\"center\">-</td>\n<td align=\"center\">-</td>\n<td align=\"center\">gin-rummy</td>\n<td align=\"center\"><a href=\"docs/games.md#gin-rummy\" rel=\"nofollow\">doc</a>, <a href=\"examples/gin_rummy_random.py\" rel=\"nofollow\">example</a></td>\n</tr></tbody></table>\n<h2>API Cheat Sheet</h2>\n<h3>How to create an environment</h3>\n<p>You can use the the following interface to make an environment. You can specify some configurations with a dictionary.</p>\n<ul>\n<li><strong>env = rlcard.make(env_id, config={})</strong>: Make an environment. <code>env_id</code> is a string of a environment; <code>config</code> is a dictionary specifying some environment configurations, which are as follows.\n<ul>\n<li><code>seed</code>: Default <code>None</code>. Set a environment local random seed for reproducing the results.</li>\n<li><code>env_num</code>: Default <code>1</code>. It specifies how many environments running in parallel. If the number is larger than 1, then the tasks will be assigned to multiple processes for acceleration.</li>\n<li><code>allow_step_back</code>: Defualt <code>False</code>. <code>True</code> if allowing <code>step_back</code> function to traverse backward in the tree.</li>\n<li><code>allow_raw_data</code>: Default <code>False</code>. <code>True</code> if allowing raw data in the <code>state</code>.</li>\n<li><code>single_agent_mode</code>: Default <code>False</code>. <code>True</code> if using single agent mode, i.e., Gym style interface with other players as pretrained/rule models.</li>\n<li><code>active_player</code>: Defualt <code>0</code>. If <code>single_agent_mode</code> is <code>True</code>, <code>active_player</code> will specify operating on which player in single agent mode.</li>\n<li><code>record_action</code>: Default <code>False</code>. If <code>True</code>, a field of <code>action_record</code> will be in the <code>state</code> to record the historical actions. This may be used for human-agent play.</li>\n</ul>\n</li>\n</ul>\n<p>Once the environemnt is made, we can access some information of the game.</p>\n<ul>\n<li><strong>env.action_num</strong>: The number of actions.</li>\n<li><strong>env.player_num</strong>: The number of players.</li>\n<li><strong>env.state_space</strong>: Ther state space of the observations.</li>\n<li><strong>env.timestep</strong>: The number of timesteps stepped by the environment.</li>\n</ul>\n<h3>What is state in RLCard</h3>\n<p>State is a Python dictionary. It will always have observation <code>state['obs']</code> and legal actions <code>state['legal_actions']</code>. If <code>allow_raw_data</code> is <code>True</code>, state will have raw observation <code>state['raw_obs']</code> and raw legal actions <code>state['raw_legal_actions']</code>.</p>\n<h3>Basic interfaces</h3>\n<p>The following interfaces provide a basic usage. It is easy to use but it has assumtions on the agent. The agent must follow <a href=\"docs/developping-algorithms.md\" rel=\"nofollow\">agent template</a>.</p>\n<ul>\n<li><strong>env.set_agents(agents)</strong>: <code>agents</code> is a list of <code>Agent</code> object. The length of the the list should equal to the number of the player in the game.</li>\n<li><strong>env.run(is_training=False)</strong>: Run a complete game and return trajectories and payoffs. The function can be used after the <code>set_agents</code> is called. If <code>is_training</code> is <code>True</code>, the function will use <code>step</code> function in the agent to play the game. If <code>is_training</code> is <code>False</code>, <code>eval_step</code> will be called instead.</li>\n</ul>\n<h3>Advanced interfaces</h3>\n<p>For advanced usage, the following interfaces allow flexible operations on the game tree. These interfaces do not make any assumtions on the agent.</p>\n<ul>\n<li><strong>env.reset()</strong>: Initialize a game. Return the state and the first player ID.</li>\n<li><strong>env.step(action, raw_action=False)</strong>: Take one step in the environment. <code>action</code> can be raw action or integer; <code>raw_action</code> should be <code>True</code> if the action is raw action (string).</li>\n<li><strong>env.step_back()</strong>: Available only when <code>allow_step_back</code> is <code>True</code>. Take one step backward. This can be used for algorithms that operate on the game tree, such as CFR.</li>\n<li><strong>env.is_over()</strong>: Return <code>True</code> if the current game is over/ Return <code>False</code> otherwise.</li>\n<li><strong>env.get_player_id()</strong>: Return the Player ID of the current player.</li>\n<li><strong>env.get_state(player_id)</strong>: Return the state corresponds to <code>player_id</code>.</li>\n<li><strong>env.get_payoffs()</strong>: In the end of the game, return a list of payoffs for all the players.</li>\n<li><strong>env.get_perfect_information()</strong>: (Currently only support some of the games) Obtain the perfect information at the current state.</li>\n</ul>\n<h3>Running with multiple processes</h3>\n<p>RLCard now supports acceleration with multiple processes. Simply change <code>env_num</code> when making the environment to indicate how many processes would be used. Currenly we only support <code>run()</code> function with multiple processes. An example is <a href=\"docs/toy-examples.md#running-multiple-processes\" rel=\"nofollow\">DQN on blackjack</a></p>\n<h2>Library Structure</h2>\n<p>The purposes of the main modules are listed as below:</p>\n<ul>\n<li><a href=\"examples\" rel=\"nofollow\">/examples</a>: Examples of using RLCard.</li>\n<li><a href=\"docs\" rel=\"nofollow\">/docs</a>: Documentation of RLCard.</li>\n<li><a href=\"tests\" rel=\"nofollow\">/tests</a>: Testing scripts for RLCard.</li>\n<li><a href=\"rlcard/agents\" rel=\"nofollow\">/rlcard/agents</a>: Reinforcement learning algorithms and human agents.</li>\n<li><a href=\"rlcard/envs\" rel=\"nofollow\">/rlcard/envs</a>: Environment wrappers (state representation, action encoding etc.)</li>\n<li><a href=\"rlcard/games\" rel=\"nofollow\">/rlcard/games</a>: Various game engines.</li>\n<li><a href=\"rlcard/models\" rel=\"nofollow\">/rlcard/models</a>: Model zoo including pre-trained models and rule models.</li>\n</ul>\n<h2>Evaluation</h2>\n<p>The perfomance is measured by winning rates through tournaments. Example outputs are as follows:\n<img alt=\"Learning Curves\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8a37194f65ba577c82548eae238538c534985385/687474703a2f2f726c636172642e6f72672f696d67732f6375727665732e706e67\"></p>\n<h2>More Documents</h2>\n<p>For more documentation, please refer to the <a href=\"docs/README.md\" rel=\"nofollow\">Documents</a> for general introductions. API documents are available at our <a href=\"http://www.rlcard.org\" rel=\"nofollow\">website</a>.</p>\n<h2>Contributing</h2>\n<p>Contribution to this project is greatly appreciated! Please create an issue for feedbacks/bugs. If you want to contribute codes, please refer to <a href=\"./CONTRIBUTING.md\" rel=\"nofollow\">Contributing Guide</a>.</p>\n<h2>Acknowledgements</h2>\n<p>We would like to thank JJ World Network Technology Co.,LTD for the generous support and all the contributors in the community.</p>\n\n          </div>"}, "last_serial": 7167698, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "cb1a754b7169f87e89acef3b575c7a8d", "sha256": "c09aca2c3891daa5af1683829fa8a83ff2ae9a5ccc7a960a2bb74d3f15ff1ae7"}, "downloads": -1, "filename": "rlcard-0.1.tar.gz", "has_sig": false, "md5_digest": "cb1a754b7169f87e89acef3b575c7a8d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18848, "upload_time": "2019-10-09T01:45:40", "upload_time_iso_8601": "2019-10-09T01:45:40.136335Z", "url": "https://files.pythonhosted.org/packages/ee/64/cc3ba5cbe64ababa463e783e4730bf8c4b4c57be8bcd13d7a1fb7b22bd34/rlcard-0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "e0ee5cb72e683a29ec1f770c636d170a", "sha256": "646c7901315e09eacec4f136d1ed80d55504d49c08bcc4959cf20153f2d8047f"}, "downloads": -1, "filename": "rlcard-0.1.1.tar.gz", "has_sig": false, "md5_digest": "e0ee5cb72e683a29ec1f770c636d170a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19908, "upload_time": "2019-10-09T02:51:37", "upload_time_iso_8601": "2019-10-09T02:51:37.961010Z", "url": "https://files.pythonhosted.org/packages/9a/c2/c03abf09dde459ee09d3208ab7b84e26d309ecd98410dc8bfccbcdd6ed64/rlcard-0.1.1.tar.gz", "yanked": false}], "0.1.10": [{"comment_text": "", "digests": {"md5": "929eaf613d6580152512acbb742c4644", "sha256": "fc75c91061b92955733fcd76507f6777454a9a3ac344cf224e659afc5ef4e115"}, "downloads": -1, "filename": "rlcard-0.1.10.tar.gz", "has_sig": false, "md5_digest": "929eaf613d6580152512acbb742c4644", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1546011, "upload_time": "2020-03-17T05:41:24", "upload_time_iso_8601": "2020-03-17T05:41:24.075622Z", "url": "https://files.pythonhosted.org/packages/07/24/d5dc247970600572e4c16c7a8ed9f83874496b28b2f5f4b6d38272ce8488/rlcard-0.1.10.tar.gz", "yanked": false}], "0.1.11": [{"comment_text": "", "digests": {"md5": "1cb08cd5ea0b5a00cd11fae8332e8d0b", "sha256": "0cd4cdbb48e140713345f767c2f7a6021963743e83a4eb3661e1af889da04d32"}, "downloads": -1, "filename": "rlcard-0.1.11.tar.gz", "has_sig": false, "md5_digest": "1cb08cd5ea0b5a00cd11fae8332e8d0b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1549533, "upload_time": "2020-03-17T19:13:10", "upload_time_iso_8601": "2020-03-17T19:13:10.012562Z", "url": "https://files.pythonhosted.org/packages/f4/df/eb09bc35e4faf0f12d2905260373983fb6088243c370c0f4c8c364e28b99/rlcard-0.1.11.tar.gz", "yanked": false}], "0.1.12": [{"comment_text": "", "digests": {"md5": "13c495a376cab2f19c9c8e53744997ec", "sha256": "8e5adca2d8630b27d7b7eb752eabdf9f47f9b8d06a92802ba947def91393de6b"}, "downloads": -1, "filename": "rlcard-0.1.12.tar.gz", "has_sig": false, "md5_digest": "13c495a376cab2f19c9c8e53744997ec", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2050489, "upload_time": "2020-03-18T18:54:29", "upload_time_iso_8601": "2020-03-18T18:54:29.982959Z", "url": "https://files.pythonhosted.org/packages/af/a1/79b102031a2959d851a8b14b8749c4262520156962b816c8a046e803a675/rlcard-0.1.12.tar.gz", "yanked": false}], "0.1.13": [{"comment_text": "", "digests": {"md5": "5918776a539ec493651dee31f55d32e8", "sha256": "6d989b98f273d25249f3df58c74ff2e67afc5f662a00a3993105314c84955225"}, "downloads": -1, "filename": "rlcard-0.1.13.tar.gz", "has_sig": false, "md5_digest": "5918776a539ec493651dee31f55d32e8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2052119, "upload_time": "2020-03-30T04:21:06", "upload_time_iso_8601": "2020-03-30T04:21:06.334782Z", "url": "https://files.pythonhosted.org/packages/d1/c2/12483aed9bd27f7162ecdeed35c92da69ca6e6371fe360c1fefd64c801c3/rlcard-0.1.13.tar.gz", "yanked": false}], "0.1.14": [{"comment_text": "", "digests": {"md5": "06c386ed60f3d2a4f18933d27da473f4", "sha256": "986ce93b594a5c46792d95cb3d07a0caf3aa1d4f567fe1543bedb8dd5f6ef996"}, "downloads": -1, "filename": "rlcard-0.1.14.tar.gz", "has_sig": false, "md5_digest": "06c386ed60f3d2a4f18933d27da473f4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2195325, "upload_time": "2020-04-08T05:14:17", "upload_time_iso_8601": "2020-04-08T05:14:17.292918Z", "url": "https://files.pythonhosted.org/packages/25/a0/93d82abdf480209cdf4ee54217c6a1b207435183c6386d34a236ed5fd9f3/rlcard-0.1.14.tar.gz", "yanked": false}], "0.1.15": [{"comment_text": "", "digests": {"md5": "f34a30231ce77d184aa73155a94a62b4", "sha256": "44c88cc5d60566dff96ded5cd12e43251f9e46ca7436cb136842e43efb91b59d"}, "downloads": -1, "filename": "rlcard-0.1.15.tar.gz", "has_sig": false, "md5_digest": "f34a30231ce77d184aa73155a94a62b4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2077109, "upload_time": "2020-04-11T20:47:57", "upload_time_iso_8601": "2020-04-11T20:47:57.300218Z", "url": "https://files.pythonhosted.org/packages/f4/ba/913242d2d7e695f93e3de9fe4d9a2e8d924255973268190447760389d9d4/rlcard-0.1.15.tar.gz", "yanked": false}], "0.1.17": [{"comment_text": "", "digests": {"md5": "4fd9c1aca1c98e0223f89d399bf93ec1", "sha256": "0c322a71f14f50943bf6bdd1a7cc4cf450b45044609f802aea09cbff11d43f44"}, "downloads": -1, "filename": "rlcard-0.1.17.tar.gz", "has_sig": false, "md5_digest": "4fd9c1aca1c98e0223f89d399bf93ec1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6640602, "upload_time": "2020-04-24T18:34:57", "upload_time_iso_8601": "2020-04-24T18:34:57.072853Z", "url": "https://files.pythonhosted.org/packages/82/78/52d7feb45fef97f42af2b1b36b5b6e383605a04c36cbed52835d6902338f/rlcard-0.1.17.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "5d3fce78e1e9cd30ea92d607ec6818a1", "sha256": "6748be9a1478fb3860bc8631ba684b7b1e6ddb2cf735b3b4a45b9b6eb71f6937"}, "downloads": -1, "filename": "rlcard-0.1.2.tar.gz", "has_sig": false, "md5_digest": "5d3fce78e1e9cd30ea92d607ec6818a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19928, "upload_time": "2019-10-09T03:07:32", "upload_time_iso_8601": "2019-10-09T03:07:32.103956Z", "url": "https://files.pythonhosted.org/packages/b9/c7/baae958658a953cae55e7c4224d45c13c14c5a8a83713cad7caadd0383d3/rlcard-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "edf39535f53b6846ffb77a5fe78aa876", "sha256": "f64e47cf82a7648c5dabf7564127c319a406c785e0c2b52812c1f9d18a099dd3"}, "downloads": -1, "filename": "rlcard-0.1.3.tar.gz", "has_sig": false, "md5_digest": "edf39535f53b6846ffb77a5fe78aa876", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 64884, "upload_time": "2019-10-10T00:14:52", "upload_time_iso_8601": "2019-10-10T00:14:52.056133Z", "url": "https://files.pythonhosted.org/packages/cf/16/c17215d4d322f7f869feca23a357885529dc583f1c7ab851687daa48c49b/rlcard-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "d103b0636359bee215856c0d72b48e9d", "sha256": "be033422cbb2d66019404b54826ee6770e83f5922ebb050d4153e5f25523fc4e"}, "downloads": -1, "filename": "rlcard-0.1.4.tar.gz", "has_sig": false, "md5_digest": "d103b0636359bee215856c0d72b48e9d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1371312, "upload_time": "2019-10-10T02:14:29", "upload_time_iso_8601": "2019-10-10T02:14:29.663070Z", "url": "https://files.pythonhosted.org/packages/57/13/0833638f36aef118f9ee209dc23239eec1477c1d311b72eb145a06563c45/rlcard-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "5687d8da7fe900b900478be06bc4720b", "sha256": "e13d815296b88d58177ce9a26b841bf0529272b9720232cbd7bf67dc46face0b"}, "downloads": -1, "filename": "rlcard-0.1.5.tar.gz", "has_sig": false, "md5_digest": "5687d8da7fe900b900478be06bc4720b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1392152, "upload_time": "2019-10-11T00:22:57", "upload_time_iso_8601": "2019-10-11T00:22:57.842731Z", "url": "https://files.pythonhosted.org/packages/0c/7e/1cbaab8583941cb412125c43ab2d13ba24097cbad1e0bce730adb87cbab5/rlcard-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "d45b06605ac2d0fb2fd6424cee8ee2d6", "sha256": "b4bcbff10459025da666edab60a04cf875ed7fa3bb711d24d18d06414c8d555b"}, "downloads": -1, "filename": "rlcard-0.1.6.tar.gz", "has_sig": false, "md5_digest": "d45b06605ac2d0fb2fd6424cee8ee2d6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1392146, "upload_time": "2019-10-11T00:33:03", "upload_time_iso_8601": "2019-10-11T00:33:03.547613Z", "url": "https://files.pythonhosted.org/packages/23/30/3eaf948ae5b7aa9df99bf0453acb9a11a1f86d84fc5dc0320f6d99974f24/rlcard-0.1.6.tar.gz", "yanked": false}], "0.1.7": [{"comment_text": "", "digests": {"md5": "41c771bb49e4fe80a11e59d26ef75d3c", "sha256": "d7235c77fd9dc8a58a86e627ca95714bda3eb04d6ad8c64d8c10e72aded4583c"}, "downloads": -1, "filename": "rlcard-0.1.7.tar.gz", "has_sig": false, "md5_digest": "41c771bb49e4fe80a11e59d26ef75d3c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1392957, "upload_time": "2019-10-12T00:34:39", "upload_time_iso_8601": "2019-10-12T00:34:39.244224Z", "url": "https://files.pythonhosted.org/packages/df/6c/35bbab3fa0019af41200612d59ad2386db6b192d0a644b88755375526f0a/rlcard-0.1.7.tar.gz", "yanked": false}], "0.1.8": [{"comment_text": "", "digests": {"md5": "a47fade89702663f5cb370a363400c45", "sha256": "f5191f19f5a656b4fa69e8167100457a1fcec2cc98f951e7bbb52c22592af280"}, "downloads": -1, "filename": "rlcard-0.1.8.tar.gz", "has_sig": false, "md5_digest": "a47fade89702663f5cb370a363400c45", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1392954, "upload_time": "2019-10-12T00:39:45", "upload_time_iso_8601": "2019-10-12T00:39:45.833063Z", "url": "https://files.pythonhosted.org/packages/3b/47/83d7d7486b4150baa70cd8aa7d36151b96e66dca176a1b1a0eb1931e8906/rlcard-0.1.8.tar.gz", "yanked": false}], "0.1.9": [{"comment_text": "", "digests": {"md5": "aca4255aba32882be497cdc6d5d94d1d", "sha256": "dc2ca4ec1dad9fb05cbd3e47788d0e6edd1b72ecb5b0f95480109f4ee3c2652f"}, "downloads": -1, "filename": "rlcard-0.1.9.tar.gz", "has_sig": false, "md5_digest": "aca4255aba32882be497cdc6d5d94d1d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1333529, "upload_time": "2020-02-18T07:39:40", "upload_time_iso_8601": "2020-02-18T07:39:40.240007Z", "url": "https://files.pythonhosted.org/packages/52/c5/f404a1dd5fcf4f1bae13f6206da6484828bcf96ad1c66e00240129abb26b/rlcard-0.1.9.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "8f4da9f03d6e613fa0dd4e42f392805c", "sha256": "b5faeee3f70a145a01e28f7032370703536a7e4103c38f29c6bcff850127d66a"}, "downloads": -1, "filename": "rlcard-0.2.0.tar.gz", "has_sig": false, "md5_digest": "8f4da9f03d6e613fa0dd4e42f392805c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6647765, "upload_time": "2020-05-01T01:51:50", "upload_time_iso_8601": "2020-05-01T01:51:50.401787Z", "url": "https://files.pythonhosted.org/packages/2f/78/833b14820bd80044636711918ce8877cfa831f76b3173895ff5939ce36cb/rlcard-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "688bbfa007c8532221468d6bcb71750c", "sha256": "21243fc3183e43b1f841577eebecda74dc5164b7ecdb2c1520dda8a0ff1bf718"}, "downloads": -1, "filename": "rlcard-0.2.1.tar.gz", "has_sig": false, "md5_digest": "688bbfa007c8532221468d6bcb71750c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6647753, "upload_time": "2020-05-04T21:02:33", "upload_time_iso_8601": "2020-05-04T21:02:33.585816Z", "url": "https://files.pythonhosted.org/packages/be/0a/73c4248a07fa5f1a669e8d20f0704b7f09188c230a44d8593bafa8651f35/rlcard-0.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "688bbfa007c8532221468d6bcb71750c", "sha256": "21243fc3183e43b1f841577eebecda74dc5164b7ecdb2c1520dda8a0ff1bf718"}, "downloads": -1, "filename": "rlcard-0.2.1.tar.gz", "has_sig": false, "md5_digest": "688bbfa007c8532221468d6bcb71750c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6647753, "upload_time": "2020-05-04T21:02:33", "upload_time_iso_8601": "2020-05-04T21:02:33.585816Z", "url": "https://files.pythonhosted.org/packages/be/0a/73c4248a07fa5f1a669e8d20f0704b7f09188c230a44d8593bafa8651f35/rlcard-0.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:02:18 2020"}