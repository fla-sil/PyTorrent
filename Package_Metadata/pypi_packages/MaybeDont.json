{"info": {"author": "Konstantin Lopuhin", "author_email": "kostia.lopuhin@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Topic :: Internet :: WWW/HTTP :: Indexing/Search"], "description": "MaybeDont\n=========\n\n.. image:: https://img.shields.io/pypi/v/MaybeDont.svg\n   :target: https://pypi.python.org/pypi/MaybeDont\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/travis/TeamHG-Memex/MaybeDont/master.svg\n   :target: http://travis-ci.org/TeamHG-Memex/MaybeDont\n   :alt: Build Status\n\n.. image:: https://codecov.io/github/TeamHG-Memex/MaybeDont/coverage.svg?branch=master\n   :target: https://codecov.io/github/TeamHG-Memex/MaybeDont?branch=master\n   :alt: Code Coverage\n\n.. contents::\n\nMaybeDont is a library that helps avoid downloading pages with duplicate\ncontent during crawling. It learns which URL components are important and\nwhich are not important during crawling, and tries to predict if the page\nwill be duplicate based on it's URL.\n\nThe idea is that if you have a crawler that just\nfollows all links, it might download a lot of duplicate pages: for example,\nfor a forum there might be pages like ``/view.php?topicId=10`` and\n``/view.php?topicId=10&start=0`` - the only difference is added ``start=0``,\nand the content of this pages is likely duplicate. If we knew that adding\n``start=0`` does not change content, then we would avoid downloading the page\n``/view.php?topicId=10&start=0`` if we have already fetched\n``/view.php?topicId=10``, and thus save time and bandwidth.\n\n\nDuplicate detector\n------------------\n\n``maybedont.DupePredictor`` collects statistics about page URLs and contents, and\nis able to predict if the new URL will bring any new content.\n\nFirst, initialize a ``DupePredictor``::\n\n    from maybedont import DupePredictor\n    dp = DupePredictor(\n        texts_sample=[page_1, page_2, page_3],\n        jaccard_threshold=0.9)  # default value\n\n``texts_sample`` is a list of page contents. It can be ommited, but it is\nrecommended to provide it: it is used to learn which parts of the page are\ncommon for a lot of site's pages, and excludes this parts from duplicate\ncomparison. This helps with pages where the content is small relative to\nthe site chrome (footer, header, etc.): without removing chrome all such\npages would be considered duplicates, as only a tiny fraction of the content\nchanges.\n\nNext, we can update ``DupePredictor`` model with downloaded pages::\n\n    dp.update_model(url_4, text_4)\n    dp.update_model(url_5, text_5)\n\nAfter a while, ``DupePredictor`` will learn which arguments in URLs\nare important, and which can be safely ignored.\n``DupePredictor.get_dupe_prob`` returns the probability of url being\na duplicate of some content that has already been seem::\n\n    dp.get_dupe_prob(url_6)\n\nRuntime overhead should be not too large: on a crawl with < 100k pages,\nexpected time to update the model is 1-5 ms, and below 1 ms\nto get the probability. All visited urls and hashes of content are stored\nin memory, along with some indexing structures.\n\n\nInstall\n-------\n\n::\n\n    pip install MaybeDont\n\n\nSpider middleware\n-----------------\n\nIf you have a `Scrapy <http://scrapy.org>`_ spider,\nor are looking for an inspiration for a spider\nmiddleware, check out ``maybedont.scrapy_middleware.AvoidDupContentMiddleware``.\nFirst, it collects an queue of documents to know better which page elements\nare common on the site, in order to exclude them from content comparison.\nAfter that it builds it's ``DupePredictor``, updates it with crawled pages\n(only textual pages are taken into account), and starts dropping requests\nfor duplicate content once it gets confident enough. Not all requests for\nduplicates are dropped: with a small probability (currenty 5%) requests\nare carried anyway. This makes duplicate detection more robust against\nchanges in site URL or content structure as the crawl progresses.\n\nTo enable the middleware, the following settings are required::\n\n    AVOID_DUP_CONTENT_ENABLED = True\n    DOWNLOADER_MIDDLEWARES['maybedont.scrapy_middleware.AvoidDupContentMiddleware'] = 200\n\nMiddleware is only applied to requests with ``avoid_dup_content`` in\n``request.meta``.\n\nOptional settings:\n\n- ``AVOID_DUP_CONTENT_THRESHOLD = 0.98`` - minimal probability when requests\n  are skipped.\n- ``AVOID_DUP_CONTENT_EXPLORATION = 0.05`` - probability of still making a\n  request that should be dropped\n- ``AVOID_DUP_CONTENT_INITIAL_QUEUE_LIMIT = 300`` - number of pages that\n  should be downloaded before DupePredictor is initialized\n\n\nHow it works\n------------\n\nDuplicate detection is based on ``MinHashLSH`` from the\n`datasketch <https://github.com/ekzhu/datasketch>`_ library. Text\n4-shingles of words are used for hashing,\nnot spanning line breaks in the extracted text.\n\nSeveral hypotheses about duplicates are tested:\n\n1. All URLs with a given URL path are the same (have the same content),\n   regardless of query parameters;\n2. All URLs which only differ in a given URL query parameter are the same\n   (e.g. session tokens can be detected this way);\n3. All URLs which have a given path and only differ in a given URL\n   query parameter are the same;\n4. All URLs which have a given path and query string and only differ\n   in a single given query parameter are the same;\n5. URLs are the same if they have same path and only differ\n   in that some of them have a given param=value query argument added;\n6. URLs are the same if they have a given path and only differ\n   in a given param=value query argument;\n\nBernoulli distribution is fit for each hypothesis.\n\n\nLicense\n-------\n\nLicense is MIT\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/TeamHG-Memex/MaybeDont", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "MaybeDont", "package_url": "https://pypi.org/project/MaybeDont/", "platform": "", "project_url": "https://pypi.org/project/MaybeDont/", "project_urls": {"Homepage": "https://github.com/TeamHG-Memex/MaybeDont"}, "release_url": "https://pypi.org/project/MaybeDont/0.1.1/", "requires_dist": null, "requires_python": "", "summary": "A component that tried to avoid downloading duplicate content", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://pypi.python.org/pypi/MaybeDont\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/18292ae94e8355d35cbcbcf4cf40d20e9d73ceb2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f4d61796265446f6e742e737667\"></a>\n<a href=\"http://travis-ci.org/TeamHG-Memex/MaybeDont\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bdca0760a2ff5fb9f68ea491a96b61930c96804b/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f5465616d48472d4d656d65782f4d61796265446f6e742f6d61737465722e737667\"></a>\n<a href=\"https://codecov.io/github/TeamHG-Memex/MaybeDont?branch=master\" rel=\"nofollow\"><img alt=\"Code Coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5c9633d7aba38ad50214074fe515463e27e63d0b/68747470733a2f2f636f6465636f762e696f2f6769746875622f5465616d48472d4d656d65782f4d61796265446f6e742f636f7665726167652e7376673f6272616e63683d6d6173746572\"></a>\n<div id=\"contents\">\n<p>Contents</p>\n<ul>\n<li><a href=\"#duplicate-detector\" id=\"id1\" rel=\"nofollow\">Duplicate detector</a></li>\n<li><a href=\"#install\" id=\"id2\" rel=\"nofollow\">Install</a></li>\n<li><a href=\"#spider-middleware\" id=\"id3\" rel=\"nofollow\">Spider middleware</a></li>\n<li><a href=\"#how-it-works\" id=\"id4\" rel=\"nofollow\">How it works</a></li>\n<li><a href=\"#license\" id=\"id5\" rel=\"nofollow\">License</a></li>\n</ul>\n</div>\n<p>MaybeDont is a library that helps avoid downloading pages with duplicate\ncontent during crawling. It learns which URL components are important and\nwhich are not important during crawling, and tries to predict if the page\nwill be duplicate based on it\u2019s URL.</p>\n<p>The idea is that if you have a crawler that just\nfollows all links, it might download a lot of duplicate pages: for example,\nfor a forum there might be pages like <tt><span class=\"pre\">/view.php?topicId=10</span></tt> and\n<tt><span class=\"pre\">/view.php?topicId=10&amp;start=0</span></tt> - the only difference is added <tt>start=0</tt>,\nand the content of this pages is likely duplicate. If we knew that adding\n<tt>start=0</tt> does not change content, then we would avoid downloading the page\n<tt><span class=\"pre\">/view.php?topicId=10&amp;start=0</span></tt> if we have already fetched\n<tt><span class=\"pre\">/view.php?topicId=10</span></tt>, and thus save time and bandwidth.</p>\n<div id=\"duplicate-detector\">\n<h2><a href=\"#id1\" rel=\"nofollow\">Duplicate detector</a></h2>\n<p><tt>maybedont.DupePredictor</tt> collects statistics about page URLs and contents, and\nis able to predict if the new URL will bring any new content.</p>\n<p>First, initialize a <tt>DupePredictor</tt>:</p>\n<pre>from maybedont import DupePredictor\ndp = DupePredictor(\n    texts_sample=[page_1, page_2, page_3],\n    jaccard_threshold=0.9)  # default value\n</pre>\n<p><tt>texts_sample</tt> is a list of page contents. It can be ommited, but it is\nrecommended to provide it: it is used to learn which parts of the page are\ncommon for a lot of site\u2019s pages, and excludes this parts from duplicate\ncomparison. This helps with pages where the content is small relative to\nthe site chrome (footer, header, etc.): without removing chrome all such\npages would be considered duplicates, as only a tiny fraction of the content\nchanges.</p>\n<p>Next, we can update <tt>DupePredictor</tt> model with downloaded pages:</p>\n<pre>dp.update_model(url_4, text_4)\ndp.update_model(url_5, text_5)\n</pre>\n<p>After a while, <tt>DupePredictor</tt> will learn which arguments in URLs\nare important, and which can be safely ignored.\n<tt>DupePredictor.get_dupe_prob</tt> returns the probability of url being\na duplicate of some content that has already been seem:</p>\n<pre>dp.get_dupe_prob(url_6)\n</pre>\n<p>Runtime overhead should be not too large: on a crawl with &lt; 100k pages,\nexpected time to update the model is 1-5 ms, and below 1 ms\nto get the probability. All visited urls and hashes of content are stored\nin memory, along with some indexing structures.</p>\n</div>\n<div id=\"install\">\n<h2><a href=\"#id2\" rel=\"nofollow\">Install</a></h2>\n<pre>pip install MaybeDont\n</pre>\n</div>\n<div id=\"spider-middleware\">\n<h2><a href=\"#id3\" rel=\"nofollow\">Spider middleware</a></h2>\n<p>If you have a <a href=\"http://scrapy.org\" rel=\"nofollow\">Scrapy</a> spider,\nor are looking for an inspiration for a spider\nmiddleware, check out <tt>maybedont.scrapy_middleware.AvoidDupContentMiddleware</tt>.\nFirst, it collects an queue of documents to know better which page elements\nare common on the site, in order to exclude them from content comparison.\nAfter that it builds it\u2019s <tt>DupePredictor</tt>, updates it with crawled pages\n(only textual pages are taken into account), and starts dropping requests\nfor duplicate content once it gets confident enough. Not all requests for\nduplicates are dropped: with a small probability (currenty 5%) requests\nare carried anyway. This makes duplicate detection more robust against\nchanges in site URL or content structure as the crawl progresses.</p>\n<p>To enable the middleware, the following settings are required:</p>\n<pre>AVOID_DUP_CONTENT_ENABLED = True\nDOWNLOADER_MIDDLEWARES['maybedont.scrapy_middleware.AvoidDupContentMiddleware'] = 200\n</pre>\n<p>Middleware is only applied to requests with <tt>avoid_dup_content</tt> in\n<tt>request.meta</tt>.</p>\n<p>Optional settings:</p>\n<ul>\n<li><tt>AVOID_DUP_CONTENT_THRESHOLD = 0.98</tt> - minimal probability when requests\nare skipped.</li>\n<li><tt>AVOID_DUP_CONTENT_EXPLORATION = 0.05</tt> - probability of still making a\nrequest that should be dropped</li>\n<li><tt>AVOID_DUP_CONTENT_INITIAL_QUEUE_LIMIT = 300</tt> - number of pages that\nshould be downloaded before DupePredictor is initialized</li>\n</ul>\n</div>\n<div id=\"how-it-works\">\n<h2><a href=\"#id4\" rel=\"nofollow\">How it works</a></h2>\n<p>Duplicate detection is based on <tt>MinHashLSH</tt> from the\n<a href=\"https://github.com/ekzhu/datasketch\" rel=\"nofollow\">datasketch</a> library. Text\n4-shingles of words are used for hashing,\nnot spanning line breaks in the extracted text.</p>\n<p>Several hypotheses about duplicates are tested:</p>\n<ol>\n<li>All URLs with a given URL path are the same (have the same content),\nregardless of query parameters;</li>\n<li>All URLs which only differ in a given URL query parameter are the same\n(e.g. session tokens can be detected this way);</li>\n<li>All URLs which have a given path and only differ in a given URL\nquery parameter are the same;</li>\n<li>All URLs which have a given path and query string and only differ\nin a single given query parameter are the same;</li>\n<li>URLs are the same if they have same path and only differ\nin that some of them have a given param=value query argument added;</li>\n<li>URLs are the same if they have a given path and only differ\nin a given param=value query argument;</li>\n</ol>\n<p>Bernoulli distribution is fit for each hypothesis.</p>\n</div>\n<div id=\"license\">\n<h2><a href=\"#id5\" rel=\"nofollow\">License</a></h2>\n<p>License is MIT</p>\n</div>\n\n          </div>"}, "last_serial": 4308900, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "6c17a630ed51f41cd2f35c5aef0a096b", "sha256": "4b0a7dc68bc8e472dedd0d1ed0348f0ba69707bc60de474a2c9f77b5508dbe68"}, "downloads": -1, "filename": "MaybeDont-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "6c17a630ed51f41cd2f35c5aef0a096b", "packagetype": "bdist_wheel", "python_version": "3.4", "requires_python": null, "size": 12345, "upload_time": "2016-06-09T18:42:18", "upload_time_iso_8601": "2016-06-09T18:42:18.964437Z", "url": "https://files.pythonhosted.org/packages/1c/c1/2000ffa75bb93c935128e1ab8ace93c5c36189f20a544d5d62822f6da3e0/MaybeDont-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "474eb0fdbbef202401d741051b9db741", "sha256": "f5fb4d38b38c58dd3b042eed74fca5d83c3c121f0be5c73d58583b877ab96231"}, "downloads": -1, "filename": "MaybeDont-0.1.0.tar.gz", "has_sig": false, "md5_digest": "474eb0fdbbef202401d741051b9db741", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8356, "upload_time": "2016-06-09T18:42:12", "upload_time_iso_8601": "2016-06-09T18:42:12.202255Z", "url": "https://files.pythonhosted.org/packages/34/88/bb3f248695834a6e19e3f0fc97b1286cfff044a5ea45dafcd4bad97de336/MaybeDont-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "516120657fce48b935f94fe4b45a8c36", "sha256": "1f7f95cb98f2342ceaf4b763ce0872385898a2c910a6eb6534493ff50c6e6fea"}, "downloads": -1, "filename": "MaybeDont-0.1.1.tar.gz", "has_sig": false, "md5_digest": "516120657fce48b935f94fe4b45a8c36", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8602, "upload_time": "2018-02-05T10:16:54", "upload_time_iso_8601": "2018-02-05T10:16:54.783830Z", "url": "https://files.pythonhosted.org/packages/63/e6/02985110c53867f105bf208d84fd86769c96d6a4d5a310fd19cc4dc32a50/MaybeDont-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "516120657fce48b935f94fe4b45a8c36", "sha256": "1f7f95cb98f2342ceaf4b763ce0872385898a2c910a6eb6534493ff50c6e6fea"}, "downloads": -1, "filename": "MaybeDont-0.1.1.tar.gz", "has_sig": false, "md5_digest": "516120657fce48b935f94fe4b45a8c36", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8602, "upload_time": "2018-02-05T10:16:54", "upload_time_iso_8601": "2018-02-05T10:16:54.783830Z", "url": "https://files.pythonhosted.org/packages/63/e6/02985110c53867f105bf208d84fd86769c96d6a4d5a310fd19cc4dc32a50/MaybeDont-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:57:06 2020"}