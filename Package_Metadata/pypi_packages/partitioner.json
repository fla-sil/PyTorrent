{"info": {"author": "Jake Ryland Williams and Andy Reagan", "author_email": "jw3477@drexel.edu", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.2", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5"], "description": "## Synopsis\n\nThis is the Python partitioner project. The partitioner module performs advanced NLP tasks essentially equivalent to tokenization (e.g., splitting texts into words), with generalizations into multiword expressions (MWE) segmentation. A definition for those unfamiliar with MWEs: \n\n\u201cA group of tokens in a sentence that cohere more strongly than ordinary syntactic combinations.\u201d\n\nThus, partitioner may be used to split texts \"phrases\" of one or more words.\n\n## Code Example\n\nTo load the module, run:\n\n\\>\\>\\> from partitioner.tools import partitioner\n\nSince the module comes with no data, running informed partitions will require acquiring the training data, which may be acquired by engaging the `.download()` method:\n\n\\>\\>\\> pa = partitioner()\n\n\\>\\>\\> pa.download()\n\nNote that the above will require responding to a prompt.\n\nAdditionally, since high-perfornace versions of the partitioner utilize the nltk package's `PerceptronTagger()` function, consider running:\n\n\\>\\>\\> import nltk\n\n\\>\\>\\> nltk.download()\n\nand download all nltk data.\n\nOnce the training data has been downloaded, the following will load all English data sets. This requires significant memory resources, but results in a high-performance model:\n\n\\>\\>\\> pa = partitioner(language = \"en\", doPOS = True, doLFD = True, maxgap = 8, q = {\"type\": 0.74, \"POS\": 0.71})\n\n\\>\\>\\> pa.partition(\"How could something like this simply pop up out of the blue?\")\n\n['How', ' ', 'could', ' ', 'something', ' ', 'like', ' ', 'this', ' ', 'simply', ' ', 'pop up', ' ', 'out of the blue', '?']\n\nThe memory overhead comes from an English Wikipedia data set. While bulky, this data set provides a huge number of named entities. To load from a specific English source, use:\n\n\\>\\>\\> pa = partitioner(language=\"en\", source=\"wiktionary\")\n\nor one of the other data sets. To view all of the available datasets, check out:\n\n\\>\\>\\> pa.datasets\n\nTo load all sets from a specific language (assuming data has been added beyond the starter data, which comes from Wikipedia), use:\n\n\\>\\>\\> pa = partitioner(language=\"es\", source=\"\")\n\n## Motivation\n\nThe original goal of the partitioner project was to create a fast, efficient, and general algorithm that segments texts into the smallest-possible meaningful units, which we refer to as phrases. This essentially coincides with the NLP task for comprehensive MWE segmentation segmentation. Reference for this modules function may be found in the following article:\n\nhttps://arxiv.org/pdf/1608.02025.pdf\n\n## Installation\n\nUsing pip from the command line:\n\n\\>\\>\\> pip install partitioner\n\nAlternatively, if using git from a command line first clone the repository:\n\n\\>\\>\\> git clone https://github.com/jakerylandwilliams/partitioner.git\n\nthen navigate the repository's main directory and run:\n\n\\>\\>\\> sudo python setup.py install\n\n## Contributors\n\nJake Ryland Williams and Andy Reagan\n\n## License\n\nApache", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jakerylandwilliams/partitioner", "keywords": "text partitioning natural language processing multiword expressions named entity recognition", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "partitioner", "package_url": "https://pypi.org/project/partitioner/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/partitioner/", "project_urls": {"Homepage": "https://github.com/jakerylandwilliams/partitioner"}, "release_url": "https://pypi.org/project/partitioner/0.1.2/", "requires_dist": null, "requires_python": "", "summary": "This is the text partitioner project for Python.", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>## Synopsis</p>\n<p>This is the Python partitioner project. The partitioner module performs advanced NLP tasks essentially equivalent to tokenization (e.g., splitting texts into words), with generalizations into multiword expressions (MWE) segmentation. A definition for those unfamiliar with MWEs:</p>\n<p>\u201cA group of tokens in a sentence that cohere more strongly than ordinary syntactic combinations.\u201d</p>\n<p>Thus, partitioner may be used to split texts \u201cphrases\u201d of one or more words.</p>\n<p>## Code Example</p>\n<p>To load the module, run:</p>\n<p>&gt;&gt;&gt; from partitioner.tools import partitioner</p>\n<p>Since the module comes with no data, running informed partitions will require acquiring the training data, which may be acquired by engaging the <cite>.download()</cite> method:</p>\n<p>&gt;&gt;&gt; pa = partitioner()</p>\n<p>&gt;&gt;&gt; pa.download()</p>\n<p>Note that the above will require responding to a prompt.</p>\n<p>Additionally, since high-perfornace versions of the partitioner utilize the nltk package\u2019s <cite>PerceptronTagger()</cite> function, consider running:</p>\n<p>&gt;&gt;&gt; import nltk</p>\n<p>&gt;&gt;&gt; nltk.download()</p>\n<p>and download all nltk data.</p>\n<p>Once the training data has been downloaded, the following will load all English data sets. This requires significant memory resources, but results in a high-performance model:</p>\n<p>&gt;&gt;&gt; pa = partitioner(language = \u201cen\u201d, doPOS = True, doLFD = True, maxgap = 8, q = {\u201ctype\u201d: 0.74, \u201cPOS\u201d: 0.71})</p>\n<p>&gt;&gt;&gt; pa.partition(\u201cHow could something like this simply pop up out of the blue?\u201d)</p>\n<p>[\u2018How\u2019, \u2018 \u2018, \u2018could\u2019, \u2018 \u2018, \u2018something\u2019, \u2018 \u2018, \u2018like\u2019, \u2018 \u2018, \u2018this\u2019, \u2018 \u2018, \u2018simply\u2019, \u2018 \u2018, \u2018pop up\u2019, \u2018 \u2018, \u2018out of the blue\u2019, \u2018?\u2019]</p>\n<p>The memory overhead comes from an English Wikipedia data set. While bulky, this data set provides a huge number of named entities. To load from a specific English source, use:</p>\n<p>&gt;&gt;&gt; pa = partitioner(language=\u201den\u201d, source=\u201dwiktionary\u201d)</p>\n<p>or one of the other data sets. To view all of the available datasets, check out:</p>\n<p>&gt;&gt;&gt; pa.datasets</p>\n<p>To load all sets from a specific language (assuming data has been added beyond the starter data, which comes from Wikipedia), use:</p>\n<p>&gt;&gt;&gt; pa = partitioner(language=\u201des\u201d, source=\u201d\u201d)</p>\n<p>## Motivation</p>\n<p>The original goal of the partitioner project was to create a fast, efficient, and general algorithm that segments texts into the smallest-possible meaningful units, which we refer to as phrases. This essentially coincides with the NLP task for comprehensive MWE segmentation segmentation. Reference for this modules function may be found in the following article:</p>\n<p><a href=\"https://arxiv.org/pdf/1608.02025.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1608.02025.pdf</a></p>\n<p>## Installation</p>\n<p>Using pip from the command line:</p>\n<p>&gt;&gt;&gt; pip install partitioner</p>\n<p>Alternatively, if using git from a command line first clone the repository:</p>\n<p>&gt;&gt;&gt; git clone <a href=\"https://github.com/jakerylandwilliams/partitioner.git\" rel=\"nofollow\">https://github.com/jakerylandwilliams/partitioner.git</a></p>\n<p>then navigate the repository\u2019s main directory and run:</p>\n<p>&gt;&gt;&gt; sudo python setup.py install</p>\n<p>## Contributors</p>\n<p>Jake Ryland Williams and Andy Reagan</p>\n<p>## License</p>\n<p>Apache</p>\n\n          </div>"}, "last_serial": 2939131, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "0ebb714d40034114bbfc9b2eaf3786a7", "sha256": "6a01ebf185789848aa89b14396dc213f316ac9dbec7707fb6e3c472a58b8c0fb"}, "downloads": -1, "filename": "partitioner-0.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "0ebb714d40034114bbfc9b2eaf3786a7", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 2927525, "upload_time": "2016-02-01T16:56:41", "upload_time_iso_8601": "2016-02-01T16:56:41.963223Z", "url": "https://files.pythonhosted.org/packages/3f/b4/c60dc026e131fa185da54e3af7acbe7cbadc8bd74189d50139ab9c3cfb99/partitioner-0.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ec1e5f32f25c3a33b1e951b442fa260b", "sha256": "cd5fcb7e87fa23f29c06939548b3144bd9a732e5812cc129964a493a51212d78"}, "downloads": -1, "filename": "partitioner-0.0.1.tar.gz", "has_sig": false, "md5_digest": "ec1e5f32f25c3a33b1e951b442fa260b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2907570, "upload_time": "2016-02-01T16:56:33", "upload_time_iso_8601": "2016-02-01T16:56:33.565744Z", "url": "https://files.pythonhosted.org/packages/2c/04/29fbdc942f5b3937267c19cd0f1df39aa0ed331495593c9c7faa44b428fa/partitioner-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "75df8b49ec41b2a92eea2a41080b834b", "sha256": "6adb94285b1b5170c557a825cabb987ba1c97faf5e2e76eb17c4cf65ef7f8132"}, "downloads": -1, "filename": "partitioner-0.0.2.tar.gz", "has_sig": false, "md5_digest": "75df8b49ec41b2a92eea2a41080b834b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2907798, "upload_time": "2016-02-03T14:58:42", "upload_time_iso_8601": "2016-02-03T14:58:42.106217Z", "url": "https://files.pythonhosted.org/packages/61/0c/019c3921c8cde7833d9eb95238380ce75a3152201a0dd28c96c00d6be98a/partitioner-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "dcba129e32b1b29ad93087d443d3473c", "sha256": "56bae9dcde336ac847182181fea9037fbbe91d38f531cdcf3d9b4dbddde145e4"}, "downloads": -1, "filename": "partitioner-0.0.3.tar.gz", "has_sig": false, "md5_digest": "dcba129e32b1b29ad93087d443d3473c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2907852, "upload_time": "2016-02-03T20:57:28", "upload_time_iso_8601": "2016-02-03T20:57:28.502095Z", "url": "https://files.pythonhosted.org/packages/66/dc/7e66a3ecbb43225b0a4f1f0eefbf62b693cbbd5248aa7873eb3ea343472a/partitioner-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "5a3997689c8db0b77d941fd9cc2dc765", "sha256": "a619a799cb5c95869423cb8f2d13342ed60d73674f519d9553d2fb3bbfc3e2bf"}, "downloads": -1, "filename": "partitioner-0.0.4.tar.gz", "has_sig": false, "md5_digest": "5a3997689c8db0b77d941fd9cc2dc765", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2907846, "upload_time": "2016-02-03T21:01:00", "upload_time_iso_8601": "2016-02-03T21:01:00.470998Z", "url": "https://files.pythonhosted.org/packages/99/18/18ee3bc81ec35b40d5b9e65c970b720548dbc2bb62ba67c00c568bf68d8a/partitioner-0.0.4.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "43e6b04408a641ff92ab9b9d091a34e0", "sha256": "6bccbadb497a3e19c295f3fbd8c915758045dcf8535c891c7804afd33bedc4a3"}, "downloads": -1, "filename": "partitioner-0.1.0.tar.gz", "has_sig": false, "md5_digest": "43e6b04408a641ff92ab9b9d091a34e0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22068, "upload_time": "2017-04-26T13:13:53", "upload_time_iso_8601": "2017-04-26T13:13:53.821738Z", "url": "https://files.pythonhosted.org/packages/20/29/82c26aa538c5e2ea6ced050bc1758c7abc9fad2ea3ff09a34cee24067feb/partitioner-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "bfc228c87f606848004e526cc649c657", "sha256": "6387423d79e86694a517c50f46cc2a2424c2ff1c2395c909fc1b6c71e90fcbfc"}, "downloads": -1, "filename": "partitioner-0.1.1.tar.gz", "has_sig": false, "md5_digest": "bfc228c87f606848004e526cc649c657", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22248, "upload_time": "2017-04-26T13:39:11", "upload_time_iso_8601": "2017-04-26T13:39:11.568806Z", "url": "https://files.pythonhosted.org/packages/ea/96/00c823fb9c042bb0d6bd44838e3aac792af7d37ad73b69b0e0d3b99ab355/partitioner-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "44f5bc748d5870fabe3fdeffe249d648", "sha256": "4401570e184e138548364448c08a979ee64c88efec9c6677bd4caac9d4d6f5a0"}, "downloads": -1, "filename": "partitioner-0.1.2.tar.gz", "has_sig": false, "md5_digest": "44f5bc748d5870fabe3fdeffe249d648", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22983, "upload_time": "2017-06-09T20:01:23", "upload_time_iso_8601": "2017-06-09T20:01:23.513326Z", "url": "https://files.pythonhosted.org/packages/f9/cf/b4f5fc5e0df01a40468ce79ee0969eba1ff69a54b1c070d9b5305792ed43/partitioner-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "44f5bc748d5870fabe3fdeffe249d648", "sha256": "4401570e184e138548364448c08a979ee64c88efec9c6677bd4caac9d4d6f5a0"}, "downloads": -1, "filename": "partitioner-0.1.2.tar.gz", "has_sig": false, "md5_digest": "44f5bc748d5870fabe3fdeffe249d648", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22983, "upload_time": "2017-06-09T20:01:23", "upload_time_iso_8601": "2017-06-09T20:01:23.513326Z", "url": "https://files.pythonhosted.org/packages/f9/cf/b4f5fc5e0df01a40468ce79ee0969eba1ff69a54b1c070d9b5305792ed43/partitioner-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:58:12 2020"}