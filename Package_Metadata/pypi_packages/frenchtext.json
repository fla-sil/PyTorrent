{"info": {"author": "Laurent Prudhon", "author_email": "laurent.prudhon@hotmail.fr", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "# frenchtext\n        > NLP library to process french text.\n\n\n        In this early pre-version, the library provides :\n        - datasets to train business-oriented french text models\n        - a characters normalization pipeline tailored for french text\n\n        ## Install\n\n        `pip install frenchtext`\n\n        ## Dependencies\n\n        - [pandas](https://pandas.pydata.org/)\n        - [pyarrow](https://arrow.apache.org/docs/python/)\n        - [requests](https://requests.readthedocs.io/en/latest/)\n        - [fastprogress](https://github.com/fastai/fastprogress)\n\n        ## Licence\n\n        APACHE licence 2.0 : https://www.apache.org/licenses/LICENSE-2.0\n\n        ## How to use\n\n        The detailed documentation for each module is available through the menu on the left side of this page.\n\n        You will find below an overview of the library.\n\n        ## French datasets\n\n        ### Data sources\n\n        The text content of the main french websites in the domain of finance and business (+ wikipedia) was extracted in september 2019 using [nlptextdoc](https://github.com/laurentprudhon/nlptextdoc).\n\n        This extraction was done as \"politely\" as possible:\n        - extract only freely and publicly available content\n        - respect the robots.txt directives of each website (pages forbidden for indexing, maximum extraction rate)\n        - detect when websites use tools to prevent indexing (like Datadome) and abort the crawl\n\n        **IMPORTANT: The original authors of the websites own the copyright on all text blocks in this dataset.**\n\n        To be able to link each text block to its original author, we track the origin URL of each text block throughout the whole process.\n\n        **YOU CAN'T REUSE THE TEXT BLOCKS FOR ANY PURPOSE EXCEPT TRAINING A NATURAL LANGUAGE PROCESSING MODEL.**\n\n        See the new European copyright rules : [European Parliament approves new copyright rules for the internet](https://www.europarl.europa.eu/news/en/headlines/priorities/copyright/20190321IPR32110/european-parliament-approves-new-copyright-rules-for-the-internet)\n\n        \"*The directive aims to make it easier for copyrighted material to be used freely through text and data mining, thereby removing a significant competitive disadvantage that European researchers currently face.*\"\n\n        => 131 websites and 2 564 755 HTML pages\n\n        ### Data preparation\n\n        The text blocks were then:\n        - deduplicated to keep only distinct text blocks for each website (forgetting part of the original document structure), \n        - tagged (but not filtered) by language (using https://fasttext.cc/docs/en/language-identification.html),\n        - grouped in categories according to the main theme of the original website,\n        - split in [Pandas](https://pandas.pydata.org/) dataframes of size < 2 GB.\n\n        => 10 categories: 'Assurance', 'Banque', 'Bourse', 'Comparateur', 'Cr\u00e9dit', 'Forum', 'Institution', 'Presse', 'SiteInfo', 'Wikipedia'\n\n        In each dataframe, the text blocks were additionnaly **SHUFFLED IN A RANDOM ORDER** to make it very difficult to reconstruct the original articles (safety measure to help protect the copyrights of the authors).\n\n        The results of this second step can be downloaded in the *config.datasets* directory, as dataframes serialized in the [feather format](https://arrow.apache.org/docs/python/ipc.html?highlight=feather#feather-format), in files named according to the 'DatasetFile' column of the datasets table.\n\n        => 19 dataset files: 'assurance', 'banque', 'bourse', 'comparateur', 'cr\u00e9dit', 'forum', 'institution', 'presse-1', 'presse-2', 'presse-3', 'presse-4', 'presse-5', 'presse-6', 'siteinfo', 'wikipedia-1', 'wikipedia-2', 'wikipedia-3', 'wikipedia-4', 'wikipedia-5'\n\n        ### Dataset size\n\n        The number of words in each text block was computed using the default french tokenizer from [spaCy](https://spacy.io/) v2.1.\n\n        This business-oriented dataset contains **2 billion french words**.\n\n        Here is a summary of the number of words contributed by each category **in millions**:\n\n        - Assurance : 12\n        - Banque : 20\n        - Bourse : 26\n        - Comparateur :\t20\n        - Cr\u00e9dit : 1\n        - Forum : 152\n        - Institution : 4\n        - Presse : 963\n        - SiteInfo : 78\n        - Wikipedia : 727\n\n        ### Dataset files\n\n        ```python\n        from frenchtext.core import *\n        from frenchtext.datasets import *\n        ```\n\n        List available dataset files :\n\n        ```python\n        datasetfiles = list_dataset_files()\n        datasetfiles\n        ```\n\n\n\n\n            ['assurance',\n             'banque',\n             'bourse',\n             'comparateur',\n             'cr\u00e9dit',\n             'forum',\n             'institution',\n             'presse-1',\n             'presse-2',\n             'presse-3',\n             'presse-4',\n             'presse-5',\n             'presse-6',\n             'siteinfo',\n             'wikipedia-1',\n             'wikipedia-2',\n             'wikipedia-3',\n             'wikipedia-4',\n             'wikipedia-5']\n\n\n\n        Source websites and number of words in each dataset file :\n\n        ```python\n        datasetsdf = list_datasets()\n        datasetsdf[[\"DatasetFile\",\"Url\",\"Pages\",\"Words\"]].iloc[80:100]\n        ```\n\n\n\n\n        <div>\n        <style scoped>\n            .dataframe tbody tr th:only-of-type {\n                vertical-align: middle;\n            }\n\n            .dataframe tbody tr th {\n                vertical-align: top;\n            }\n\n            .dataframe thead th {\n                text-align: right;\n            }\n        </style>\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr style=\"text-align: right;\">\n              <th></th>\n              <th>DatasetFile</th>\n              <th>Url</th>\n              <th>Pages</th>\n              <th>Words</th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <th>80</th>\n              <td>comparateur</td>\n              <td>https://www.panorabanques.com/</td>\n              <td>4341</td>\n              <td>2584038</td>\n            </tr>\n            <tr>\n              <th>81</th>\n              <td>cr\u00e9dit</td>\n              <td>https://www.cetelem.fr/</td>\n              <td>274</td>\n              <td>157191</td>\n            </tr>\n            <tr>\n              <th>82</th>\n              <td>cr\u00e9dit</td>\n              <td>https://www.cofidis.fr/</td>\n              <td>347</td>\n              <td>243904</td>\n            </tr>\n            <tr>\n              <th>83</th>\n              <td>cr\u00e9dit</td>\n              <td>https://www.cofinoga.fr/</td>\n              <td>413</td>\n              <td>86796</td>\n            </tr>\n            <tr>\n              <th>84</th>\n              <td>cr\u00e9dit</td>\n              <td>https://www.sofinco.fr/</td>\n              <td>916</td>\n              <td>597221</td>\n            </tr>\n            <tr>\n              <th>85</th>\n              <td>cr\u00e9dit</td>\n              <td>https://www.younited-credit.com/</td>\n              <td>1341</td>\n              <td>665115</td>\n            </tr>\n            <tr>\n              <th>86</th>\n              <td>forum</td>\n              <td>https://droit-finances.commentcamarche.com/</td>\n              <td>96450</td>\n              <td>56120562</td>\n            </tr>\n            <tr>\n              <th>87</th>\n              <td>forum</td>\n              <td>http://forum.doctissimo.fr/famille/argent-budg...</td>\n              <td>26981</td>\n              <td>61020453</td>\n            </tr>\n            <tr>\n              <th>88</th>\n              <td>forum</td>\n              <td>http://forum.doctissimo.fr/viepratique/finance...</td>\n              <td>5745</td>\n              <td>4962230</td>\n            </tr>\n            <tr>\n              <th>89</th>\n              <td>forum</td>\n              <td>http://forum.doctissimo.fr/viepratique/Impots/...</td>\n              <td>2338</td>\n              <td>1422143</td>\n            </tr>\n            <tr>\n              <th>90</th>\n              <td>forum</td>\n              <td>https://forum.lesarnaques.com/assurance-automo...</td>\n              <td>3530</td>\n              <td>3085101</td>\n            </tr>\n            <tr>\n              <th>91</th>\n              <td>forum</td>\n              <td>https://forum.lesarnaques.com/banque/</td>\n              <td>6206</td>\n              <td>5766116</td>\n            </tr>\n            <tr>\n              <th>92</th>\n              <td>forum</td>\n              <td>https://www.60millions-mag.com/forum/</td>\n              <td>3692</td>\n              <td>2222882</td>\n            </tr>\n            <tr>\n              <th>93</th>\n              <td>forum</td>\n              <td>https://www.boursorama.com/patrimoine/forum/</td>\n              <td>13020</td>\n              <td>10497065</td>\n            </tr>\n            <tr>\n              <th>94</th>\n              <td>forum</td>\n              <td>https://www.cbanque.com/forums/</td>\n              <td>12098</td>\n              <td>7702002</td>\n            </tr>\n            <tr>\n              <th>95</th>\n              <td>institution</td>\n              <td>https://acpr.banque-france.fr/</td>\n              <td>470</td>\n              <td>51397</td>\n            </tr>\n            <tr>\n              <th>96</th>\n              <td>institution</td>\n              <td>https://www.banque-france.fr/</td>\n              <td>728</td>\n              <td>75101</td>\n            </tr>\n            <tr>\n              <th>97</th>\n              <td>institution</td>\n              <td>https://www.ffa-assurance.fr/</td>\n              <td>301</td>\n              <td>146499</td>\n            </tr>\n            <tr>\n              <th>98</th>\n              <td>institution</td>\n              <td>https://www.economie.gouv.fr/</td>\n              <td>2720</td>\n              <td>159663</td>\n            </tr>\n            <tr>\n              <th>99</th>\n              <td>institution</td>\n              <td>https://www.impots.gouv.fr/portail/</td>\n              <td>1631</td>\n              <td>653735</td>\n            </tr>\n          </tbody>\n        </table>\n        </div>\n\n\n\n        ### Download dataset files\n\n        ```python\n        download_dataset_file(\"assurance\")\n        ```\n\n            Downloading dataset file : assurance (17 MB)\n\n\n        ```python\n        download_all_datasets()\n        ```\n\n            Downloading dataset file : assurance (17 MB)\n            Downloading dataset file : banque (28 MB)\n            Downloading dataset file : bourse (38 MB)\n            Downloading dataset file : comparateur (28 MB)\n            Downloading dataset file : cr\u00e9dit (2 MB)\n            Downloading dataset file : forum (220 MB)\n            Downloading dataset file : institution (5 MB)\n            Downloading dataset file : presse-1 (218 MB)\n            Downloading dataset file : presse-2 (196 MB)\n            Downloading dataset file : presse-3 (190 MB)\n            Downloading dataset file : presse-4 (234 MB)\n            Downloading dataset file : presse-5 (269 MB)\n            Downloading dataset file : presse-6 (334 MB)\n            Downloading dataset file : siteinfo (116 MB)\n            Downloading dataset file : wikipedia-1 (131 MB)\n            Downloading dataset file : wikipedia-2 (182 MB)\n            Downloading dataset file : wikipedia-3 (263 MB)\n            Downloading dataset file : wikipedia-4 (269 MB)\n            Downloading dataset file : wikipedia-5 (267 MB)\n\n\n        You can change the local directory where the dataset files are downloaded :\n\n        ```python\n        config.datasets\n        ```\n\n\n\n\n            PosixPath('/home/laurent/.frenchtext/datasets')\n\n\n\n        ```python\n        config[\"datasets_path\"] = \"/tmp/datasets\"\n        config.datasets.mkdir(parents=True, exist_ok=True)\n        ```\n\n        ```python\n        config.datasets\n        ```\n\n\n\n\n            PosixPath('/tmp/datasets')\n\n\n\n        ### Read dataset files\n\n        ```python\n        datasetdf = read_dataset_file(\"assurance\")\n        datasetdf\n        ```\n\n            Loaded dataframe for dataset assurance : 563613 text blocks\n\n\n\n\n\n        <div>\n        <style scoped>\n            .dataframe tbody tr th:only-of-type {\n                vertical-align: middle;\n            }\n\n            .dataframe tbody tr th {\n                vertical-align: top;\n            }\n\n            .dataframe thead th {\n                text-align: right;\n            }\n        </style>\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr style=\"text-align: right;\">\n              <th></th>\n              <th>Website</th>\n              <th>DocId</th>\n              <th>DocEltType</th>\n              <th>DocEltCmd</th>\n              <th>NestingLevel</th>\n              <th>Text</th>\n              <th>Lang</th>\n              <th>Words</th>\n              <th>Unique</th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <th>0</th>\n              <td>11</td>\n              <td>22332</td>\n              <td>ListItem</td>\n              <td>Text</td>\n              <td>2</td>\n              <td>5 tournages catastrophe pour un assureur</td>\n              <td>fr</td>\n              <td>6</td>\n              <td>True</td>\n            </tr>\n            <tr>\n              <th>1</th>\n              <td>74</td>\n              <td>710</td>\n              <td>Section</td>\n              <td>Start</td>\n              <td>1</td>\n              <td>Tout connaitre sur la nouvelle formation post-...</td>\n              <td>fr</td>\n              <td>7</td>\n              <td>True</td>\n            </tr>\n            <tr>\n              <th>2</th>\n              <td>11</td>\n              <td>12082</td>\n              <td>TextBlock</td>\n              <td>Text</td>\n              <td>1</td>\n              <td>Votre Agent Mandataire AXA - Civry Marie Claud...</td>\n              <td>?</td>\n              <td>18</td>\n              <td>True</td>\n            </tr>\n            <tr>\n              <th>3</th>\n              <td>87</td>\n              <td>461</td>\n              <td>TextBlock</td>\n              <td>Text</td>\n              <td>4</td>\n              <td>60 ans et 4 mois</td>\n              <td>fr</td>\n              <td>5</td>\n              <td>True</td>\n            </tr>\n            <tr>\n              <th>4</th>\n              <td>7</td>\n              <td>200</td>\n              <td>TextBlock</td>\n              <td>Text</td>\n              <td>1</td>\n              <td>Mon devis sur mesure</td>\n              <td>fr</td>\n              <td>4</td>\n              <td>True</td>\n            </tr>\n            <tr>\n              <th>...</th>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n            </tr>\n            <tr>\n              <th>563608</th>\n              <td>138</td>\n              <td>255</td>\n              <td>Section</td>\n              <td>Start</td>\n              <td>2</td>\n              <td>Les autres pouvoirs de police</td>\n              <td>fr</td>\n              <td>5</td>\n              <td>True</td>\n            </tr>\n            <tr>\n              <th>563609</th>\n              <td>11</td>\n              <td>19483</td>\n              <td>TextBlock</td>\n              <td>Text</td>\n              <td>1</td>\n              <td>Yves Nicolau assurance Laon</td>\n              <td>?</td>\n              <td>4</td>\n              <td>True</td>\n            </tr>\n            <tr>\n              <th>563610</th>\n              <td>106</td>\n              <td>1644</td>\n              <td>ListItem</td>\n              <td>Text</td>\n              <td>3</td>\n              <td>Ev\u00e8nements sportifs</td>\n              <td>fr</td>\n              <td>2</td>\n              <td>True</td>\n            </tr>\n            <tr>\n              <th>563611</th>\n              <td>58</td>\n              <td>4155</td>\n              <td>Section</td>\n              <td>Start</td>\n              <td>1</td>\n              <td>Agence Groupama Chalon</td>\n              <td>?</td>\n              <td>3</td>\n              <td>True</td>\n            </tr>\n            <tr>\n              <th>563612</th>\n              <td>10</td>\n              <td>150</td>\n              <td>TextBlock</td>\n              <td>Text</td>\n              <td>2</td>\n              <td>Nos agences d'assurance Aviva \u00e0 OYONNAX sont h...</td>\n              <td>fr</td>\n              <td>26</td>\n              <td>True</td>\n            </tr>\n          </tbody>\n        </table>\n        <p>563613 rows \u00d7 9 columns</p>\n        </div>\n\n\n\n        ### Access text blocks in dataset files\n\n        Filter and iterate over the rows of a dataset file :\n\n        ```python\n        rowsiterator = get_rows_from_datasetdf(datasetdf, minwords=None, maxwords=5, lang=\"?\")\n        show_first_rows(rowsiterator,10)\n        ```\n\n            12 - COORDONNEES\n            41 - 01 30 41 67 33\n            49 - Dmitriy G.\n            57 - Les atouts du Multisupport CONFIANCE\n            74 - 01XXL meribel hiver\n            76 - Garantie en cas de vol\n            87 - Par AXA, le 01/08/2016\n            96 - mgr@enderby.eu\n            127 - 18 place De Strasbourg\n            131 - Saint Gaudens\n\n\n        Filter and iterate over the text blocks of a full dataset (across multiple files) :\n\n        ```python\n        textiterator = get_textblocks_from_dataset(\"Assurance\", minwords=None, maxwords=10, lang=\"fr\")\n        show_first_textblocks(textiterator,skip=2000,count=10)\n        ```\n\n            Loaded dataframe for dataset assurance : 563613 text blocks\n            2001 - R\u00e9\u00e9quipement \u00e0 neuf \u00e0 vie\n            2002 - D\u00e9finition Conducteur secondaire- Lexique\n            2003 - Comment \u00e9viter les fraudes\n            2004 - Comment demander un remboursement sant\u00e9 - GENERALI\n            2005 - Simulateur pour conna\u00eetre les obligations de votre accord de branche\n            2006 - Compl\u00e9mentaire Epargne retraite des ind\u00e9pendants et TNS - Malakoff M\u00e9d\u00e9ric\n            2007 - Experts-Comptables, d\u00e9couvrez la mission \u00e9pargne salariale\n            2008 - Vous n\u2019\u00eates pas encore client :\n            2009 - Actualit\u00e9s (Page 6) | ameli.fr | Pharmacien\n            2010 - D\u00e9pression : quelle prise en charge ? - Matmut\n\n\n        Access a specific row :\n\n        ```python\n        get_text_from_rowindex(datasetdf,100)\n        ```\n\n\n\n\n            'Les inondations de plaine : d\u00e9bordement de cours d\u2019eau avec une dur\u00e9e d\u2019immersion longue (pr\u00e9visibles plusieurs jours ou heures \u00e0 l\u2019avance).'\n\n\n\n        Find text blocks with a specific char or substring :\n\n        ```python\n        find_textblocks_with_chars(datasetdf,\"r\u00e9troviseur\",count=20,ctxsize=15)\n        ```\n\n\n\n\n            350594     ore dans notre r\u00e9troviseur gauche lorsque \n            149029     de glace ? Les r\u00e9troviseurs ainsi que les \n            51349      ace. Quant aux r\u00e9troviseurs, ils le sont d\n            310354     vant, arri\u00e8re, r\u00e9troviseurs et vitres lat\u00e9\n            489866    \\naussi dans le r\u00e9troviseur pour ne pas se \n            364550     \u00f4t\u00e9 ou sous le r\u00e9troviseur int\u00e9rieur de vo\n            560539     tionnement des r\u00e9troviseurs.              \n            560700     \u00e9 (pare-brise, r\u00e9troviseurs\u2026),            \n            223621     riorations des r\u00e9troviseurs et des phares.\n            543903     es miroirs des r\u00e9troviseurs lorsqu\u2019ils peu\n            502075      logo dans son r\u00e9troviseur et par un signa\n            53237      vous cassez le r\u00e9troviseur d\u2019une voiture. \n            310456      \u00e9raflures, un r\u00e9troviseur ab\u00eem\u00e9, ou un au\n            375158     ant, moteur de r\u00e9troviseurs\u2026              \n            539914     nt et arri\u00e8re, r\u00e9troviseurs int\u00e9rieurs et \n            171367     t utilisez vos r\u00e9troviseurs               \n            485058      ainsi que les r\u00e9troviseurs ne sont pas ga\n            277390     ant, moteur de r\u00e9troviseurs...            \n            20222      sont offerts : r\u00e9troviseurs \u00e9lectriques, c\n            317634     res, y compris r\u00e9troviseurs et feux       \n            Name: Text, dtype: object\n\n\n\n        ```python\n        find_textblocks_with_chars(datasetdf,64257,count=10,wrap=True)\n        ```\n\n\n\n\n            175413    x besoins de diversi[\ufb01]cation des placements\n            337398    e 30 villes ont b\u00e9n\u00e9[\ufb01]ci\u00e9 de ces animations\n            265114    nt r\u00e8glementaire et [\ufb01]nancier, nous accompa\n            74267          La Fondation a [\ufb01]nanc\u00e9 depuis 2009, l\u2019\n            424584    tion de l\u2019\u00e9quilibre [\ufb01]nancier des r\u00e9gimes d\n            219195    d, J\u00e9r\u00f4me Powell con[\ufb01]rmera que, dans l\u2019att\n            489511    s besoins de diversi[\ufb01]cation de la client\u00e8l\n            517563    si en pr\u00e9sence d\u2019un [\ufb01]nancement par cr\u00e9dit,\n            479694    nt r\u00e8glementaire et [\ufb01]nancier, La Mondiale \n            252202    n de disponibilit\u00e9s [\ufb01]nanci\u00e8res mais aussi,\n            Name: Text, dtype: object\n\n\n\n        ### Track the source URL for each text block \n\n        Optionally download and read urls file to track the origin of each text block :\n\n        ```python\n        urlsdf = read_urls_file()\n        urlsdf.head()\n        ```\n\n            Loaded datasets urls : 2668787 urls\n\n\n\n\n\n        <div>\n        <style scoped>\n            .dataframe tbody tr th:only-of-type {\n                vertical-align: middle;\n            }\n\n            .dataframe tbody tr th {\n                vertical-align: top;\n            }\n\n            .dataframe thead th {\n                text-align: right;\n            }\n        </style>\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr style=\"text-align: right;\">\n              <th></th>\n              <th>Website</th>\n              <th>DocId</th>\n              <th>DocUrl</th>\n              <th>Words</th>\n              <th>fr</th>\n              <th>en</th>\n              <th>de</th>\n              <th>es</th>\n              <th>?</th>\n              <th>%fr</th>\n              <th>%en</th>\n              <th>%de</th>\n              <th>%es</th>\n              <th>%?</th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <th>0</th>\n              <td>4</td>\n              <td>1</td>\n              <td>https://www.afer.fr/</td>\n              <td>573.0</td>\n              <td>524.0</td>\n              <td>3.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>46.0</td>\n              <td>0.914485</td>\n              <td>0.005236</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.080279</td>\n            </tr>\n            <tr>\n              <th>1</th>\n              <td>4</td>\n              <td>2</td>\n              <td>https://www.afer.fr/afer/adhesion/</td>\n              <td>74.0</td>\n              <td>74.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>1.000000</td>\n              <td>0.000000</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.000000</td>\n            </tr>\n            <tr>\n              <th>2</th>\n              <td>4</td>\n              <td>3</td>\n              <td>https://www.afer.fr/afer/adhesion/adherent-ass...</td>\n              <td>475.0</td>\n              <td>457.0</td>\n              <td>5.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>13.0</td>\n              <td>0.962105</td>\n              <td>0.010526</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.027368</td>\n            </tr>\n            <tr>\n              <th>3</th>\n              <td>4</td>\n              <td>4</td>\n              <td>https://www.afer.fr/afer/adhesion/adherer-assu...</td>\n              <td>519.0</td>\n              <td>519.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>1.000000</td>\n              <td>0.000000</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.000000</td>\n            </tr>\n            <tr>\n              <th>4</th>\n              <td>4</td>\n              <td>5</td>\n              <td>https://www.afer.fr/afer/adhesion/parrainage-a...</td>\n              <td>355.0</td>\n              <td>345.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>10.0</td>\n              <td>0.971831</td>\n              <td>0.000000</td>\n              <td>0.0</td>\n              <td>0.0</td>\n              <td>0.028169</td>\n            </tr>\n          </tbody>\n        </table>\n        </div>\n\n\n\n        ```python\n        get_text_from_rowindex(datasetdf,100)\n        ```\n\n\n\n\n            'Les inondations de plaine : d\u00e9bordement de cours d\u2019eau avec une dur\u00e9e d\u2019immersion longue (pr\u00e9visibles plusieurs jours ou heures \u00e0 l\u2019avance).'\n\n\n\n        ```python\n        get_url_from_rowindex(datasetdf, 100)\n        ```\n\n\n\n\n            'https://www.maif.fr/conseils-prevention/risques-majeurs/inondation.html'\n\n\n\n        ## Characters normalization pipeline\n\n        ### Motivation\n\n        French datasets often contain several thousands distinct Unicode characters.\n\n        Characters stats in Wikipedia dataset :\n        - 35.6 billion chars\n        - 13 502 distinct Unicode chars\n\n        Characters stats in Business dataset :\n        - 27.5 billion chars\n        - 3 763 distinct Unicode chars\n\n        We need to reduce the number of distinct characters fed to our natural language processing applications, for three reasons :\n        - chars considered by the user as visually equivalent will often produce a different application behavior : this is a huge problem for the user experience\n        - with so many chars, the designer of the NLP application will not be able to reason about all possible combinations : this could harm the explainability of the system\n        - this huge number of distinct characters brings a significant amount complexity the NLP models will have to deal with\n\n        Characters stats in Wikipedia dataset :\n        - Only 1316 chars more frequent than 1 in 100 million\n        - 99.9987 % of Wikipedia chars would be preserved if we only kept the frequent chars\n\n        Characters stats in Business dataset :\n        - Only 531 chars more frequent than 1 in 100 million\n        - 99.9996 % of Business chars would be preserved if we only kept the frequent chars\n\n        We can be smarter than that and replace rare chars with equivalent (or mostly equivalent) more frequent chars to preserve a maximum of information.\n\n        ### Target characters set\n\n        After a detailed study of all the frequent chars, the goal is to design a noramization pipeline which can retain as much information as possible while greatly reducing the number of dinstinct chars.\n\n        We saw before that it is possible to preserve 99.9996% of the original chars while keeping only 500 distinct chars. By being clever and replacing equivalent chars, we can divide this number by 2 and still retain the same amount of information.\n\n        It may then be useful to limit the number of distinct characters after normalization to **255 distinct characters** : \n        - if needed, french text chars can then be encoded with a single byte\n        - the list of supported chars can be memorized by NLP application developers and users\n\n        ```python\n        from frenchtext.core import *\n        from frenchtext.chars import *\n        ```\n\n        255 supported characters after normalization : \n\n        ```python\n        import pandas as pd\n        dfcharsnorm = pd.read_csv(chardatadir / \"charset-fr.csv\", sep=\";\")\n        dfcharsnorm\n        ```\n\n\n\n\n        <div>\n        <style scoped>\n            .dataframe tbody tr th:only-of-type {\n                vertical-align: middle;\n            }\n\n            .dataframe tbody tr th {\n                vertical-align: top;\n            }\n\n            .dataframe thead th {\n                text-align: right;\n            }\n        </style>\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr style=\"text-align: right;\">\n              <th></th>\n              <th>FrCode</th>\n              <th>Category</th>\n              <th>SubCategory</th>\n              <th>Code</th>\n              <th>Char</th>\n              <th>CharName</th>\n              <th>CountBusiness</th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <th>0</th>\n              <td>0</td>\n              <td>separator</td>\n              <td>control</td>\n              <td>0</td>\n              <td>NaN</td>\n              <td>Reserved - End of string</td>\n              <td>0</td>\n            </tr>\n            <tr>\n              <th>1</th>\n              <td>1</td>\n              <td>separator</td>\n              <td>space</td>\n              <td>32</td>\n              <td></td>\n              <td>Space</td>\n              <td>88494564</td>\n            </tr>\n            <tr>\n              <th>2</th>\n              <td>2</td>\n              <td>separator</td>\n              <td>space</td>\n              <td>10</td>\n              <td>\\n</td>\n              <td>Char 10</td>\n              <td>9588147</td>\n            </tr>\n            <tr>\n              <th>3</th>\n              <td>3</td>\n              <td>separator</td>\n              <td>space</td>\n              <td>9</td>\n              <td>\\t</td>\n              <td>Char 9</td>\n              <td>1522053</td>\n            </tr>\n            <tr>\n              <th>4</th>\n              <td>4</td>\n              <td>separator</td>\n              <td>punctuation</td>\n              <td>44</td>\n              <td>,</td>\n              <td>Comma</td>\n              <td>286106887</td>\n            </tr>\n            <tr>\n              <th>...</th>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n              <td>...</td>\n            </tr>\n            <tr>\n              <th>251</th>\n              <td>251</td>\n              <td>emoticon</td>\n              <td>object</td>\n              <td>9792</td>\n              <td>\u2640</td>\n              <td>Female Sign</td>\n              <td>515</td>\n            </tr>\n            <tr>\n              <th>252</th>\n              <td>252</td>\n              <td>emoticon</td>\n              <td>object</td>\n              <td>127881</td>\n              <td>\ud83c\udf89</td>\n              <td>Party Popper</td>\n              <td>356</td>\n            </tr>\n            <tr>\n              <th>253</th>\n              <td>253</td>\n              <td>emoticon</td>\n              <td>object</td>\n              <td>9997</td>\n              <td>\u270d</td>\n              <td>Writing Hand</td>\n              <td>157</td>\n            </tr>\n            <tr>\n              <th>254</th>\n              <td>254</td>\n              <td>emoticon</td>\n              <td>object</td>\n              <td>9993</td>\n              <td>\u2709</td>\n              <td>Envelope</td>\n              <td>55</td>\n            </tr>\n            <tr>\n              <th>255</th>\n              <td>255</td>\n              <td>emoticon</td>\n              <td>object</td>\n              <td>10013</td>\n              <td>\u271d</td>\n              <td>Latin Cross</td>\n              <td>22</td>\n            </tr>\n          </tbody>\n        </table>\n        <p>256 rows \u00d7 7 columns</p>\n        </div>\n\n\n\n        The table below shows the number of chars in each category (after normalization) **per 100 million characters** :\n\n        ```python\n        dfblocks = dfcharsnorm.groupby(by=[\"Category\",\"SubCategory\"]).agg({\"Char\":[\"count\",\"sum\"],\"CountBusiness\":\"sum\"})\n        dfblocks[\"CountBusiness\"] = (dfblocks[\"CountBusiness\"] / 27577304956 * 100000000).astype(int)\n        dfblocks\n        ```\n\n\n\n\n        <div>\n        <style scoped>\n            .dataframe tbody tr th:only-of-type {\n                vertical-align: middle;\n            }\n\n            .dataframe tbody tr th {\n                vertical-align: top;\n            }\n\n            .dataframe thead tr th {\n                text-align: left;\n            }\n\n            .dataframe thead tr:last-of-type th {\n                text-align: right;\n            }\n        </style>\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr>\n              <th></th>\n              <th></th>\n              <th colspan=\"2\" halign=\"left\">Char</th>\n              <th>CountBusiness</th>\n            </tr>\n            <tr>\n              <th></th>\n              <th></th>\n              <th>count</th>\n              <th>sum</th>\n              <th>sum</th>\n            </tr>\n            <tr>\n              <th>Category</th>\n              <th>SubCategory</th>\n              <th></th>\n              <th></th>\n              <th></th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <th rowspan=\"3\" valign=\"top\">emoticon</th>\n              <th>hand</th>\n              <td>12</td>\n              <td>\ud83d\udcaa\ud83d\udc49\ud83d\udc4d\ud83d\udc4f\ud83d\ude4f\ud83d\ude4c\ud83d\udc47\ud83d\udc4a\ud83d\udc4e\ud83d\udc4c\u270c\u270a</td>\n              <td>42</td>\n            </tr>\n            <tr>\n              <th>head</th>\n              <td>28</td>\n              <td>\ud83d\ude42\ud83d\ude09\ud83d\ude00\ud83d\ude02\ud83d\ude01\ud83d\ude0a\ud83d\ude41\ud83d\ude05\ud83d\ude0d\ud83d\ude03\ud83d\ude21\ud83e\udd23\ud83d\ude04\ud83e\udd14\ud83d\ude0e\ud83d\ude2d\ud83d\udc79\ud83d\ude31\ud83d\ude1c\ud83d\ude0b\ud83e\udd29\ud83d\ude44\ud83d\ude06\ud83d\ude1b\ud83e\udd2a\ud83d\ude22\ud83d\ude07\ud83e\udd26</td>\n              <td>233</td>\n            </tr>\n            <tr>\n              <th>object</th>\n              <td>16</td>\n              <td>\u26a0\ud83d\udd34\ud83d\udd25\ud83c\udfc6\u26bd\ud83d\udca1\ud83d\udea8\ud83d\udca5\u26a1\u266b\u2642\u2640\ud83c\udf89\u270d\u2709\u271d</td>\n              <td>60</td>\n            </tr>\n            <tr>\n              <th rowspan=\"6\" valign=\"top\">letter</th>\n              <th>digit</th>\n              <td>10</td>\n              <td>0123549876</td>\n              <td>3271115</td>\n            </tr>\n            <tr>\n              <th>encoding</th>\n              <td>3</td>\n              <td>\u00c3\ufffd\ufffc</td>\n              <td>249</td>\n            </tr>\n            <tr>\n              <th>greek</th>\n              <td>2</td>\n              <td>\u03bb\u03c0</td>\n              <td>2</td>\n            </tr>\n            <tr>\n              <th>latin-fr</th>\n              <td>84</td>\n              <td>abcdefghijklmnopqrstuvwxyz\u00e0\u00e2\u00e4\u00e7\u00e8\u00e9\u00ea\u00eb\u00ee\u00ef\u00f4\u00f6\u00f9\u00fb\u00fc\u00ffABCD...</td>\n              <td>91437146</td>\n            </tr>\n            <tr>\n              <th>latin-other</th>\n              <td>25</td>\n              <td>\u00e1\u00e3\u00e5\u0107\u010d\u0117\u011f\u0131\u00ed\u00ec\u0144\u00f1\u00f3\u00f2\u00f5\u00f8\u0161\u015f\u00df\u00fa\u00c1\u00c5\u0160\u00da\u017d</td>\n              <td>712</td>\n            </tr>\n            <tr>\n              <th>other</th>\n              <td>5</td>\n              <td>_&amp;@\\#</td>\n              <td>40814</td>\n            </tr>\n            <tr>\n              <th rowspan=\"3\" valign=\"top\">separator</th>\n              <th>control</th>\n              <td>0</td>\n              <td>0</td>\n              <td>0</td>\n            </tr>\n            <tr>\n              <th>punctuation</th>\n              <td>23</td>\n              <td>,'.-:/\")(?!\u00bb\u00ab|\u2026;[]}{\u2022\u00bf\u00a1</td>\n              <td>4684722</td>\n            </tr>\n            <tr>\n              <th>space</th>\n              <td>3</td>\n              <td>\\n\\t</td>\n              <td>361183</td>\n            </tr>\n            <tr>\n              <th rowspan=\"5\" valign=\"top\">symbol</th>\n              <th>currency</th>\n              <td>6</td>\n              <td>\u20ac$\u00a4\u00a3\u00a5\u00a2</td>\n              <td>21099</td>\n            </tr>\n            <tr>\n              <th>math</th>\n              <td>14</td>\n              <td>=&gt;+&lt;^~\u00d7\u2264\u00f7\u2265\u00b1\u2260\u221e\u221a</td>\n              <td>50056</td>\n            </tr>\n            <tr>\n              <th>shape</th>\n              <td>15</td>\n              <td>*\u2713\u21d2\u2665\u00a6\u2192\u2605\u00af\u2193\u274c\u2750\u2020\u2191\u2190\u2194</td>\n              <td>7954</td>\n            </tr>\n            <tr>\n              <th>sign</th>\n              <td>3</td>\n              <td>\u00a9\u00ae\u2122</td>\n              <td>1754</td>\n            </tr>\n            <tr>\n              <th>unit</th>\n              <td>6</td>\n              <td>%\u00b0\u00a7\u00b5\u00d8\u2030</td>\n              <td>102213</td>\n            </tr>\n          </tbody>\n        </table>\n        </div>\n\n\n\n        ### Normalization pipeline overview\n\n        The normalization pipeline applies the following **14 steps**, which are explained and illustrated in the sections below.\n\n        - Fix encoding errors\n          - fix windows1252 text read as iso8859-1\n          - fix utf8 text read as windows1252\n          - fix windows1252 text read as utf8\n          - merge Unicode combining chars\n          - ignore control chars\n        - Remove display attributes\n          - replace latin letter symbols\n          - replace latin letter ligatures\n          - replace latin number symbols\n        - Normalize visually equivalent chars\n          - replace equivalent chars \n          - replace cyrillic and greek chars looking like latin letters\n        - Encode infrequent chars while losing a little bit of information \n          - replace infrequent latin letters with diacritics\n          - replace infrequent chars from other scripts\n          - replace infrequent symbols \n          - ignore remaining chars with no glyph \n\n        The statistics below count the number of chars normalized **for 1 million chars** in 4 distinct parts of the french datasets : business websites, forums, news, wikipedia.\n\n        The first line of the table below shows that :\n        - in 1 million chars extracted from forum pages (raw users input), 41.8 chars will be encoding errors (windows1252 read as iso8859-1)\n        - in 1 million chars extracted from wikipedia (curated content), only 0.006 chars will be encoding errors\n\n        These numbers show that **characters normalization is much more important in real world applications** than in academic papers based on clean wikipedia text. \n\n        ```python\n        normstats = pd.read_csv(chardatadir / \"stats\" / \"normalization.total.stats.csv\")\n        normstats[[\"Transform\",\"FreqBusiness\",\"FreqForum\",\"FreqPresse\",\"FreqWikipedia\"]]\n        ```\n\n\n\n\n        <div>\n        <style scoped>\n            .dataframe tbody tr th:only-of-type {\n                vertical-align: middle;\n            }\n\n            .dataframe tbody tr th {\n                vertical-align: top;\n            }\n\n            .dataframe thead th {\n                text-align: right;\n            }\n        </style>\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr style=\"text-align: right;\">\n              <th></th>\n              <th>Transform</th>\n              <th>FreqBusiness</th>\n              <th>FreqForum</th>\n              <th>FreqPresse</th>\n              <th>FreqWikipedia</th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <th>0</th>\n              <td>Fix encoding errors : windows1252 read as iso8...</td>\n              <td>0.510560</td>\n              <td>41.818746</td>\n              <td>0.813485</td>\n              <td>0.006025</td>\n            </tr>\n            <tr>\n              <th>1</th>\n              <td>Fix encoding errors : utf8 read as windows1252</td>\n              <td>0.126815</td>\n              <td>0.058024</td>\n              <td>0.072456</td>\n              <td>0.001037</td>\n            </tr>\n            <tr>\n              <th>2</th>\n              <td>Fix encoding errors :  windows1252 read as utf8</td>\n              <td>0.000000</td>\n              <td>0.000000</td>\n              <td>0.019315</td>\n              <td>0.000000</td>\n            </tr>\n            <tr>\n              <th>3</th>\n              <td>Merge Unicode combining chars</td>\n              <td>2.811983</td>\n              <td>0.432638</td>\n              <td>0.568146</td>\n              <td>0.000140</td>\n            </tr>\n            <tr>\n              <th>4</th>\n              <td>Ignore control chars</td>\n              <td>6.450737</td>\n              <td>349.052995</td>\n              <td>6.454367</td>\n              <td>4.118586</td>\n            </tr>\n            <tr>\n              <th>5</th>\n              <td>Replace latin letter symbols</td>\n              <td>0.019360</td>\n              <td>0.039701</td>\n              <td>0.297372</td>\n              <td>0.150550</td>\n            </tr>\n            <tr>\n              <th>6</th>\n              <td>Replace latin letter ligatures</td>\n              <td>6.603815</td>\n              <td>6.541480</td>\n              <td>10.097290</td>\n              <td>17.204422</td>\n            </tr>\n            <tr>\n              <th>7</th>\n              <td>Replace latin number symbols</td>\n              <td>2.528338</td>\n              <td>4.162482</td>\n              <td>2.560933</td>\n              <td>0.429792</td>\n            </tr>\n            <tr>\n              <th>8</th>\n              <td>Normalize equivalent chars</td>\n              <td>814.327384</td>\n              <td>1248.410777</td>\n              <td>684.333730</td>\n              <td>242.391239</td>\n            </tr>\n            <tr>\n              <th>9</th>\n              <td>Replace cyrillic and greek chars looking like ...</td>\n              <td>0.062432</td>\n              <td>0.760424</td>\n              <td>0.491996</td>\n              <td>7.479907</td>\n            </tr>\n            <tr>\n              <th>10</th>\n              <td>Replace infrequent chars : latin letters with ...</td>\n              <td>0.063782</td>\n              <td>0.078384</td>\n              <td>0.099106</td>\n              <td>9.124948</td>\n            </tr>\n            <tr>\n              <th>11</th>\n              <td>Replace infrequent chars : other scripts</td>\n              <td>0.085694</td>\n              <td>0.468776</td>\n              <td>1.192548</td>\n              <td>16.612142</td>\n            </tr>\n            <tr>\n              <th>12</th>\n              <td>Replace infrequent chars : symbols</td>\n              <td>0.139271</td>\n              <td>0.159821</td>\n              <td>0.399064</td>\n              <td>0.073566</td>\n            </tr>\n            <tr>\n              <th>13</th>\n              <td>Replace infrequent chars : chars to ignore</td>\n              <td>0.018910</td>\n              <td>0.044282</td>\n              <td>0.021320</td>\n              <td>0.016423</td>\n            </tr>\n          </tbody>\n        </table>\n        </div>\n\n\n\n        Most frequent chars replaced from equivalent characters :\n\n        ```python\n        replacestats = pd.read_csv(chardatadir / \"stats\" / \"normalization.layer8.stats.csv\")\n        replacestats[[\"Char\",\"CharName\",\"FreqBusiness\",\"FreqForum\",\"FreqPresse\",\"FreqWikipedia\"]].head(20)\n        ```\n\n\n\n\n        <div>\n        <style scoped>\n            .dataframe tbody tr th:only-of-type {\n                vertical-align: middle;\n            }\n\n            .dataframe tbody tr th {\n                vertical-align: top;\n            }\n\n            .dataframe thead th {\n                text-align: right;\n            }\n        </style>\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr style=\"text-align: right;\">\n              <th></th>\n              <th>Char</th>\n              <th>CharName</th>\n              <th>FreqBusiness</th>\n              <th>FreqForum</th>\n              <th>FreqPresse</th>\n              <th>FreqWikipedia</th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <th>0</th>\n              <td>'</td>\n              <td>Apostrophe</td>\n              <td>486.034805</td>\n              <td>160.264219</td>\n              <td>376.104982</td>\n              <td>134.658673</td>\n            </tr>\n            <tr>\n              <th>1</th>\n              <td></td>\n              <td>Space</td>\n              <td>310.411117</td>\n              <td>1082.845985</td>\n              <td>288.635983</td>\n              <td>87.877649</td>\n            </tr>\n            <tr>\n              <th>2</th>\n              <td>-</td>\n              <td>Hyphen-Minus</td>\n              <td>14.431203</td>\n              <td>2.903761</td>\n              <td>12.828203</td>\n              <td>16.223154</td>\n            </tr>\n            <tr>\n              <th>3</th>\n              <td>\u00ab</td>\n              <td>Left-Pointing Double Angle Quotation Mark</td>\n              <td>1.429478</td>\n              <td>0.680513</td>\n              <td>3.002426</td>\n              <td>0.559632</td>\n            </tr>\n            <tr>\n              <th>4</th>\n              <td>\u00bb</td>\n              <td>Right-Pointing Double Angle Quotation Mark</td>\n              <td>1.323524</td>\n              <td>0.533926</td>\n              <td>2.461880</td>\n              <td>0.544134</td>\n            </tr>\n            <tr>\n              <th>5</th>\n              <td>|</td>\n              <td>Vertical Line</td>\n              <td>0.003452</td>\n              <td>0.001018</td>\n              <td>0.005488</td>\n              <td>0.875894</td>\n            </tr>\n            <tr>\n              <th>6</th>\n              <td>\u2022</td>\n              <td>Bullet</td>\n              <td>0.204104</td>\n              <td>0.243295</td>\n              <td>0.189664</td>\n              <td>0.543237</td>\n            </tr>\n            <tr>\n              <th>7</th>\n              <td>.</td>\n              <td>Full Stop</td>\n              <td>0.059280</td>\n              <td>0.078893</td>\n              <td>0.856230</td>\n              <td>0.069278</td>\n            </tr>\n            <tr>\n              <th>8</th>\n              <td>\"</td>\n              <td>Quotation Mark</td>\n              <td>0.085093</td>\n              <td>0.023413</td>\n              <td>0.011504</td>\n              <td>0.292385</td>\n            </tr>\n            <tr>\n              <th>9</th>\n              <td>:</td>\n              <td>Colon</td>\n              <td>0.000150</td>\n              <td>0.000509</td>\n              <td>0.000053</td>\n              <td>0.169047</td>\n            </tr>\n            <tr>\n              <th>10</th>\n              <td>\u00b0</td>\n              <td>Degree Sign</td>\n              <td>0.148726</td>\n              <td>0.181199</td>\n              <td>0.014618</td>\n              <td>0.078302</td>\n            </tr>\n            <tr>\n              <th>11</th>\n              <td>\u00e9</td>\n              <td>Latin Small Letter E With Acute</td>\n              <td>0.001651</td>\n              <td>0.006108</td>\n              <td>0.003166</td>\n              <td>0.101114</td>\n            </tr>\n            <tr>\n              <th>12</th>\n              <td>\u2190</td>\n              <td>Leftwards Arrow</td>\n              <td>0.000000</td>\n              <td>0.000000</td>\n              <td>0.000158</td>\n              <td>0.047194</td>\n            </tr>\n            <tr>\n              <th>13</th>\n              <td>=</td>\n              <td>Equals Sign</td>\n              <td>0.004802</td>\n              <td>0.029012</td>\n              <td>0.000686</td>\n              <td>0.041589</td>\n            </tr>\n            <tr>\n              <th>14</th>\n              <td>\u2192</td>\n              <td>Rightwards Arrow</td>\n              <td>0.026113</td>\n              <td>0.002545</td>\n              <td>0.034302</td>\n              <td>0.015862</td>\n            </tr>\n            <tr>\n              <th>15</th>\n              <td>d</td>\n              <td>Latin Small Letter D</td>\n              <td>0.000000</td>\n              <td>0.024940</td>\n              <td>0.000000</td>\n              <td>0.036405</td>\n            </tr>\n            <tr>\n              <th>16</th>\n              <td>&lt;</td>\n              <td>Less-Than Sign</td>\n              <td>0.004202</td>\n              <td>0.142007</td>\n              <td>0.001267</td>\n              <td>0.024073</td>\n            </tr>\n            <tr>\n              <th>17</th>\n              <td>,</td>\n              <td>Comma</td>\n              <td>0.006453</td>\n              <td>0.101288</td>\n              <td>0.004538</td>\n              <td>0.022756</td>\n            </tr>\n            <tr>\n              <th>18</th>\n              <td>\u2193</td>\n              <td>Downwards Arrow</td>\n              <td>0.007504</td>\n              <td>0.001527</td>\n              <td>0.011188</td>\n              <td>0.021888</td>\n            </tr>\n            <tr>\n              <th>19</th>\n              <td>\u2605</td>\n              <td>Black Star</td>\n              <td>0.001351</td>\n              <td>0.013743</td>\n              <td>0.022006</td>\n              <td>0.011686</td>\n            </tr>\n          </tbody>\n        </table>\n        </div>\n\n\n\n        For example, list of all Unicode chars wich will be projected to a regular 'apostrophe' :\n\n        ```python\n        replacechars = pd.read_csv(chardatadir / \"normalizedchars.csv\", sep=';')\n        replacechars[replacechars[\"NormChar\"]==\"'\"][[\"Code\",\"Char\",\"CharName\"]]\n        ```\n\n\n\n\n        <div>\n        <style scoped>\n            .dataframe tbody tr th:only-of-type {\n                vertical-align: middle;\n            }\n\n            .dataframe tbody tr th {\n                vertical-align: top;\n            }\n\n            .dataframe thead th {\n                text-align: right;\n            }\n        </style>\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr style=\"text-align: right;\">\n              <th></th>\n              <th>Code</th>\n              <th>Char</th>\n              <th>CharName</th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <th>23</th>\n              <td>96</td>\n              <td>`</td>\n              <td>Grave Accent</td>\n            </tr>\n            <tr>\n              <th>24</th>\n              <td>180</td>\n              <td>\u00b4</td>\n              <td>Acute Accent</td>\n            </tr>\n            <tr>\n              <th>25</th>\n              <td>697</td>\n              <td>\u02b9</td>\n              <td>Modifier Letter Prime</td>\n            </tr>\n            <tr>\n              <th>26</th>\n              <td>699</td>\n              <td>\u02bb</td>\n              <td>Modifier Letter Turned Comma</td>\n            </tr>\n            <tr>\n              <th>27</th>\n              <td>700</td>\n              <td>\u02bc</td>\n              <td>Modifier Letter Apostrophe</td>\n            </tr>\n            <tr>\n              <th>28</th>\n              <td>702</td>\n              <td>\u02be</td>\n              <td>Modifier Letter Right Half Ring</td>\n            </tr>\n            <tr>\n              <th>29</th>\n              <td>703</td>\n              <td>\u02bf</td>\n              <td>Modifier Letter Left Half Ring</td>\n            </tr>\n            <tr>\n              <th>30</th>\n              <td>712</td>\n              <td>\u02c8</td>\n              <td>Modifier Letter Vertical Line</td>\n            </tr>\n            <tr>\n              <th>31</th>\n              <td>714</td>\n              <td>\u02ca</td>\n              <td>Modifier Letter Acute Accent</td>\n            </tr>\n            <tr>\n              <th>32</th>\n              <td>715</td>\n              <td>\u02cb</td>\n              <td>Modifier Letter Grave Accent</td>\n            </tr>\n            <tr>\n              <th>33</th>\n              <td>729</td>\n              <td>\u02d9</td>\n              <td>Dot Above</td>\n            </tr>\n            <tr>\n              <th>34</th>\n              <td>8216</td>\n              <td>\u2018</td>\n              <td>Left Single Quotation Mark</td>\n            </tr>\n            <tr>\n              <th>35</th>\n              <td>8217</td>\n              <td>\u2019</td>\n              <td>Right Single Quotation Mark</td>\n            </tr>\n            <tr>\n              <th>36</th>\n              <td>8219</td>\n              <td>\u201b</td>\n              <td>Single High-Reversed-9 Quotation Mark</td>\n            </tr>\n            <tr>\n              <th>37</th>\n              <td>8223</td>\n              <td>\u201f</td>\n              <td>Double High-Reversed-9 Quotation Mark</td>\n            </tr>\n            <tr>\n              <th>38</th>\n              <td>8242</td>\n              <td>\u2032</td>\n              <td>Prime</td>\n            </tr>\n          </tbody>\n        </table>\n        </div>\n\n\n\n        Frequency of characters from other scripts (chinese, arabic, cyrillic ...) :\n\n        ```python\n        scriptsstats = pd.read_csv(chardatadir / \"stats\" / \"normalization.layer11.stats.csv\")\n        scriptsstats[[\"CharFamily\",\"FreqBusiness\",\"FreqForum\",\"FreqPresse\",\"FreqWikipedia\"]]\n        ```\n\n\n\n\n        <div>\n        <style scoped>\n            .dataframe tbody tr th:only-of-type {\n                vertical-align: middle;\n            }\n\n            .dataframe tbody tr th {\n                vertical-align: top;\n            }\n\n            .dataframe thead th {\n                text-align: right;\n            }\n        </style>\n        <table border=\"1\" class=\"dataframe\">\n          <thead>\n            <tr style=\"text-align: right;\">\n              <th></th>\n              <th>CharFamily</th>\n              <th>FreqBusiness</th>\n              <th>FreqForum</th>\n              <th>FreqPresse</th>\n              <th>FreqWikipedia</th>\n            </tr>\n          </thead>\n          <tbody>\n            <tr>\n              <th>0</th>\n              <td>ChineseJapaneseKorean</td>\n              <td>0.012456</td>\n              <td>0.177127</td>\n              <td>0.194677</td>\n              <td>4.059173</td>\n            </tr>\n            <tr>\n              <th>1</th>\n              <td>Arabic</td>\n              <td>0.012306</td>\n              <td>0.026467</td>\n              <td>0.460280</td>\n              <td>3.140120</td>\n            </tr>\n            <tr>\n              <th>2</th>\n              <td>Cyrillic</td>\n              <td>0.024462</td>\n              <td>0.166438</td>\n              <td>0.237159</td>\n              <td>3.118961</td>\n            </tr>\n            <tr>\n              <th>3</th>\n              <td>Greek</td>\n              <td>0.016058</td>\n              <td>0.022904</td>\n              <td>0.031347</td>\n              <td>2.423996</td>\n            </tr>\n            <tr>\n              <th>4</th>\n              <td>Hebrew</td>\n              <td>0.000150</td>\n              <td>0.000000</td>\n              <td>0.184914</td>\n              <td>1.132155</td>\n            </tr>\n            <tr>\n              <th>5</th>\n              <td>Other</td>\n              <td>0.000750</td>\n              <td>0.029012</td>\n              <td>0.004063</td>\n              <td>0.800871</td>\n            </tr>\n            <tr>\n              <th>6</th>\n              <td>Indian</td>\n              <td>0.000750</td>\n              <td>0.037665</td>\n              <td>0.033458</td>\n              <td>0.737955</td>\n            </tr>\n            <tr>\n              <th>7</th>\n              <td>Phonetic</td>\n              <td>0.002401</td>\n              <td>0.001527</td>\n              <td>0.001636</td>\n              <td>0.298579</td>\n            </tr>\n            <tr>\n              <th>8</th>\n              <td>Latin</td>\n              <td>0.013507</td>\n              <td>0.006108</td>\n              <td>0.007283</td>\n              <td>0.269377</td>\n            </tr>\n            <tr>\n              <th>9</th>\n              <td>Math</td>\n              <td>0.001801</td>\n              <td>0.000509</td>\n              <td>0.000528</td>\n              <td>0.240707</td>\n            </tr>\n            <tr>\n              <th>10</th>\n              <td>LaoThai</td>\n              <td>0.000000</td>\n              <td>0.001018</td>\n              <td>0.033194</td>\n              <td>0.217867</td>\n            </tr>\n            <tr>\n              <th>11</th>\n              <td>Armenian</td>\n              <td>0.001051</td>\n              <td>0.000000</td>\n              <td>0.004011</td>\n              <td>0.172382</td>\n            </tr>\n          </tbody>\n        </table>\n        </div>\n\n\n\n        ### Normalization pipeline API\n\n        Initialize a text normalizer :\n\n        ```python\n        %time norm = TextNormalizer()\n        norm\n        ```\n\n            CPU times: user 1.83 s, sys: 15.6 ms, total: 1.84 s\n            Wall time: 2 s\n\n\n\n\n\n            1 - Fix encoding errors : windows1252 read as iso8859-1\n            2 - Fix encoding errors : utf8 read as windows1252\n            3 - Fix encoding errors :  windows1252 read as utf8\n            4 - Merge Unicode combining chars\n            5 - Ignore control chars\n            6 - Replace latin letter symbols\n            7 - Replace latin letter ligatures\n            8 - Replace latin number symbols\n            9 - Normalize equivalent chars\n            10 - Replace cyrillic and greek chars looking like latin letters\n            11 - Replace infrequent chars : latin letters with diacritics\n            12 - Replace infrequent chars : other scripts\n            13 - Replace infrequent chars : symbols\n            14 - Replace infrequent chars : chars to ignore\n\n\n\n        Normalize text :\n\n        ```python\n        teststring = chr(127995)+\"\u2460 l`\"+chr(156)+\"uv\"+chr(127)+\"re est\u00a8 \"+chr(147)+\"belle\"+chr(148)+\"\u00b8 \u00c3  \u00c2\u00bd \u00e2\u201a\u00ac e\u0301nie\u0300me \u00e2\u20ac\u00b0 \"+chr(133)+\" \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\"\n        teststring\n        ```\n\n\n\n\n            '\ud83c\udffb\u2460 l`\\x9cuv\\x7fre est\u00a8 \\x93belle\\x94\u00b8 \u00c3  \u00c2\u00bd \u00e2\u201a\u00ac e\u0301nie\u0300me \u00e2\u20ac\u00b0 \\x85 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01'\n\n\n\n        ```python\n        result = norm(teststring)\n        result\n        ```\n\n\n\n\n            (1) l'oeuvre est \u00abbelle\u00bb, \u00c3  1/2 \u20ac \u00e9ni\u00e8me \u2030 \u2026 (EfficAce) !\n\n\n\n        Describe the changes applied by the normalization pipeline :\n\n        ```python\n        print(result.describeChanges())\n        ```\n\n            Fix encoding errors : windows1252 read as iso8859-1\n             < \ud83c\udffb\u2460 l` [\u009c] uv\u007fre est\u00a8  [\u0093] belle [\u0094] \u00b8 \u00c3  \u00c2\u00bd \u00e2\u201a\u00ac e\u0301nie\u0300me \u00e2\u20ac\u00b0  [\n]  \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n             < \ud83c\udffb\u2460 l` [\u0153] uv\u007fre est\u00a8  [\u201c] belle [\u201d] \u00b8 \u00c3  \u00c2\u00bd \u00e2\u201a\u00ac e\u0301nie\u0300me \u00e2\u20ac\u00b0  [\u2026]  \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n            Fix encoding errors : utf8 read as windows1252\n             < \ud83c\udffb\u2460 l`\u0153uv\u007fre est\u00a8 \u201cbelle\u201d\u00b8 \u00c3   [\u00c2\u00bd]   [\u00e2\u201a\u00ac]  e\u0301nie\u0300me  [\u00e2\u20ac\u00b0]  \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n             < \ud83c\udffb\u2460 l`\u0153uv\u007fre est\u00a8 \u201cbelle\u201d\u00b8 \u00c3   [\u00bd_]   [\u20ac__]  e\u0301nie\u0300me  [\u2030__]  \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n            Merge Unicode combining chars\n             < \ud83c\udffb\u2460 l`\u0153uv\u007fre est\u00a8 \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac  [e\u0301] ni [e\u0300] me \u2030 \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n             < \ud83c\udffb\u2460 l`\u0153uv\u007fre est\u00a8 \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac  [\u00e9_] ni [\u00e8_] me \u2030 \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n            Ignore control chars\n             <  [\ud83c\udffb] \u2460 l`\u0153uv [\u007f] re est [\u00a8]  \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n             <  [_] \u2460 l`\u0153uv [_] re est [_]  \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n            Replace latin letter symbols\n             < \u2460 l`\u0153uvre est \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207d [\ud83c\uddea] \ufb03c [\ud83c\udde6] ce\u207e \uff01\n             < \u2460 l`\u0153uvre est \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207d [E] \ufb03c [A] ce\u207e \uff01\n            Replace latin letter ligatures\n             < \u2460 l` [\u0153 ] uvre est \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207dE [\ufb03  ] cAce\u207e \uff01\n             < \u2460 l` [oe] uvre est \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207dE [ffi] cAce\u207e \uff01\n            Replace latin number symbols\n             <  [\u2460  ]  l`oeuvre est \u201cbelle\u201d\u00b8 \u00c3   [\u00bd  ]  \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207dEfficAce\u207e \uff01\n             <  [(1)]  l`oeuvre est \u201cbelle\u201d\u00b8 \u00c3   [1/2]  \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207dEfficAce\u207e \uff01\n            Normalize equivalent chars\n             < (1) l [`] oeuvre est  [\u201c] belle [\u201d]  [\u00b8]  \u00c3  1/2 \u20ac \u00e9ni\u00e8me \u2030 \u2026  [\u207d] EfficAce [\u207e]   [\uff01] \n             < (1) l ['] oeuvre est  [\u00ab] belle [\u00bb]  [,]  \u00c3  1/2 \u20ac \u00e9ni\u00e8me \u2030 \u2026  [(] EfficAce [)]   [!] \n\n\n\n        Compute spans for equivalent substrings before and after normalization :\n\n        ```python\n        result.output[0:12]\n        ```\n\n\n\n\n            \"(1) l'oeuvre\"\n\n\n\n        ```python\n        result.input[result.mapOutputIndexToInput(0):result.mapOutputIndexToInput(12)]\n        ```\n\n\n\n\n            '\ud83c\udffb\u2460 l`\\x9cuv\\x7fre'\n\n\n\n        ```python\n        result.output[3:10]\n        ```\n\n\n\n\n            \" l'oeuv\"\n\n\n\n        ```python\n        result.input[result.mapOutputIndexToInput(3):result.mapOutputIndexToInput(10)]\n        ```\n\n\n\n\n            ' l`\\x9cuv\\x7f'\n\n\n\n        Performance test : **2500 sentences per second** => fast enough but will be optimized in a later version.\n\n        ```python\n        %timeit -n100 norm(teststring)\n        ```\n\n            397 \u00b5s \u00b1 89.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n        ### Appendix : Unicode utility functions\n\n        Unicode characters properties :\n\n        ```python\n        charname(\"\ud83d\ude42\")\n        ```\n\n\n\n\n            'Slightly Smiling Face'\n\n\n\n        ```python\n        charcategory(\"\ud83d\ude42\")\n        ```\n\n\n\n\n            'Symbol'\n\n\n\n        ```python\n        charsubcategory(\"\ud83d\ude42\")\n        ```\n\n\n\n\n            'Other'\n\n\n\n        ```python\n        charblock(\"\ud83d\ude42\")\n        ```\n\n\n\n\n            'Emoticons'\n\n\n\n        ```python\n        blockfamily('Emoticons')\n        ```\n\n\n\n\n            'Symbols'\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/laurentprudhon/frenchtext", "keywords": "NLP french text", "license": "Apache Software License 2.0", "maintainer": "", "maintainer_email": "", "name": "frenchtext", "package_url": "https://pypi.org/project/frenchtext/", "platform": "", "project_url": "https://pypi.org/project/frenchtext/", "project_urls": {"Homepage": "https://github.com/laurentprudhon/frenchtext"}, "release_url": "https://pypi.org/project/frenchtext/0.0.5/", "requires_dist": ["numpy", "pandas", "pyarrow", "matplotlib", "requests", "fastprogress"], "requires_python": ">=3.6", "summary": "NLP library to process french text", "version": "0.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>frenchtext</h1>\n<pre><code>    &gt; NLP library to process french text.\n\n\n    In this early pre-version, the library provides :\n    - datasets to train business-oriented french text models\n    - a characters normalization pipeline tailored for french text\n\n    ## Install\n\n    `pip install frenchtext`\n\n    ## Dependencies\n\n    - [pandas](https://pandas.pydata.org/)\n    - [pyarrow](https://arrow.apache.org/docs/python/)\n    - [requests](https://requests.readthedocs.io/en/latest/)\n    - [fastprogress](https://github.com/fastai/fastprogress)\n\n    ## Licence\n\n    APACHE licence 2.0 : https://www.apache.org/licenses/LICENSE-2.0\n\n    ## How to use\n\n    The detailed documentation for each module is available through the menu on the left side of this page.\n\n    You will find below an overview of the library.\n\n    ## French datasets\n\n    ### Data sources\n\n    The text content of the main french websites in the domain of finance and business (+ wikipedia) was extracted in september 2019 using [nlptextdoc](https://github.com/laurentprudhon/nlptextdoc).\n\n    This extraction was done as \"politely\" as possible:\n    - extract only freely and publicly available content\n    - respect the robots.txt directives of each website (pages forbidden for indexing, maximum extraction rate)\n    - detect when websites use tools to prevent indexing (like Datadome) and abort the crawl\n\n    **IMPORTANT: The original authors of the websites own the copyright on all text blocks in this dataset.**\n\n    To be able to link each text block to its original author, we track the origin URL of each text block throughout the whole process.\n\n    **YOU CAN'T REUSE THE TEXT BLOCKS FOR ANY PURPOSE EXCEPT TRAINING A NATURAL LANGUAGE PROCESSING MODEL.**\n\n    See the new European copyright rules : [European Parliament approves new copyright rules for the internet](https://www.europarl.europa.eu/news/en/headlines/priorities/copyright/20190321IPR32110/european-parliament-approves-new-copyright-rules-for-the-internet)\n\n    \"*The directive aims to make it easier for copyrighted material to be used freely through text and data mining, thereby removing a significant competitive disadvantage that European researchers currently face.*\"\n\n    =&gt; 131 websites and 2 564 755 HTML pages\n\n    ### Data preparation\n\n    The text blocks were then:\n    - deduplicated to keep only distinct text blocks for each website (forgetting part of the original document structure), \n    - tagged (but not filtered) by language (using https://fasttext.cc/docs/en/language-identification.html),\n    - grouped in categories according to the main theme of the original website,\n    - split in [Pandas](https://pandas.pydata.org/) dataframes of size &lt; 2 GB.\n\n    =&gt; 10 categories: 'Assurance', 'Banque', 'Bourse', 'Comparateur', 'Cr\u00e9dit', 'Forum', 'Institution', 'Presse', 'SiteInfo', 'Wikipedia'\n\n    In each dataframe, the text blocks were additionnaly **SHUFFLED IN A RANDOM ORDER** to make it very difficult to reconstruct the original articles (safety measure to help protect the copyrights of the authors).\n\n    The results of this second step can be downloaded in the *config.datasets* directory, as dataframes serialized in the [feather format](https://arrow.apache.org/docs/python/ipc.html?highlight=feather#feather-format), in files named according to the 'DatasetFile' column of the datasets table.\n\n    =&gt; 19 dataset files: 'assurance', 'banque', 'bourse', 'comparateur', 'cr\u00e9dit', 'forum', 'institution', 'presse-1', 'presse-2', 'presse-3', 'presse-4', 'presse-5', 'presse-6', 'siteinfo', 'wikipedia-1', 'wikipedia-2', 'wikipedia-3', 'wikipedia-4', 'wikipedia-5'\n\n    ### Dataset size\n\n    The number of words in each text block was computed using the default french tokenizer from [spaCy](https://spacy.io/) v2.1.\n\n    This business-oriented dataset contains **2 billion french words**.\n\n    Here is a summary of the number of words contributed by each category **in millions**:\n\n    - Assurance : 12\n    - Banque : 20\n    - Bourse : 26\n    - Comparateur :\t20\n    - Cr\u00e9dit : 1\n    - Forum : 152\n    - Institution : 4\n    - Presse : 963\n    - SiteInfo : 78\n    - Wikipedia : 727\n\n    ### Dataset files\n\n    ```python\n    from frenchtext.core import *\n    from frenchtext.datasets import *\n    ```\n\n    List available dataset files :\n\n    ```python\n    datasetfiles = list_dataset_files()\n    datasetfiles\n    ```\n\n\n\n\n        ['assurance',\n         'banque',\n         'bourse',\n         'comparateur',\n         'cr\u00e9dit',\n         'forum',\n         'institution',\n         'presse-1',\n         'presse-2',\n         'presse-3',\n         'presse-4',\n         'presse-5',\n         'presse-6',\n         'siteinfo',\n         'wikipedia-1',\n         'wikipedia-2',\n         'wikipedia-3',\n         'wikipedia-4',\n         'wikipedia-5']\n\n\n\n    Source websites and number of words in each dataset file :\n\n    ```python\n    datasetsdf = list_datasets()\n    datasetsdf[[\"DatasetFile\",\"Url\",\"Pages\",\"Words\"]].iloc[80:100]\n    ```\n\n\n\n\n    &lt;div&gt;\n    &lt;style scoped&gt;\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n\n        .dataframe thead th {\n            text-align: right;\n        }\n    &lt;/style&gt;\n    &lt;table border=\"1\" class=\"dataframe\"&gt;\n      &lt;thead&gt;\n        &lt;tr style=\"text-align: right;\"&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;DatasetFile&lt;/th&gt;\n          &lt;th&gt;Url&lt;/th&gt;\n          &lt;th&gt;Pages&lt;/th&gt;\n          &lt;th&gt;Words&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;th&gt;80&lt;/th&gt;\n          &lt;td&gt;comparateur&lt;/td&gt;\n          &lt;td&gt;https://www.panorabanques.com/&lt;/td&gt;\n          &lt;td&gt;4341&lt;/td&gt;\n          &lt;td&gt;2584038&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;81&lt;/th&gt;\n          &lt;td&gt;cr\u00e9dit&lt;/td&gt;\n          &lt;td&gt;https://www.cetelem.fr/&lt;/td&gt;\n          &lt;td&gt;274&lt;/td&gt;\n          &lt;td&gt;157191&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;82&lt;/th&gt;\n          &lt;td&gt;cr\u00e9dit&lt;/td&gt;\n          &lt;td&gt;https://www.cofidis.fr/&lt;/td&gt;\n          &lt;td&gt;347&lt;/td&gt;\n          &lt;td&gt;243904&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;83&lt;/th&gt;\n          &lt;td&gt;cr\u00e9dit&lt;/td&gt;\n          &lt;td&gt;https://www.cofinoga.fr/&lt;/td&gt;\n          &lt;td&gt;413&lt;/td&gt;\n          &lt;td&gt;86796&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;84&lt;/th&gt;\n          &lt;td&gt;cr\u00e9dit&lt;/td&gt;\n          &lt;td&gt;https://www.sofinco.fr/&lt;/td&gt;\n          &lt;td&gt;916&lt;/td&gt;\n          &lt;td&gt;597221&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;85&lt;/th&gt;\n          &lt;td&gt;cr\u00e9dit&lt;/td&gt;\n          &lt;td&gt;https://www.younited-credit.com/&lt;/td&gt;\n          &lt;td&gt;1341&lt;/td&gt;\n          &lt;td&gt;665115&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;86&lt;/th&gt;\n          &lt;td&gt;forum&lt;/td&gt;\n          &lt;td&gt;https://droit-finances.commentcamarche.com/&lt;/td&gt;\n          &lt;td&gt;96450&lt;/td&gt;\n          &lt;td&gt;56120562&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;87&lt;/th&gt;\n          &lt;td&gt;forum&lt;/td&gt;\n          &lt;td&gt;http://forum.doctissimo.fr/famille/argent-budg...&lt;/td&gt;\n          &lt;td&gt;26981&lt;/td&gt;\n          &lt;td&gt;61020453&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;88&lt;/th&gt;\n          &lt;td&gt;forum&lt;/td&gt;\n          &lt;td&gt;http://forum.doctissimo.fr/viepratique/finance...&lt;/td&gt;\n          &lt;td&gt;5745&lt;/td&gt;\n          &lt;td&gt;4962230&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;89&lt;/th&gt;\n          &lt;td&gt;forum&lt;/td&gt;\n          &lt;td&gt;http://forum.doctissimo.fr/viepratique/Impots/...&lt;/td&gt;\n          &lt;td&gt;2338&lt;/td&gt;\n          &lt;td&gt;1422143&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;90&lt;/th&gt;\n          &lt;td&gt;forum&lt;/td&gt;\n          &lt;td&gt;https://forum.lesarnaques.com/assurance-automo...&lt;/td&gt;\n          &lt;td&gt;3530&lt;/td&gt;\n          &lt;td&gt;3085101&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;91&lt;/th&gt;\n          &lt;td&gt;forum&lt;/td&gt;\n          &lt;td&gt;https://forum.lesarnaques.com/banque/&lt;/td&gt;\n          &lt;td&gt;6206&lt;/td&gt;\n          &lt;td&gt;5766116&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;92&lt;/th&gt;\n          &lt;td&gt;forum&lt;/td&gt;\n          &lt;td&gt;https://www.60millions-mag.com/forum/&lt;/td&gt;\n          &lt;td&gt;3692&lt;/td&gt;\n          &lt;td&gt;2222882&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;93&lt;/th&gt;\n          &lt;td&gt;forum&lt;/td&gt;\n          &lt;td&gt;https://www.boursorama.com/patrimoine/forum/&lt;/td&gt;\n          &lt;td&gt;13020&lt;/td&gt;\n          &lt;td&gt;10497065&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;94&lt;/th&gt;\n          &lt;td&gt;forum&lt;/td&gt;\n          &lt;td&gt;https://www.cbanque.com/forums/&lt;/td&gt;\n          &lt;td&gt;12098&lt;/td&gt;\n          &lt;td&gt;7702002&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;95&lt;/th&gt;\n          &lt;td&gt;institution&lt;/td&gt;\n          &lt;td&gt;https://acpr.banque-france.fr/&lt;/td&gt;\n          &lt;td&gt;470&lt;/td&gt;\n          &lt;td&gt;51397&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;96&lt;/th&gt;\n          &lt;td&gt;institution&lt;/td&gt;\n          &lt;td&gt;https://www.banque-france.fr/&lt;/td&gt;\n          &lt;td&gt;728&lt;/td&gt;\n          &lt;td&gt;75101&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;97&lt;/th&gt;\n          &lt;td&gt;institution&lt;/td&gt;\n          &lt;td&gt;https://www.ffa-assurance.fr/&lt;/td&gt;\n          &lt;td&gt;301&lt;/td&gt;\n          &lt;td&gt;146499&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;98&lt;/th&gt;\n          &lt;td&gt;institution&lt;/td&gt;\n          &lt;td&gt;https://www.economie.gouv.fr/&lt;/td&gt;\n          &lt;td&gt;2720&lt;/td&gt;\n          &lt;td&gt;159663&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;99&lt;/th&gt;\n          &lt;td&gt;institution&lt;/td&gt;\n          &lt;td&gt;https://www.impots.gouv.fr/portail/&lt;/td&gt;\n          &lt;td&gt;1631&lt;/td&gt;\n          &lt;td&gt;653735&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;/div&gt;\n\n\n\n    ### Download dataset files\n\n    ```python\n    download_dataset_file(\"assurance\")\n    ```\n\n        Downloading dataset file : assurance (17 MB)\n\n\n    ```python\n    download_all_datasets()\n    ```\n\n        Downloading dataset file : assurance (17 MB)\n        Downloading dataset file : banque (28 MB)\n        Downloading dataset file : bourse (38 MB)\n        Downloading dataset file : comparateur (28 MB)\n        Downloading dataset file : cr\u00e9dit (2 MB)\n        Downloading dataset file : forum (220 MB)\n        Downloading dataset file : institution (5 MB)\n        Downloading dataset file : presse-1 (218 MB)\n        Downloading dataset file : presse-2 (196 MB)\n        Downloading dataset file : presse-3 (190 MB)\n        Downloading dataset file : presse-4 (234 MB)\n        Downloading dataset file : presse-5 (269 MB)\n        Downloading dataset file : presse-6 (334 MB)\n        Downloading dataset file : siteinfo (116 MB)\n        Downloading dataset file : wikipedia-1 (131 MB)\n        Downloading dataset file : wikipedia-2 (182 MB)\n        Downloading dataset file : wikipedia-3 (263 MB)\n        Downloading dataset file : wikipedia-4 (269 MB)\n        Downloading dataset file : wikipedia-5 (267 MB)\n\n\n    You can change the local directory where the dataset files are downloaded :\n\n    ```python\n    config.datasets\n    ```\n\n\n\n\n        PosixPath('/home/laurent/.frenchtext/datasets')\n\n\n\n    ```python\n    config[\"datasets_path\"] = \"/tmp/datasets\"\n    config.datasets.mkdir(parents=True, exist_ok=True)\n    ```\n\n    ```python\n    config.datasets\n    ```\n\n\n\n\n        PosixPath('/tmp/datasets')\n\n\n\n    ### Read dataset files\n\n    ```python\n    datasetdf = read_dataset_file(\"assurance\")\n    datasetdf\n    ```\n\n        Loaded dataframe for dataset assurance : 563613 text blocks\n\n\n\n\n\n    &lt;div&gt;\n    &lt;style scoped&gt;\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n\n        .dataframe thead th {\n            text-align: right;\n        }\n    &lt;/style&gt;\n    &lt;table border=\"1\" class=\"dataframe\"&gt;\n      &lt;thead&gt;\n        &lt;tr style=\"text-align: right;\"&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;Website&lt;/th&gt;\n          &lt;th&gt;DocId&lt;/th&gt;\n          &lt;th&gt;DocEltType&lt;/th&gt;\n          &lt;th&gt;DocEltCmd&lt;/th&gt;\n          &lt;th&gt;NestingLevel&lt;/th&gt;\n          &lt;th&gt;Text&lt;/th&gt;\n          &lt;th&gt;Lang&lt;/th&gt;\n          &lt;th&gt;Words&lt;/th&gt;\n          &lt;th&gt;Unique&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;th&gt;0&lt;/th&gt;\n          &lt;td&gt;11&lt;/td&gt;\n          &lt;td&gt;22332&lt;/td&gt;\n          &lt;td&gt;ListItem&lt;/td&gt;\n          &lt;td&gt;Text&lt;/td&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;5 tournages catastrophe pour un assureur&lt;/td&gt;\n          &lt;td&gt;fr&lt;/td&gt;\n          &lt;td&gt;6&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;1&lt;/th&gt;\n          &lt;td&gt;74&lt;/td&gt;\n          &lt;td&gt;710&lt;/td&gt;\n          &lt;td&gt;Section&lt;/td&gt;\n          &lt;td&gt;Start&lt;/td&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;Tout connaitre sur la nouvelle formation post-...&lt;/td&gt;\n          &lt;td&gt;fr&lt;/td&gt;\n          &lt;td&gt;7&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;2&lt;/th&gt;\n          &lt;td&gt;11&lt;/td&gt;\n          &lt;td&gt;12082&lt;/td&gt;\n          &lt;td&gt;TextBlock&lt;/td&gt;\n          &lt;td&gt;Text&lt;/td&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;Votre Agent Mandataire AXA - Civry Marie Claud...&lt;/td&gt;\n          &lt;td&gt;?&lt;/td&gt;\n          &lt;td&gt;18&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;3&lt;/th&gt;\n          &lt;td&gt;87&lt;/td&gt;\n          &lt;td&gt;461&lt;/td&gt;\n          &lt;td&gt;TextBlock&lt;/td&gt;\n          &lt;td&gt;Text&lt;/td&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;60 ans et 4 mois&lt;/td&gt;\n          &lt;td&gt;fr&lt;/td&gt;\n          &lt;td&gt;5&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;4&lt;/th&gt;\n          &lt;td&gt;7&lt;/td&gt;\n          &lt;td&gt;200&lt;/td&gt;\n          &lt;td&gt;TextBlock&lt;/td&gt;\n          &lt;td&gt;Text&lt;/td&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;Mon devis sur mesure&lt;/td&gt;\n          &lt;td&gt;fr&lt;/td&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;...&lt;/th&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;563608&lt;/th&gt;\n          &lt;td&gt;138&lt;/td&gt;\n          &lt;td&gt;255&lt;/td&gt;\n          &lt;td&gt;Section&lt;/td&gt;\n          &lt;td&gt;Start&lt;/td&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;Les autres pouvoirs de police&lt;/td&gt;\n          &lt;td&gt;fr&lt;/td&gt;\n          &lt;td&gt;5&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;563609&lt;/th&gt;\n          &lt;td&gt;11&lt;/td&gt;\n          &lt;td&gt;19483&lt;/td&gt;\n          &lt;td&gt;TextBlock&lt;/td&gt;\n          &lt;td&gt;Text&lt;/td&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;Yves Nicolau assurance Laon&lt;/td&gt;\n          &lt;td&gt;?&lt;/td&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;563610&lt;/th&gt;\n          &lt;td&gt;106&lt;/td&gt;\n          &lt;td&gt;1644&lt;/td&gt;\n          &lt;td&gt;ListItem&lt;/td&gt;\n          &lt;td&gt;Text&lt;/td&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;Ev\u00e8nements sportifs&lt;/td&gt;\n          &lt;td&gt;fr&lt;/td&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;563611&lt;/th&gt;\n          &lt;td&gt;58&lt;/td&gt;\n          &lt;td&gt;4155&lt;/td&gt;\n          &lt;td&gt;Section&lt;/td&gt;\n          &lt;td&gt;Start&lt;/td&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;Agence Groupama Chalon&lt;/td&gt;\n          &lt;td&gt;?&lt;/td&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;563612&lt;/th&gt;\n          &lt;td&gt;10&lt;/td&gt;\n          &lt;td&gt;150&lt;/td&gt;\n          &lt;td&gt;TextBlock&lt;/td&gt;\n          &lt;td&gt;Text&lt;/td&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;Nos agences d'assurance Aviva \u00e0 OYONNAX sont h...&lt;/td&gt;\n          &lt;td&gt;fr&lt;/td&gt;\n          &lt;td&gt;26&lt;/td&gt;\n          &lt;td&gt;True&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;p&gt;563613 rows \u00d7 9 columns&lt;/p&gt;\n    &lt;/div&gt;\n\n\n\n    ### Access text blocks in dataset files\n\n    Filter and iterate over the rows of a dataset file :\n\n    ```python\n    rowsiterator = get_rows_from_datasetdf(datasetdf, minwords=None, maxwords=5, lang=\"?\")\n    show_first_rows(rowsiterator,10)\n    ```\n\n        12 - COORDONNEES\n        41 - 01 30 41 67 33\n        49 - Dmitriy G.\n        57 - Les atouts du Multisupport CONFIANCE\n        74 - 01XXL meribel hiver\n        76 - Garantie en cas de vol\n        87 - Par AXA, le 01/08/2016\n        96 - mgr@enderby.eu\n        127 - 18 place De Strasbourg\n        131 - Saint Gaudens\n\n\n    Filter and iterate over the text blocks of a full dataset (across multiple files) :\n\n    ```python\n    textiterator = get_textblocks_from_dataset(\"Assurance\", minwords=None, maxwords=10, lang=\"fr\")\n    show_first_textblocks(textiterator,skip=2000,count=10)\n    ```\n\n        Loaded dataframe for dataset assurance : 563613 text blocks\n        2001 - R\u00e9\u00e9quipement \u00e0 neuf \u00e0 vie\n        2002 - D\u00e9finition Conducteur secondaire- Lexique\n        2003 - Comment \u00e9viter les fraudes\n        2004 - Comment demander un remboursement sant\u00e9 - GENERALI\n        2005 - Simulateur pour conna\u00eetre les obligations de votre accord de branche\n        2006 - Compl\u00e9mentaire Epargne retraite des ind\u00e9pendants et TNS - Malakoff M\u00e9d\u00e9ric\n        2007 - Experts-Comptables, d\u00e9couvrez la mission \u00e9pargne salariale\n        2008 - Vous n\u2019\u00eates pas encore client :\n        2009 - Actualit\u00e9s (Page 6) | ameli.fr | Pharmacien\n        2010 - D\u00e9pression : quelle prise en charge ? - Matmut\n\n\n    Access a specific row :\n\n    ```python\n    get_text_from_rowindex(datasetdf,100)\n    ```\n\n\n\n\n        'Les inondations de plaine : d\u00e9bordement de cours d\u2019eau avec une dur\u00e9e d\u2019immersion longue (pr\u00e9visibles plusieurs jours ou heures \u00e0 l\u2019avance).'\n\n\n\n    Find text blocks with a specific char or substring :\n\n    ```python\n    find_textblocks_with_chars(datasetdf,\"r\u00e9troviseur\",count=20,ctxsize=15)\n    ```\n\n\n\n\n        350594     ore dans notre r\u00e9troviseur gauche lorsque \n        149029     de glace ? Les r\u00e9troviseurs ainsi que les \n        51349      ace. Quant aux r\u00e9troviseurs, ils le sont d\n        310354     vant, arri\u00e8re, r\u00e9troviseurs et vitres lat\u00e9\n        489866    \\naussi dans le r\u00e9troviseur pour ne pas se \n        364550     \u00f4t\u00e9 ou sous le r\u00e9troviseur int\u00e9rieur de vo\n        560539     tionnement des r\u00e9troviseurs.              \n        560700     \u00e9 (pare-brise, r\u00e9troviseurs\u2026),            \n        223621     riorations des r\u00e9troviseurs et des phares.\n        543903     es miroirs des r\u00e9troviseurs lorsqu\u2019ils peu\n        502075      logo dans son r\u00e9troviseur et par un signa\n        53237      vous cassez le r\u00e9troviseur d\u2019une voiture. \n        310456      \u00e9raflures, un r\u00e9troviseur ab\u00eem\u00e9, ou un au\n        375158     ant, moteur de r\u00e9troviseurs\u2026              \n        539914     nt et arri\u00e8re, r\u00e9troviseurs int\u00e9rieurs et \n        171367     t utilisez vos r\u00e9troviseurs               \n        485058      ainsi que les r\u00e9troviseurs ne sont pas ga\n        277390     ant, moteur de r\u00e9troviseurs...            \n        20222      sont offerts : r\u00e9troviseurs \u00e9lectriques, c\n        317634     res, y compris r\u00e9troviseurs et feux       \n        Name: Text, dtype: object\n\n\n\n    ```python\n    find_textblocks_with_chars(datasetdf,64257,count=10,wrap=True)\n    ```\n\n\n\n\n        175413    x besoins de diversi[\ufb01]cation des placements\n        337398    e 30 villes ont b\u00e9n\u00e9[\ufb01]ci\u00e9 de ces animations\n        265114    nt r\u00e8glementaire et [\ufb01]nancier, nous accompa\n        74267          La Fondation a [\ufb01]nanc\u00e9 depuis 2009, l\u2019\n        424584    tion de l\u2019\u00e9quilibre [\ufb01]nancier des r\u00e9gimes d\n        219195    d, J\u00e9r\u00f4me Powell con[\ufb01]rmera que, dans l\u2019att\n        489511    s besoins de diversi[\ufb01]cation de la client\u00e8l\n        517563    si en pr\u00e9sence d\u2019un [\ufb01]nancement par cr\u00e9dit,\n        479694    nt r\u00e8glementaire et [\ufb01]nancier, La Mondiale \n        252202    n de disponibilit\u00e9s [\ufb01]nanci\u00e8res mais aussi,\n        Name: Text, dtype: object\n\n\n\n    ### Track the source URL for each text block \n\n    Optionally download and read urls file to track the origin of each text block :\n\n    ```python\n    urlsdf = read_urls_file()\n    urlsdf.head()\n    ```\n\n        Loaded datasets urls : 2668787 urls\n\n\n\n\n\n    &lt;div&gt;\n    &lt;style scoped&gt;\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n\n        .dataframe thead th {\n            text-align: right;\n        }\n    &lt;/style&gt;\n    &lt;table border=\"1\" class=\"dataframe\"&gt;\n      &lt;thead&gt;\n        &lt;tr style=\"text-align: right;\"&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;Website&lt;/th&gt;\n          &lt;th&gt;DocId&lt;/th&gt;\n          &lt;th&gt;DocUrl&lt;/th&gt;\n          &lt;th&gt;Words&lt;/th&gt;\n          &lt;th&gt;fr&lt;/th&gt;\n          &lt;th&gt;en&lt;/th&gt;\n          &lt;th&gt;de&lt;/th&gt;\n          &lt;th&gt;es&lt;/th&gt;\n          &lt;th&gt;?&lt;/th&gt;\n          &lt;th&gt;%fr&lt;/th&gt;\n          &lt;th&gt;%en&lt;/th&gt;\n          &lt;th&gt;%de&lt;/th&gt;\n          &lt;th&gt;%es&lt;/th&gt;\n          &lt;th&gt;%?&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;th&gt;0&lt;/th&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;https://www.afer.fr/&lt;/td&gt;\n          &lt;td&gt;573.0&lt;/td&gt;\n          &lt;td&gt;524.0&lt;/td&gt;\n          &lt;td&gt;3.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;46.0&lt;/td&gt;\n          &lt;td&gt;0.914485&lt;/td&gt;\n          &lt;td&gt;0.005236&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.080279&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;1&lt;/th&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;https://www.afer.fr/afer/adhesion/&lt;/td&gt;\n          &lt;td&gt;74.0&lt;/td&gt;\n          &lt;td&gt;74.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;1.000000&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;2&lt;/th&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;https://www.afer.fr/afer/adhesion/adherent-ass...&lt;/td&gt;\n          &lt;td&gt;475.0&lt;/td&gt;\n          &lt;td&gt;457.0&lt;/td&gt;\n          &lt;td&gt;5.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;13.0&lt;/td&gt;\n          &lt;td&gt;0.962105&lt;/td&gt;\n          &lt;td&gt;0.010526&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.027368&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;3&lt;/th&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;https://www.afer.fr/afer/adhesion/adherer-assu...&lt;/td&gt;\n          &lt;td&gt;519.0&lt;/td&gt;\n          &lt;td&gt;519.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;1.000000&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;4&lt;/th&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;5&lt;/td&gt;\n          &lt;td&gt;https://www.afer.fr/afer/adhesion/parrainage-a...&lt;/td&gt;\n          &lt;td&gt;355.0&lt;/td&gt;\n          &lt;td&gt;345.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;10.0&lt;/td&gt;\n          &lt;td&gt;0.971831&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.0&lt;/td&gt;\n          &lt;td&gt;0.028169&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;/div&gt;\n\n\n\n    ```python\n    get_text_from_rowindex(datasetdf,100)\n    ```\n\n\n\n\n        'Les inondations de plaine : d\u00e9bordement de cours d\u2019eau avec une dur\u00e9e d\u2019immersion longue (pr\u00e9visibles plusieurs jours ou heures \u00e0 l\u2019avance).'\n\n\n\n    ```python\n    get_url_from_rowindex(datasetdf, 100)\n    ```\n\n\n\n\n        'https://www.maif.fr/conseils-prevention/risques-majeurs/inondation.html'\n\n\n\n    ## Characters normalization pipeline\n\n    ### Motivation\n\n    French datasets often contain several thousands distinct Unicode characters.\n\n    Characters stats in Wikipedia dataset :\n    - 35.6 billion chars\n    - 13 502 distinct Unicode chars\n\n    Characters stats in Business dataset :\n    - 27.5 billion chars\n    - 3 763 distinct Unicode chars\n\n    We need to reduce the number of distinct characters fed to our natural language processing applications, for three reasons :\n    - chars considered by the user as visually equivalent will often produce a different application behavior : this is a huge problem for the user experience\n    - with so many chars, the designer of the NLP application will not be able to reason about all possible combinations : this could harm the explainability of the system\n    - this huge number of distinct characters brings a significant amount complexity the NLP models will have to deal with\n\n    Characters stats in Wikipedia dataset :\n    - Only 1316 chars more frequent than 1 in 100 million\n    - 99.9987 % of Wikipedia chars would be preserved if we only kept the frequent chars\n\n    Characters stats in Business dataset :\n    - Only 531 chars more frequent than 1 in 100 million\n    - 99.9996 % of Business chars would be preserved if we only kept the frequent chars\n\n    We can be smarter than that and replace rare chars with equivalent (or mostly equivalent) more frequent chars to preserve a maximum of information.\n\n    ### Target characters set\n\n    After a detailed study of all the frequent chars, the goal is to design a noramization pipeline which can retain as much information as possible while greatly reducing the number of dinstinct chars.\n\n    We saw before that it is possible to preserve 99.9996% of the original chars while keeping only 500 distinct chars. By being clever and replacing equivalent chars, we can divide this number by 2 and still retain the same amount of information.\n\n    It may then be useful to limit the number of distinct characters after normalization to **255 distinct characters** : \n    - if needed, french text chars can then be encoded with a single byte\n    - the list of supported chars can be memorized by NLP application developers and users\n\n    ```python\n    from frenchtext.core import *\n    from frenchtext.chars import *\n    ```\n\n    255 supported characters after normalization : \n\n    ```python\n    import pandas as pd\n    dfcharsnorm = pd.read_csv(chardatadir / \"charset-fr.csv\", sep=\";\")\n    dfcharsnorm\n    ```\n\n\n\n\n    &lt;div&gt;\n    &lt;style scoped&gt;\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n\n        .dataframe thead th {\n            text-align: right;\n        }\n    &lt;/style&gt;\n    &lt;table border=\"1\" class=\"dataframe\"&gt;\n      &lt;thead&gt;\n        &lt;tr style=\"text-align: right;\"&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;FrCode&lt;/th&gt;\n          &lt;th&gt;Category&lt;/th&gt;\n          &lt;th&gt;SubCategory&lt;/th&gt;\n          &lt;th&gt;Code&lt;/th&gt;\n          &lt;th&gt;Char&lt;/th&gt;\n          &lt;th&gt;CharName&lt;/th&gt;\n          &lt;th&gt;CountBusiness&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;th&gt;0&lt;/th&gt;\n          &lt;td&gt;0&lt;/td&gt;\n          &lt;td&gt;separator&lt;/td&gt;\n          &lt;td&gt;control&lt;/td&gt;\n          &lt;td&gt;0&lt;/td&gt;\n          &lt;td&gt;NaN&lt;/td&gt;\n          &lt;td&gt;Reserved - End of string&lt;/td&gt;\n          &lt;td&gt;0&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;1&lt;/th&gt;\n          &lt;td&gt;1&lt;/td&gt;\n          &lt;td&gt;separator&lt;/td&gt;\n          &lt;td&gt;space&lt;/td&gt;\n          &lt;td&gt;32&lt;/td&gt;\n          &lt;td&gt;&lt;/td&gt;\n          &lt;td&gt;Space&lt;/td&gt;\n          &lt;td&gt;88494564&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;2&lt;/th&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;separator&lt;/td&gt;\n          &lt;td&gt;space&lt;/td&gt;\n          &lt;td&gt;10&lt;/td&gt;\n          &lt;td&gt;\\n&lt;/td&gt;\n          &lt;td&gt;Char 10&lt;/td&gt;\n          &lt;td&gt;9588147&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;3&lt;/th&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;separator&lt;/td&gt;\n          &lt;td&gt;space&lt;/td&gt;\n          &lt;td&gt;9&lt;/td&gt;\n          &lt;td&gt;\\t&lt;/td&gt;\n          &lt;td&gt;Char 9&lt;/td&gt;\n          &lt;td&gt;1522053&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;4&lt;/th&gt;\n          &lt;td&gt;4&lt;/td&gt;\n          &lt;td&gt;separator&lt;/td&gt;\n          &lt;td&gt;punctuation&lt;/td&gt;\n          &lt;td&gt;44&lt;/td&gt;\n          &lt;td&gt;,&lt;/td&gt;\n          &lt;td&gt;Comma&lt;/td&gt;\n          &lt;td&gt;286106887&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;...&lt;/th&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n          &lt;td&gt;...&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;251&lt;/th&gt;\n          &lt;td&gt;251&lt;/td&gt;\n          &lt;td&gt;emoticon&lt;/td&gt;\n          &lt;td&gt;object&lt;/td&gt;\n          &lt;td&gt;9792&lt;/td&gt;\n          &lt;td&gt;\u2640&lt;/td&gt;\n          &lt;td&gt;Female Sign&lt;/td&gt;\n          &lt;td&gt;515&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;252&lt;/th&gt;\n          &lt;td&gt;252&lt;/td&gt;\n          &lt;td&gt;emoticon&lt;/td&gt;\n          &lt;td&gt;object&lt;/td&gt;\n          &lt;td&gt;127881&lt;/td&gt;\n          &lt;td&gt;\ud83c\udf89&lt;/td&gt;\n          &lt;td&gt;Party Popper&lt;/td&gt;\n          &lt;td&gt;356&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;253&lt;/th&gt;\n          &lt;td&gt;253&lt;/td&gt;\n          &lt;td&gt;emoticon&lt;/td&gt;\n          &lt;td&gt;object&lt;/td&gt;\n          &lt;td&gt;9997&lt;/td&gt;\n          &lt;td&gt;\u270d&lt;/td&gt;\n          &lt;td&gt;Writing Hand&lt;/td&gt;\n          &lt;td&gt;157&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;254&lt;/th&gt;\n          &lt;td&gt;254&lt;/td&gt;\n          &lt;td&gt;emoticon&lt;/td&gt;\n          &lt;td&gt;object&lt;/td&gt;\n          &lt;td&gt;9993&lt;/td&gt;\n          &lt;td&gt;\u2709&lt;/td&gt;\n          &lt;td&gt;Envelope&lt;/td&gt;\n          &lt;td&gt;55&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;255&lt;/th&gt;\n          &lt;td&gt;255&lt;/td&gt;\n          &lt;td&gt;emoticon&lt;/td&gt;\n          &lt;td&gt;object&lt;/td&gt;\n          &lt;td&gt;10013&lt;/td&gt;\n          &lt;td&gt;\u271d&lt;/td&gt;\n          &lt;td&gt;Latin Cross&lt;/td&gt;\n          &lt;td&gt;22&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;p&gt;256 rows \u00d7 7 columns&lt;/p&gt;\n    &lt;/div&gt;\n\n\n\n    The table below shows the number of chars in each category (after normalization) **per 100 million characters** :\n\n    ```python\n    dfblocks = dfcharsnorm.groupby(by=[\"Category\",\"SubCategory\"]).agg({\"Char\":[\"count\",\"sum\"],\"CountBusiness\":\"sum\"})\n    dfblocks[\"CountBusiness\"] = (dfblocks[\"CountBusiness\"] / 27577304956 * 100000000).astype(int)\n    dfblocks\n    ```\n\n\n\n\n    &lt;div&gt;\n    &lt;style scoped&gt;\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n\n        .dataframe thead tr th {\n            text-align: left;\n        }\n\n        .dataframe thead tr:last-of-type th {\n            text-align: right;\n        }\n    &lt;/style&gt;\n    &lt;table border=\"1\" class=\"dataframe\"&gt;\n      &lt;thead&gt;\n        &lt;tr&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th colspan=\"2\" halign=\"left\"&gt;Char&lt;/th&gt;\n          &lt;th&gt;CountBusiness&lt;/th&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;count&lt;/th&gt;\n          &lt;th&gt;sum&lt;/th&gt;\n          &lt;th&gt;sum&lt;/th&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;Category&lt;/th&gt;\n          &lt;th&gt;SubCategory&lt;/th&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;th rowspan=\"3\" valign=\"top\"&gt;emoticon&lt;/th&gt;\n          &lt;th&gt;hand&lt;/th&gt;\n          &lt;td&gt;12&lt;/td&gt;\n          &lt;td&gt;\ud83d\udcaa\ud83d\udc49\ud83d\udc4d\ud83d\udc4f\ud83d\ude4f\ud83d\ude4c\ud83d\udc47\ud83d\udc4a\ud83d\udc4e\ud83d\udc4c\u270c\u270a&lt;/td&gt;\n          &lt;td&gt;42&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;head&lt;/th&gt;\n          &lt;td&gt;28&lt;/td&gt;\n          &lt;td&gt;\ud83d\ude42\ud83d\ude09\ud83d\ude00\ud83d\ude02\ud83d\ude01\ud83d\ude0a\ud83d\ude41\ud83d\ude05\ud83d\ude0d\ud83d\ude03\ud83d\ude21\ud83e\udd23\ud83d\ude04\ud83e\udd14\ud83d\ude0e\ud83d\ude2d\ud83d\udc79\ud83d\ude31\ud83d\ude1c\ud83d\ude0b\ud83e\udd29\ud83d\ude44\ud83d\ude06\ud83d\ude1b\ud83e\udd2a\ud83d\ude22\ud83d\ude07\ud83e\udd26&lt;/td&gt;\n          &lt;td&gt;233&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;object&lt;/th&gt;\n          &lt;td&gt;16&lt;/td&gt;\n          &lt;td&gt;\u26a0\ud83d\udd34\ud83d\udd25\ud83c\udfc6\u26bd\ud83d\udca1\ud83d\udea8\ud83d\udca5\u26a1\u266b\u2642\u2640\ud83c\udf89\u270d\u2709\u271d&lt;/td&gt;\n          &lt;td&gt;60&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th rowspan=\"6\" valign=\"top\"&gt;letter&lt;/th&gt;\n          &lt;th&gt;digit&lt;/th&gt;\n          &lt;td&gt;10&lt;/td&gt;\n          &lt;td&gt;0123549876&lt;/td&gt;\n          &lt;td&gt;3271115&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;encoding&lt;/th&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;\u00c3\ufffd\ufffc&lt;/td&gt;\n          &lt;td&gt;249&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;greek&lt;/th&gt;\n          &lt;td&gt;2&lt;/td&gt;\n          &lt;td&gt;\u03bb\u03c0&lt;/td&gt;\n          &lt;td&gt;2&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;latin-fr&lt;/th&gt;\n          &lt;td&gt;84&lt;/td&gt;\n          &lt;td&gt;abcdefghijklmnopqrstuvwxyz\u00e0\u00e2\u00e4\u00e7\u00e8\u00e9\u00ea\u00eb\u00ee\u00ef\u00f4\u00f6\u00f9\u00fb\u00fc\u00ffABCD...&lt;/td&gt;\n          &lt;td&gt;91437146&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;latin-other&lt;/th&gt;\n          &lt;td&gt;25&lt;/td&gt;\n          &lt;td&gt;\u00e1\u00e3\u00e5\u0107\u010d\u0117\u011f\u0131\u00ed\u00ec\u0144\u00f1\u00f3\u00f2\u00f5\u00f8\u0161\u015f\u00df\u00fa\u00c1\u00c5\u0160\u00da\u017d&lt;/td&gt;\n          &lt;td&gt;712&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;other&lt;/th&gt;\n          &lt;td&gt;5&lt;/td&gt;\n          &lt;td&gt;_&amp;amp;@\\#&lt;/td&gt;\n          &lt;td&gt;40814&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th rowspan=\"3\" valign=\"top\"&gt;separator&lt;/th&gt;\n          &lt;th&gt;control&lt;/th&gt;\n          &lt;td&gt;0&lt;/td&gt;\n          &lt;td&gt;0&lt;/td&gt;\n          &lt;td&gt;0&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;punctuation&lt;/th&gt;\n          &lt;td&gt;23&lt;/td&gt;\n          &lt;td&gt;,'.-:/\")(?!\u00bb\u00ab|\u2026;[]}{\u2022\u00bf\u00a1&lt;/td&gt;\n          &lt;td&gt;4684722&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;space&lt;/th&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;\\n\\t&lt;/td&gt;\n          &lt;td&gt;361183&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th rowspan=\"5\" valign=\"top\"&gt;symbol&lt;/th&gt;\n          &lt;th&gt;currency&lt;/th&gt;\n          &lt;td&gt;6&lt;/td&gt;\n          &lt;td&gt;\u20ac$\u00a4\u00a3\u00a5\u00a2&lt;/td&gt;\n          &lt;td&gt;21099&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;math&lt;/th&gt;\n          &lt;td&gt;14&lt;/td&gt;\n          &lt;td&gt;=&amp;gt;+&amp;lt;^~\u00d7\u2264\u00f7\u2265\u00b1\u2260\u221e\u221a&lt;/td&gt;\n          &lt;td&gt;50056&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;shape&lt;/th&gt;\n          &lt;td&gt;15&lt;/td&gt;\n          &lt;td&gt;*\u2713\u21d2\u2665\u00a6\u2192\u2605\u00af\u2193\u274c\u2750\u2020\u2191\u2190\u2194&lt;/td&gt;\n          &lt;td&gt;7954&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;sign&lt;/th&gt;\n          &lt;td&gt;3&lt;/td&gt;\n          &lt;td&gt;\u00a9\u00ae\u2122&lt;/td&gt;\n          &lt;td&gt;1754&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;unit&lt;/th&gt;\n          &lt;td&gt;6&lt;/td&gt;\n          &lt;td&gt;%\u00b0\u00a7\u00b5\u00d8\u2030&lt;/td&gt;\n          &lt;td&gt;102213&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;/div&gt;\n\n\n\n    ### Normalization pipeline overview\n\n    The normalization pipeline applies the following **14 steps**, which are explained and illustrated in the sections below.\n\n    - Fix encoding errors\n      - fix windows1252 text read as iso8859-1\n      - fix utf8 text read as windows1252\n      - fix windows1252 text read as utf8\n      - merge Unicode combining chars\n      - ignore control chars\n    - Remove display attributes\n      - replace latin letter symbols\n      - replace latin letter ligatures\n      - replace latin number symbols\n    - Normalize visually equivalent chars\n      - replace equivalent chars \n      - replace cyrillic and greek chars looking like latin letters\n    - Encode infrequent chars while losing a little bit of information \n      - replace infrequent latin letters with diacritics\n      - replace infrequent chars from other scripts\n      - replace infrequent symbols \n      - ignore remaining chars with no glyph \n\n    The statistics below count the number of chars normalized **for 1 million chars** in 4 distinct parts of the french datasets : business websites, forums, news, wikipedia.\n\n    The first line of the table below shows that :\n    - in 1 million chars extracted from forum pages (raw users input), 41.8 chars will be encoding errors (windows1252 read as iso8859-1)\n    - in 1 million chars extracted from wikipedia (curated content), only 0.006 chars will be encoding errors\n\n    These numbers show that **characters normalization is much more important in real world applications** than in academic papers based on clean wikipedia text. \n\n    ```python\n    normstats = pd.read_csv(chardatadir / \"stats\" / \"normalization.total.stats.csv\")\n    normstats[[\"Transform\",\"FreqBusiness\",\"FreqForum\",\"FreqPresse\",\"FreqWikipedia\"]]\n    ```\n\n\n\n\n    &lt;div&gt;\n    &lt;style scoped&gt;\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n\n        .dataframe thead th {\n            text-align: right;\n        }\n    &lt;/style&gt;\n    &lt;table border=\"1\" class=\"dataframe\"&gt;\n      &lt;thead&gt;\n        &lt;tr style=\"text-align: right;\"&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;Transform&lt;/th&gt;\n          &lt;th&gt;FreqBusiness&lt;/th&gt;\n          &lt;th&gt;FreqForum&lt;/th&gt;\n          &lt;th&gt;FreqPresse&lt;/th&gt;\n          &lt;th&gt;FreqWikipedia&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;th&gt;0&lt;/th&gt;\n          &lt;td&gt;Fix encoding errors : windows1252 read as iso8...&lt;/td&gt;\n          &lt;td&gt;0.510560&lt;/td&gt;\n          &lt;td&gt;41.818746&lt;/td&gt;\n          &lt;td&gt;0.813485&lt;/td&gt;\n          &lt;td&gt;0.006025&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;1&lt;/th&gt;\n          &lt;td&gt;Fix encoding errors : utf8 read as windows1252&lt;/td&gt;\n          &lt;td&gt;0.126815&lt;/td&gt;\n          &lt;td&gt;0.058024&lt;/td&gt;\n          &lt;td&gt;0.072456&lt;/td&gt;\n          &lt;td&gt;0.001037&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;2&lt;/th&gt;\n          &lt;td&gt;Fix encoding errors :  windows1252 read as utf8&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.019315&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;3&lt;/th&gt;\n          &lt;td&gt;Merge Unicode combining chars&lt;/td&gt;\n          &lt;td&gt;2.811983&lt;/td&gt;\n          &lt;td&gt;0.432638&lt;/td&gt;\n          &lt;td&gt;0.568146&lt;/td&gt;\n          &lt;td&gt;0.000140&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;4&lt;/th&gt;\n          &lt;td&gt;Ignore control chars&lt;/td&gt;\n          &lt;td&gt;6.450737&lt;/td&gt;\n          &lt;td&gt;349.052995&lt;/td&gt;\n          &lt;td&gt;6.454367&lt;/td&gt;\n          &lt;td&gt;4.118586&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;5&lt;/th&gt;\n          &lt;td&gt;Replace latin letter symbols&lt;/td&gt;\n          &lt;td&gt;0.019360&lt;/td&gt;\n          &lt;td&gt;0.039701&lt;/td&gt;\n          &lt;td&gt;0.297372&lt;/td&gt;\n          &lt;td&gt;0.150550&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;6&lt;/th&gt;\n          &lt;td&gt;Replace latin letter ligatures&lt;/td&gt;\n          &lt;td&gt;6.603815&lt;/td&gt;\n          &lt;td&gt;6.541480&lt;/td&gt;\n          &lt;td&gt;10.097290&lt;/td&gt;\n          &lt;td&gt;17.204422&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;7&lt;/th&gt;\n          &lt;td&gt;Replace latin number symbols&lt;/td&gt;\n          &lt;td&gt;2.528338&lt;/td&gt;\n          &lt;td&gt;4.162482&lt;/td&gt;\n          &lt;td&gt;2.560933&lt;/td&gt;\n          &lt;td&gt;0.429792&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;8&lt;/th&gt;\n          &lt;td&gt;Normalize equivalent chars&lt;/td&gt;\n          &lt;td&gt;814.327384&lt;/td&gt;\n          &lt;td&gt;1248.410777&lt;/td&gt;\n          &lt;td&gt;684.333730&lt;/td&gt;\n          &lt;td&gt;242.391239&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;9&lt;/th&gt;\n          &lt;td&gt;Replace cyrillic and greek chars looking like ...&lt;/td&gt;\n          &lt;td&gt;0.062432&lt;/td&gt;\n          &lt;td&gt;0.760424&lt;/td&gt;\n          &lt;td&gt;0.491996&lt;/td&gt;\n          &lt;td&gt;7.479907&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;10&lt;/th&gt;\n          &lt;td&gt;Replace infrequent chars : latin letters with ...&lt;/td&gt;\n          &lt;td&gt;0.063782&lt;/td&gt;\n          &lt;td&gt;0.078384&lt;/td&gt;\n          &lt;td&gt;0.099106&lt;/td&gt;\n          &lt;td&gt;9.124948&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;11&lt;/th&gt;\n          &lt;td&gt;Replace infrequent chars : other scripts&lt;/td&gt;\n          &lt;td&gt;0.085694&lt;/td&gt;\n          &lt;td&gt;0.468776&lt;/td&gt;\n          &lt;td&gt;1.192548&lt;/td&gt;\n          &lt;td&gt;16.612142&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;12&lt;/th&gt;\n          &lt;td&gt;Replace infrequent chars : symbols&lt;/td&gt;\n          &lt;td&gt;0.139271&lt;/td&gt;\n          &lt;td&gt;0.159821&lt;/td&gt;\n          &lt;td&gt;0.399064&lt;/td&gt;\n          &lt;td&gt;0.073566&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;13&lt;/th&gt;\n          &lt;td&gt;Replace infrequent chars : chars to ignore&lt;/td&gt;\n          &lt;td&gt;0.018910&lt;/td&gt;\n          &lt;td&gt;0.044282&lt;/td&gt;\n          &lt;td&gt;0.021320&lt;/td&gt;\n          &lt;td&gt;0.016423&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;/div&gt;\n\n\n\n    Most frequent chars replaced from equivalent characters :\n\n    ```python\n    replacestats = pd.read_csv(chardatadir / \"stats\" / \"normalization.layer8.stats.csv\")\n    replacestats[[\"Char\",\"CharName\",\"FreqBusiness\",\"FreqForum\",\"FreqPresse\",\"FreqWikipedia\"]].head(20)\n    ```\n\n\n\n\n    &lt;div&gt;\n    &lt;style scoped&gt;\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n\n        .dataframe thead th {\n            text-align: right;\n        }\n    &lt;/style&gt;\n    &lt;table border=\"1\" class=\"dataframe\"&gt;\n      &lt;thead&gt;\n        &lt;tr style=\"text-align: right;\"&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;Char&lt;/th&gt;\n          &lt;th&gt;CharName&lt;/th&gt;\n          &lt;th&gt;FreqBusiness&lt;/th&gt;\n          &lt;th&gt;FreqForum&lt;/th&gt;\n          &lt;th&gt;FreqPresse&lt;/th&gt;\n          &lt;th&gt;FreqWikipedia&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;th&gt;0&lt;/th&gt;\n          &lt;td&gt;'&lt;/td&gt;\n          &lt;td&gt;Apostrophe&lt;/td&gt;\n          &lt;td&gt;486.034805&lt;/td&gt;\n          &lt;td&gt;160.264219&lt;/td&gt;\n          &lt;td&gt;376.104982&lt;/td&gt;\n          &lt;td&gt;134.658673&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;1&lt;/th&gt;\n          &lt;td&gt;&lt;/td&gt;\n          &lt;td&gt;Space&lt;/td&gt;\n          &lt;td&gt;310.411117&lt;/td&gt;\n          &lt;td&gt;1082.845985&lt;/td&gt;\n          &lt;td&gt;288.635983&lt;/td&gt;\n          &lt;td&gt;87.877649&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;2&lt;/th&gt;\n          &lt;td&gt;-&lt;/td&gt;\n          &lt;td&gt;Hyphen-Minus&lt;/td&gt;\n          &lt;td&gt;14.431203&lt;/td&gt;\n          &lt;td&gt;2.903761&lt;/td&gt;\n          &lt;td&gt;12.828203&lt;/td&gt;\n          &lt;td&gt;16.223154&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;3&lt;/th&gt;\n          &lt;td&gt;\u00ab&lt;/td&gt;\n          &lt;td&gt;Left-Pointing Double Angle Quotation Mark&lt;/td&gt;\n          &lt;td&gt;1.429478&lt;/td&gt;\n          &lt;td&gt;0.680513&lt;/td&gt;\n          &lt;td&gt;3.002426&lt;/td&gt;\n          &lt;td&gt;0.559632&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;4&lt;/th&gt;\n          &lt;td&gt;\u00bb&lt;/td&gt;\n          &lt;td&gt;Right-Pointing Double Angle Quotation Mark&lt;/td&gt;\n          &lt;td&gt;1.323524&lt;/td&gt;\n          &lt;td&gt;0.533926&lt;/td&gt;\n          &lt;td&gt;2.461880&lt;/td&gt;\n          &lt;td&gt;0.544134&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;5&lt;/th&gt;\n          &lt;td&gt;|&lt;/td&gt;\n          &lt;td&gt;Vertical Line&lt;/td&gt;\n          &lt;td&gt;0.003452&lt;/td&gt;\n          &lt;td&gt;0.001018&lt;/td&gt;\n          &lt;td&gt;0.005488&lt;/td&gt;\n          &lt;td&gt;0.875894&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;6&lt;/th&gt;\n          &lt;td&gt;\u2022&lt;/td&gt;\n          &lt;td&gt;Bullet&lt;/td&gt;\n          &lt;td&gt;0.204104&lt;/td&gt;\n          &lt;td&gt;0.243295&lt;/td&gt;\n          &lt;td&gt;0.189664&lt;/td&gt;\n          &lt;td&gt;0.543237&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;7&lt;/th&gt;\n          &lt;td&gt;.&lt;/td&gt;\n          &lt;td&gt;Full Stop&lt;/td&gt;\n          &lt;td&gt;0.059280&lt;/td&gt;\n          &lt;td&gt;0.078893&lt;/td&gt;\n          &lt;td&gt;0.856230&lt;/td&gt;\n          &lt;td&gt;0.069278&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;8&lt;/th&gt;\n          &lt;td&gt;\"&lt;/td&gt;\n          &lt;td&gt;Quotation Mark&lt;/td&gt;\n          &lt;td&gt;0.085093&lt;/td&gt;\n          &lt;td&gt;0.023413&lt;/td&gt;\n          &lt;td&gt;0.011504&lt;/td&gt;\n          &lt;td&gt;0.292385&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;9&lt;/th&gt;\n          &lt;td&gt;:&lt;/td&gt;\n          &lt;td&gt;Colon&lt;/td&gt;\n          &lt;td&gt;0.000150&lt;/td&gt;\n          &lt;td&gt;0.000509&lt;/td&gt;\n          &lt;td&gt;0.000053&lt;/td&gt;\n          &lt;td&gt;0.169047&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;10&lt;/th&gt;\n          &lt;td&gt;\u00b0&lt;/td&gt;\n          &lt;td&gt;Degree Sign&lt;/td&gt;\n          &lt;td&gt;0.148726&lt;/td&gt;\n          &lt;td&gt;0.181199&lt;/td&gt;\n          &lt;td&gt;0.014618&lt;/td&gt;\n          &lt;td&gt;0.078302&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;11&lt;/th&gt;\n          &lt;td&gt;\u00e9&lt;/td&gt;\n          &lt;td&gt;Latin Small Letter E With Acute&lt;/td&gt;\n          &lt;td&gt;0.001651&lt;/td&gt;\n          &lt;td&gt;0.006108&lt;/td&gt;\n          &lt;td&gt;0.003166&lt;/td&gt;\n          &lt;td&gt;0.101114&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;12&lt;/th&gt;\n          &lt;td&gt;\u2190&lt;/td&gt;\n          &lt;td&gt;Leftwards Arrow&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.000158&lt;/td&gt;\n          &lt;td&gt;0.047194&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;13&lt;/th&gt;\n          &lt;td&gt;=&lt;/td&gt;\n          &lt;td&gt;Equals Sign&lt;/td&gt;\n          &lt;td&gt;0.004802&lt;/td&gt;\n          &lt;td&gt;0.029012&lt;/td&gt;\n          &lt;td&gt;0.000686&lt;/td&gt;\n          &lt;td&gt;0.041589&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;14&lt;/th&gt;\n          &lt;td&gt;\u2192&lt;/td&gt;\n          &lt;td&gt;Rightwards Arrow&lt;/td&gt;\n          &lt;td&gt;0.026113&lt;/td&gt;\n          &lt;td&gt;0.002545&lt;/td&gt;\n          &lt;td&gt;0.034302&lt;/td&gt;\n          &lt;td&gt;0.015862&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;15&lt;/th&gt;\n          &lt;td&gt;d&lt;/td&gt;\n          &lt;td&gt;Latin Small Letter D&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.024940&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.036405&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;16&lt;/th&gt;\n          &lt;td&gt;&amp;lt;&lt;/td&gt;\n          &lt;td&gt;Less-Than Sign&lt;/td&gt;\n          &lt;td&gt;0.004202&lt;/td&gt;\n          &lt;td&gt;0.142007&lt;/td&gt;\n          &lt;td&gt;0.001267&lt;/td&gt;\n          &lt;td&gt;0.024073&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;17&lt;/th&gt;\n          &lt;td&gt;,&lt;/td&gt;\n          &lt;td&gt;Comma&lt;/td&gt;\n          &lt;td&gt;0.006453&lt;/td&gt;\n          &lt;td&gt;0.101288&lt;/td&gt;\n          &lt;td&gt;0.004538&lt;/td&gt;\n          &lt;td&gt;0.022756&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;18&lt;/th&gt;\n          &lt;td&gt;\u2193&lt;/td&gt;\n          &lt;td&gt;Downwards Arrow&lt;/td&gt;\n          &lt;td&gt;0.007504&lt;/td&gt;\n          &lt;td&gt;0.001527&lt;/td&gt;\n          &lt;td&gt;0.011188&lt;/td&gt;\n          &lt;td&gt;0.021888&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;19&lt;/th&gt;\n          &lt;td&gt;\u2605&lt;/td&gt;\n          &lt;td&gt;Black Star&lt;/td&gt;\n          &lt;td&gt;0.001351&lt;/td&gt;\n          &lt;td&gt;0.013743&lt;/td&gt;\n          &lt;td&gt;0.022006&lt;/td&gt;\n          &lt;td&gt;0.011686&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;/div&gt;\n\n\n\n    For example, list of all Unicode chars wich will be projected to a regular 'apostrophe' :\n\n    ```python\n    replacechars = pd.read_csv(chardatadir / \"normalizedchars.csv\", sep=';')\n    replacechars[replacechars[\"NormChar\"]==\"'\"][[\"Code\",\"Char\",\"CharName\"]]\n    ```\n\n\n\n\n    &lt;div&gt;\n    &lt;style scoped&gt;\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n\n        .dataframe thead th {\n            text-align: right;\n        }\n    &lt;/style&gt;\n    &lt;table border=\"1\" class=\"dataframe\"&gt;\n      &lt;thead&gt;\n        &lt;tr style=\"text-align: right;\"&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;Code&lt;/th&gt;\n          &lt;th&gt;Char&lt;/th&gt;\n          &lt;th&gt;CharName&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;th&gt;23&lt;/th&gt;\n          &lt;td&gt;96&lt;/td&gt;\n          &lt;td&gt;`&lt;/td&gt;\n          &lt;td&gt;Grave Accent&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;24&lt;/th&gt;\n          &lt;td&gt;180&lt;/td&gt;\n          &lt;td&gt;\u00b4&lt;/td&gt;\n          &lt;td&gt;Acute Accent&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;25&lt;/th&gt;\n          &lt;td&gt;697&lt;/td&gt;\n          &lt;td&gt;\u02b9&lt;/td&gt;\n          &lt;td&gt;Modifier Letter Prime&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;26&lt;/th&gt;\n          &lt;td&gt;699&lt;/td&gt;\n          &lt;td&gt;\u02bb&lt;/td&gt;\n          &lt;td&gt;Modifier Letter Turned Comma&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;27&lt;/th&gt;\n          &lt;td&gt;700&lt;/td&gt;\n          &lt;td&gt;\u02bc&lt;/td&gt;\n          &lt;td&gt;Modifier Letter Apostrophe&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;28&lt;/th&gt;\n          &lt;td&gt;702&lt;/td&gt;\n          &lt;td&gt;\u02be&lt;/td&gt;\n          &lt;td&gt;Modifier Letter Right Half Ring&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;29&lt;/th&gt;\n          &lt;td&gt;703&lt;/td&gt;\n          &lt;td&gt;\u02bf&lt;/td&gt;\n          &lt;td&gt;Modifier Letter Left Half Ring&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;30&lt;/th&gt;\n          &lt;td&gt;712&lt;/td&gt;\n          &lt;td&gt;\u02c8&lt;/td&gt;\n          &lt;td&gt;Modifier Letter Vertical Line&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;31&lt;/th&gt;\n          &lt;td&gt;714&lt;/td&gt;\n          &lt;td&gt;\u02ca&lt;/td&gt;\n          &lt;td&gt;Modifier Letter Acute Accent&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;32&lt;/th&gt;\n          &lt;td&gt;715&lt;/td&gt;\n          &lt;td&gt;\u02cb&lt;/td&gt;\n          &lt;td&gt;Modifier Letter Grave Accent&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;33&lt;/th&gt;\n          &lt;td&gt;729&lt;/td&gt;\n          &lt;td&gt;\u02d9&lt;/td&gt;\n          &lt;td&gt;Dot Above&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;34&lt;/th&gt;\n          &lt;td&gt;8216&lt;/td&gt;\n          &lt;td&gt;\u2018&lt;/td&gt;\n          &lt;td&gt;Left Single Quotation Mark&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;35&lt;/th&gt;\n          &lt;td&gt;8217&lt;/td&gt;\n          &lt;td&gt;\u2019&lt;/td&gt;\n          &lt;td&gt;Right Single Quotation Mark&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;36&lt;/th&gt;\n          &lt;td&gt;8219&lt;/td&gt;\n          &lt;td&gt;\u201b&lt;/td&gt;\n          &lt;td&gt;Single High-Reversed-9 Quotation Mark&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;37&lt;/th&gt;\n          &lt;td&gt;8223&lt;/td&gt;\n          &lt;td&gt;\u201f&lt;/td&gt;\n          &lt;td&gt;Double High-Reversed-9 Quotation Mark&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;38&lt;/th&gt;\n          &lt;td&gt;8242&lt;/td&gt;\n          &lt;td&gt;\u2032&lt;/td&gt;\n          &lt;td&gt;Prime&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;/div&gt;\n\n\n\n    Frequency of characters from other scripts (chinese, arabic, cyrillic ...) :\n\n    ```python\n    scriptsstats = pd.read_csv(chardatadir / \"stats\" / \"normalization.layer11.stats.csv\")\n    scriptsstats[[\"CharFamily\",\"FreqBusiness\",\"FreqForum\",\"FreqPresse\",\"FreqWikipedia\"]]\n    ```\n\n\n\n\n    &lt;div&gt;\n    &lt;style scoped&gt;\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n\n        .dataframe thead th {\n            text-align: right;\n        }\n    &lt;/style&gt;\n    &lt;table border=\"1\" class=\"dataframe\"&gt;\n      &lt;thead&gt;\n        &lt;tr style=\"text-align: right;\"&gt;\n          &lt;th&gt;&lt;/th&gt;\n          &lt;th&gt;CharFamily&lt;/th&gt;\n          &lt;th&gt;FreqBusiness&lt;/th&gt;\n          &lt;th&gt;FreqForum&lt;/th&gt;\n          &lt;th&gt;FreqPresse&lt;/th&gt;\n          &lt;th&gt;FreqWikipedia&lt;/th&gt;\n        &lt;/tr&gt;\n      &lt;/thead&gt;\n      &lt;tbody&gt;\n        &lt;tr&gt;\n          &lt;th&gt;0&lt;/th&gt;\n          &lt;td&gt;ChineseJapaneseKorean&lt;/td&gt;\n          &lt;td&gt;0.012456&lt;/td&gt;\n          &lt;td&gt;0.177127&lt;/td&gt;\n          &lt;td&gt;0.194677&lt;/td&gt;\n          &lt;td&gt;4.059173&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;1&lt;/th&gt;\n          &lt;td&gt;Arabic&lt;/td&gt;\n          &lt;td&gt;0.012306&lt;/td&gt;\n          &lt;td&gt;0.026467&lt;/td&gt;\n          &lt;td&gt;0.460280&lt;/td&gt;\n          &lt;td&gt;3.140120&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;2&lt;/th&gt;\n          &lt;td&gt;Cyrillic&lt;/td&gt;\n          &lt;td&gt;0.024462&lt;/td&gt;\n          &lt;td&gt;0.166438&lt;/td&gt;\n          &lt;td&gt;0.237159&lt;/td&gt;\n          &lt;td&gt;3.118961&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;3&lt;/th&gt;\n          &lt;td&gt;Greek&lt;/td&gt;\n          &lt;td&gt;0.016058&lt;/td&gt;\n          &lt;td&gt;0.022904&lt;/td&gt;\n          &lt;td&gt;0.031347&lt;/td&gt;\n          &lt;td&gt;2.423996&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;4&lt;/th&gt;\n          &lt;td&gt;Hebrew&lt;/td&gt;\n          &lt;td&gt;0.000150&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.184914&lt;/td&gt;\n          &lt;td&gt;1.132155&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;5&lt;/th&gt;\n          &lt;td&gt;Other&lt;/td&gt;\n          &lt;td&gt;0.000750&lt;/td&gt;\n          &lt;td&gt;0.029012&lt;/td&gt;\n          &lt;td&gt;0.004063&lt;/td&gt;\n          &lt;td&gt;0.800871&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;6&lt;/th&gt;\n          &lt;td&gt;Indian&lt;/td&gt;\n          &lt;td&gt;0.000750&lt;/td&gt;\n          &lt;td&gt;0.037665&lt;/td&gt;\n          &lt;td&gt;0.033458&lt;/td&gt;\n          &lt;td&gt;0.737955&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;7&lt;/th&gt;\n          &lt;td&gt;Phonetic&lt;/td&gt;\n          &lt;td&gt;0.002401&lt;/td&gt;\n          &lt;td&gt;0.001527&lt;/td&gt;\n          &lt;td&gt;0.001636&lt;/td&gt;\n          &lt;td&gt;0.298579&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;8&lt;/th&gt;\n          &lt;td&gt;Latin&lt;/td&gt;\n          &lt;td&gt;0.013507&lt;/td&gt;\n          &lt;td&gt;0.006108&lt;/td&gt;\n          &lt;td&gt;0.007283&lt;/td&gt;\n          &lt;td&gt;0.269377&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;9&lt;/th&gt;\n          &lt;td&gt;Math&lt;/td&gt;\n          &lt;td&gt;0.001801&lt;/td&gt;\n          &lt;td&gt;0.000509&lt;/td&gt;\n          &lt;td&gt;0.000528&lt;/td&gt;\n          &lt;td&gt;0.240707&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;10&lt;/th&gt;\n          &lt;td&gt;LaoThai&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.001018&lt;/td&gt;\n          &lt;td&gt;0.033194&lt;/td&gt;\n          &lt;td&gt;0.217867&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;th&gt;11&lt;/th&gt;\n          &lt;td&gt;Armenian&lt;/td&gt;\n          &lt;td&gt;0.001051&lt;/td&gt;\n          &lt;td&gt;0.000000&lt;/td&gt;\n          &lt;td&gt;0.004011&lt;/td&gt;\n          &lt;td&gt;0.172382&lt;/td&gt;\n        &lt;/tr&gt;\n      &lt;/tbody&gt;\n    &lt;/table&gt;\n    &lt;/div&gt;\n\n\n\n    ### Normalization pipeline API\n\n    Initialize a text normalizer :\n\n    ```python\n    %time norm = TextNormalizer()\n    norm\n    ```\n\n        CPU times: user 1.83 s, sys: 15.6 ms, total: 1.84 s\n        Wall time: 2 s\n\n\n\n\n\n        1 - Fix encoding errors : windows1252 read as iso8859-1\n        2 - Fix encoding errors : utf8 read as windows1252\n        3 - Fix encoding errors :  windows1252 read as utf8\n        4 - Merge Unicode combining chars\n        5 - Ignore control chars\n        6 - Replace latin letter symbols\n        7 - Replace latin letter ligatures\n        8 - Replace latin number symbols\n        9 - Normalize equivalent chars\n        10 - Replace cyrillic and greek chars looking like latin letters\n        11 - Replace infrequent chars : latin letters with diacritics\n        12 - Replace infrequent chars : other scripts\n        13 - Replace infrequent chars : symbols\n        14 - Replace infrequent chars : chars to ignore\n\n\n\n    Normalize text :\n\n    ```python\n    teststring = chr(127995)+\"\u2460 l`\"+chr(156)+\"uv\"+chr(127)+\"re est\u00a8 \"+chr(147)+\"belle\"+chr(148)+\"\u00b8 \u00c3  \u00c2\u00bd \u00e2\u201a\u00ac e\u0301nie\u0300me \u00e2\u20ac\u00b0 \"+chr(133)+\" \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\"\n    teststring\n    ```\n\n\n\n\n        '\ud83c\udffb\u2460 l`\\x9cuv\\x7fre est\u00a8 \\x93belle\\x94\u00b8 \u00c3  \u00c2\u00bd \u00e2\u201a\u00ac e\u0301nie\u0300me \u00e2\u20ac\u00b0 \\x85 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01'\n\n\n\n    ```python\n    result = norm(teststring)\n    result\n    ```\n\n\n\n\n        (1) l'oeuvre est \u00abbelle\u00bb, \u00c3  1/2 \u20ac \u00e9ni\u00e8me \u2030 \u2026 (EfficAce) !\n\n\n\n    Describe the changes applied by the normalization pipeline :\n\n    ```python\n    print(result.describeChanges())\n    ```\n\n        Fix encoding errors : windows1252 read as iso8859-1\n         &lt; \ud83c\udffb\u2460 l` [\u009c] uv\u007fre est\u00a8  [\u0093] belle [\u0094] \u00b8 \u00c3  \u00c2\u00bd \u00e2\u201a\u00ac e\u0301nie\u0300me \u00e2\u20ac\u00b0  [\n</code></pre>\n<p>]  \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n&lt; \ud83c\udffb\u2460 l<code>[\u0153] uv\u007fre est\u00a8 [\u201c] belle [\u201d] \u00b8 \u00c3 \u00c2\u00bd \u00e2\u201a\u00ac e\u0301nie\u0300me \u00e2\u20ac\u00b0 [\u2026] \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01 Fix encoding errors : utf8 read as windows1252 &lt; \ud83c\udffb\u2460 l</code>\u0153uv\u007fre est\u00a8 \u201cbelle\u201d\u00b8 \u00c3   [\u00c2\u00bd]   [\u00e2\u201a\u00ac]  e\u0301nie\u0300me  [\u00e2\u20ac\u00b0]  \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n&lt; \ud83c\udffb\u2460 l<code>\u0153uv\u007fre est\u00a8 \u201cbelle\u201d\u00b8 \u00c3 [\u00bd_] [\u20ac__] e\u0301nie\u0300me [\u2030__] \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01 Merge Unicode combining chars &lt; \ud83c\udffb\u2460 l</code>\u0153uv\u007fre est\u00a8 \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac  [e\u0301] ni [e\u0300] me \u2030 \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n&lt; \ud83c\udffb\u2460 l<code>\u0153uv\u007fre est\u00a8 \u201cbelle\u201d\u00b8 \u00c3 \u00bd \u20ac [\u00e9_] ni [\u00e8_] me \u2030 \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01 Ignore control chars &lt; [\ud83c\udffb] \u2460 l</code>\u0153uv [\u007f] re est [\u00a8]  \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01\n&lt;  [_] \u2460 l<code>\u0153uv [_] re est [_] \u201cbelle\u201d\u00b8 \u00c3 \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207d\ud83c\uddea\ufb03c\ud83c\udde6ce\u207e \uff01 Replace latin letter symbols &lt; \u2460 l</code>\u0153uvre est \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207d [\ud83c\uddea] \ufb03c [\ud83c\udde6] ce\u207e \uff01\n&lt; \u2460 l<code>\u0153uvre est \u201cbelle\u201d\u00b8 \u00c3 \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207d [E] \ufb03c [A] ce\u207e \uff01 Replace latin letter ligatures &lt; \u2460 l</code> [\u0153 ] uvre est \u201cbelle\u201d\u00b8 \u00c3  \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207dE [\ufb03  ] cAce\u207e \uff01\n&lt; \u2460 l<code>[oe] uvre est \u201cbelle\u201d\u00b8 \u00c3 \u00bd \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207dE [ffi] cAce\u207e \uff01 Replace latin number symbols &lt; [\u2460 ] l</code>oeuvre est \u201cbelle\u201d\u00b8 \u00c3   [\u00bd  ]  \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207dEfficAce\u207e \uff01\n&lt;  [(1)]  l<code>oeuvre est \u201cbelle\u201d\u00b8 \u00c3 [1/2] \u20ac \u00e9ni\u00e8me \u2030 \u2026 \u207dEfficAce\u207e \uff01 Normalize equivalent chars &lt; (1) l [</code>] oeuvre est  [\u201c] belle [\u201d]  [\u00b8]  \u00c3  1/2 \u20ac \u00e9ni\u00e8me \u2030 \u2026  [\u207d] EfficAce [\u207e]   [\uff01]\n&lt; (1) l ['] oeuvre est  [\u00ab] belle [\u00bb]  [,]  \u00c3  1/2 \u20ac \u00e9ni\u00e8me \u2030 \u2026  [(] EfficAce [)]   [!]</p>\n<pre><code>    Compute spans for equivalent substrings before and after normalization :\n\n    ```python\n    result.output[0:12]\n    ```\n\n\n\n\n        \"(1) l'oeuvre\"\n\n\n\n    ```python\n    result.input[result.mapOutputIndexToInput(0):result.mapOutputIndexToInput(12)]\n    ```\n\n\n\n\n        '\ud83c\udffb\u2460 l`\\x9cuv\\x7fre'\n\n\n\n    ```python\n    result.output[3:10]\n    ```\n\n\n\n\n        \" l'oeuv\"\n\n\n\n    ```python\n    result.input[result.mapOutputIndexToInput(3):result.mapOutputIndexToInput(10)]\n    ```\n\n\n\n\n        ' l`\\x9cuv\\x7f'\n\n\n\n    Performance test : **2500 sentences per second** =&gt; fast enough but will be optimized in a later version.\n\n    ```python\n    %timeit -n100 norm(teststring)\n    ```\n\n        397 \u00b5s \u00b1 89.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n    ### Appendix : Unicode utility functions\n\n    Unicode characters properties :\n\n    ```python\n    charname(\"\ud83d\ude42\")\n    ```\n\n\n\n\n        'Slightly Smiling Face'\n\n\n\n    ```python\n    charcategory(\"\ud83d\ude42\")\n    ```\n\n\n\n\n        'Symbol'\n\n\n\n    ```python\n    charsubcategory(\"\ud83d\ude42\")\n    ```\n\n\n\n\n        'Other'\n\n\n\n    ```python\n    charblock(\"\ud83d\ude42\")\n    ```\n\n\n\n\n        'Emoticons'\n\n\n\n    ```python\n    blockfamily('Emoticons')\n    ```\n\n\n\n\n        'Symbols'\n</code></pre>\n\n          </div>"}, "last_serial": 6736981, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "e97d9a90fc5c2ab10e50c8b7dcf0ed7f", "sha256": "f18f311d69bf84521c7875a1de9baee30c51b967e6de952717beb68937e232d5"}, "downloads": -1, "filename": "frenchtext-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "e97d9a90fc5c2ab10e50c8b7dcf0ed7f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 9984, "upload_time": "2020-02-06T23:28:31", "upload_time_iso_8601": "2020-02-06T23:28:31.484337Z", "url": "https://files.pythonhosted.org/packages/79/03/4c05314bd54577dfbd46e96578cdeb988e1bdb40979a8fad1bd5d7d264c1/frenchtext-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8aa7ebbdd7e270f158b6426ddc617aad", "sha256": "ed4d3a003bbcd868302445e344f2555a3c0d60f32b8f55b33106d4cac746f179"}, "downloads": -1, "filename": "frenchtext-0.0.1.tar.gz", "has_sig": false, "md5_digest": "8aa7ebbdd7e270f158b6426ddc617aad", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 634801, "upload_time": "2020-02-06T23:28:34", "upload_time_iso_8601": "2020-02-06T23:28:34.292317Z", "url": "https://files.pythonhosted.org/packages/66/b7/aefd48907739700813b2141291485abeaad6d6cf8610e5b2210559681190/frenchtext-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "b58a6ef76ccf799615d6da47268d5809", "sha256": "6e7997c430325c87d0bd76ebbd43d4ea1557234026f7c1c957625c1e95432e1d"}, "downloads": -1, "filename": "frenchtext-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "b58a6ef76ccf799615d6da47268d5809", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 16983, "upload_time": "2020-02-26T23:56:45", "upload_time_iso_8601": "2020-02-26T23:56:45.442585Z", "url": "https://files.pythonhosted.org/packages/2c/c6/96082b1f87df5449672e1d627049be3776f45ccaba6f0f1716864367f765/frenchtext-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7ef7c3bfcd32e449f7816272cf09589b", "sha256": "bd99be9652e525ac36b1be573802973d0d365392fc900391aefd4ac0e5d427ae"}, "downloads": -1, "filename": "frenchtext-0.0.2.tar.gz", "has_sig": false, "md5_digest": "7ef7c3bfcd32e449f7816272cf09589b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 640085, "upload_time": "2020-02-26T23:56:47", "upload_time_iso_8601": "2020-02-26T23:56:47.513256Z", "url": "https://files.pythonhosted.org/packages/cb/7c/2222703b19046447959725684e25f0bf5b5c370eac996983103826dfd6dd/frenchtext-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "4da599fa06172f863fdd0694bbce7040", "sha256": "8253b4698cb80598c8caa4a1b1609addf311a798165493b9f2b8beee1d3893e9"}, "downloads": -1, "filename": "frenchtext-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "4da599fa06172f863fdd0694bbce7040", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 28462, "upload_time": "2020-02-27T23:28:08", "upload_time_iso_8601": "2020-02-27T23:28:08.729902Z", "url": "https://files.pythonhosted.org/packages/b6/2e/606ae5ca3592bdd930316a6f605edfaf2877119655f6fbfed698f75bac7d/frenchtext-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "009244662bb23ba67727742b0ff520b6", "sha256": "c40d86e59097481fc7bde098939c175fa691330ea9e9f5a7ff25da42521e5212"}, "downloads": -1, "filename": "frenchtext-0.0.3.tar.gz", "has_sig": false, "md5_digest": "009244662bb23ba67727742b0ff520b6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 673852, "upload_time": "2020-02-27T23:28:11", "upload_time_iso_8601": "2020-02-27T23:28:11.062300Z", "url": "https://files.pythonhosted.org/packages/2d/74/6ccb4a1beb92bf8f614bf942959db39a7be712d0e736b4728a3117f40a7c/frenchtext-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "43e64a74ee6ca8825774ff7addefc2fa", "sha256": "7f45ab1cadf2a3ea3676d037c28dc8206d34a1f94a1bba0a36f00539579e6c1e"}, "downloads": -1, "filename": "frenchtext-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "43e64a74ee6ca8825774ff7addefc2fa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 29542, "upload_time": "2020-03-01T18:35:29", "upload_time_iso_8601": "2020-03-01T18:35:29.390375Z", "url": "https://files.pythonhosted.org/packages/b5/1e/12c165e0a1053d221dca0b3e02ce2a74c22046a87594ffab0ca6a3828c6b/frenchtext-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1d547b1800447a7b23e3af5a966f1a4e", "sha256": "ad6fe12569e9bfa09a89b19aa4958a633747dea47a3cbe53fc2bef1040de4069"}, "downloads": -1, "filename": "frenchtext-0.0.4.tar.gz", "has_sig": false, "md5_digest": "1d547b1800447a7b23e3af5a966f1a4e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 54317, "upload_time": "2020-03-01T18:35:31", "upload_time_iso_8601": "2020-03-01T18:35:31.170097Z", "url": "https://files.pythonhosted.org/packages/13/da/3dcfe61a28245bb02a029c1252aac2c20a750d3ecc8212667972aa7b7a20/frenchtext-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "0abba8daae8fe137f686cefb37f71e36", "sha256": "f2636389ae62354928aa28522340b9244144a037b569efa521bc833fb6a19cb3"}, "downloads": -1, "filename": "frenchtext-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "0abba8daae8fe137f686cefb37f71e36", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 677132, "upload_time": "2020-03-02T22:20:37", "upload_time_iso_8601": "2020-03-02T22:20:37.246797Z", "url": "https://files.pythonhosted.org/packages/f6/3b/21018b678b95b3a01920c00aba0bd933eb82b167efd7ce8d91ed89d39834/frenchtext-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "486d40bb1a2f65baf0b8b870c08cb515", "sha256": "396f6894638414aa0114e6a9a7c8165eb373a6b3c4e6d05a6fa68a56c67037f7"}, "downloads": -1, "filename": "frenchtext-0.0.5.tar.gz", "has_sig": false, "md5_digest": "486d40bb1a2f65baf0b8b870c08cb515", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 677652, "upload_time": "2020-03-02T22:20:39", "upload_time_iso_8601": "2020-03-02T22:20:39.026592Z", "url": "https://files.pythonhosted.org/packages/d1/fa/5f8a4f38576a2727b592fefc2b6899f4447591d3029d3e4dfb53e1bfe744/frenchtext-0.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0abba8daae8fe137f686cefb37f71e36", "sha256": "f2636389ae62354928aa28522340b9244144a037b569efa521bc833fb6a19cb3"}, "downloads": -1, "filename": "frenchtext-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "0abba8daae8fe137f686cefb37f71e36", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 677132, "upload_time": "2020-03-02T22:20:37", "upload_time_iso_8601": "2020-03-02T22:20:37.246797Z", "url": "https://files.pythonhosted.org/packages/f6/3b/21018b678b95b3a01920c00aba0bd933eb82b167efd7ce8d91ed89d39834/frenchtext-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "486d40bb1a2f65baf0b8b870c08cb515", "sha256": "396f6894638414aa0114e6a9a7c8165eb373a6b3c4e6d05a6fa68a56c67037f7"}, "downloads": -1, "filename": "frenchtext-0.0.5.tar.gz", "has_sig": false, "md5_digest": "486d40bb1a2f65baf0b8b870c08cb515", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 677652, "upload_time": "2020-03-02T22:20:39", "upload_time_iso_8601": "2020-03-02T22:20:39.026592Z", "url": "https://files.pythonhosted.org/packages/d1/fa/5f8a4f38576a2727b592fefc2b6899f4447591d3029d3e4dfb53e1bfe744/frenchtext-0.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 01:00:25 2020"}