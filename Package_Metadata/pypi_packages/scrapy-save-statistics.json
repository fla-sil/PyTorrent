{"info": {"author": "Light Ning", "author_email": "lightning1141@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Internet :: WWW/HTTP"], "description": "Save statistics extension for `Scrapy <http://scrapy.org/>`__\n=============================================================\n\nSave statistics to mongo for analytics.\n\nInstall\n-------\n\nThe quick way:\n\n::\n\n    pip install scrapy-save-statistics\n\nOr install from GitHub:\n\n::\n\n    pip install git+git://github.com/light4/scrapy-save-statistics.git@master\n\nOr checkout the source and run:\n\n::\n\n    python setup.py install\n\nsettings.py\n-----------\n\nMongodb settings for save statistics, need a *statistics* database.\n\n::\n\n    MONGO_HOST = \"127.0.0.1\"\n    MONGO_PORT = 27017\n    MONGO_DB = \"myspider\"\n    MONGO_STATISTICS = \"statistics\"\n\n    EXTENSIONS = {\n        'scrapy_save_statistics.SaveStatistics': 100,\n    }\n\nSpider\n-------\n\nSpider must have *statistics* attributes and contains spider_url.\nWe'll save that info to mongodb.\n\n::\n\n    class TestSpider(scrapy.Spider):\n        name = \"test\"\n\n        def __init__(self, name=None, **kwargs):\n            super(TestSpider, self).__init__(name=name, **kwargs)\n            self.statistics = []\n\n        def parse(self, response):\n            crawl_info = {'spider_url': spider_url,\n                          'expected_crawl_num': expected_crawl_num,\n                          'pages': total_page}\n            self.statistics.append(crawl_info)\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/light4/scrapy-save-statistics", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "scrapy-save-statistics", "package_url": "https://pypi.org/project/scrapy-save-statistics/", "platform": "", "project_url": "https://pypi.org/project/scrapy-save-statistics/", "project_urls": {"Homepage": "https://github.com/light4/scrapy-save-statistics"}, "release_url": "https://pypi.org/project/scrapy-save-statistics/0.2/", "requires_dist": ["Scrapy (>=1.0)"], "requires_python": "", "summary": "Scrapy Save Statistics: Save statistics extension for Scrapy", "version": "0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Save statistics to mongo for analytics.</p>\n<div id=\"install\">\n<h2>Install</h2>\n<p>The quick way:</p>\n<pre>pip install scrapy-save-statistics\n</pre>\n<p>Or install from GitHub:</p>\n<pre>pip install git+git://github.com/light4/scrapy-save-statistics.git@master\n</pre>\n<p>Or checkout the source and run:</p>\n<pre>python setup.py install\n</pre>\n</div>\n<div id=\"settings-py\">\n<h2>settings.py</h2>\n<p>Mongodb settings for save statistics, need a <em>statistics</em> database.</p>\n<pre>MONGO_HOST = \"127.0.0.1\"\nMONGO_PORT = 27017\nMONGO_DB = \"myspider\"\nMONGO_STATISTICS = \"statistics\"\n\nEXTENSIONS = {\n    'scrapy_save_statistics.SaveStatistics': 100,\n}\n</pre>\n</div>\n<div id=\"spider\">\n<h2>Spider</h2>\n<p>Spider must have <em>statistics</em> attributes and contains spider_url.\nWe\u2019ll save that info to mongodb.</p>\n<pre>class TestSpider(scrapy.Spider):\n    name = \"test\"\n\n    def __init__(self, name=None, **kwargs):\n        super(TestSpider, self).__init__(name=name, **kwargs)\n        self.statistics = []\n\n    def parse(self, response):\n        crawl_info = {'spider_url': spider_url,\n                      'expected_crawl_num': expected_crawl_num,\n                      'pages': total_page}\n        self.statistics.append(crawl_info)\n</pre>\n</div>\n\n          </div>"}, "last_serial": 2662439, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "740237605ef08271fae1289b97c09008", "sha256": "bdfd590a1d068968b38b71f9744288f8671beb0cb2fbb481c40338cab08f5a77"}, "downloads": -1, "filename": "scrapy_save_statistics-0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "740237605ef08271fae1289b97c09008", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4545, "upload_time": "2017-02-16T08:55:07", "upload_time_iso_8601": "2017-02-16T08:55:07.520294Z", "url": "https://files.pythonhosted.org/packages/83/89/554f4471110477f445262ae2f381841fc5c1fa543047149aae52639d6ba5/scrapy_save_statistics-0.1-py2.py3-none-any.whl", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "571d7b75a8fa663a540ed5b7c30b0dc7", "sha256": "aeb9743d06da179c480c5747cd086b1c19c5d0757295b72e950bc43c223f7e53"}, "downloads": -1, "filename": "scrapy_save_statistics-0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "571d7b75a8fa663a540ed5b7c30b0dc7", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4432, "upload_time": "2017-02-23T03:53:09", "upload_time_iso_8601": "2017-02-23T03:53:09.140373Z", "url": "https://files.pythonhosted.org/packages/c0/dd/d05e4c25af97e1bd384e1666b12d59e9a452f032b975e182a1dea6b67be2/scrapy_save_statistics-0.2-py2.py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "571d7b75a8fa663a540ed5b7c30b0dc7", "sha256": "aeb9743d06da179c480c5747cd086b1c19c5d0757295b72e950bc43c223f7e53"}, "downloads": -1, "filename": "scrapy_save_statistics-0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "571d7b75a8fa663a540ed5b7c30b0dc7", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4432, "upload_time": "2017-02-23T03:53:09", "upload_time_iso_8601": "2017-02-23T03:53:09.140373Z", "url": "https://files.pythonhosted.org/packages/c0/dd/d05e4c25af97e1bd384e1666b12d59e9a452f032b975e182a1dea6b67be2/scrapy_save_statistics-0.2-py2.py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 02:56:41 2020"}