{"info": {"author": "Fei Zhan", "author_email": "enfeizhan@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Programming Language :: Python"], "description": "=========\nClusteror\n=========\n\n* Unveils internal \"invisible\" patterns automatically\n* https://github.com/enfeizhan/clusteror\n* Fei Zhan\n* License: MIT License\n\nVersion support\n===============\nThis package is developed in Python 3.4. However support for Python 2 will be\nin the next release.\n\nDescription\n===========\n\nThis is a tool for discovering patterns existing in a dataset. It can be useful\nin segmenting customers based on their demographic, geographic, and past\nbehaviours in a commercial marketing environment. Users are encouraged to\nadventure with it in other scenarios.\n\nUnder the hood, a pretraining of \n`(Stacked) Denoising Autoencoder <https://en.wikipedia.org/wiki/Autoencoder>`__\nis implemented in\n`Python deep learning <http://deeplearning.net/tutorial/>`__ library\n`Theano <http://deeplearning.net/software/theano/>`__. Therefore, an installation\nof Theano is mandate. About how to install Theano, please read\n`Theano installation guide <http://deeplearning.net/software/theano/install.html>`__.\n\nIf you are lucky to have Nvidia graphic card, with proper setup Theano can\nparallel the calculation to be able to scale up to large datasets.\n\nWhy we need it?\n===============\n\nThe purpose of Unsupervised Machine Learning is to find underlying patterns\nwithout being provided with known categories as Supervised Classification\nMachine Learning. While it sounds like the most leveraged use case of Machine\nLearning as it does not rely on expensive labels that generally need to be\ncreated by human, its efficiency is debatable for commonly seen algorithms.\n\nRecall the very first clustering algorithm in machine learning lectures,\nK-Means. It looks amazing for beginners. The randomly initiated centroids\nseem to be intelligent to know where they should gravitate to. At least for\nthe example datasets for illustrating purposes.\n\nBut real life problems aren't that straightforward. It's not unlikely you are\ngiven a dataset that looks like this in a 2D space:\n\n.. image:: tests/readme_pics/original_problem.png\n\nUnfortunately this is the best K-Means can do and you probably scratch you head\nwondering what you can do to solve this seemingly straightforward problem.\n\n.. image:: tests/readme_pics/bad_kmeans.png\n\nThe cost function of K-Means instructs centroids to search for points\nlocated in a spharical region centring around them. This assumption of what\na cluster is no doubt fails when a cluster of points resides in a stripe\nshape like in the example above.\n\nHow can we do better?\n=====================\nThis \"unexpected\" (actually this is a well-known fact) failure stems from that\nK-Means fumbles in higher dimensional space. While this terminates K-Means to\nbe an awesome clustering tool that is deployable in extensive environments,\nits performance in one dimensional space has been ignored largely due to\nexamples in two dimension are more illustrative.\n\nHaving this in mind, we would work on to search ways of reducing high\ndimensional dataset to one dimension. As this is truly the cause responsible\nfor all strugglings in clustering high dimensional data.\n\nFortunately, the progress in Deep Learning sheds new light on this issue. As\nlabeled dataset is expensive, experts in this area came up with the salient\nidea of Pre-training, which enormously reduced the amount of labeled dataset\nneeded to train a deep learning neural network.\n\nIn a nutshell, pre-training updates the parameters in neural network such that\nan input can traverse the network forward and back therefore reproduce itself.\nA cost function is defined regarding the difference between the input and\nreproduced input instead of input and output.\n\nWhile it's highly recommended to read research articles to gain more detail,\na characteristic property of this process is tremendously useful for reducing\ndataset in high dimension down to lower dimension.\n\nThe central idea is given the input can be loyally or nearly loyally\nreproduced the neural network can be expected to do two things:\n\n* one-to-one mapping input to output; A different input results in a different\n  output.\n* Similar inputs lead to close outputs.\n\nThese two characteristics ensure clustering done in low-dimensional space\nis equivalent to do the clustering in high dimension.\n\nA picture is worth a thousand words. The chart below is the result of a three\nlayer neural network:\n\n.. image:: tests/readme_pics/leveraged_kmeans.png\n\nThe magic lies in how it looks in the mapped one dimensional space:\n\n.. image:: tests/readme_pics/hist.png\n\nWithout too much explanation, the left blue bars are the from the right blue\npoints and the right red bars from the left red points. As there isn't\nconcept for a sphere in 1d, K-Means works perfectly.\n\nWhat's in the package and what's coming up?\n===========================================\nAt the moment, you can do the following with the tools provided by the\npackage:\n\n* Dimension reduction to 1d.\n* Clustering in 1d and assign cluster ids back to the original dataset.\n* Certain useful plotting tools to display the efficiency of the clustering.\n\nThis project is still in alpha stage. In the imminent release, you can expect\n\n* Tools to searching for the drivers that distinct the clusters.\n* More support in Python versions.\n\nNote\n====\n\nThis project has been set up using PyScaffold 2.5.7. For details and usage\ninformation on PyScaffold see http://pyscaffold.readthedocs.org/.", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://...", "keywords": "", "license": "none", "maintainer": "", "maintainer_email": "", "name": "clusteror", "package_url": "https://pypi.org/project/clusteror/", "platform": "any", "project_url": "https://pypi.org/project/clusteror/", "project_urls": {"Homepage": "http://..."}, "release_url": "https://pypi.org/project/clusteror/0.0.2/", "requires_dist": null, "requires_python": "", "summary": "Dataset pattern discovery tool", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <ul>\n<li>Unveils internal \u201cinvisible\u201d patterns automatically</li>\n<li><a href=\"https://github.com/enfeizhan/clusteror\" rel=\"nofollow\">https://github.com/enfeizhan/clusteror</a></li>\n<li>Fei Zhan</li>\n<li>License: MIT License</li>\n</ul>\n<div id=\"version-support\">\n<h2>Version support</h2>\n<p>This package is developed in Python 3.4. However support for Python 2 will be\nin the next release.</p>\n</div>\n<div id=\"description\">\n<h2>Description</h2>\n<p>This is a tool for discovering patterns existing in a dataset. It can be useful\nin segmenting customers based on their demographic, geographic, and past\nbehaviours in a commercial marketing environment. Users are encouraged to\nadventure with it in other scenarios.</p>\n<p>Under the hood, a pretraining of\n<a href=\"https://en.wikipedia.org/wiki/Autoencoder\" rel=\"nofollow\">(Stacked) Denoising Autoencoder</a>\nis implemented in\n<a href=\"http://deeplearning.net/tutorial/\" rel=\"nofollow\">Python deep learning</a> library\n<a href=\"http://deeplearning.net/software/theano/\" rel=\"nofollow\">Theano</a>. Therefore, an installation\nof Theano is mandate. About how to install Theano, please read\n<a href=\"http://deeplearning.net/software/theano/install.html\" rel=\"nofollow\">Theano installation guide</a>.</p>\n<p>If you are lucky to have Nvidia graphic card, with proper setup Theano can\nparallel the calculation to be able to scale up to large datasets.</p>\n</div>\n<div id=\"why-we-need-it\">\n<h2>Why we need it?</h2>\n<p>The purpose of Unsupervised Machine Learning is to find underlying patterns\nwithout being provided with known categories as Supervised Classification\nMachine Learning. While it sounds like the most leveraged use case of Machine\nLearning as it does not rely on expensive labels that generally need to be\ncreated by human, its efficiency is debatable for commonly seen algorithms.</p>\n<p>Recall the very first clustering algorithm in machine learning lectures,\nK-Means. It looks amazing for beginners. The randomly initiated centroids\nseem to be intelligent to know where they should gravitate to. At least for\nthe example datasets for illustrating purposes.</p>\n<p>But real life problems aren\u2019t that straightforward. It\u2019s not unlikely you are\ngiven a dataset that looks like this in a 2D space:</p>\n<img alt=\"tests/readme_pics/original_problem.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b4d0312542138942abc0d142e91d757e543019f4/74657374732f726561646d655f706963732f6f726967696e616c5f70726f626c656d2e706e67\">\n<p>Unfortunately this is the best K-Means can do and you probably scratch you head\nwondering what you can do to solve this seemingly straightforward problem.</p>\n<img alt=\"tests/readme_pics/bad_kmeans.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/edd73c7e5d93a2b6f386694c75662a376cf93c4a/74657374732f726561646d655f706963732f6261645f6b6d65616e732e706e67\">\n<p>The cost function of K-Means instructs centroids to search for points\nlocated in a spharical region centring around them. This assumption of what\na cluster is no doubt fails when a cluster of points resides in a stripe\nshape like in the example above.</p>\n</div>\n<div id=\"how-can-we-do-better\">\n<h2>How can we do better?</h2>\n<p>This \u201cunexpected\u201d (actually this is a well-known fact) failure stems from that\nK-Means fumbles in higher dimensional space. While this terminates K-Means to\nbe an awesome clustering tool that is deployable in extensive environments,\nits performance in one dimensional space has been ignored largely due to\nexamples in two dimension are more illustrative.</p>\n<p>Having this in mind, we would work on to search ways of reducing high\ndimensional dataset to one dimension. As this is truly the cause responsible\nfor all strugglings in clustering high dimensional data.</p>\n<p>Fortunately, the progress in Deep Learning sheds new light on this issue. As\nlabeled dataset is expensive, experts in this area came up with the salient\nidea of Pre-training, which enormously reduced the amount of labeled dataset\nneeded to train a deep learning neural network.</p>\n<p>In a nutshell, pre-training updates the parameters in neural network such that\nan input can traverse the network forward and back therefore reproduce itself.\nA cost function is defined regarding the difference between the input and\nreproduced input instead of input and output.</p>\n<p>While it\u2019s highly recommended to read research articles to gain more detail,\na characteristic property of this process is tremendously useful for reducing\ndataset in high dimension down to lower dimension.</p>\n<p>The central idea is given the input can be loyally or nearly loyally\nreproduced the neural network can be expected to do two things:</p>\n<ul>\n<li>one-to-one mapping input to output; A different input results in a different\noutput.</li>\n<li>Similar inputs lead to close outputs.</li>\n</ul>\n<p>These two characteristics ensure clustering done in low-dimensional space\nis equivalent to do the clustering in high dimension.</p>\n<p>A picture is worth a thousand words. The chart below is the result of a three\nlayer neural network:</p>\n<img alt=\"tests/readme_pics/leveraged_kmeans.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/25b0d6a1b8b70134c054a1aae0258c67aaec20cd/74657374732f726561646d655f706963732f6c65766572616765645f6b6d65616e732e706e67\">\n<p>The magic lies in how it looks in the mapped one dimensional space:</p>\n<img alt=\"tests/readme_pics/hist.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/aa0d806bc3194f7333ef044dd175e85d8c5c9f48/74657374732f726561646d655f706963732f686973742e706e67\">\n<p>Without too much explanation, the left blue bars are the from the right blue\npoints and the right red bars from the left red points. As there isn\u2019t\nconcept for a sphere in 1d, K-Means works perfectly.</p>\n</div>\n<div id=\"what-s-in-the-package-and-what-s-coming-up\">\n<h2>What\u2019s in the package and what\u2019s coming up?</h2>\n<p>At the moment, you can do the following with the tools provided by the\npackage:</p>\n<ul>\n<li>Dimension reduction to 1d.</li>\n<li>Clustering in 1d and assign cluster ids back to the original dataset.</li>\n<li>Certain useful plotting tools to display the efficiency of the clustering.</li>\n</ul>\n<p>This project is still in alpha stage. In the imminent release, you can expect</p>\n<ul>\n<li>Tools to searching for the drivers that distinct the clusters.</li>\n<li>More support in Python versions.</li>\n</ul>\n</div>\n<div id=\"note\">\n<h2>Note</h2>\n<p>This project has been set up using PyScaffold 2.5.7. For details and usage\ninformation on PyScaffold see <a href=\"http://pyscaffold.readthedocs.org/\" rel=\"nofollow\">http://pyscaffold.readthedocs.org/</a>.</p>\n</div>\n\n          </div>"}, "last_serial": 2515815, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "420a9f869d3e408011ae244f93854fa2", "sha256": "650c01f279a72279ae3c9b1863ccc94940bde5854d3813c2450770930874d3a6"}, "downloads": -1, "filename": "clusteror-0.0.1.tar.gz", "has_sig": false, "md5_digest": "420a9f869d3e408011ae244f93854fa2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2259721, "upload_time": "2016-12-08T01:23:37", "upload_time_iso_8601": "2016-12-08T01:23:37.510052Z", "url": "https://files.pythonhosted.org/packages/67/23/62349c9fc766ead2928bb565b8cf3385a34a39e0e63d006651678d3d6c01/clusteror-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "d677de10b05f5cf55f00b088e11ab48c", "sha256": "7b1686679eb142c0a2fcdd5bff0b5318f0f1bd2a82e12eeb73231eb87f42afae"}, "downloads": -1, "filename": "clusteror-0.0.2.tar.gz", "has_sig": false, "md5_digest": "d677de10b05f5cf55f00b088e11ab48c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2258519, "upload_time": "2016-12-13T06:34:29", "upload_time_iso_8601": "2016-12-13T06:34:29.823892Z", "url": "https://files.pythonhosted.org/packages/78/21/b8f05ca9015962b2d53bb1ad099dd7e9b05c097fdd3be95943261efa25d6/clusteror-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d677de10b05f5cf55f00b088e11ab48c", "sha256": "7b1686679eb142c0a2fcdd5bff0b5318f0f1bd2a82e12eeb73231eb87f42afae"}, "downloads": -1, "filename": "clusteror-0.0.2.tar.gz", "has_sig": false, "md5_digest": "d677de10b05f5cf55f00b088e11ab48c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2258519, "upload_time": "2016-12-13T06:34:29", "upload_time_iso_8601": "2016-12-13T06:34:29.823892Z", "url": "https://files.pythonhosted.org/packages/78/21/b8f05ca9015962b2d53bb1ad099dd7e9b05c097fdd3be95943261efa25d6/clusteror-0.0.2.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:18:39 2020"}