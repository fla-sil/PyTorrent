{"info": {"author": "Joy Ghosh", "author_email": "joyghosh826@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Build Tools"], "description": "=================\nU.S.News-Scrapper\n=================\n\nU.S.News Scrapper is a Python library that collect data from the website of usnews_ and output those data in a file for offline usage. Till now, it is only capable of collecting graduate schools data and output it in .xls format. After generating the .xls file, it will be opened by default excel file opener.\n*Visit github_ page for detailed informations.*\n\nSetup\n=====\n*Visit github_ page for detailed informations.*\n\n    | $ pip install usnews_scrapper\n\n\nUsage\n=====\nusage: python usnews_scrapper.py [-h] -u URL [-o OUTPUTFILENAME] [-p PAUSETIME] [--from STARTPAGE] [--to ENDPAGE]\n\nCollects data from usnews and generates excel file\n\noptional arguments:\n-h, --help            \t\t        Show this help message and exit\n-u URL, --url URL     \t\t        The usnews address to collect data from. Put the URL within qoutes i.e. \" or ' .\n-o OUTPUTFILENAME     \t\t        The output file name without extension.\n-p PAUSETIME, --pause PAUSETIME             The pause time between loading pages from usnews.\n--from STARTPAGE      \t\t        The page number from which the scrapper starts working.\n--to ENDPAGE          \t\t        The page number to which the scrapper works.\n\n\nExamples\n========\n\nCopy the address of the page from usnews website and in the Command Prompt and enter this command -\n\n    | $ cd USNews-Scrapper\n    | $ python usnews_scrapper.py --url=\"https://www.usnews.com/best-graduate-schools/top-science-schools/computer-science-rankings\" -o file_name -p 2 --from=2 --to=5 \n\nThe output file will be saved in current directory under the name of file_name_*.xls \n\nAuthors\n=======\n\n* *Joy Ghosh* - www.ijoyghosh.com_\n\n.. _usnews: https://www.usnews.com/best-graduate-schools\n.. _pip: https://pip.pypa.io/en/stable/\n.. _www.ijoyghosh.com : https://www.ijoyghosh.com\n.. _github : https://github.com/OvroAbir/USNews-Scrapper", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/OvroAbir/USNews-Scrapper/archive/v0.3.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://www.ijoyghosh.com", "keywords": "scraper,usnews,graduate,grad,school,university,crawler", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "usnews-scrapper", "package_url": "https://pypi.org/project/usnews-scrapper/", "platform": "", "project_url": "https://pypi.org/project/usnews-scrapper/", "project_urls": {"Download": "https://github.com/OvroAbir/USNews-Scrapper/archive/v0.3.tar.gz", "Homepage": "https://www.ijoyghosh.com"}, "release_url": "https://pypi.org/project/usnews-scrapper/v0.1/", "requires_dist": null, "requires_python": "", "summary": "Collects Grad School data from https://www.usnews.com and gives output in a .xls file.", "version": "v0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>U.S.News Scrapper is a Python library that collect data from the website of <a href=\"https://www.usnews.com/best-graduate-schools\" rel=\"nofollow\">usnews</a> and output those data in a file for offline usage. Till now, it is only capable of collecting graduate schools data and output it in .xls format. After generating the .xls file, it will be opened by default excel file opener.\n<em>Visit github_ page for detailed informations.</em></p>\n<div id=\"setup\">\n<h2>Setup</h2>\n<p><em>Visit github_ page for detailed informations.</em></p>\n<blockquote>\n<div>\n<div>$ pip install usnews_scrapper</div>\n</div>\n</blockquote>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>usage: python usnews_scrapper.py [-h] -u URL [-o OUTPUTFILENAME] [-p PAUSETIME] [\u2013from STARTPAGE] [\u2013to ENDPAGE]</p>\n<p>Collects data from usnews and generates excel file</p>\n<p>optional arguments:\n-h, \u2013help                              Show this help message and exit\n-u URL, \u2013url URL                       The usnews address to collect data from. Put the URL within qoutes i.e. \u201d or \u2018 .\n-o OUTPUTFILENAME                       The output file name without extension.\n-p PAUSETIME, \u2013pause PAUSETIME             The pause time between loading pages from usnews.\n\u2013from STARTPAGE                        The page number from which the scrapper starts working.\n\u2013to ENDPAGE                            The page number to which the scrapper works.</p>\n</div>\n<div id=\"examples\">\n<h2>Examples</h2>\n<p>Copy the address of the page from usnews website and in the Command Prompt and enter this command -</p>\n<blockquote>\n<div>\n<div>$ cd USNews-Scrapper</div>\n<div>$ python usnews_scrapper.py \u2013url=\u201d<a href=\"https://www.usnews.com/best-graduate-schools/top-science-schools/computer-science-rankings\" rel=\"nofollow\">https://www.usnews.com/best-graduate-schools/top-science-schools/computer-science-rankings</a>\u201d -o file_name -p 2 \u2013from=2 \u2013to=5</div>\n</div>\n</blockquote>\n<p>The output file will be saved in current directory under the name of file_name_*.xls</p>\n</div>\n<div id=\"authors\">\n<h2>Authors</h2>\n<ul>\n<li><em>Joy Ghosh</em> - <a href=\"https://www.ijoyghosh.com\" rel=\"nofollow\">www.ijoyghosh.com</a></li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 5487407, "releases": {"v0.1": [{"comment_text": "", "digests": {"md5": "2fc88dee57f4604604267e5d77fc52b7", "sha256": "61d9a1ec00ca65a1302ecda5ae5e567fedd0a46f2de60e6ab1640ce8c519fb10"}, "downloads": -1, "filename": "usnews_scrapper-v0.1.tar.gz", "has_sig": false, "md5_digest": "2fc88dee57f4604604267e5d77fc52b7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5882, "upload_time": "2019-07-04T15:57:58", "upload_time_iso_8601": "2019-07-04T15:57:58.602146Z", "url": "https://files.pythonhosted.org/packages/c3/88/cc60a244d2d047bcc1532275f8e244f59e59dca237737f01bb6d0396ce01/usnews_scrapper-v0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2fc88dee57f4604604267e5d77fc52b7", "sha256": "61d9a1ec00ca65a1302ecda5ae5e567fedd0a46f2de60e6ab1640ce8c519fb10"}, "downloads": -1, "filename": "usnews_scrapper-v0.1.tar.gz", "has_sig": false, "md5_digest": "2fc88dee57f4604604267e5d77fc52b7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5882, "upload_time": "2019-07-04T15:57:58", "upload_time_iso_8601": "2019-07-04T15:57:58.602146Z", "url": "https://files.pythonhosted.org/packages/c3/88/cc60a244d2d047bcc1532275f8e244f59e59dca237737f01bb6d0396ce01/usnews_scrapper-v0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:39:00 2020"}