{"info": {"author": "Andrew Wrigley", "author_email": "andrew@wrigley.io", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: MacOS :: MacOS X", "Operating System :: POSIX :: Linux", "Operating System :: Unix", "Programming Language :: C++", "Programming Language :: Python :: 3 :: Only", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# Tensor Belief Propagation \n\n[![Build Status](https://travis-ci.org/akxlr/tbp.svg?branch=master)](https://travis-ci.org/akxlr/tbp)\n\n[Tensor Belief Propagation](http://proceedings.mlr.press/v70/wrigley17a/wrigley17a.pdf) (TBP) is an experimental algorithm for\napproximate inference in discrete graphical models [1]. It takes a factor graph in [.uai](#other-file-formats) or [.fg](#other-file-formats) format and outputs approximate marginals for\neach variable.\n\n[1] [Wrigley, Andrew, Wee Sun Lee, and Nan Ye. \"Tensor Belief Propagation.\" International Conference on Machine Learning. 2017.](http://proceedings.mlr.press/v70/wrigley17a/wrigley17a.pdf)\n\n## Requirements\n\n * Linux or OSX\n * Python 3.6+\n\n## Installation\n\nInstall libDAI prerequisites:\n```\n# Linux\n$ sudo apt-get install g++ make doxygen graphviz libboost-dev libboost-graph-dev libboost-program-options-dev libboost-test-dev libgmp-dev cimg-dev\n\n# OSX\n$ brew install boost gmp doxygen graphviz\n```\n\nInstall tbp with the Python package manager `pip`:\n```bash\n$ pip install tbp\n...\nSuccessfully installed tbp-X.X.X\n```\nThis will take a while as libDAI must be compiled.\n\n## Usage\nTBP takes a factor graph in either [.fg](#other-file-formats) or [.uai](#other-file-formats) format as input, and\noutputs the approximate marginal distribution of each variable in [.MAR format](#other-file-formats).\nThis involves two steps \u2014 first, all potential functions in the graph must be decomposed\ninto sums of rank-1 tensors yielding a decomposed factor graph (.dfg). Then, the\nmessage passing procedure must be run on the decomposed graph to give approximate marginals.\n\n### Command line\nAfter installation, the command line utility `tbp` is available to do either or both of these steps. For usage\ninstructions, run `tbp --help`. \n\n#### Examples\n\nDecompose the factor graph `ising_8x8.fg` and find marginals:\n```bash\n$ tbp tests/ising_8x8.fg\n64 2 0.594961 0.405039 2 ... 0.608573 0.391427\n```\n\nDecompose input potentials into 3 rank-1 components and save the resulting decomposed graph (but don't find marginals):\n```bash\n$ tbp tests/ising_8x8.fg -r 3 -o tests/ising_8x8.dfg --verbosity 2\nReading graph tests/ising_8x8.fg (libDAI format)...\nDecomposing input graph (r=3 terms per factor)...\nSuccessfully saved decomposed graph to tests/ising_8x8.dfg.\n```\n\nDecompose the factor graph `Promedus_11.uai` after applying some evidence, find marginals using TBP with sample size of 1000, and save the output\nto `out.MAR`:\n```bash\n$ tbp tests/uai/MAR_prob/Promedus_11.uai -e tests/uai/MAR_prob/Promedus_11.uai.evid -k 1000 -o out.MAR --verbosity 2\nReading graph tests/uai/MAR_prob/Promedus_11.uai (UAI format)...\nApplying evidence file tests/uai/MAR_prob/Promedus_11.uai.evid...\nDecomposing input graph (r=4 terms per factor)...\nRunning TBP with sample size K=1000...\nSuccessfully saved marginals to out.MAR.\n```\n\n\n### Python library\nThe `tbp` package can also be used directly from Python, for example:\n\n```python\nimport tbp\n\n# Load a factor graph in .uai format\ng = tbp.load_uai_graph('tests/uai/MAR_prob/linkage_11.uai')\n\n# Apply evidence (fixed variable assignments)\ng.apply_evidence('tests/uai/MAR_prob/linkage_11.uai.evid')\n\n# Decompose each factor into a weighted sum of 4 rank-1 tensors\ndg = g.decompose(r=4)\n\n# Run TBP to find marginals with sample size of 10000\nmar = dg.tbp_marg(K=10000)\n\n```\n\n### Troubleshooting\n#### Installing into a virtual environment\nIf `pip install` has issues with dependencies or version conflicts, you can install the necessary\n packages into a virtual environment (a project-specific folder rather than globally on your system):\n```bash\n$ sudo pip3 install virtualenv  # pip or pip3, depending on your system\n$ virtualenv -p python3 venv    # create venv folder to store packages\n$ source venv/bin/activate      # activate virtual environment\n$ pip install tbp               # install tbp into venv folder\n```\nNow when you invoke `tbp`, the local versions will be used.\n\n#### Building from GitHub clone\nTo use the `tbp` Python package from source without installation via `pip install`, libDAI must first be compiled:\n \n```bash\n$ git clone git@github.com:akxlr/tbp.git\n$ cd tbp/libdai\n$ cp Makefile.<platform> Makefile.conf  # Choose <platform> according to your platform\n$ make\n...\nlibDAI built successfully!\n```\nThis produces a utility `libdai/utils/dfgmarg` which is symlinked from `tbp/dfgmarg` and used during inference. See [libDAI README](libdai/README) for full installation instructions.\n\n\n## Using MATLAB for the decomposition\nThe decomposition of potential functions uses the non-negative CP decomposition algorithm in the Tensorly tensor \nlibrary. As an alternative to TensorLy, the [MATLAB Tensor Toolbox](http://www.sandia.gov/~tgkolda/TensorToolbox) can be used\n(this was what we used in [1]). To use this instead of Tensorly:\n\n * Install MATLAB\n * Install the\n [MATLAB Python API](https://www.mathworks.com/help/matlab/matlab_external/install-the-matlab-engine-for-python.html)\n * Install the [MATLAB Tensor Toolbox](http://www.sandia.gov/~tgkolda/TensorToolbox)\n \nYou can now replace `method='tensorly'` with `method='matlab'` when calling decomposition functions in [core.py](python/tbp/core.py).\n\n\n## File formats\n\n### .dfg (decomposed factor graph)\nWe created the `.dfg` file format based on\n[libDAI's .fg file format](https://staff.fnwi.uva.nl/j.m.mooij/libDAI/doc/fileformats.html)\nto represent decomposed factor graphs. A decomposed factor graph is a\nfactor graph with all factors represented as sums of rank-1 tensors rather than multidimensional tables.\n\nThe first line of a `.dfg` file contains the number of factors in the graph, followed by a blank line. Then, factors\nare described in turn by blocks separated by a single blank line. Each factor block is structured as follows:\n\n     1. n_terms\n     2. <weights>\n     3. n_variables\n     4. <variable indices>\n     5. <variable cardinalities>\n     6. n_nonzero_1\n     7. 1 0.5\n     8. 3 0.1\n     9. 4 0.1\n    10. ...\n    11. n_nonzero_2\n    12. 1 0.5\n    13. 3 0.1\n    14. 4 0.3\n    15. ...\n\nIn the header section of the factor block (lines 1-5), `n_terms` is the number of terms in the decomposition and\n`<weights>`, `<variable indices>` and `<variable cardinalities>` are self-explanatory space-separated lists of length `n_terms`,\n`n_variables` and `n_variables` respectively. \n\nThe remainder of the factor block (line 6 onwards) describes\na series of `n_variables` 2D matrices that together describe the `n_terms` rank-1 tensors.\nEach matrix corresponds to a single variable and has shape `(cardinality, n_terms)`, where `cardinality` is\nthe cardinality of the variable and `n_terms` is the number of rank-1 terms in the decomposition (constant\nfor all variables). Each matrix begins with the\nnumber of nonzero values in the matrix, followed by a series of `index value` pairs describing the nonzero\nentries of the matrix in column-major order. See\n[libDAI's documentation](https://staff.fnwi.uva.nl/j.m.mooij/libDAI/doc/fileformats.html) for examples of how to\nreshape these lists back into matrices.\n\n The *i*th rank-1 tensor is constructed by taking the outer product of the *i*th columns of\nall matrices. The complete factor is then reconstructed by adding up these rank-1 tensors and weighting\naccording to `<weights>`.\n\n### Other file formats\nOther file formats used in this project are:\n\n * `.fg` (libDAI factor graph): https://staff.fnwi.uva.nl/j.m.mooij/libDAI/doc/fileformats.html\n * `.uai` (UAI factor graph): http://www.hlt.utdallas.edu/~vgogate/uai14-competition/modelformat.html\n * `.MAR` (marginals): http://www.hlt.utdallas.edu/~vgogate/uai14-competition/resformat.html\n * `.evid` (evidence): http://www.hlt.utdallas.edu/~vgogate/uai14-competition/evidformat.html\n\n## To do\n * ICML experiments - finish cleaning code used for experiments (see `icml17.py` for partial code)\n * Rewrite code that loads .uai files to handle all problems (currently breaks on some)\n * Deal with Z <= 0 warning from C++ code\n * Clean up C++ code and compiler warnings\n * Add more tests\n\n## Feedback\nBug reports, suggestions and comments are welcome. Please email [andrew@wrigley.io](mailto:andrew@wrigley.io) or use the issue tracker.\n\n## License\n\nSee [LICENSE.txt](LICENSE.txt) (MIT).\n\n## Acknowledgments\n\n* [libDAI](https://staff.fnwi.uva.nl/j.m.mooij/libDAI/) (included in [libdai](libdai) folder with modifications; libDAI's junction tree implementation is used for the message passing step)\n* [Eigen](http://eigen.tuxfamily.org/) (version 3.3.4 included in [libdai/vendor/include](libdai/vendor/include) folder)\n* [TensorLy](https://github.com/tensorly/tensorly) (used to perform initial non-negative CP decomposition of potential functions)\n* [MATLAB Tensor Toolbox](http://www.sandia.gov/~tgkolda/TensorToolbox)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/akxlr/tbp/archive/0.1.1.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/akxlr/tbp", "keywords": "tensor,belief,propagation,tensor belief propagation,graphical models,machine learning,inference", "license": "MIT", "maintainer": "Andrew Wrigley", "maintainer_email": "andrew@wrigley.io", "name": "tbp", "package_url": "https://pypi.org/project/tbp/", "platform": "", "project_url": "https://pypi.org/project/tbp/", "project_urls": {"Download": "https://github.com/akxlr/tbp/archive/0.1.1.tar.gz", "Homepage": "https://github.com/akxlr/tbp"}, "release_url": "https://pypi.org/project/tbp/0.1.1/", "requires_dist": null, "requires_python": "", "summary": "Tensor Belief Propagation - algorithm for approximate inference in discrete graphical models", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Tensor Belief Propagation</h1>\n<p><a href=\"https://travis-ci.org/akxlr/tbp\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2915c1ec2c1ee2dc3e9008bfd6a87821e22bfdd9/68747470733a2f2f7472617669732d63692e6f72672f616b786c722f7462702e7376673f6272616e63683d6d6173746572\"></a></p>\n<p><a href=\"http://proceedings.mlr.press/v70/wrigley17a/wrigley17a.pdf\" rel=\"nofollow\">Tensor Belief Propagation</a> (TBP) is an experimental algorithm for\napproximate inference in discrete graphical models [1]. It takes a factor graph in <a href=\"#other-file-formats\" rel=\"nofollow\">.uai</a> or <a href=\"#other-file-formats\" rel=\"nofollow\">.fg</a> format and outputs approximate marginals for\neach variable.</p>\n<p>[1] <a href=\"http://proceedings.mlr.press/v70/wrigley17a/wrigley17a.pdf\" rel=\"nofollow\">Wrigley, Andrew, Wee Sun Lee, and Nan Ye. \"Tensor Belief Propagation.\" International Conference on Machine Learning. 2017.</a></p>\n<h2>Requirements</h2>\n<ul>\n<li>Linux or OSX</li>\n<li>Python 3.6+</li>\n</ul>\n<h2>Installation</h2>\n<p>Install libDAI prerequisites:</p>\n<pre><code># Linux\n$ sudo apt-get install g++ make doxygen graphviz libboost-dev libboost-graph-dev libboost-program-options-dev libboost-test-dev libgmp-dev cimg-dev\n\n# OSX\n$ brew install boost gmp doxygen graphviz\n</code></pre>\n<p>Install tbp with the Python package manager <code>pip</code>:</p>\n<pre>$ pip install tbp\n...\nSuccessfully installed tbp-X.X.X\n</pre>\n<p>This will take a while as libDAI must be compiled.</p>\n<h2>Usage</h2>\n<p>TBP takes a factor graph in either <a href=\"#other-file-formats\" rel=\"nofollow\">.fg</a> or <a href=\"#other-file-formats\" rel=\"nofollow\">.uai</a> format as input, and\noutputs the approximate marginal distribution of each variable in <a href=\"#other-file-formats\" rel=\"nofollow\">.MAR format</a>.\nThis involves two steps \u2014 first, all potential functions in the graph must be decomposed\ninto sums of rank-1 tensors yielding a decomposed factor graph (.dfg). Then, the\nmessage passing procedure must be run on the decomposed graph to give approximate marginals.</p>\n<h3>Command line</h3>\n<p>After installation, the command line utility <code>tbp</code> is available to do either or both of these steps. For usage\ninstructions, run <code>tbp --help</code>.</p>\n<h4>Examples</h4>\n<p>Decompose the factor graph <code>ising_8x8.fg</code> and find marginals:</p>\n<pre>$ tbp tests/ising_8x8.fg\n<span class=\"m\">64</span> <span class=\"m\">2</span> <span class=\"m\">0</span>.594961 <span class=\"m\">0</span>.405039 <span class=\"m\">2</span> ... <span class=\"m\">0</span>.608573 <span class=\"m\">0</span>.391427\n</pre>\n<p>Decompose input potentials into 3 rank-1 components and save the resulting decomposed graph (but don't find marginals):</p>\n<pre>$ tbp tests/ising_8x8.fg -r <span class=\"m\">3</span> -o tests/ising_8x8.dfg --verbosity <span class=\"m\">2</span>\nReading graph tests/ising_8x8.fg <span class=\"o\">(</span>libDAI format<span class=\"o\">)</span>...\nDecomposing input graph <span class=\"o\">(</span><span class=\"nv\">r</span><span class=\"o\">=</span><span class=\"m\">3</span> terms per factor<span class=\"o\">)</span>...\nSuccessfully saved decomposed graph to tests/ising_8x8.dfg.\n</pre>\n<p>Decompose the factor graph <code>Promedus_11.uai</code> after applying some evidence, find marginals using TBP with sample size of 1000, and save the output\nto <code>out.MAR</code>:</p>\n<pre>$ tbp tests/uai/MAR_prob/Promedus_11.uai -e tests/uai/MAR_prob/Promedus_11.uai.evid -k <span class=\"m\">1000</span> -o out.MAR --verbosity <span class=\"m\">2</span>\nReading graph tests/uai/MAR_prob/Promedus_11.uai <span class=\"o\">(</span>UAI format<span class=\"o\">)</span>...\nApplying evidence file tests/uai/MAR_prob/Promedus_11.uai.evid...\nDecomposing input graph <span class=\"o\">(</span><span class=\"nv\">r</span><span class=\"o\">=</span><span class=\"m\">4</span> terms per factor<span class=\"o\">)</span>...\nRunning TBP with sample size <span class=\"nv\">K</span><span class=\"o\">=</span><span class=\"m\">1000</span>...\nSuccessfully saved marginals to out.MAR.\n</pre>\n<h3>Python library</h3>\n<p>The <code>tbp</code> package can also be used directly from Python, for example:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">tbp</span>\n\n<span class=\"c1\"># Load a factor graph in .uai format</span>\n<span class=\"n\">g</span> <span class=\"o\">=</span> <span class=\"n\">tbp</span><span class=\"o\">.</span><span class=\"n\">load_uai_graph</span><span class=\"p\">(</span><span class=\"s1\">'tests/uai/MAR_prob/linkage_11.uai'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Apply evidence (fixed variable assignments)</span>\n<span class=\"n\">g</span><span class=\"o\">.</span><span class=\"n\">apply_evidence</span><span class=\"p\">(</span><span class=\"s1\">'tests/uai/MAR_prob/linkage_11.uai.evid'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Decompose each factor into a weighted sum of 4 rank-1 tensors</span>\n<span class=\"n\">dg</span> <span class=\"o\">=</span> <span class=\"n\">g</span><span class=\"o\">.</span><span class=\"n\">decompose</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run TBP to find marginals with sample size of 10000</span>\n<span class=\"n\">mar</span> <span class=\"o\">=</span> <span class=\"n\">dg</span><span class=\"o\">.</span><span class=\"n\">tbp_marg</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"o\">=</span><span class=\"mi\">10000</span><span class=\"p\">)</span>\n</pre>\n<h3>Troubleshooting</h3>\n<h4>Installing into a virtual environment</h4>\n<p>If <code>pip install</code> has issues with dependencies or version conflicts, you can install the necessary\npackages into a virtual environment (a project-specific folder rather than globally on your system):</p>\n<pre>$ sudo pip3 install virtualenv  <span class=\"c1\"># pip or pip3, depending on your system</span>\n$ virtualenv -p python3 venv    <span class=\"c1\"># create venv folder to store packages</span>\n$ <span class=\"nb\">source</span> venv/bin/activate      <span class=\"c1\"># activate virtual environment</span>\n$ pip install tbp               <span class=\"c1\"># install tbp into venv folder</span>\n</pre>\n<p>Now when you invoke <code>tbp</code>, the local versions will be used.</p>\n<h4>Building from GitHub clone</h4>\n<p>To use the <code>tbp</code> Python package from source without installation via <code>pip install</code>, libDAI must first be compiled:</p>\n<pre>$ git clone git@github.com:akxlr/tbp.git\n$ <span class=\"nb\">cd</span> tbp/libdai\n$ cp Makefile.&lt;platform&gt; Makefile.conf  <span class=\"c1\"># Choose &lt;platform&gt; according to your platform</span>\n$ make\n...\nlibDAI built successfully!\n</pre>\n<p>This produces a utility <code>libdai/utils/dfgmarg</code> which is symlinked from <code>tbp/dfgmarg</code> and used during inference. See <a href=\"libdai/README\" rel=\"nofollow\">libDAI README</a> for full installation instructions.</p>\n<h2>Using MATLAB for the decomposition</h2>\n<p>The decomposition of potential functions uses the non-negative CP decomposition algorithm in the Tensorly tensor\nlibrary. As an alternative to TensorLy, the <a href=\"http://www.sandia.gov/%7Etgkolda/TensorToolbox\" rel=\"nofollow\">MATLAB Tensor Toolbox</a> can be used\n(this was what we used in [1]). To use this instead of Tensorly:</p>\n<ul>\n<li>Install MATLAB</li>\n<li>Install the\n<a href=\"https://www.mathworks.com/help/matlab/matlab_external/install-the-matlab-engine-for-python.html\" rel=\"nofollow\">MATLAB Python API</a></li>\n<li>Install the <a href=\"http://www.sandia.gov/%7Etgkolda/TensorToolbox\" rel=\"nofollow\">MATLAB Tensor Toolbox</a></li>\n</ul>\n<p>You can now replace <code>method='tensorly'</code> with <code>method='matlab'</code> when calling decomposition functions in <a href=\"python/tbp/core.py\" rel=\"nofollow\">core.py</a>.</p>\n<h2>File formats</h2>\n<h3>.dfg (decomposed factor graph)</h3>\n<p>We created the <code>.dfg</code> file format based on\n<a href=\"https://staff.fnwi.uva.nl/j.m.mooij/libDAI/doc/fileformats.html\" rel=\"nofollow\">libDAI's .fg file format</a>\nto represent decomposed factor graphs. A decomposed factor graph is a\nfactor graph with all factors represented as sums of rank-1 tensors rather than multidimensional tables.</p>\n<p>The first line of a <code>.dfg</code> file contains the number of factors in the graph, followed by a blank line. Then, factors\nare described in turn by blocks separated by a single blank line. Each factor block is structured as follows:</p>\n<pre><code> 1. n_terms\n 2. &lt;weights&gt;\n 3. n_variables\n 4. &lt;variable indices&gt;\n 5. &lt;variable cardinalities&gt;\n 6. n_nonzero_1\n 7. 1 0.5\n 8. 3 0.1\n 9. 4 0.1\n10. ...\n11. n_nonzero_2\n12. 1 0.5\n13. 3 0.1\n14. 4 0.3\n15. ...\n</code></pre>\n<p>In the header section of the factor block (lines 1-5), <code>n_terms</code> is the number of terms in the decomposition and\n<code>&lt;weights&gt;</code>, <code>&lt;variable indices&gt;</code> and <code>&lt;variable cardinalities&gt;</code> are self-explanatory space-separated lists of length <code>n_terms</code>,\n<code>n_variables</code> and <code>n_variables</code> respectively.</p>\n<p>The remainder of the factor block (line 6 onwards) describes\na series of <code>n_variables</code> 2D matrices that together describe the <code>n_terms</code> rank-1 tensors.\nEach matrix corresponds to a single variable and has shape <code>(cardinality, n_terms)</code>, where <code>cardinality</code> is\nthe cardinality of the variable and <code>n_terms</code> is the number of rank-1 terms in the decomposition (constant\nfor all variables). Each matrix begins with the\nnumber of nonzero values in the matrix, followed by a series of <code>index value</code> pairs describing the nonzero\nentries of the matrix in column-major order. See\n<a href=\"https://staff.fnwi.uva.nl/j.m.mooij/libDAI/doc/fileformats.html\" rel=\"nofollow\">libDAI's documentation</a> for examples of how to\nreshape these lists back into matrices.</p>\n<p>The <em>i</em>th rank-1 tensor is constructed by taking the outer product of the <em>i</em>th columns of\nall matrices. The complete factor is then reconstructed by adding up these rank-1 tensors and weighting\naccording to <code>&lt;weights&gt;</code>.</p>\n<h3>Other file formats</h3>\n<p>Other file formats used in this project are:</p>\n<ul>\n<li><code>.fg</code> (libDAI factor graph): <a href=\"https://staff.fnwi.uva.nl/j.m.mooij/libDAI/doc/fileformats.html\" rel=\"nofollow\">https://staff.fnwi.uva.nl/j.m.mooij/libDAI/doc/fileformats.html</a></li>\n<li><code>.uai</code> (UAI factor graph): <a href=\"http://www.hlt.utdallas.edu/%7Evgogate/uai14-competition/modelformat.html\" rel=\"nofollow\">http://www.hlt.utdallas.edu/~vgogate/uai14-competition/modelformat.html</a></li>\n<li><code>.MAR</code> (marginals): <a href=\"http://www.hlt.utdallas.edu/%7Evgogate/uai14-competition/resformat.html\" rel=\"nofollow\">http://www.hlt.utdallas.edu/~vgogate/uai14-competition/resformat.html</a></li>\n<li><code>.evid</code> (evidence): <a href=\"http://www.hlt.utdallas.edu/%7Evgogate/uai14-competition/evidformat.html\" rel=\"nofollow\">http://www.hlt.utdallas.edu/~vgogate/uai14-competition/evidformat.html</a></li>\n</ul>\n<h2>To do</h2>\n<ul>\n<li>ICML experiments - finish cleaning code used for experiments (see <code>icml17.py</code> for partial code)</li>\n<li>Rewrite code that loads .uai files to handle all problems (currently breaks on some)</li>\n<li>Deal with Z &lt;= 0 warning from C++ code</li>\n<li>Clean up C++ code and compiler warnings</li>\n<li>Add more tests</li>\n</ul>\n<h2>Feedback</h2>\n<p>Bug reports, suggestions and comments are welcome. Please email <a href=\"mailto:andrew@wrigley.io\">andrew@wrigley.io</a> or use the issue tracker.</p>\n<h2>License</h2>\n<p>See <a href=\"LICENSE.txt\" rel=\"nofollow\">LICENSE.txt</a> (MIT).</p>\n<h2>Acknowledgments</h2>\n<ul>\n<li><a href=\"https://staff.fnwi.uva.nl/j.m.mooij/libDAI/\" rel=\"nofollow\">libDAI</a> (included in <a href=\"libdai\" rel=\"nofollow\">libdai</a> folder with modifications; libDAI's junction tree implementation is used for the message passing step)</li>\n<li><a href=\"http://eigen.tuxfamily.org/\" rel=\"nofollow\">Eigen</a> (version 3.3.4 included in <a href=\"libdai/vendor/include\" rel=\"nofollow\">libdai/vendor/include</a> folder)</li>\n<li><a href=\"https://github.com/tensorly/tensorly\" rel=\"nofollow\">TensorLy</a> (used to perform initial non-negative CP decomposition of potential functions)</li>\n<li><a href=\"http://www.sandia.gov/%7Etgkolda/TensorToolbox\" rel=\"nofollow\">MATLAB Tensor Toolbox</a></li>\n</ul>\n\n          </div>"}, "last_serial": 6644283, "releases": {"0.0.38": [{"comment_text": "", "digests": {"md5": "91d44565d037e63bc2c5c802be5d9979", "sha256": "dd3046aa8c5b2fa505456699bbc5980151b656707a5bb85d4e92d561997f8091"}, "downloads": -1, "filename": "tbp-0.0.38.tar.gz", "has_sig": false, "md5_digest": "91d44565d037e63bc2c5c802be5d9979", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11209237, "upload_time": "2018-01-04T08:45:32", "upload_time_iso_8601": "2018-01-04T08:45:32.906647Z", "url": "https://files.pythonhosted.org/packages/ff/b4/99b46da11ca951f5ab863323834ac966a047adcc0547d4dc66f36a3bd86f/tbp-0.0.38.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "2be8372dea901da15d3423bb50771e3a", "sha256": "b4f55da176401bebdb9cc152f2d8ae9c8baee8fd14d8bf080254fd815b6833bb"}, "downloads": -1, "filename": "tbp-0.1.0.tar.gz", "has_sig": false, "md5_digest": "2be8372dea901da15d3423bb50771e3a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11204930, "upload_time": "2018-01-16T07:23:49", "upload_time_iso_8601": "2018-01-16T07:23:49.779469Z", "url": "https://files.pythonhosted.org/packages/52/3d/1d6537f0def3084ca7dcd5d4758d752d16651267ac80a0dbb00538311772/tbp-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "ad4d2635b660b8d594dfec4ff5deb77e", "sha256": "7473045c4cf55eb22b81ab194de0c11e9cdb54256bdbca685bc6c081079c5bf3"}, "downloads": -1, "filename": "tbp-0.1.1.tar.gz", "has_sig": false, "md5_digest": "ad4d2635b660b8d594dfec4ff5deb77e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 46162700, "upload_time": "2020-02-17T06:19:51", "upload_time_iso_8601": "2020-02-17T06:19:51.447009Z", "url": "https://files.pythonhosted.org/packages/c3/13/6d45fc51fefb3a579a0f7ea2ebe283c64754a9e47f62ea8cbbd1539fb50d/tbp-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ad4d2635b660b8d594dfec4ff5deb77e", "sha256": "7473045c4cf55eb22b81ab194de0c11e9cdb54256bdbca685bc6c081079c5bf3"}, "downloads": -1, "filename": "tbp-0.1.1.tar.gz", "has_sig": false, "md5_digest": "ad4d2635b660b8d594dfec4ff5deb77e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 46162700, "upload_time": "2020-02-17T06:19:51", "upload_time_iso_8601": "2020-02-17T06:19:51.447009Z", "url": "https://files.pythonhosted.org/packages/c3/13/6d45fc51fefb3a579a0f7ea2ebe283c64754a9e47f62ea8cbbd1539fb50d/tbp-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:57:28 2020"}