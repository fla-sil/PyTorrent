{"info": {"author": "Nils Reimers, Gregor Geigle", "author_email": "Rnils@web.de", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# Sentence Transformers: Multilingual Sentence Embeddings using BERT / RoBERTa / XLM-RoBERTa & Co. with PyTorch\nBERT / RoBERTa / XLM-RoBERTa produces out-of-the-box rather bad sentence embeddings. This repository fine-tunes BERT / RoBERTa / DistilBERT / ALBERT / XLNet with a siamese or triplet network structure to produce semantically meaningful sentence embeddings that can be used in unsupervised scenarios: Semantic textual similarity via cosine-similarity, clustering, semantic search.\n\n\nWe provide an increasing number of **state-of-the-art pretrained models** that can be used to derive sentence embeddings. See [Pretrained Models](#pretrained-models). Details of the implemented approaches can be found in our publication: [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (EMNLP 2019).\n\n\nYou can use this code to easily **train your own sentence embeddings**, that are tuned for your specific task. We provide various dataset readers and you can tune sentence embeddings with different loss function, depending on the structure of your dataset. For further details, see [Train your own Sentence Embeddings](#Training).\n\n\n\n## Setup\nWe recommend Python 3.6 or higher. The model is implemented with PyTorch (at least 1.0.1) using [transformers v2.8.0](https://github.com/huggingface/transformers).\nThe code does **not** work with Python 2.7.\n\n**With pip**\n\nInstall the model with `pip`:\n```\npip install -U sentence-transformers\n```\n\n**From source**\n\nClone this repository and install it with `pip`:\n````\npip install -e .\n```` \n\n\n\n## Getting Started\n\n### Sentences Embedding with a Pretrained Model\n[This example](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/basic_embedding.py) shows you how to use an already trained Sentence Transformer model to embed sentences for another task.\n\nFirst download a pretrained model.\n````\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\n````\nThen provide some sentences to the model.\n````\nsentences = ['This framework generates embeddings for each input sentence',\n    'Sentences are passed as a list of string.', \n    'The quick brown fox jumps over the lazy dog.']\nsentence_embeddings = model.encode(sentences)\n````\nAnd that's it already. We now have a list of numpy arrays with the embeddings.\n````\nfor sentence, embedding in zip(sentences, sentence_embeddings):\n    print(\"Sentence:\", sentence)\n    print(\"Embedding:\", embedding)\n    print(\"\")\n````\n\n## Training\nThis framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. You have various options to choose from in order to get perfect sentence embeddings for your specific task. \n\n### Dataset Download\nFirst, you should download some datasets. For this run the [examples/datasets/get_data.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/datasets/get_data.py):\n```\npython examples/datasets/get_data.py\n```\n\nIt will download some [datasets](https://github.com/UKPLab/sentence-transformers/blob/master/examples/datasets) and store them on your disk.\n\n\n### Model Training from Scratch\n[training_nli.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_transformers/training_nli.py) fine-tunes BERT (and other transformer models) from the pre-trained model as provided by Google & Co. It tunes the model on Natural Language Inference (NLI) data. Given two sentences, the model should classify if these two sentence entail, contradict, or are neutral to each other. For this, the two sentences are passed to a transformer model to generate fixed-sized sentence embeddings. These sentence embeddings are then passed to a softmax classifier to derive the final label (entail, contradict, neutral). This generates sentence embeddings that are useful also for other tasks like clustering or semantic textual similarity.\n\n\nFirst, we define a sequential model of how a sentence is mapped to a fixed size sentence embedding:\n```\n# Use BERT for mapping tokens to embeddings\nword_embedding_model = models.Transformer('bert-base-uncased')\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=False,\n                               pooling_mode_max_tokens=False)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n```\n\nFirst, we use the BERT model (instantiated from bert-base-uncased) to map tokens in a sentence to the output embeddings from BERT. The next layer in our model is a Pooling model: In that case, we perform mean-pooling. You can also perform max-pooling or use the embedding from the CLS token. You can also combine multiple poolings together.\n\nThese two modules (word_embedding_model and pooling_model) form our SentenceTransformer. Each sentence is now passed first through the word_embedding_model and then through the pooling_model to give fixed sized sentence vectors.\n\n\nNext, we specify a train dataloader:\n```\nnli_reader = NLIDataReader('datasets/AllNLI')\n\ntrain_data = SentencesDataset(nli_reader.get_examples('train.gz'), model=model)\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntrain_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n```\n\nThe `NLIDataReader` reads the AllNLI dataset and we generate a dataloader that is suitable for training the Sentence Transformer model. As training loss, we use a Softmax Classifier.\n\nNext, we also specify a dev-set. The dev-set is used to evaluate the sentence embedding model on some unseen data. Note, the dev-set can be any data, in this case, we evaluate on the dev-set of the STS benchmark dataset.  The `evaluator` computes the performance metric, in this case, the cosine-similarity between sentence embeddings are computed and the Spearman-correlation to the gold scores is computed.\n\n```\nsts_reader = STSBenchmarkDataReader('datasets/stsbenchmark')\ndev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\ndev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\nevaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n```\n\n The training then looks like this:\n ```\nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          evaluator=evaluator,\n          epochs=num_epochs,\n          evaluation_steps=1000,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path\n          )\n```\n\n\n\n### Continue Training on Other Data\n[training_stsbenchmark_continue_training.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_transformers/training_stsbenchmark_continue_training.py) shows an example where training on a fine-tuned model is continued. In that example, we use a sentence transformer model that was first fine-tuned on the NLI dataset and then continue training on the training data from the STS benchmark.\n\nFirst, we load a pre-trained model from the server:\n```\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\n```\n\n\nThe next steps are as before. We specify training and dev data:\n```\nsts_reader = STSBenchmarkDataReader('datasets/stsbenchmark', normalize_scores=True)\ntrain_data = SentencesDataset(sts_reader.get_examples('sts-train.csv'), model)\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\ntrain_loss = losses.CosineSimilarityLoss(model=model)\n\ndev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\ndev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\nevaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n```\n\nIn that example, we use CosineSimilarityLoss, which computes the cosine similarity between two sentences and compares this score with a provided gold similarity score.\n\nThen we can train as before:\n```\nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          evaluator=evaluator,\n          epochs=num_epochs,\n          evaluation_steps=1000,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path)\n```\n\n\n## Loading SentenceTransformer Models\nLoading trained models is easy. You can specify a path:\n```\nmodel = SentenceTransformer('./my/path/to/model/')\n```\nNote: It is important that a / or \\ is the path, otherwise, it is not recognized as a path.\n\nYou can also host the training output on a server and download it:\n ```\nmodel = SentenceTransformer('http://www.server.com/path/to/model/my_model.zip')\n```\nWith the first call, the model is downloaded and stored in the local torch cache-folder (`~/.cache/torch/sentence_transformers`). In order to work, you must zip all files and subfolders of your model. \n\nWe also provide several pre-trained models, that can be loaded by just passing a name:\n\n ```\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\n```\n\nThis downloads the `bert-base-nli-mean-tokens` from our server and stores it locally.\n\n## Loading custom BERT models\nIf you have fine-tuned BERT (or similar models) and you want to use it to generate sentence embeddings, you must construct an appropriate sentence transformer model from it. This is possible by using this code:\n\n```\n# Use BERT for mapping tokens to embeddings\nword_embedding_model = models.Transformer('path/to/your/BERT/model')\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=False,\n                               pooling_mode_max_tokens=False)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n```\n\n## Pretrained Models\nWe provide the following models. You can use them in the following way:\n ```\nmodel = SentenceTransformer('name_of_model')\n```\n\n\n\n### English Pre-Trained Models\nIn the following you find selected models that were trained on English data only. For the full list of available models, see [SentenceTransformer Pretrained Models](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit?usp=sharing). See the next section for multi-lingual models.\n\n**Trained on NLI data**\n\nThese models were trained on SNLI and MultiNLI dataset to create universal sentence embeddings. For more details, see: [nli-models.md](https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md).\n- **bert-base-nli-mean-tokens**: BERT-base model with mean-tokens pooling. Performance: STSbenchmark: 77.12\n- **bert-large-nli-mean-tokens**: BERT-large with mean-tokens pooling. Performance: STSbenchmark: 79.19\n- **roberta-base-nli-mean-tokens**: RoBERTa-base with mean-tokens pooling. Performance: STSbenchmark: 77.49\n- **roberta-large-nli-mean-tokens**: RoBERTa-base with mean-tokens pooling. Performance: STSbenchmark: 78.69\n- **distilbert-base-nli-mean-tokens**: DistilBERT-base with mean-tokens pooling. Performance: STSbenchmark: 76.97\n\n\n**Trained on STS data**\n\nThese models were first fine-tuned on the AllNLI datasent, then on train set of STS benchmark. They are specifically well suited for semantic textual similarity. For more details, see: [sts-models.md](https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md).\n- **bert-base-nli-stsb-mean-tokens**: Performance: STSbenchmark: 85.14\n- **bert-large-nli-stsb-mean-tokens**: Performance: STSbenchmark: 85.29\n- **roberta-base-nli-stsb-mean-tokens**: Performance: STSbenchmark: 85.44\n- **roberta-large-nli-stsb-mean-tokens**: Performance: STSbenchmark: 86.39\n- **distilbert-base-nli-stsb-mean-tokens**: Performance: STSbenchmark: 84.38\n\n\n### Multilingual Models\nThe following models can be used for languages other than English. The vector spaces for the included languages are aligned, i.e., two sentences are mapped to the same point in vector space independent of the language. The models can be used for cross-lingual tasks. For more details see [multilingual-models.md](https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/multilingual-models.md).\n\n- **distiluse-base-multilingual-cased**: Supported languages: Arabic, Chinese, Dutch, English, French, German,  Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish. Performance on the extended STS2017: 80.1\n\n\n\n## Performance\n\nExtensive evaluation is currently undergoing, but here we provide some preliminary results.\n\n| Model    | STS benchmark | SentEval  |\n| ----------------------------------|:-----: |:---:   |\n| Avg. GloVe embeddings             | 58.02  | 81.52  |\n| BERT-as-a-service avg. embeddings | 46.35  | 84.04  |\n| BERT-as-a-service CLS-vector      | 16.50  | 84.66  |\n| InferSent - GloVe                 | 68.03  | 85.59  |\n| Universal Sentence Encoder        | 74.92  | 85.10  |\n|**Sentence Transformer Models**    ||\n| bert-base-nli-mean-tokens         | 77.12  | 86.37 |\n| bert-large-nli-mean-tokens        | 79.19  | 87.78 |\n| bert-base-nli-stsb-mean-tokens    | 85.14  | 86.07 |\n| bert-large-nli-stsb-mean-tokens   | 85.29 | 86.66|\n\n\n## Loss Functions\nWe implemented various loss-functions that allow training of sentence embeddings from various datasets. These loss-functions are in the package `sentence_transformers.losses`.\n \n- *SoftmaxLoss*: Given the sentence embeddings of two sentences, trains a softmax-classifier. Useful for training on datasets like NLI.\n- *CosineSimilarityLoss*: Given a sentence pair and a gold similarity score (either between -1 and 1 or between 0 and 1), computes the cosine similarity between the sentence embeddings and minimizes the mean squared error loss.\n- *TripletLoss*: Given a triplet (anchor, positive example, negative example), minimizes the [triplet loss](https://en.wikipedia.org/wiki/Triplet_loss).\n- *BatchHardTripletLoss*: Implements the *batch hard triplet loss* from the paper [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737). Each batch must contain multiple examples from the same class. The loss optimizes then the distance between the most-distance positive pair and the closest negative-pair.\n- *MultipleNegativesRankingLoss*: Each batch has one positive pair, all other pairs are treated as negative examples. The loss was used in the papers [Efficient Natural Language Response Suggestion for Smart Reply](https://arxiv.org/pdf/1705.00652.pdf) and [Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model](https://arxiv.org/pdf/1810.12836.pdf).\n\n## Models\nThis framework implements various modules, that can be used sequentially to map a sentence to a sentence embedding. The different modules can be found in the package `sentence_transformers.models`. Each pipeline consists of the following modules.\n\n\n**Word Embeddings:** These models map tokens to token embeddings.\n- **[Transformer](sentence_transformers/models/Transformer.py)**: You can use any huggingface [pretrained models](https://huggingface.co/transformers/pretrained_models.html) including BERT, RoBERTa, DistilBERT, ALBERT, XLNet, XLM-RoBERTa, ELECTRA, FlauBERT, CamemBERT... \n- **[WordEmbeddings](sentence_transformers/models/WordEmbeddings.py)**: Uses traditional word embeddings like word2vec or GloVe to map tokens to vectors. Example: [training_stsbenchmark_avg_word_embeddings.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_avg_word_embeddings.py)\n\n**Embedding Transformations:** These models transform token embeddings in some way\n- **[LSTM](sentence_transformers/models/LSTM.py)**: Runs a bidirectional LSTM. Example: [training_stsbenchmark_bilstm.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_bilstm.py).\n- **[CNN](sentence_transformers/models/CNN.py)**: Runs a CNN model with multiple kernel sizes. Example: [training_stsbenchmark_cnn.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_cnn.py).\n- **[WordWeights](sentence_transformers/models/WordWeights.py)**: This model can be used after WordEmbeddings and before Pooling to apply a weighting to the token embeddings, for example, a tf-idf weighting. Example: [training_stsbenchmark_tf-idf_word_embeddings.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_tf-idf_word_embeddings.py).\n- **[Pooling](sentence_transformers/models/Pooling.py)**: After tokens are mapped to embeddings, we apply the pooling, where you can compute a mean/max-pooling or use the CLS-token embedding (for BERT and XLNet). You can also combine multiple poolings.\n- **[WeightedLayerPooling](sentence_transformers/models/WeightedLayerPooling.py)**: Learns a weighted pooling of all hidden layer of transformer models like BERT. Requires that the model has set output_hidden_states to true.\n- **[WKPooling](sentence_transformers/models/WKPooling.py)**: Pooling based on the paper of *[SBERT-WK](https://arxiv.org/abs/2002.06652)*. Note, WKPooling uses QR decomposition which must run on the CPU. This makes the pooling rather slow. For some models, WKPooling leads to a performance improvement.\n\n**Sentence Embeddings Models:** These models map a sentence directly to a fixed size sentence embedding:\n- **[BoW](sentence_transformers/models/BoW.py)**: Computes a fixed size bag-of-words (BoW) representation of the input text. Can be initialized with IDF-values to create a tf-idf vector. Note that this model is not trainable. Example: [training_stsbenchmark_bow.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_bow.py)\n\n\n**Sentence Embeddings Transformations:** These models can be added once we have a fixed size sentence embedding.\n- **[Dense](sentence_transformers/models/Pooling.py)**: A fully-connected feed-forward network to create a Deep Averaging Network (DAN). You can stack multiple Dense models. Example: [training_stsbenchmark_avg_word_embeddings.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_avg_word_embeddings.py)\n\n\n\n## Multitask Training\nThis code allows multi-task learning with training data from different datasets and with different loss-functions. For an example, see [training_multi-task.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_transformers/training_multi-task.py).\n\n\n## Application Examples\nWe present some examples, how the generated sentence embeddings can be used for downstream applications.\n\n### Semantic Search\nSemantic search is the task of finding similar sentences to a given sentence. See [semantic_search.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic_search.py).\n\nWe first generate an embedding for all sentences in a corpus:\n```\nembedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = ['A man is eating food.',\n          'A man is eating a piece of bread.',\n          'The girl is carrying a baby.',\n          'A man is riding a horse.',\n          'A woman is playing violin.',\n          'Two men pushed carts through the woods.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'A cheetah is running behind its prey.']\n\ncorpus_embeddings = embedder.encode(corpus)\n```\n\nThen, we generate the embeddings for different query sentences:\n```\nqueries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\nquery_embeddings = embedder.encode(queries)\n```\n\nWe then use scipy to find the most-similar embeddings for queries in the corpus:\n```\nfor query, query_embedding in zip(queries, query_embeddings):\n    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n```\n\nThe output looks like this:\n```\nQuery: A man is eating pasta.\nTop 5 most similar sentences in corpus:\nA man is eating a piece of bread. (Score: 0.8518)\nA man is eating a food. (Score: 0.8020)\nA monkey is playing drums. (Score: 0.4167)\nA man is riding a horse. (Score: 0.2621)\nA man is riding a white horse on an enclosed ground. (Score: 0.2379)\n\n\nQuery: Someone in a gorilla costume is playing a set of drums.\nTop 5 most similar sentences in corpus:\nA monkey is playing drums. (Score: 0.8514)\nA man is eating a piece of bread. (Score: 0.3671)\nA man is eating a food. (Score: 0.3559)\nA man is riding a horse. (Score: 0.3153)\nThe girl is carrying a baby. (Score: 0.2589)\n\n\nQuery: A cheetah chases prey on across a field.\nTop 5 most similar sentences in corpus:\nA cheetah is running behind its prey. (Score: 0.9073)\nTwo men pushed carts through the woods. (Score: 0.3896)\nA man is riding a horse. (Score: 0.3789)\nA man is riding a white horse on an enclosed ground. (Score: 0.3544)\nA monkey is playing drums. (Score: 0.3435)\n\n```\n\n\n### Clustering\n[clustering.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering.py) depicts an example to cluster similar sentences based on their sentence embedding similarity.\n\nAs before, we first compute an embedding for each sentence:\n```\nembedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = ['A man is eating food.',\n          'A man is eating a piece of bread.',\n          'A man is eating pasta.',\n          'The girl is carrying a baby.',\n          'The baby is carried by the woman',\n          'A man is riding a horse.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'Someone in a gorilla costume is playing a set of drums.',\n          'A cheetah is running behind its prey.',\n          'A cheetah chases prey on across a field.']\n\ncorpus_embeddings = embedder.encode(corpus)\n```\n\nThen, we perform k-means clustering using sklearn:\n```\nfrom sklearn.cluster import KMeans\n\nnum_clusters = 5\nclustering_model = KMeans(n_clusters=num_clusters)\nclustering_model.fit(corpus_embeddings)\ncluster_assignment = clustering_model.labels_\n```\n\nThe output looks like this:\n```\nCluster  1\n['The girl is carrying a baby.', 'The baby is carried by the woman']\n\nCluster  2\n['A cheetah is running behind its prey.', 'A cheetah chases prey on across a field.']\n\nCluster  3\n['A monkey is playing drums.', 'Someone in a gorilla costume is playing a set of drums.']\n\nCluster  4\n['A man is eating food.', 'A man is eating a piece of bread.', 'A man is eating pasta.']\n\nCluster  5\n['A man is riding a horse.', 'A man is riding a white horse on an enclosed ground.']\n```\n\n## Citing & Authors\nIf you find this repository helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n``` \n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n```\n\n\nThe main contributors of this repository are:\n- [Nils Reimers](https://github.com/nreimers)\n- [Gregor Geigle](https://github.com/aaronsom)\n\nContact person: Nils Reimers, info@nils-reimers.de\n\nhttps://www.ukp.tu-darmstadt.de/\n\n\nDon't hesitate to send us an e-mail or report an issue, if something is broken (and it shouldn't be) or if you have further questions.\n\n> This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/UKPLab/sentence-transformers/archive/v0.2.6.zip", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/UKPLab/sentence-transformers", "keywords": "Transformer Networks BERT XLNet sentence embedding PyTorch NLP deep learning", "license": "Apache License 2.0", "maintainer": "", "maintainer_email": "", "name": "sentence-transformers", "package_url": "https://pypi.org/project/sentence-transformers/", "platform": "", "project_url": "https://pypi.org/project/sentence-transformers/", "project_urls": {"Download": "https://github.com/UKPLab/sentence-transformers/archive/v0.2.6.zip", "Homepage": "https://github.com/UKPLab/sentence-transformers"}, "release_url": "https://pypi.org/project/sentence-transformers/0.2.6.1/", "requires_dist": null, "requires_python": "", "summary": "Sentence Embeddings using BERT / RoBERTa / XLNet", "version": "0.2.6.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Sentence Transformers: Multilingual Sentence Embeddings using BERT / RoBERTa / XLM-RoBERTa &amp; Co. with PyTorch</h1>\n<p>BERT / RoBERTa / XLM-RoBERTa produces out-of-the-box rather bad sentence embeddings. This repository fine-tunes BERT / RoBERTa / DistilBERT / ALBERT / XLNet with a siamese or triplet network structure to produce semantically meaningful sentence embeddings that can be used in unsupervised scenarios: Semantic textual similarity via cosine-similarity, clustering, semantic search.</p>\n<p>We provide an increasing number of <strong>state-of-the-art pretrained models</strong> that can be used to derive sentence embeddings. See <a href=\"#pretrained-models\" rel=\"nofollow\">Pretrained Models</a>. Details of the implemented approaches can be found in our publication: <a href=\"https://arxiv.org/abs/1908.10084\" rel=\"nofollow\">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a> (EMNLP 2019).</p>\n<p>You can use this code to easily <strong>train your own sentence embeddings</strong>, that are tuned for your specific task. We provide various dataset readers and you can tune sentence embeddings with different loss function, depending on the structure of your dataset. For further details, see <a href=\"#Training\" rel=\"nofollow\">Train your own Sentence Embeddings</a>.</p>\n<h2>Setup</h2>\n<p>We recommend Python 3.6 or higher. The model is implemented with PyTorch (at least 1.0.1) using <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">transformers v2.8.0</a>.\nThe code does <strong>not</strong> work with Python 2.7.</p>\n<p><strong>With pip</strong></p>\n<p>Install the model with <code>pip</code>:</p>\n<pre><code>pip install -U sentence-transformers\n</code></pre>\n<p><strong>From source</strong></p>\n<p>Clone this repository and install it with <code>pip</code>:</p>\n<pre><code>pip install -e .\n</code></pre>\n<h2>Getting Started</h2>\n<h3>Sentences Embedding with a Pretrained Model</h3>\n<p><a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/basic_embedding.py\" rel=\"nofollow\">This example</a> shows you how to use an already trained Sentence Transformer model to embed sentences for another task.</p>\n<p>First download a pretrained model.</p>\n<pre><code>from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')\n</code></pre>\n<p>Then provide some sentences to the model.</p>\n<pre><code>sentences = ['This framework generates embeddings for each input sentence',\n    'Sentences are passed as a list of string.', \n    'The quick brown fox jumps over the lazy dog.']\nsentence_embeddings = model.encode(sentences)\n</code></pre>\n<p>And that's it already. We now have a list of numpy arrays with the embeddings.</p>\n<pre><code>for sentence, embedding in zip(sentences, sentence_embeddings):\n    print(\"Sentence:\", sentence)\n    print(\"Embedding:\", embedding)\n    print(\"\")\n</code></pre>\n<h2>Training</h2>\n<p>This framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. You have various options to choose from in order to get perfect sentence embeddings for your specific task.</p>\n<h3>Dataset Download</h3>\n<p>First, you should download some datasets. For this run the <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/datasets/get_data.py\" rel=\"nofollow\">examples/datasets/get_data.py</a>:</p>\n<pre><code>python examples/datasets/get_data.py\n</code></pre>\n<p>It will download some <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/datasets\" rel=\"nofollow\">datasets</a> and store them on your disk.</p>\n<h3>Model Training from Scratch</h3>\n<p><a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_transformers/training_nli.py\" rel=\"nofollow\">training_nli.py</a> fine-tunes BERT (and other transformer models) from the pre-trained model as provided by Google &amp; Co. It tunes the model on Natural Language Inference (NLI) data. Given two sentences, the model should classify if these two sentence entail, contradict, or are neutral to each other. For this, the two sentences are passed to a transformer model to generate fixed-sized sentence embeddings. These sentence embeddings are then passed to a softmax classifier to derive the final label (entail, contradict, neutral). This generates sentence embeddings that are useful also for other tasks like clustering or semantic textual similarity.</p>\n<p>First, we define a sequential model of how a sentence is mapped to a fixed size sentence embedding:</p>\n<pre><code># Use BERT for mapping tokens to embeddings\nword_embedding_model = models.Transformer('bert-base-uncased')\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=False,\n                               pooling_mode_max_tokens=False)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n</code></pre>\n<p>First, we use the BERT model (instantiated from bert-base-uncased) to map tokens in a sentence to the output embeddings from BERT. The next layer in our model is a Pooling model: In that case, we perform mean-pooling. You can also perform max-pooling or use the embedding from the CLS token. You can also combine multiple poolings together.</p>\n<p>These two modules (word_embedding_model and pooling_model) form our SentenceTransformer. Each sentence is now passed first through the word_embedding_model and then through the pooling_model to give fixed sized sentence vectors.</p>\n<p>Next, we specify a train dataloader:</p>\n<pre><code>nli_reader = NLIDataReader('datasets/AllNLI')\n\ntrain_data = SentencesDataset(nli_reader.get_examples('train.gz'), model=model)\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntrain_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n</code></pre>\n<p>The <code>NLIDataReader</code> reads the AllNLI dataset and we generate a dataloader that is suitable for training the Sentence Transformer model. As training loss, we use a Softmax Classifier.</p>\n<p>Next, we also specify a dev-set. The dev-set is used to evaluate the sentence embedding model on some unseen data. Note, the dev-set can be any data, in this case, we evaluate on the dev-set of the STS benchmark dataset.  The <code>evaluator</code> computes the performance metric, in this case, the cosine-similarity between sentence embeddings are computed and the Spearman-correlation to the gold scores is computed.</p>\n<pre><code>sts_reader = STSBenchmarkDataReader('datasets/stsbenchmark')\ndev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\ndev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\nevaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n</code></pre>\n<p>The training then looks like this:</p>\n<pre><code>model.fit(train_objectives=[(train_dataloader, train_loss)],\n         evaluator=evaluator,\n         epochs=num_epochs,\n         evaluation_steps=1000,\n         warmup_steps=warmup_steps,\n         output_path=model_save_path\n         )\n</code></pre>\n<h3>Continue Training on Other Data</h3>\n<p><a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_transformers/training_stsbenchmark_continue_training.py\" rel=\"nofollow\">training_stsbenchmark_continue_training.py</a> shows an example where training on a fine-tuned model is continued. In that example, we use a sentence transformer model that was first fine-tuned on the NLI dataset and then continue training on the training data from the STS benchmark.</p>\n<p>First, we load a pre-trained model from the server:</p>\n<pre><code>model = SentenceTransformer('bert-base-nli-mean-tokens')\n</code></pre>\n<p>The next steps are as before. We specify training and dev data:</p>\n<pre><code>sts_reader = STSBenchmarkDataReader('datasets/stsbenchmark', normalize_scores=True)\ntrain_data = SentencesDataset(sts_reader.get_examples('sts-train.csv'), model)\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\ntrain_loss = losses.CosineSimilarityLoss(model=model)\n\ndev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\ndev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\nevaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n</code></pre>\n<p>In that example, we use CosineSimilarityLoss, which computes the cosine similarity between two sentences and compares this score with a provided gold similarity score.</p>\n<p>Then we can train as before:</p>\n<pre><code>model.fit(train_objectives=[(train_dataloader, train_loss)],\n          evaluator=evaluator,\n          epochs=num_epochs,\n          evaluation_steps=1000,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path)\n</code></pre>\n<h2>Loading SentenceTransformer Models</h2>\n<p>Loading trained models is easy. You can specify a path:</p>\n<pre><code>model = SentenceTransformer('./my/path/to/model/')\n</code></pre>\n<p>Note: It is important that a / or \\ is the path, otherwise, it is not recognized as a path.</p>\n<p>You can also host the training output on a server and download it:</p>\n<pre><code>model = SentenceTransformer('http://www.server.com/path/to/model/my_model.zip')\n</code></pre>\n<p>With the first call, the model is downloaded and stored in the local torch cache-folder (<code>~/.cache/torch/sentence_transformers</code>). In order to work, you must zip all files and subfolders of your model.</p>\n<p>We also provide several pre-trained models, that can be loaded by just passing a name:</p>\n<pre><code>model = SentenceTransformer('bert-base-nli-mean-tokens')\n</code></pre>\n<p>This downloads the <code>bert-base-nli-mean-tokens</code> from our server and stores it locally.</p>\n<h2>Loading custom BERT models</h2>\n<p>If you have fine-tuned BERT (or similar models) and you want to use it to generate sentence embeddings, you must construct an appropriate sentence transformer model from it. This is possible by using this code:</p>\n<pre><code># Use BERT for mapping tokens to embeddings\nword_embedding_model = models.Transformer('path/to/your/BERT/model')\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=False,\n                               pooling_mode_max_tokens=False)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n</code></pre>\n<h2>Pretrained Models</h2>\n<p>We provide the following models. You can use them in the following way:</p>\n<pre><code>model = SentenceTransformer('name_of_model')\n</code></pre>\n<h3>English Pre-Trained Models</h3>\n<p>In the following you find selected models that were trained on English data only. For the full list of available models, see <a href=\"https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit?usp=sharing\" rel=\"nofollow\">SentenceTransformer Pretrained Models</a>. See the next section for multi-lingual models.</p>\n<p><strong>Trained on NLI data</strong></p>\n<p>These models were trained on SNLI and MultiNLI dataset to create universal sentence embeddings. For more details, see: <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md\" rel=\"nofollow\">nli-models.md</a>.</p>\n<ul>\n<li><strong>bert-base-nli-mean-tokens</strong>: BERT-base model with mean-tokens pooling. Performance: STSbenchmark: 77.12</li>\n<li><strong>bert-large-nli-mean-tokens</strong>: BERT-large with mean-tokens pooling. Performance: STSbenchmark: 79.19</li>\n<li><strong>roberta-base-nli-mean-tokens</strong>: RoBERTa-base with mean-tokens pooling. Performance: STSbenchmark: 77.49</li>\n<li><strong>roberta-large-nli-mean-tokens</strong>: RoBERTa-base with mean-tokens pooling. Performance: STSbenchmark: 78.69</li>\n<li><strong>distilbert-base-nli-mean-tokens</strong>: DistilBERT-base with mean-tokens pooling. Performance: STSbenchmark: 76.97</li>\n</ul>\n<p><strong>Trained on STS data</strong></p>\n<p>These models were first fine-tuned on the AllNLI datasent, then on train set of STS benchmark. They are specifically well suited for semantic textual similarity. For more details, see: <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md\" rel=\"nofollow\">sts-models.md</a>.</p>\n<ul>\n<li><strong>bert-base-nli-stsb-mean-tokens</strong>: Performance: STSbenchmark: 85.14</li>\n<li><strong>bert-large-nli-stsb-mean-tokens</strong>: Performance: STSbenchmark: 85.29</li>\n<li><strong>roberta-base-nli-stsb-mean-tokens</strong>: Performance: STSbenchmark: 85.44</li>\n<li><strong>roberta-large-nli-stsb-mean-tokens</strong>: Performance: STSbenchmark: 86.39</li>\n<li><strong>distilbert-base-nli-stsb-mean-tokens</strong>: Performance: STSbenchmark: 84.38</li>\n</ul>\n<h3>Multilingual Models</h3>\n<p>The following models can be used for languages other than English. The vector spaces for the included languages are aligned, i.e., two sentences are mapped to the same point in vector space independent of the language. The models can be used for cross-lingual tasks. For more details see <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/multilingual-models.md\" rel=\"nofollow\">multilingual-models.md</a>.</p>\n<ul>\n<li><strong>distiluse-base-multilingual-cased</strong>: Supported languages: Arabic, Chinese, Dutch, English, French, German,  Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish. Performance on the extended STS2017: 80.1</li>\n</ul>\n<h2>Performance</h2>\n<p>Extensive evaluation is currently undergoing, but here we provide some preliminary results.</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th align=\"center\">STS benchmark</th>\n<th align=\"center\">SentEval</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Avg. GloVe embeddings</td>\n<td align=\"center\">58.02</td>\n<td align=\"center\">81.52</td>\n</tr>\n<tr>\n<td>BERT-as-a-service avg. embeddings</td>\n<td align=\"center\">46.35</td>\n<td align=\"center\">84.04</td>\n</tr>\n<tr>\n<td>BERT-as-a-service CLS-vector</td>\n<td align=\"center\">16.50</td>\n<td align=\"center\">84.66</td>\n</tr>\n<tr>\n<td>InferSent - GloVe</td>\n<td align=\"center\">68.03</td>\n<td align=\"center\">85.59</td>\n</tr>\n<tr>\n<td>Universal Sentence Encoder</td>\n<td align=\"center\">74.92</td>\n<td align=\"center\">85.10</td>\n</tr>\n<tr>\n<td><strong>Sentence Transformer Models</strong></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>bert-base-nli-mean-tokens</td>\n<td align=\"center\">77.12</td>\n<td align=\"center\">86.37</td>\n</tr>\n<tr>\n<td>bert-large-nli-mean-tokens</td>\n<td align=\"center\">79.19</td>\n<td align=\"center\">87.78</td>\n</tr>\n<tr>\n<td>bert-base-nli-stsb-mean-tokens</td>\n<td align=\"center\">85.14</td>\n<td align=\"center\">86.07</td>\n</tr>\n<tr>\n<td>bert-large-nli-stsb-mean-tokens</td>\n<td align=\"center\">85.29</td>\n<td align=\"center\">86.66</td>\n</tr></tbody></table>\n<h2>Loss Functions</h2>\n<p>We implemented various loss-functions that allow training of sentence embeddings from various datasets. These loss-functions are in the package <code>sentence_transformers.losses</code>.</p>\n<ul>\n<li><em>SoftmaxLoss</em>: Given the sentence embeddings of two sentences, trains a softmax-classifier. Useful for training on datasets like NLI.</li>\n<li><em>CosineSimilarityLoss</em>: Given a sentence pair and a gold similarity score (either between -1 and 1 or between 0 and 1), computes the cosine similarity between the sentence embeddings and minimizes the mean squared error loss.</li>\n<li><em>TripletLoss</em>: Given a triplet (anchor, positive example, negative example), minimizes the <a href=\"https://en.wikipedia.org/wiki/Triplet_loss\" rel=\"nofollow\">triplet loss</a>.</li>\n<li><em>BatchHardTripletLoss</em>: Implements the <em>batch hard triplet loss</em> from the paper <a href=\"https://arxiv.org/abs/1703.07737\" rel=\"nofollow\">In Defense of the Triplet Loss for Person Re-Identification</a>. Each batch must contain multiple examples from the same class. The loss optimizes then the distance between the most-distance positive pair and the closest negative-pair.</li>\n<li><em>MultipleNegativesRankingLoss</em>: Each batch has one positive pair, all other pairs are treated as negative examples. The loss was used in the papers <a href=\"https://arxiv.org/pdf/1705.00652.pdf\" rel=\"nofollow\">Efficient Natural Language Response Suggestion for Smart Reply</a> and <a href=\"https://arxiv.org/pdf/1810.12836.pdf\" rel=\"nofollow\">Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model</a>.</li>\n</ul>\n<h2>Models</h2>\n<p>This framework implements various modules, that can be used sequentially to map a sentence to a sentence embedding. The different modules can be found in the package <code>sentence_transformers.models</code>. Each pipeline consists of the following modules.</p>\n<p><strong>Word Embeddings:</strong> These models map tokens to token embeddings.</p>\n<ul>\n<li><strong><a href=\"sentence_transformers/models/Transformer.py\" rel=\"nofollow\">Transformer</a></strong>: You can use any huggingface <a href=\"https://huggingface.co/transformers/pretrained_models.html\" rel=\"nofollow\">pretrained models</a> including BERT, RoBERTa, DistilBERT, ALBERT, XLNet, XLM-RoBERTa, ELECTRA, FlauBERT, CamemBERT...</li>\n<li><strong><a href=\"sentence_transformers/models/WordEmbeddings.py\" rel=\"nofollow\">WordEmbeddings</a></strong>: Uses traditional word embeddings like word2vec or GloVe to map tokens to vectors. Example: <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_avg_word_embeddings.py\" rel=\"nofollow\">training_stsbenchmark_avg_word_embeddings.py</a></li>\n</ul>\n<p><strong>Embedding Transformations:</strong> These models transform token embeddings in some way</p>\n<ul>\n<li><strong><a href=\"sentence_transformers/models/LSTM.py\" rel=\"nofollow\">LSTM</a></strong>: Runs a bidirectional LSTM. Example: <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_bilstm.py\" rel=\"nofollow\">training_stsbenchmark_bilstm.py</a>.</li>\n<li><strong><a href=\"sentence_transformers/models/CNN.py\" rel=\"nofollow\">CNN</a></strong>: Runs a CNN model with multiple kernel sizes. Example: <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_cnn.py\" rel=\"nofollow\">training_stsbenchmark_cnn.py</a>.</li>\n<li><strong><a href=\"sentence_transformers/models/WordWeights.py\" rel=\"nofollow\">WordWeights</a></strong>: This model can be used after WordEmbeddings and before Pooling to apply a weighting to the token embeddings, for example, a tf-idf weighting. Example: <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_tf-idf_word_embeddings.py\" rel=\"nofollow\">training_stsbenchmark_tf-idf_word_embeddings.py</a>.</li>\n<li><strong><a href=\"sentence_transformers/models/Pooling.py\" rel=\"nofollow\">Pooling</a></strong>: After tokens are mapped to embeddings, we apply the pooling, where you can compute a mean/max-pooling or use the CLS-token embedding (for BERT and XLNet). You can also combine multiple poolings.</li>\n<li><strong><a href=\"sentence_transformers/models/WeightedLayerPooling.py\" rel=\"nofollow\">WeightedLayerPooling</a></strong>: Learns a weighted pooling of all hidden layer of transformer models like BERT. Requires that the model has set output_hidden_states to true.</li>\n<li><strong><a href=\"sentence_transformers/models/WKPooling.py\" rel=\"nofollow\">WKPooling</a></strong>: Pooling based on the paper of <em><a href=\"https://arxiv.org/abs/2002.06652\" rel=\"nofollow\">SBERT-WK</a></em>. Note, WKPooling uses QR decomposition which must run on the CPU. This makes the pooling rather slow. For some models, WKPooling leads to a performance improvement.</li>\n</ul>\n<p><strong>Sentence Embeddings Models:</strong> These models map a sentence directly to a fixed size sentence embedding:</p>\n<ul>\n<li><strong><a href=\"sentence_transformers/models/BoW.py\" rel=\"nofollow\">BoW</a></strong>: Computes a fixed size bag-of-words (BoW) representation of the input text. Can be initialized with IDF-values to create a tf-idf vector. Note that this model is not trainable. Example: <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_bow.py\" rel=\"nofollow\">training_stsbenchmark_bow.py</a></li>\n</ul>\n<p><strong>Sentence Embeddings Transformations:</strong> These models can be added once we have a fixed size sentence embedding.</p>\n<ul>\n<li><strong><a href=\"sentence_transformers/models/Pooling.py\" rel=\"nofollow\">Dense</a></strong>: A fully-connected feed-forward network to create a Deep Averaging Network (DAN). You can stack multiple Dense models. Example: <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_basic_models/training_stsbenchmark_avg_word_embeddings.py\" rel=\"nofollow\">training_stsbenchmark_avg_word_embeddings.py</a></li>\n</ul>\n<h2>Multitask Training</h2>\n<p>This code allows multi-task learning with training data from different datasets and with different loss-functions. For an example, see <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_transformers/training_multi-task.py\" rel=\"nofollow\">training_multi-task.py</a>.</p>\n<h2>Application Examples</h2>\n<p>We present some examples, how the generated sentence embeddings can be used for downstream applications.</p>\n<h3>Semantic Search</h3>\n<p>Semantic search is the task of finding similar sentences to a given sentence. See <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic_search.py\" rel=\"nofollow\">semantic_search.py</a>.</p>\n<p>We first generate an embedding for all sentences in a corpus:</p>\n<pre><code>embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = ['A man is eating food.',\n          'A man is eating a piece of bread.',\n          'The girl is carrying a baby.',\n          'A man is riding a horse.',\n          'A woman is playing violin.',\n          'Two men pushed carts through the woods.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'A cheetah is running behind its prey.']\n\ncorpus_embeddings = embedder.encode(corpus)\n</code></pre>\n<p>Then, we generate the embeddings for different query sentences:</p>\n<pre><code>queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\nquery_embeddings = embedder.encode(queries)\n</code></pre>\n<p>We then use scipy to find the most-similar embeddings for queries in the corpus:</p>\n<pre><code>for query, query_embedding in zip(queries, query_embeddings):\n    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n</code></pre>\n<p>The output looks like this:</p>\n<pre><code>Query: A man is eating pasta.\nTop 5 most similar sentences in corpus:\nA man is eating a piece of bread. (Score: 0.8518)\nA man is eating a food. (Score: 0.8020)\nA monkey is playing drums. (Score: 0.4167)\nA man is riding a horse. (Score: 0.2621)\nA man is riding a white horse on an enclosed ground. (Score: 0.2379)\n\n\nQuery: Someone in a gorilla costume is playing a set of drums.\nTop 5 most similar sentences in corpus:\nA monkey is playing drums. (Score: 0.8514)\nA man is eating a piece of bread. (Score: 0.3671)\nA man is eating a food. (Score: 0.3559)\nA man is riding a horse. (Score: 0.3153)\nThe girl is carrying a baby. (Score: 0.2589)\n\n\nQuery: A cheetah chases prey on across a field.\nTop 5 most similar sentences in corpus:\nA cheetah is running behind its prey. (Score: 0.9073)\nTwo men pushed carts through the woods. (Score: 0.3896)\nA man is riding a horse. (Score: 0.3789)\nA man is riding a white horse on an enclosed ground. (Score: 0.3544)\nA monkey is playing drums. (Score: 0.3435)\n\n</code></pre>\n<h3>Clustering</h3>\n<p><a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering.py\" rel=\"nofollow\">clustering.py</a> depicts an example to cluster similar sentences based on their sentence embedding similarity.</p>\n<p>As before, we first compute an embedding for each sentence:</p>\n<pre><code>embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = ['A man is eating food.',\n          'A man is eating a piece of bread.',\n          'A man is eating pasta.',\n          'The girl is carrying a baby.',\n          'The baby is carried by the woman',\n          'A man is riding a horse.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'Someone in a gorilla costume is playing a set of drums.',\n          'A cheetah is running behind its prey.',\n          'A cheetah chases prey on across a field.']\n\ncorpus_embeddings = embedder.encode(corpus)\n</code></pre>\n<p>Then, we perform k-means clustering using sklearn:</p>\n<pre><code>from sklearn.cluster import KMeans\n\nnum_clusters = 5\nclustering_model = KMeans(n_clusters=num_clusters)\nclustering_model.fit(corpus_embeddings)\ncluster_assignment = clustering_model.labels_\n</code></pre>\n<p>The output looks like this:</p>\n<pre><code>Cluster  1\n['The girl is carrying a baby.', 'The baby is carried by the woman']\n\nCluster  2\n['A cheetah is running behind its prey.', 'A cheetah chases prey on across a field.']\n\nCluster  3\n['A monkey is playing drums.', 'Someone in a gorilla costume is playing a set of drums.']\n\nCluster  4\n['A man is eating food.', 'A man is eating a piece of bread.', 'A man is eating pasta.']\n\nCluster  5\n['A man is riding a horse.', 'A man is riding a white horse on an enclosed ground.']\n</code></pre>\n<h2>Citing &amp; Authors</h2>\n<p>If you find this repository helpful, feel free to cite our publication <a href=\"https://arxiv.org/abs/1908.10084\" rel=\"nofollow\">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>:</p>\n<pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>\n<p>The main contributors of this repository are:</p>\n<ul>\n<li><a href=\"https://github.com/nreimers\" rel=\"nofollow\">Nils Reimers</a></li>\n<li><a href=\"https://github.com/aaronsom\" rel=\"nofollow\">Gregor Geigle</a></li>\n</ul>\n<p>Contact person: Nils Reimers, <a href=\"mailto:info@nils-reimers.de\">info@nils-reimers.de</a></p>\n<p><a href=\"https://www.ukp.tu-darmstadt.de/\" rel=\"nofollow\">https://www.ukp.tu-darmstadt.de/</a></p>\n<p>Don't hesitate to send us an e-mail or report an issue, if something is broken (and it shouldn't be) or if you have further questions.</p>\n<blockquote>\n<p>This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication.</p>\n</blockquote>\n\n          </div>"}, "last_serial": 7032524, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "8751213b82cc26b268a84100967b8c15", "sha256": "d39aa508f4053437610017f3ebc8592d7266e1e7e9fdfc265719b5b96849c231"}, "downloads": -1, "filename": "sentence-transformers-0.1.0.tar.gz", "has_sig": false, "md5_digest": "8751213b82cc26b268a84100967b8c15", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 35761, "upload_time": "2019-07-25T08:09:22", "upload_time_iso_8601": "2019-07-25T08:09:22.676208Z", "url": "https://files.pythonhosted.org/packages/6d/84/bba42b8266508ae5d80e2e1c1356e64543a50dfb5d6fe9cc23461532b980/sentence-transformers-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "13569be4b1a01bc7f83f138ad5b4a2be", "sha256": "1e379fa1ba2b08df1b301a13c6259f8261a4840b06ad2f70305fca7e6627b5b5"}, "downloads": -1, "filename": "sentence-transformers-0.2.0.tar.gz", "has_sig": false, "md5_digest": "13569be4b1a01bc7f83f138ad5b4a2be", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28872, "upload_time": "2019-08-16T08:19:41", "upload_time_iso_8601": "2019-08-16T08:19:41.036872Z", "url": "https://files.pythonhosted.org/packages/30/fb/44898f60dc7030c1044ac8bd80714fe25215f59061395c44756f05de3aec/sentence-transformers-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "6acf783ffbf0f47250a27d65837ac28e", "sha256": "7254beb598df166bea979c955e99fc78f2cdcf0e344b2284fd15ed6b0323ce2d"}, "downloads": -1, "filename": "sentence-transformers-0.2.1.tar.gz", "has_sig": false, "md5_digest": "6acf783ffbf0f47250a27d65837ac28e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 42595, "upload_time": "2019-08-16T21:11:03", "upload_time_iso_8601": "2019-08-16T21:11:03.365340Z", "url": "https://files.pythonhosted.org/packages/60/61/1a71005f79756d9809d1ce31b3270ac5aa9098ff085c30d57dcce1c96a1e/sentence-transformers-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "9c165758c68b2e7c792d68cf6c52fb14", "sha256": "b1ee992a48fe0c3f0a0c430e6706a3e833ad9fd89ef7135fb475bde27deb73e8"}, "downloads": -1, "filename": "sentence-transformers-0.2.2.tar.gz", "has_sig": false, "md5_digest": "9c165758c68b2e7c792d68cf6c52fb14", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 44867, "upload_time": "2019-08-19T14:36:34", "upload_time_iso_8601": "2019-08-19T14:36:34.795937Z", "url": "https://files.pythonhosted.org/packages/2f/c7/dc7c24a339dad89c33c4dc95c17f6bac4973ecc96bad4c34b27c3377f2ef/sentence-transformers-0.2.2.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "8e633b0277dbe6538227f15775e58b74", "sha256": "86030a9e2c20a9fe2121c60de5a0f5d9a65316fdd0d3deffe219937da508d593"}, "downloads": -1, "filename": "sentence-transformers-0.2.3.tar.gz", "has_sig": false, "md5_digest": "8e633b0277dbe6538227f15775e58b74", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45083, "upload_time": "2019-08-20T17:17:50", "upload_time_iso_8601": "2019-08-20T17:17:50.616960Z", "url": "https://files.pythonhosted.org/packages/85/17/9edba42c29fda04f2eb8597bb4de380f0f43d65e317969070c04510d93eb/sentence-transformers-0.2.3.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "1836a2ac82146b5f5a6b08fffa3c28b0", "sha256": "46f74f4cef699725c13aa3dd4079a01f0583cb497f78ce50c8f5114c9539a97b"}, "downloads": -1, "filename": "sentence-transformers-0.2.4.tar.gz", "has_sig": false, "md5_digest": "1836a2ac82146b5f5a6b08fffa3c28b0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 49099, "upload_time": "2019-12-06T14:13:42", "upload_time_iso_8601": "2019-12-06T14:13:42.124781Z", "url": "https://files.pythonhosted.org/packages/b4/02/ed23ad96c1457d28e045e81454ee065469dc0122522abd9af582b84a2891/sentence-transformers-0.2.4.tar.gz", "yanked": false}], "0.2.4.1": [{"comment_text": "", "digests": {"md5": "597e9f31bffac242d76db38224c611a0", "sha256": "a7d6487d1f3d3bd58795064f3b0962fb57ff373b2d92c4c4d51baf509ccfa937"}, "downloads": -1, "filename": "sentence-transformers-0.2.4.1.tar.gz", "has_sig": false, "md5_digest": "597e9f31bffac242d76db38224c611a0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 49102, "upload_time": "2019-12-06T14:23:54", "upload_time_iso_8601": "2019-12-06T14:23:54.346669Z", "url": "https://files.pythonhosted.org/packages/b9/6e/5c98f5f26698276bacd09077b039fa1a00797ed080a628ee844bd9f281d4/sentence-transformers-0.2.4.1.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "43075ae249c4dcb91ce04194c74026df", "sha256": "fae26d38cc452c5baa5aeca1a200b48415929d51ae93720609afee5130979b79"}, "downloads": -1, "filename": "sentence-transformers-0.2.5.tar.gz", "has_sig": false, "md5_digest": "43075ae249c4dcb91ce04194c74026df", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 49483, "upload_time": "2020-01-10T09:31:21", "upload_time_iso_8601": "2020-01-10T09:31:21.956176Z", "url": "https://files.pythonhosted.org/packages/c9/91/c85ddef872d5bb39949386930c1f834ac382e145fcd30155b09d6fb65c5a/sentence-transformers-0.2.5.tar.gz", "yanked": false}], "0.2.5.1": [{"comment_text": "", "digests": {"md5": "809311adde6d78c657eeb93d7ed7c3c4", "sha256": "6a0086b0c1ce571f6ea03aa1e1f508acbcb5f4b477e8297269df77587169c574"}, "downloads": -1, "filename": "sentence-transformers-0.2.5.1.tar.gz", "has_sig": false, "md5_digest": "809311adde6d78c657eeb93d7ed7c3c4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 52239, "upload_time": "2020-03-13T10:19:44", "upload_time_iso_8601": "2020-03-13T10:19:44.942498Z", "url": "https://files.pythonhosted.org/packages/07/32/e3d405806ea525fd74c2c79164c3f7bc0b0b9811f27990484c6d6874c76f/sentence-transformers-0.2.5.1.tar.gz", "yanked": false}], "0.2.6": [{"comment_text": "", "digests": {"md5": "d1688d59f3968bb3790fb25b7811c226", "sha256": "a5f01a7b0fd03c0f8a3920712ebf3698b1ed331e87123c5864ed927346f00df5"}, "downloads": -1, "filename": "sentence-transformers-0.2.6.tar.gz", "has_sig": false, "md5_digest": "d1688d59f3968bb3790fb25b7811c226", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 55631, "upload_time": "2020-04-16T14:14:43", "upload_time_iso_8601": "2020-04-16T14:14:43.804740Z", "url": "https://files.pythonhosted.org/packages/51/9d/cef25b5faabdc1b54d218012ee821292312e139e76cc40105c824ad024bb/sentence-transformers-0.2.6.tar.gz", "yanked": false}], "0.2.6.1": [{"comment_text": "", "digests": {"md5": "d1f5d1cac9cfa2fddc675a40b3124512", "sha256": "68250e1e272ad7013c879a633deca710bbaf7b8cec4080095e88904b93eed128"}, "downloads": -1, "filename": "sentence-transformers-0.2.6.1.tar.gz", "has_sig": false, "md5_digest": "d1f5d1cac9cfa2fddc675a40b3124512", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 55609, "upload_time": "2020-04-16T14:24:46", "upload_time_iso_8601": "2020-04-16T14:24:46.610636Z", "url": "https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d1f5d1cac9cfa2fddc675a40b3124512", "sha256": "68250e1e272ad7013c879a633deca710bbaf7b8cec4080095e88904b93eed128"}, "downloads": -1, "filename": "sentence-transformers-0.2.6.1.tar.gz", "has_sig": false, "md5_digest": "d1f5d1cac9cfa2fddc675a40b3124512", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 55609, "upload_time": "2020-04-16T14:24:46", "upload_time_iso_8601": "2020-04-16T14:24:46.610636Z", "url": "https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:05 2020"}