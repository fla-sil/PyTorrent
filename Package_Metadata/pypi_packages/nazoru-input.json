{"info": {"author": "Makoto Shimazu", "author_email": "shimazu@google.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Environment :: No Input/Output (Daemon)", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Utilities"], "description": "Gboard Physical Handwriting Version\n===================================\n\nGboard Physical Handwriting Version is a device which translates your\nscribble on your keyboard into a character. You can make your own Gboard\nPhysical Handwriting Version by printing your own printed circuit board\n(PCB). Also, you can train your own model to recognize a customized set\nof characters. This repository provides circuit diagram, the board\nlayout and software to recognize your stroke over the keyboard as a\ncharacter.\n\nSoftware Usage\n--------------\n\nInput Characters\n~~~~~~~~~~~~~~~~\n\n.. code:: shell\n\n     $ pip install .\n     $ nazoru-input\n\nBy running the commands above, you can make your own machine into an\ninput device which accepts scribbles on the connected keyboard and send\ncharacters via bluetooth. At the beginning, this script scans\nconnected keyboards and starts listening to inputs from one of the\nkeyboards. Then it translates a sequence of keydowns into a predicted\ncharacter considering pressed timings, and send the character to the\ntarget device paired by bluetooth.\n\nIf you want to try it for development, you can use ``-e`` option.\n\n.. code:: shell\n\n     $ pip install -e .\n     $ nazoru-input\n\nTraining Model\n~~~~~~~~~~~~~~\n\n.. code:: shell\n\n     $ pip install .\n     $ nazoru-training ./data/strokes.zip\n\nWe have a script to generate a trained model which recognizes input\ncharacters from scribbles. This script renders input stroke data into\nimages to extract useful features for prediction considering position of\nthe key and timing of keyboard events. Rendered images are fed into the\nneural network model and the optimizer tunes the model to fit the data.\nOnce the training is done, the script outputs the trained graph, which\nyou can use for your own device. In the case where you install\n``nazoru-training`` from pip, you can find ``strokes.zip`` at here:\nhttps://github.com/google/mozc-devices/mozc-nazoru/data/strokes.zip\n\nYou can change some configurations by passing command line flags (e.g.\npath to the input/output files, hyper-parameters). Run\n``nazoru-training --help`` for details.\n\nHardware Setup\n--------------\n\nPrinted Circuit Board\n~~~~~~~~~~~~~~~~~~~~~\nGboard Physical Handwriting Version uses Raspberry Pi Zero for the\nkeyboard input recognition and RN42 module for Bluetooth connection to\nyour laptop. You can check the wiring at ``board/schematic.png``. Also,\nthe original CAD data in EAGLE format is available\n(``board/nazoru-stack.sch`` and ``board/nazoru-stack.brd``). The board\nhas non-connected pads and connectors for SPI and I2C. The connector\nitself should be compatible with other Raspberry Pi, but we tested it\nonly on Raspberry Pi Zero W.\n\n.. image:: https://raw.githubusercontent.com/google/mozc-devices/master/mozc-nazoru/board/schematic.png\n   :width: 1000px\n\nRaspberry Pi Setup\n~~~~~~~~~~~~~~~~~~\n\n**Step 0 - Prepare your Raspberry Pi**\n\n  Please prepare your Raspberry Pi, SD card initialized by RASPBIAN\n  image, and RN42 module. Connect your Raspberry Pi with RN42 as the\n  schematic shows. Please make sure you can have access to the internet\n  and also it has enough disk space to install packages on the following\n  steps.\n\n**Step 1 - Setup UART to RN42**\n\n  If you try it on Raspberry Pi Zero W or Raspberry Pi 3, you need to\n  have additional settings for the serial communication because they\n  equipped a wireless module connected by the UART. See details at `an\n  official document\n  <https://www.raspberrypi.org/documentation/configuration/uart.md>`_.\n  In short, you need to add ``enable_uart=1`` to ``/boot/config.txt`` on\n  your Raspberry Pi.\n\n**Step 2 - Initial setup for RN42**\n\n  You need to write your initial setup to RN42. At first, install screen\n  and open ``/dev/serial0`` for configuration.\n\n  .. code:: shell\n\n    $ sudo apt install screen\n    $ sudo screen /dev/serial0 115200\n\n  After that, please type the following commands. Note that you need to\n  type ENTER after input commands. For example, please type ``$$$``\n  and ENTER to execute ``$$$`` command.\n\n  1. ``$$$`` : Get into the command mode. The green LED will blink\n     faster.\n  2. ``+`` : You can see what you type.\n  3. ``SD,0540`` : Set the device class to keyboard.\n  4. ``S~,6`` : Set the profile to HID.\n  5. ``SH,0200`` : Set the HID flag to keyboard.\n  6. ``SN,nazoru-input`` : Set the device name as nazoru-input. You\n     can name it as you want.\n  7. ``R,1`` : Reboot RN42.\n\n  You can quit the screen by ``C-a k``.\n\n**Step 3 - Download and install nazoru-input**\n\n  We provide a service file at ``data/nazoru.service`` to launch\n  ``nazoru-input`` when booting. You can install it by uncomment\n  ``data_files`` entry in ``setup.py``. Also, before installing this\n  package, We'd strongly recommend you to install some  package from apt\n  repository as follows, so that you can install pre-built packages.\n\n  .. code:: shell\n\n    $ sudo apt install git python-pip python-numpy python-cairocffi \\\n      python-h5py python-imaging python-scipy libblas-dev liblapack-dev \\\n      python-dev libatlas-base-dev gfortran python-setuptools \\\n      python-html5lib\n    $ sudo pip install http://ci.tensorflow.org/view/Nightly/job/nightly-pi-zero/219/artifact/output-artifacts/tensorflow-1.6.0-cp27-none-any.whl\n    $ git clone https://github.com/google/mozc-devices\n    $ cd mozc-devices/mozc-nazoru\n    $ sudo pip install . # If you want to develop nazoru-input, please use 'pip install -e .' instead.\n\n**Step 4 - Enjoy!**\n\n  .. code:: shell\n\n    $ sudo nazoru-input # If you miss sudo, nazoru-input may use a DummyBluetooth object.\n\nTraining Data Format\n--------------------\n\nWe are providing the raw training data at ``data/strokes.zip``. Once you\nuncompress the zip file, you will get a ``.ndjson`` file which contains\nall entries (we call them **strokes**) we have used for training.\n\nEach stroke entry contains the following field:\n\n+----------+-----------+-------------------------------------------+\n| Key      | Type      | Description                               |\n+==========+===========+===========================================+\n| id       | integer   | A unique identifier across all strokes.   |\n+----------+-----------+-------------------------------------------+\n| writer   | string    | A unique identifier of writer.            |\n+----------+-----------+-------------------------------------------+\n| kana     | string    | Label of the character drawn.             |\n+----------+-----------+-------------------------------------------+\n| events   | list      | List of keyboard events.                  |\n+----------+-----------+-------------------------------------------+\n\nEach event is a 3-tuple of (``key``, ``event type``, ``time``). ``key``\ndescribes the key on which the event happened. ``event type`` describes\nwhat type of event happened. It should be \"down\" (keydown) or \"up\"\n(keyup). ``time`` describes the consumed time until the event is fired\nin millisecond.\n\nFor example, the entry below denotes a stoke of \"\u307b\n(\\\\u307b)\" accompanied with a sequence of keyboard events\nstarting from the keydown event on \"t\" and ending at the keyup event on\n\"l\" which was fired 1.005 seconds later after it started recording.\n\n.. code:: json\n\n    {\n      \"id\": 5788999721418752,\n      \"writer\": \"ffb0dac6b8be3faa81da320e29a2ba72\",\n      \"kana\": \"\\u307b\",\n      \"events\": [\n        [\"t\", \"down\", 0],\n        [\"g\", \"down\", 40],\n        ...\n        [\"l\", \"down\", 966],\n        [\"l\", \"up\", 1005]\n      ]\n    }\n\nYou can also prepare your own dataset in ``.ndjson`` format and rebuild\nthe model on it. The list of kanas to recognize is in\n``src/nazoru/lib.py``. You can update that if you want to modify the set\nof characters.\n\nNetwork Structure\n-----------------\n\nData Preprocessing\n~~~~~~~~~~~~~~~~~~\n\nEach stroke entry is rendered to a square image before any training\nruns. The script (``nazoru-training``) renders strokes in various ways\nto extract useful features. Our default settings extract 10 features\nfrom each stroke entry: 8 directional features and 2 temporal features\non 16x16 square canvas; this means that the input shape is 16x16x10 by\ndefault.\n\nConvolutional Network\n~~~~~~~~~~~~~~~~~~~~~\n\nRendered inputs are fed into a convolutional neural network designed for\nthis task. Body structure looks like:\n\n-  Convolutional layer (kernel size: 3x3, filter size: 32, stride: 2,\n   activation: Relu)\n-  Separatable convolutional layer (kernel size: 3x3, filter size: 64,\n   stride: 1, activation: Relu)\n-  Separatable convolutional layer (kernel size: 3x3, filter size: 128,\n   stride: 2, activation: Relu)\n-  Separatable convolutional layer (kernel size: 3x3, filter size: 128,\n   stride: 1, activation: Relu)\n\nFor more details about the separatable convolutional layers, please\nrefer to `MobileNet <https://arxiv.org/abs/1704.04861>`__ architecture.\n\nAuthors\n-------\n\nMachine Learning:\n\n  Shuhei Iitsuka <tushuhei@google.com>\n\nHardwares, system setups:\n\n  Makoto Shimazu <shimazu@google.com>\n\nLicense\n-------\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may\nnot use this file except in compliance with the License. You may obtain\na copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://landing.google.com/tegaki", "keywords": "", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "nazoru-input", "package_url": "https://pypi.org/project/nazoru-input/", "platform": "", "project_url": "https://pypi.org/project/nazoru-input/", "project_urls": {"Homepage": "https://landing.google.com/tegaki"}, "release_url": "https://pypi.org/project/nazoru-input/0.1.1/", "requires_dist": null, "requires_python": "", "summary": "Package for Gboard Physical Handwriting Version", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Gboard Physical Handwriting Version is a device which translates your\nscribble on your keyboard into a character. You can make your own Gboard\nPhysical Handwriting Version by printing your own printed circuit board\n(PCB). Also, you can train your own model to recognize a customized set\nof characters. This repository provides circuit diagram, the board\nlayout and software to recognize your stroke over the keyboard as a\ncharacter.</p>\n<div id=\"software-usage\">\n<h2>Software Usage</h2>\n<div id=\"input-characters\">\n<h3>Input Characters</h3>\n<pre>$ pip install .\n$ nazoru-input\n</pre>\n<p>By running the commands above, you can make your own machine into an\ninput device which accepts scribbles on the connected keyboard and send\ncharacters via bluetooth. At the beginning, this script scans\nconnected keyboards and starts listening to inputs from one of the\nkeyboards. Then it translates a sequence of keydowns into a predicted\ncharacter considering pressed timings, and send the character to the\ntarget device paired by bluetooth.</p>\n<p>If you want to try it for development, you can use <tt><span class=\"pre\">-e</span></tt> option.</p>\n<pre>$ pip install -e .\n$ nazoru-input\n</pre>\n</div>\n<div id=\"training-model\">\n<h3>Training Model</h3>\n<pre>$ pip install .\n$ nazoru-training ./data/strokes.zip\n</pre>\n<p>We have a script to generate a trained model which recognizes input\ncharacters from scribbles. This script renders input stroke data into\nimages to extract useful features for prediction considering position of\nthe key and timing of keyboard events. Rendered images are fed into the\nneural network model and the optimizer tunes the model to fit the data.\nOnce the training is done, the script outputs the trained graph, which\nyou can use for your own device. In the case where you install\n<tt><span class=\"pre\">nazoru-training</span></tt> from pip, you can find <tt>strokes.zip</tt> at here:\n<a href=\"https://github.com/google/mozc-devices/mozc-nazoru/data/strokes.zip\" rel=\"nofollow\">https://github.com/google/mozc-devices/mozc-nazoru/data/strokes.zip</a></p>\n<p>You can change some configurations by passing command line flags (e.g.\npath to the input/output files, hyper-parameters). Run\n<tt><span class=\"pre\">nazoru-training</span> <span class=\"pre\">--help</span></tt> for details.</p>\n</div>\n</div>\n<div id=\"hardware-setup\">\n<h2>Hardware Setup</h2>\n<div id=\"printed-circuit-board\">\n<h3>Printed Circuit Board</h3>\n<p>Gboard Physical Handwriting Version uses Raspberry Pi Zero for the\nkeyboard input recognition and RN42 module for Bluetooth connection to\nyour laptop. You can check the wiring at <tt>board/schematic.png</tt>. Also,\nthe original CAD data in EAGLE format is available\n(<tt><span class=\"pre\">board/nazoru-stack.sch</span></tt> and <tt><span class=\"pre\">board/nazoru-stack.brd</span></tt>). The board\nhas non-connected pads and connectors for SPI and I2C. The connector\nitself should be compatible with other Raspberry Pi, but we tested it\nonly on Raspberry Pi Zero W.</p>\n<img alt=\"https://raw.githubusercontent.com/google/mozc-devices/master/mozc-nazoru/board/schematic.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ae0ac6e65cb3a623a8dbe7b728dc71363be00a33/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f676f6f676c652f6d6f7a632d646576696365732f6d61737465722f6d6f7a632d6e617a6f72752f626f6172642f736368656d617469632e706e67\" width=\"1000px\">\n</div>\n<div id=\"raspberry-pi-setup\">\n<h3>Raspberry Pi Setup</h3>\n<p><strong>Step 0 - Prepare your Raspberry Pi</strong></p>\n<blockquote>\nPlease prepare your Raspberry Pi, SD card initialized by RASPBIAN\nimage, and RN42 module. Connect your Raspberry Pi with RN42 as the\nschematic shows. Please make sure you can have access to the internet\nand also it has enough disk space to install packages on the following\nsteps.</blockquote>\n<p><strong>Step 1 - Setup UART to RN42</strong></p>\n<blockquote>\nIf you try it on Raspberry Pi Zero W or Raspberry Pi 3, you need to\nhave additional settings for the serial communication because they\nequipped a wireless module connected by the UART. See details at <a href=\"https://www.raspberrypi.org/documentation/configuration/uart.md\" rel=\"nofollow\">an\nofficial document</a>.\nIn short, you need to add <tt>enable_uart=1</tt> to <tt>/boot/config.txt</tt> on\nyour Raspberry Pi.</blockquote>\n<p><strong>Step 2 - Initial setup for RN42</strong></p>\n<blockquote>\n<p>You need to write your initial setup to RN42. At first, install screen\nand open <tt>/dev/serial0</tt> for configuration.</p>\n<pre>$ sudo apt install screen\n$ sudo screen /dev/serial0 <span class=\"m\">115200</span>\n</pre>\n<p>After that, please type the following commands. Note that you need to\ntype ENTER after input commands. For example, please type <tt>$$$</tt>\nand ENTER to execute <tt>$$$</tt> command.</p>\n<ol>\n<li><tt>$$$</tt> : Get into the command mode. The green LED will blink\nfaster.</li>\n<li><tt>+</tt> : You can see what you type.</li>\n<li><tt>SD,0540</tt> : Set the device class to keyboard.</li>\n<li><tt><span class=\"pre\">S~,6</span></tt> : Set the profile to HID.</li>\n<li><tt>SH,0200</tt> : Set the HID flag to keyboard.</li>\n<li><tt><span class=\"pre\">SN,nazoru-input</span></tt> : Set the device name as nazoru-input. You\ncan name it as you want.</li>\n<li><tt>R,1</tt> : Reboot RN42.</li>\n</ol>\n<p>You can quit the screen by <tt><span class=\"pre\">C-a</span> k</tt>.</p>\n</blockquote>\n<p><strong>Step 3 - Download and install nazoru-input</strong></p>\n<blockquote>\n<p>We provide a service file at <tt>data/nazoru.service</tt> to launch\n<tt><span class=\"pre\">nazoru-input</span></tt> when booting. You can install it by uncomment\n<tt>data_files</tt> entry in <tt>setup.py</tt>. Also, before installing this\npackage, We\u2019d strongly recommend you to install some  package from apt\nrepository as follows, so that you can install pre-built packages.</p>\n<pre>$ sudo apt install git python-pip python-numpy python-cairocffi <span class=\"se\">\\\n</span>  python-h5py python-imaging python-scipy libblas-dev liblapack-dev <span class=\"se\">\\\n</span>  python-dev libatlas-base-dev gfortran python-setuptools <span class=\"se\">\\\n</span>  python-html5lib\n$ sudo pip install http://ci.tensorflow.org/view/Nightly/job/nightly-pi-zero/219/artifact/output-artifacts/tensorflow-1.6.0-cp27-none-any.whl\n$ git clone https://github.com/google/mozc-devices\n$ <span class=\"nb\">cd</span> mozc-devices/mozc-nazoru\n$ sudo pip install . <span class=\"c1\"># If you want to develop nazoru-input, please use 'pip install -e .' instead.</span>\n</pre>\n</blockquote>\n<p><strong>Step 4 - Enjoy!</strong></p>\n<blockquote>\n<pre>$ sudo nazoru-input <span class=\"c1\"># If you miss sudo, nazoru-input may use a DummyBluetooth object.</span>\n</pre>\n</blockquote>\n</div>\n</div>\n<div id=\"training-data-format\">\n<h2>Training Data Format</h2>\n<p>We are providing the raw training data at <tt>data/strokes.zip</tt>. Once you\nuncompress the zip file, you will get a <tt>.ndjson</tt> file which contains\nall entries (we call them <strong>strokes</strong>) we have used for training.</p>\n<p>Each stroke entry contains the following field:</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>Key</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>id</td>\n<td>integer</td>\n<td>A unique identifier across all strokes.</td>\n</tr>\n<tr><td>writer</td>\n<td>string</td>\n<td>A unique identifier of writer.</td>\n</tr>\n<tr><td>kana</td>\n<td>string</td>\n<td>Label of the character drawn.</td>\n</tr>\n<tr><td>events</td>\n<td>list</td>\n<td>List of keyboard events.</td>\n</tr>\n</tbody>\n</table>\n<p>Each event is a 3-tuple of (<tt>key</tt>, <tt>event type</tt>, <tt>time</tt>). <tt>key</tt>\ndescribes the key on which the event happened. <tt>event type</tt> describes\nwhat type of event happened. It should be \u201cdown\u201d (keydown) or \u201cup\u201d\n(keyup). <tt>time</tt> describes the consumed time until the event is fired\nin millisecond.</p>\n<p>For example, the entry below denotes a stoke of \u201c\u307b\n(\\u307b)\u201d accompanied with a sequence of keyboard events\nstarting from the keydown event on \u201ct\u201d and ending at the keyup event on\n\u201cl\u201d which was fired 1.005 seconds later after it started recording.</p>\n<pre><span class=\"p\">{</span>\n  <span class=\"nt\">\"id\"</span><span class=\"p\">:</span> <span class=\"mi\">5788999721418752</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"writer\"</span><span class=\"p\">:</span> <span class=\"s2\">\"ffb0dac6b8be3faa81da320e29a2ba72\"</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"kana\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\\u307b\"</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"events\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"t\"</span><span class=\"p\">,</span> <span class=\"s2\">\"down\"</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"g\"</span><span class=\"p\">,</span> <span class=\"s2\">\"down\"</span><span class=\"p\">,</span> <span class=\"mi\">40</span><span class=\"p\">],</span>\n    <span class=\"err\">...</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"l\"</span><span class=\"p\">,</span> <span class=\"s2\">\"down\"</span><span class=\"p\">,</span> <span class=\"mi\">966</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"l\"</span><span class=\"p\">,</span> <span class=\"s2\">\"up\"</span><span class=\"p\">,</span> <span class=\"mi\">1005</span><span class=\"p\">]</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</pre>\n<p>You can also prepare your own dataset in <tt>.ndjson</tt> format and rebuild\nthe model on it. The list of kanas to recognize is in\n<tt>src/nazoru/lib.py</tt>. You can update that if you want to modify the set\nof characters.</p>\n</div>\n<div id=\"network-structure\">\n<h2>Network Structure</h2>\n<div id=\"data-preprocessing\">\n<h3>Data Preprocessing</h3>\n<p>Each stroke entry is rendered to a square image before any training\nruns. The script (<tt><span class=\"pre\">nazoru-training</span></tt>) renders strokes in various ways\nto extract useful features. Our default settings extract 10 features\nfrom each stroke entry: 8 directional features and 2 temporal features\non 16x16 square canvas; this means that the input shape is 16x16x10 by\ndefault.</p>\n</div>\n<div id=\"convolutional-network\">\n<h3>Convolutional Network</h3>\n<p>Rendered inputs are fed into a convolutional neural network designed for\nthis task. Body structure looks like:</p>\n<ul>\n<li>Convolutional layer (kernel size: 3x3, filter size: 32, stride: 2,\nactivation: Relu)</li>\n<li>Separatable convolutional layer (kernel size: 3x3, filter size: 64,\nstride: 1, activation: Relu)</li>\n<li>Separatable convolutional layer (kernel size: 3x3, filter size: 128,\nstride: 2, activation: Relu)</li>\n<li>Separatable convolutional layer (kernel size: 3x3, filter size: 128,\nstride: 1, activation: Relu)</li>\n</ul>\n<p>For more details about the separatable convolutional layers, please\nrefer to <a href=\"https://arxiv.org/abs/1704.04861\" rel=\"nofollow\">MobileNet</a> architecture.</p>\n</div>\n</div>\n<div id=\"authors\">\n<h2>Authors</h2>\n<p>Machine Learning:</p>\n<blockquote>\nShuhei Iitsuka &lt;<a href=\"mailto:tushuhei%40google.com\">tushuhei<span>@</span>google<span>.</span>com</a>&gt;</blockquote>\n<p>Hardwares, system setups:</p>\n<blockquote>\nMakoto Shimazu &lt;<a href=\"mailto:shimazu%40google.com\">shimazu<span>@</span>google<span>.</span>com</a>&gt;</blockquote>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may\nnot use this file except in compliance with the License. You may obtain\na copy of the License at</p>\n<blockquote>\n<a href=\"http://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\">http://www.apache.org/licenses/LICENSE-2.0</a></blockquote>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \u201cAS IS\u201d BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>\n</div>\n\n          </div>"}, "last_serial": 3723130, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "fc6865a85dff7076dab36d53c4a2b3c0", "sha256": "48cd812c63ae8b825b1c87ecd012ef8c7abb9853d9d4fc7f1e626228e8545691"}, "downloads": -1, "filename": "nazoru-input-0.1.tar.gz", "has_sig": false, "md5_digest": "fc6865a85dff7076dab36d53c4a2b3c0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 189320, "upload_time": "2018-03-31T15:03:12", "upload_time_iso_8601": "2018-03-31T15:03:12.819731Z", "url": "https://files.pythonhosted.org/packages/e0/21/9a71a63c704d0e61c3518cc92ef3d411bb8ec633d62d4e39790f55c44f70/nazoru-input-0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "9ff55bf877d91193a44f31df680e74e3", "sha256": "f3990b368e082ba3df3201b646ced38da2d2be495db23e2bd0b14d5ea421c23b"}, "downloads": -1, "filename": "nazoru-input-0.1.1.tar.gz", "has_sig": false, "md5_digest": "9ff55bf877d91193a44f31df680e74e3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 189354, "upload_time": "2018-03-31T15:12:53", "upload_time_iso_8601": "2018-03-31T15:12:53.592128Z", "url": "https://files.pythonhosted.org/packages/67/23/d7795d5caa4840bda066f7832366bdad1ee8a2b574e5db25ee10ad4736fa/nazoru-input-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "9ff55bf877d91193a44f31df680e74e3", "sha256": "f3990b368e082ba3df3201b646ced38da2d2be495db23e2bd0b14d5ea421c23b"}, "downloads": -1, "filename": "nazoru-input-0.1.1.tar.gz", "has_sig": false, "md5_digest": "9ff55bf877d91193a44f31df680e74e3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 189354, "upload_time": "2018-03-31T15:12:53", "upload_time_iso_8601": "2018-03-31T15:12:53.592128Z", "url": "https://files.pythonhosted.org/packages/67/23/d7795d5caa4840bda066f7832366bdad1ee8a2b574e5db25ee10ad4736fa/nazoru-input-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:05 2020"}