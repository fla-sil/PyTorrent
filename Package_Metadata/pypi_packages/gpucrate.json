{"info": {"author": "Justin Riley", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "# gpucrate\n\n[![build status](https://secure.travis-ci.org/jtriley/gpucrate.png?branch=master)](https://secure.travis-ci.org/jtriley/gpucrate)\n\ngpucrate creates hard-linked GPU driver (currently just NVIDIA) volumes for use\nwith docker, singularity, etc. This allows the exact system drivers to be\nlinked into a container without needing to maintain a separate container per\ndriver version.\n\n## Installation\nTo install gpucrate use the `pip` command:\n\n```\n$ pip install gpucrate\n```\n\nor in a [virtual environment](https://virtualenv.pypa.io/en/stable/):\n\n```\n$ virtualenv gpucrate\n$ source gpucrate/bin/activate\n$ pip install gpucrate\n```\n\n## Usage\nTo create a driver volume for your system's current GPU driver:\n\n```\n$ sudo gpucrate create\n```\n\nThis will create a hard-linked driver volume directory in `/usr/local/gpucrate`\nby default that can be used to link the drivers into a container. Here's an\nexample volume for driver version `367.48`:\n\n```\n$ find /usr/local/gpucrate/367.48/\n/usr/local/gpucrate/367.48/\n/usr/local/gpucrate/367.48/bin\n/usr/local/gpucrate/367.48/bin/nvidia-cuda-mps-server\n/usr/local/gpucrate/367.48/bin/nvidia-debugdump\n/usr/local/gpucrate/367.48/bin/nvidia-persistenced\n/usr/local/gpucrate/367.48/bin/nvidia-cuda-mps-control\n/usr/local/gpucrate/367.48/bin/nvidia-smi\n/usr/local/gpucrate/367.48/lib\n/usr/local/gpucrate/367.48/lib64\n/usr/local/gpucrate/367.48/lib64/libnvcuvid.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-ml.so.1\n/usr/local/gpucrate/367.48/lib64/libnvidia-eglcore.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-glcore.so.367.48\n/usr/local/gpucrate/367.48/lib64/libcuda.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-opencl.so.1\n/usr/local/gpucrate/367.48/lib64/libnvcuvid.so.1\n/usr/local/gpucrate/367.48/lib64/libnvidia-ifr.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-ml.so.367.48\n/usr/local/gpucrate/367.48/lib64/libcuda.so.1\n/usr/local/gpucrate/367.48/lib64/libnvidia-encode.so.1\n/usr/local/gpucrate/367.48/lib64/libnvidia-tls.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-egl-wayland.so.367.48\n/usr/local/gpucrate/367.48/lib64/libOpenGL.so.0\n/usr/local/gpucrate/367.48/lib64/libcuda.so\n/usr/local/gpucrate/367.48/lib64/libnvidia-compiler.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-fatbinaryloader.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-opencl.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-ptxjitcompiler.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-fbc.so.1\n/usr/local/gpucrate/367.48/lib64/libnvidia-fbc.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-glsi.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-encode.so.367.48\n/usr/local/gpucrate/367.48/lib64/libnvidia-ifr.so.1\n```\n\nBy default gpucrate creates driver volumes in `/usr/local/gpucrate`. You can\nchange this via gpucrate's config file:\n\n```\necho 'volume_root: /path/to/volume/root' > /etc/gpucrate/config.yaml\n```\n\nor via the `GPUCRATE_VOLUME_ROOT` environment variable:\n\n```\nexport GPUCRATE_VOLUME_ROOT=\"/path/to/volume/root\"\n```\n\n### Using with Singularity\n**NOTE**: singularity-gpu requires Singularity 2.4+\n\nOnce a volume has been created for the currently active driver you can now use\nthe singularity wrapper `singularity-gpu` to run GPU-enabled containers.\n\nAs an example lets convert the [tensorflow/tensorflow:latest-gpu](https://hub.docker.com/r/tensorflow/tensorflow/)\ndocker image to a singularity image:\n\n```\n$ singularity build tensorflow.img docker://tensorflow/tensorflow:latest-gpu\n```\n\nNow use the `singularity-gpu` wrapper to run any singularity command as normal\nonly with the host's exact GPU driver linked in:\n\n```\n$ singularity-gpu exec tensorflow.img python -c 'import tensorflow'\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n```\n\nBy default singularity-gpu injects the required environment for NVIDIA/CUDA\ninside the container at run time. If this causes issues or you'd like to\ndisable this for any reason set the following in the gpucrate config file:\n\n```\necho 'manage_environment: false' > /etc/gpucrate/config.yaml\n```\n\nor use the `GPUCRATE_MANAGE_ENVIRONMENT` environment variable:\n\n```\nexport GPUCRATE_MANAGE_ENVIRONMENT=\"false\"\n```\n\n#### Container Requirements\nThe singularity-gpu wrapper uses the same conventions as NVIDIA's upstream\ndocker containers:\n\n1. NVIDIA driver volume binds to /usr/local/nvidia inside the container\n2. CUDA lives in /usr/local/cuda\n\nIf you have `enable overlay no` in your singularity config you'll need to\nensure that /usr/local/nvidia exists inside the container before attempting to\nuse `singularity-gpu`.\n\n### Using with Docker\nIt's much easier to just use [nvidia-docker](https://github.com/NVIDIA/nvidia-docker).\nIf you still insist try this (not tested and you'll need to adjust the devices,\nvolume root, and driver version for your system):\n\n```\n$ docker run -ti --rm \\\n--device=/dev/nvidiactl \\\n--device=/dev/nvidia-uvm \\\n--device=/dev/nvidia0 \\\n--device=/dev/nvidia1 \\\n--device=/dev/nvidia2\n--device=/dev/nvidia3 \\\n--volume-driver=nvidia-docker \\\n--volume=/usr/local/gpucrate/<driver_version>:/usr/local/nvidia:ro nvidia/cuda \\\nnvidia-smi\n```\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jtriley/gpucrate", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "gpucrate", "package_url": "https://pypi.org/project/gpucrate/", "platform": "", "project_url": "https://pypi.org/project/gpucrate/", "project_urls": {"Homepage": "https://github.com/jtriley/gpucrate"}, "release_url": "https://pypi.org/project/gpucrate/0.1.0/", "requires_dist": null, "requires_python": "", "summary": "gpucrate creates hard-linked GPU driver volumes for use with docker, singularity, etc.", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p># gpucrate</p>\n<p>[![build status](<a href=\"https://secure.travis-ci.org/jtriley/gpucrate.png?branch=master)%5D(https://secure.travis-ci.org/jtriley/gpucrate\" rel=\"nofollow\">https://secure.travis-ci.org/jtriley/gpucrate.png?branch=master)](https://secure.travis-ci.org/jtriley/gpucrate</a>)</p>\n<p>gpucrate creates hard-linked GPU driver (currently just NVIDIA) volumes for use\nwith docker, singularity, etc. This allows the exact system drivers to be\nlinked into a container without needing to maintain a separate container per\ndriver version.</p>\n<p>## Installation\nTo install gpucrate use the <cite>pip</cite> command:</p>\n<p><tt>`\n$ pip install gpucrate\n`</tt></p>\n<p>or in a [virtual environment](<a href=\"https://virtualenv.pypa.io/en/stable/\" rel=\"nofollow\">https://virtualenv.pypa.io/en/stable/</a>):</p>\n<p><tt>`\n$ virtualenv gpucrate\n$ source gpucrate/bin/activate\n$ pip install gpucrate\n`</tt></p>\n<p>## Usage\nTo create a driver volume for your system\u2019s current GPU driver:</p>\n<p><tt>`\n$ sudo gpucrate create\n`</tt></p>\n<p>This will create a hard-linked driver volume directory in <cite>/usr/local/gpucrate</cite>\nby default that can be used to link the drivers into a container. Here\u2019s an\nexample volume for driver version <cite>367.48</cite>:</p>\n<p><tt>`\n$ find /usr/local/gpucrate/367.48/\n/usr/local/gpucrate/367.48/\n/usr/local/gpucrate/367.48/bin\n<span class=\"pre\">/usr/local/gpucrate/367.48/bin/nvidia-cuda-mps-server</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/bin/nvidia-debugdump</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/bin/nvidia-persistenced</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/bin/nvidia-cuda-mps-control</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/bin/nvidia-smi</span>\n/usr/local/gpucrate/367.48/lib\n/usr/local/gpucrate/367.48/lib64\n/usr/local/gpucrate/367.48/lib64/libnvcuvid.so.367.48\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-ml.so.1</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-eglcore.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-glcore.so.367.48</span>\n/usr/local/gpucrate/367.48/lib64/libcuda.so.367.48\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-opencl.so.1</span>\n/usr/local/gpucrate/367.48/lib64/libnvcuvid.so.1\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-ifr.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-ml.so.367.48</span>\n/usr/local/gpucrate/367.48/lib64/libcuda.so.1\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-encode.so.1</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-tls.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-egl-wayland.so.367.48</span>\n/usr/local/gpucrate/367.48/lib64/libOpenGL.so.0\n/usr/local/gpucrate/367.48/lib64/libcuda.so\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-compiler.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-fatbinaryloader.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-opencl.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-ptxjitcompiler.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-fbc.so.1</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-fbc.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-glsi.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-encode.so.367.48</span>\n<span class=\"pre\">/usr/local/gpucrate/367.48/lib64/libnvidia-ifr.so.1</span>\n`</tt></p>\n<p>By default gpucrate creates driver volumes in <cite>/usr/local/gpucrate</cite>. You can\nchange this via gpucrate\u2019s config file:</p>\n<p><tt>`\necho 'volume_root: /path/to/volume/root' &gt; /etc/gpucrate/config.yaml\n`</tt></p>\n<p>or via the <cite>GPUCRATE_VOLUME_ROOT</cite> environment variable:</p>\n<p><tt>`\nexport <span class=\"pre\">GPUCRATE_VOLUME_ROOT=\"/path/to/volume/root\"</span>\n`</tt></p>\n<p>### Using with Singularity\n<strong>NOTE</strong>: singularity-gpu requires Singularity 2.4+</p>\n<p>Once a volume has been created for the currently active driver you can now use\nthe singularity wrapper <cite>singularity-gpu</cite> to run GPU-enabled containers.</p>\n<p>As an example lets convert the [tensorflow/tensorflow:latest-gpu](https://hub.docker.com/r/tensorflow/tensorflow/)\ndocker image to a singularity image:</p>\n<p><tt>`\n$ singularity build tensorflow.img <span class=\"pre\">docker://tensorflow/tensorflow:latest-gpu</span>\n`</tt></p>\n<p>Now use the <cite>singularity-gpu</cite> wrapper to run any singularity command as normal\nonly with the host\u2019s exact GPU driver linked in:</p>\n<p><tt>`\n$ <span class=\"pre\">singularity-gpu</span> exec tensorflow.img python <span class=\"pre\">-c</span> 'import tensorflow'\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n`</tt></p>\n<p>By default singularity-gpu injects the required environment for NVIDIA/CUDA\ninside the container at run time. If this causes issues or you\u2019d like to\ndisable this for any reason set the following in the gpucrate config file:</p>\n<p><tt>`\necho 'manage_environment: false' &gt; /etc/gpucrate/config.yaml\n`</tt></p>\n<p>or use the <cite>GPUCRATE_MANAGE_ENVIRONMENT</cite> environment variable:</p>\n<p><tt>`\nexport <span class=\"pre\">GPUCRATE_MANAGE_ENVIRONMENT=\"false\"</span>\n`</tt></p>\n<p>#### Container Requirements\nThe singularity-gpu wrapper uses the same conventions as NVIDIA\u2019s upstream\ndocker containers:</p>\n<ol>\n<li>NVIDIA driver volume binds to /usr/local/nvidia inside the container</li>\n<li>CUDA lives in /usr/local/cuda</li>\n</ol>\n<p>If you have <cite>enable overlay no</cite> in your singularity config you\u2019ll need to\nensure that /usr/local/nvidia exists inside the container before attempting to\nuse <cite>singularity-gpu</cite>.</p>\n<p>### Using with Docker\nIt\u2019s much easier to just use [nvidia-docker](<a href=\"https://github.com/NVIDIA/nvidia-docker\" rel=\"nofollow\">https://github.com/NVIDIA/nvidia-docker</a>).\nIf you still insist try this (not tested and you\u2019ll need to adjust the devices,\nvolume root, and driver version for your system):</p>\n<p><tt>`\n$ docker run <span class=\"pre\">-ti</span> <span class=\"pre\">--rm</span> \\\n<span class=\"pre\">--device=/dev/nvidiactl</span> \\\n<span class=\"pre\">--device=/dev/nvidia-uvm</span> \\\n<span class=\"pre\">--device=/dev/nvidia0</span> \\\n<span class=\"pre\">--device=/dev/nvidia1</span> \\\n<span class=\"pre\">--device=/dev/nvidia2</span>\n<span class=\"pre\">--device=/dev/nvidia3</span> \\\n<span class=\"pre\">--volume-driver=nvidia-docker</span> \\\n<span class=\"pre\">--volume=/usr/local/gpucrate/&lt;driver_version&gt;:/usr/local/nvidia:ro</span> nvidia/cuda \\\n<span class=\"pre\">nvidia-smi</span>\n`</tt></p>\n\n          </div>"}, "last_serial": 3343055, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "336f91b97e7646e4529272759f612690", "sha256": "03fb9a90b351a2b2608857a99c04e0dfe39c2851340f93f3fabb923c6a103a47"}, "downloads": -1, "filename": "gpucrate-0.1.0-py2-none-any.whl", "has_sig": false, "md5_digest": "336f91b97e7646e4529272759f612690", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 34181, "upload_time": "2017-11-17T23:47:07", "upload_time_iso_8601": "2017-11-17T23:47:07.644876Z", "url": "https://files.pythonhosted.org/packages/da/8f/43eb2999fdd3844d41575de063340537ca7d1a4118429bcc22a68e877718/gpucrate-0.1.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "57edffc9779eb88489c24a5c4953353a", "sha256": "d198b4a0128f77e4c62953825f63bfb95480d89ec644c056b476f50bae2334ba"}, "downloads": -1, "filename": "gpucrate-0.1.0.tar.gz", "has_sig": false, "md5_digest": "57edffc9779eb88489c24a5c4953353a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 27423, "upload_time": "2017-11-17T23:46:43", "upload_time_iso_8601": "2017-11-17T23:46:43.482139Z", "url": "https://files.pythonhosted.org/packages/0d/08/d1d1c7682e719b71a7a84b1aeaed441d547cf9997a3f203f1f717e56a5ad/gpucrate-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "336f91b97e7646e4529272759f612690", "sha256": "03fb9a90b351a2b2608857a99c04e0dfe39c2851340f93f3fabb923c6a103a47"}, "downloads": -1, "filename": "gpucrate-0.1.0-py2-none-any.whl", "has_sig": false, "md5_digest": "336f91b97e7646e4529272759f612690", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 34181, "upload_time": "2017-11-17T23:47:07", "upload_time_iso_8601": "2017-11-17T23:47:07.644876Z", "url": "https://files.pythonhosted.org/packages/da/8f/43eb2999fdd3844d41575de063340537ca7d1a4118429bcc22a68e877718/gpucrate-0.1.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "57edffc9779eb88489c24a5c4953353a", "sha256": "d198b4a0128f77e4c62953825f63bfb95480d89ec644c056b476f50bae2334ba"}, "downloads": -1, "filename": "gpucrate-0.1.0.tar.gz", "has_sig": false, "md5_digest": "57edffc9779eb88489c24a5c4953353a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 27423, "upload_time": "2017-11-17T23:46:43", "upload_time_iso_8601": "2017-11-17T23:46:43.482139Z", "url": "https://files.pythonhosted.org/packages/0d/08/d1d1c7682e719b71a7a84b1aeaed441d547cf9997a3f203f1f717e56a5ad/gpucrate-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:55:09 2020"}