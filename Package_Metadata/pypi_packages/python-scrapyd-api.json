{"info": {"author": "Darian Moody", "author_email": "mail@djm.org.uk", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Natural Language :: English", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Topic :: Internet :: WWW/HTTP"], "description": "# python-scrapyd-api\n\n[![The PyPI version](https://badge.fury.io/py/python-scrapyd-api.png)][pypi] [![Build status on Travis-CI](https://travis-ci.org/djm/python-scrapyd-api.png?branch=master)](https://travis-ci.org/djm/python-scrapyd-api) [![Coverage status on Coveralls](https://coveralls.io/repos/djm/python-scrapyd-api/badge.png)](https://coveralls.io/r/djm/python-scrapyd-api) [![Documentation status on ReadTheDocs](https://readthedocs.org/projects/python-scrapyd-api/badge/?version=latest)][docs]\n\nA Python wrapper for working with [Scrapyd][scrapyd]'s [API][scrapyd-api-docs].\n\nCurrent released version: 2.1.2 (see [history][history]).\n\nAllows a Python application to talk to, and therefore control, the\n[Scrapy][scrapy] daemon: [Scrapyd][scrapyd].\n\n* Supports Python 2.6, 2.7, 3.3 & 3.4\n* Free software: BSD license\n* [Full documentation][docs]\n* On the [Python Package Index (PyPI)][pypi]\n* Scrapyd's [API Documentation][scrapyd-api-docs]\n\n[scrapy]: http://scrapy.org/\n[scrapyd]: https://github.com/scrapy/scrapyd\n[scrapyd-api-docs]: http://scrapyd.readthedocs.org/en/latest/api.html\n[history]: https://github.com/djm/python-scrapyd-api/blob/master/HISTORY.md\n[pypi]: https://pypi.python.org/pypi/python-scrapyd-api/\n[docs]: http://python-scrapyd-api.readthedocs.org/en/latest/\n\n## Install\n\nEasiest installation is via `pip`:\n\n```bash\npip install python-scrapyd-api\n```\n\n## Quick Usage\n\nPlease refer to the [full documentation][docs] for more detailed usage but to get you started:\n\n```python\n>>> from scrapyd_api import ScrapydAPI\n>>> scrapyd = ScrapydAPI('http://localhost:6800')\n```\n\n**Add a project** egg as a new version:\n\n```python\n>>> egg = open('some_egg.egg', 'rb')\n>>> scrapyd.add_version('project_name', 'version_name', egg)\n# Returns the number of spiders in the project.\n3\n>>> egg.close()\n```\n\n**Cancel a scheduled job**:\n\n```python\n>>> scrapyd.cancel('project_name', '14a6599ef67111e38a0e080027880ca6')\n# Returns the \"previous state\" of the job before it was cancelled: 'running' or 'pending'.\n'running'\n```\n\n**Delete a project** and all sibling versions:\n\n```python\n>>> scrapyd.delete_project('project_name')\n# Returns True if the request was met with an OK response.\nTrue\n```\n\n**Delete a version** of a project:\n\n```python\n>>> scrapyd.delete_version('project_name', 'version_name')\n# Returns True if the request was met with an OK response.\nTrue\n```\n\n**Request status** of a job:\n\n```python\n>>> scrapyd.job_status('project_name', '14a6599ef67111e38a0e080027880ca6')\n# Returns 'running', 'pending', 'finished' or '' for unknown state.\n'running'\n```\n\n**List all jobs** registered:\n\n```python\n>>> scrapyd.list_jobs('project_name')\n# Returns a dict of running, finished and pending job lists.\n{\n    'pending': [\n        {\n            u'id': u'24c35...f12ae',\n            u'spider': u'spider_name'\n        },\n    ],\n    'running': [\n        {\n            u'id': u'14a65...b27ce',\n            u'spider': u'spider_name',\n            u'start_time': u'2014-06-17 22:45:31.975358'\n        },\n    ],\n    'finished': [\n        {\n            u'id': u'34c23...b21ba',\n            u'spider': u'spider_name',\n            u'start_time': u'2014-06-17 22:45:31.975358',\n            u'end_time': u'2014-06-23 14:01:18.209680'\n        }\n    ]\n}\n```\n\n**List all projects** registered:\n\n```python\n>>> scrapyd.list_projects()\n[u'ecom_project', u'estate_agent_project', u'car_project']\n```\n\n**List all spiders** available to a given project:\n\n```python\n>>> scrapyd.list_spiders('project_name')\n[u'raw_spider', u'js_enhanced_spider', u'selenium_spider']\n```\n\n**List all versions** registered to a given project:\n\n```python\n>>> scrapyd.list_versions('project_name'):\n[u'345', u'346', u'347', u'348']\n```\n\n**Schedule a job** to run with a specific spider:\n\n```python\n# Schedule a job to run with a specific spider.\n>>> scrapyd.schedule('project_name', 'spider_name')\n# Returns the Scrapyd job id.\nu'14a6599ef67111e38a0e080027880ca6'\n```\n\n**Schedule a job** to run while passing override settings:\n\n```python\n>>> settings = {'DOWNLOAD_DELAY': 2}\n>>> scrapyd.schedule('project_name', 'spider_name', settings=settings)\nu'25b6588ef67333e38a0e080027880de7'\n```\n\n**Schedule a job** to run while passing extra attributes to spider initialisation:\n\n```python\n>>> scrapyd.schedule('project_name', 'spider_name', extra_attribute='value')\n# NB: 'project', 'spider' and 'settings' are reserved kwargs for this\n# method and therefore these names should be avoided when trying to pass\n# extra attributes to the spider init.\nu'25b6588ef67333e38a0e080027880de7'\n```\n\n\n## Setting up the project to contribute code\n\nPlease see [CONTRIBUTING.md][contributing].  This will guide you through our pull request\nguidelines, project setup and testing requirements.\n\n[contributing]: https://github.com/djm/python-scrapyd-api/blob/master/CONTRIBUTING.md\n\n## License\n\n2-clause BSD. See the full [LICENSE][license].\n\n[license]: https://github.com/djm/python-scrapyd-api/blob/master/LICENSE\n\n\n# History\n\n## 2.1.1 (2018-04-01)\n\n* Base set of docs converted to markdown (README, AUTHORS, CONTRIBUTING, HISTORY)\n\n## 2.1.0 (2018-03-31)\n\n* Introduces the `timeout` keyword argument, which allows the caller to specify\n  a timeout after which requests to the scrapyd server give up. This works as\n  per the underlying `requests` library, and raises `requests.exceptions.Timeout`\n  when the timeout is exceeded. See docs for usage.\n\n\n## 2.0.1 (2016-02-27)\n\nv2.0.0 shipped with docs which were slightly out of date for the cancel\nendpoint, this release corrects that.\n\n## 2.0.0 (2016-02-27)\n\nWhy Version 2? This package has been production ready and stable in use\nfor over a year now, so it's ready  to commit to a stable API /w semver.\nVersion 1 has deliberately been skipped to make it absolutely clear that\nthis release contains a breaking change:\n\nBreaking changes:\n\n* The cancel job endpoint now returns the previous state of the successfully\n  cancelled spider rather than a simple boolean True/False. This change was\n  made because:\n    a) the boolean return was relatively useless and actually hiding data the\n       scrapyd API passes us as part of the cancel endpoint response.\n    b) before this change, the method would have returned `True` only if the\n       cancelled job was previously running, and this resulted in us incorrectly\n       reporting `False` when a *pending* job was cancelled.\n  This may require no changes to your codebase but nevertheless it is a change\n  in a public API, thus the requirement for major version bumping.\n\nOther changes:\n\n* The cancel job endpoint now accepts a `signal` keyword argument which is\n  the termination signal Scrapyd uses to cancel the spider job. If not\n  specified, the value is not sent to the scrapyd endpoint at all, therefore\n  allows scrapyd control over which default signal gets used (currently `TERM`).\n\n\n## 0.2.0 (2015-01-14)\n\n* Added the new ``job_status`` method which can retrieve the job status of a\n  specific job from a project. See docs for usage.\n* Increased and improved test coverage.\n\n## 0.1.0 (2014-09-16)\n\n* First release on PyPI.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/djm/python-scrapyd-api", "keywords": "python-scrapyd-api scrapyd scrapy api wrapper", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "python-scrapyd-api", "package_url": "https://pypi.org/project/python-scrapyd-api/", "platform": "", "project_url": "https://pypi.org/project/python-scrapyd-api/", "project_urls": {"Homepage": "https://github.com/djm/python-scrapyd-api"}, "release_url": "https://pypi.org/project/python-scrapyd-api/2.1.2/", "requires_dist": null, "requires_python": "", "summary": "A Python wrapper for working with the Scrapyd API", "version": "2.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>python-scrapyd-api</h1>\n<p><a href=\"https://pypi.python.org/pypi/python-scrapyd-api/\" rel=\"nofollow\"><img alt=\"The PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0f061e4a3ba63f791e765ee42b7de5bcaec6248e/68747470733a2f2f62616467652e667572792e696f2f70792f707974686f6e2d736372617079642d6170692e706e67\"></a> <a href=\"https://travis-ci.org/djm/python-scrapyd-api\" rel=\"nofollow\"><img alt=\"Build status on Travis-CI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5ae3632e9c3b2c9a526ceb45ebaa6daaa557ed7e/68747470733a2f2f7472617669732d63692e6f72672f646a6d2f707974686f6e2d736372617079642d6170692e706e673f6272616e63683d6d6173746572\"></a> <a href=\"https://coveralls.io/r/djm/python-scrapyd-api\" rel=\"nofollow\"><img alt=\"Coverage status on Coveralls\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/951d0c11a5b47dd5fce8ba761cdc6e2d3b65c372/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f646a6d2f707974686f6e2d736372617079642d6170692f62616467652e706e67\"></a> <a href=\"http://python-scrapyd-api.readthedocs.org/en/latest/\" rel=\"nofollow\"><img alt=\"Documentation status on ReadTheDocs\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/407406512c58729e3b5910d8c54bf737a1e4263f/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f707974686f6e2d736372617079642d6170692f62616467652f3f76657273696f6e3d6c6174657374\"></a></p>\n<p>A Python wrapper for working with <a href=\"https://github.com/scrapy/scrapyd\" rel=\"nofollow\">Scrapyd</a>'s <a href=\"http://scrapyd.readthedocs.org/en/latest/api.html\" rel=\"nofollow\">API</a>.</p>\n<p>Current released version: 2.1.2 (see <a href=\"https://github.com/djm/python-scrapyd-api/blob/master/HISTORY.md\" rel=\"nofollow\">history</a>).</p>\n<p>Allows a Python application to talk to, and therefore control, the\n<a href=\"http://scrapy.org/\" rel=\"nofollow\">Scrapy</a> daemon: <a href=\"https://github.com/scrapy/scrapyd\" rel=\"nofollow\">Scrapyd</a>.</p>\n<ul>\n<li>Supports Python 2.6, 2.7, 3.3 &amp; 3.4</li>\n<li>Free software: BSD license</li>\n<li><a href=\"http://python-scrapyd-api.readthedocs.org/en/latest/\" rel=\"nofollow\">Full documentation</a></li>\n<li>On the <a href=\"https://pypi.python.org/pypi/python-scrapyd-api/\" rel=\"nofollow\">Python Package Index (PyPI)</a></li>\n<li>Scrapyd's <a href=\"http://scrapyd.readthedocs.org/en/latest/api.html\" rel=\"nofollow\">API Documentation</a></li>\n</ul>\n<h2>Install</h2>\n<p>Easiest installation is via <code>pip</code>:</p>\n<pre>pip install python-scrapyd-api\n</pre>\n<h2>Quick Usage</h2>\n<p>Please refer to the <a href=\"http://python-scrapyd-api.readthedocs.org/en/latest/\" rel=\"nofollow\">full documentation</a> for more detailed usage but to get you started:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">scrapyd_api</span> <span class=\"kn\">import</span> <span class=\"n\">ScrapydAPI</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span> <span class=\"o\">=</span> <span class=\"n\">ScrapydAPI</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:6800'</span><span class=\"p\">)</span>\n</pre>\n<p><strong>Add a project</strong> egg as a new version:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">egg</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'some_egg.egg'</span><span class=\"p\">,</span> <span class=\"s1\">'rb'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">add_version</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">,</span> <span class=\"s1\">'version_name'</span><span class=\"p\">,</span> <span class=\"n\">egg</span><span class=\"p\">)</span>\n<span class=\"c1\"># Returns the number of spiders in the project.</span>\n<span class=\"mi\">3</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">egg</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Cancel a scheduled job</strong>:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">cancel</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">,</span> <span class=\"s1\">'14a6599ef67111e38a0e080027880ca6'</span><span class=\"p\">)</span>\n<span class=\"c1\"># Returns the \"previous state\" of the job before it was cancelled: 'running' or 'pending'.</span>\n<span class=\"s1\">'running'</span>\n</pre>\n<p><strong>Delete a project</strong> and all sibling versions:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">delete_project</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">)</span>\n<span class=\"c1\"># Returns True if the request was met with an OK response.</span>\n<span class=\"kc\">True</span>\n</pre>\n<p><strong>Delete a version</strong> of a project:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">delete_version</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">,</span> <span class=\"s1\">'version_name'</span><span class=\"p\">)</span>\n<span class=\"c1\"># Returns True if the request was met with an OK response.</span>\n<span class=\"kc\">True</span>\n</pre>\n<p><strong>Request status</strong> of a job:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">job_status</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">,</span> <span class=\"s1\">'14a6599ef67111e38a0e080027880ca6'</span><span class=\"p\">)</span>\n<span class=\"c1\"># Returns 'running', 'pending', 'finished' or '' for unknown state.</span>\n<span class=\"s1\">'running'</span>\n</pre>\n<p><strong>List all jobs</strong> registered:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">list_jobs</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">)</span>\n<span class=\"c1\"># Returns a dict of running, finished and pending job lists.</span>\n<span class=\"p\">{</span>\n    <span class=\"s1\">'pending'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"sa\">u</span><span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s1\">'24c35...f12ae'</span><span class=\"p\">,</span>\n            <span class=\"sa\">u</span><span class=\"s1\">'spider'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s1\">'spider_name'</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s1\">'running'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"sa\">u</span><span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s1\">'14a65...b27ce'</span><span class=\"p\">,</span>\n            <span class=\"sa\">u</span><span class=\"s1\">'spider'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s1\">'spider_name'</span><span class=\"p\">,</span>\n            <span class=\"sa\">u</span><span class=\"s1\">'start_time'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s1\">'2014-06-17 22:45:31.975358'</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"s1\">'finished'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"sa\">u</span><span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s1\">'34c23...b21ba'</span><span class=\"p\">,</span>\n            <span class=\"sa\">u</span><span class=\"s1\">'spider'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s1\">'spider_name'</span><span class=\"p\">,</span>\n            <span class=\"sa\">u</span><span class=\"s1\">'start_time'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s1\">'2014-06-17 22:45:31.975358'</span><span class=\"p\">,</span>\n            <span class=\"sa\">u</span><span class=\"s1\">'end_time'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s1\">'2014-06-23 14:01:18.209680'</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</pre>\n<p><strong>List all projects</strong> registered:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">list_projects</span><span class=\"p\">()</span>\n<span class=\"p\">[</span><span class=\"sa\">u</span><span class=\"s1\">'ecom_project'</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s1\">'estate_agent_project'</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s1\">'car_project'</span><span class=\"p\">]</span>\n</pre>\n<p><strong>List all spiders</strong> available to a given project:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">list_spiders</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"sa\">u</span><span class=\"s1\">'raw_spider'</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s1\">'js_enhanced_spider'</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s1\">'selenium_spider'</span><span class=\"p\">]</span>\n</pre>\n<p><strong>List all versions</strong> registered to a given project:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">list_versions</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">):</span>\n<span class=\"p\">[</span><span class=\"sa\">u</span><span class=\"s1\">'345'</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s1\">'346'</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s1\">'347'</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s1\">'348'</span><span class=\"p\">]</span>\n</pre>\n<p><strong>Schedule a job</strong> to run with a specific spider:</p>\n<pre><span class=\"c1\"># Schedule a job to run with a specific spider.</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">schedule</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">,</span> <span class=\"s1\">'spider_name'</span><span class=\"p\">)</span>\n<span class=\"c1\"># Returns the Scrapyd job id.</span>\n<span class=\"sa\">u</span><span class=\"s1\">'14a6599ef67111e38a0e080027880ca6'</span>\n</pre>\n<p><strong>Schedule a job</strong> to run while passing override settings:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">settings</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">'DOWNLOAD_DELAY'</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">}</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">schedule</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">,</span> <span class=\"s1\">'spider_name'</span><span class=\"p\">,</span> <span class=\"n\">settings</span><span class=\"o\">=</span><span class=\"n\">settings</span><span class=\"p\">)</span>\n<span class=\"sa\">u</span><span class=\"s1\">'25b6588ef67333e38a0e080027880de7'</span>\n</pre>\n<p><strong>Schedule a job</strong> to run while passing extra attributes to spider initialisation:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">scrapyd</span><span class=\"o\">.</span><span class=\"n\">schedule</span><span class=\"p\">(</span><span class=\"s1\">'project_name'</span><span class=\"p\">,</span> <span class=\"s1\">'spider_name'</span><span class=\"p\">,</span> <span class=\"n\">extra_attribute</span><span class=\"o\">=</span><span class=\"s1\">'value'</span><span class=\"p\">)</span>\n<span class=\"c1\"># NB: 'project', 'spider' and 'settings' are reserved kwargs for this</span>\n<span class=\"c1\"># method and therefore these names should be avoided when trying to pass</span>\n<span class=\"c1\"># extra attributes to the spider init.</span>\n<span class=\"sa\">u</span><span class=\"s1\">'25b6588ef67333e38a0e080027880de7'</span>\n</pre>\n<h2>Setting up the project to contribute code</h2>\n<p>Please see <a href=\"https://github.com/djm/python-scrapyd-api/blob/master/CONTRIBUTING.md\" rel=\"nofollow\">CONTRIBUTING.md</a>.  This will guide you through our pull request\nguidelines, project setup and testing requirements.</p>\n<h2>License</h2>\n<p>2-clause BSD. See the full <a href=\"https://github.com/djm/python-scrapyd-api/blob/master/LICENSE\" rel=\"nofollow\">LICENSE</a>.</p>\n<h1>History</h1>\n<h2>2.1.1 (2018-04-01)</h2>\n<ul>\n<li>Base set of docs converted to markdown (README, AUTHORS, CONTRIBUTING, HISTORY)</li>\n</ul>\n<h2>2.1.0 (2018-03-31)</h2>\n<ul>\n<li>Introduces the <code>timeout</code> keyword argument, which allows the caller to specify\na timeout after which requests to the scrapyd server give up. This works as\nper the underlying <code>requests</code> library, and raises <code>requests.exceptions.Timeout</code>\nwhen the timeout is exceeded. See docs for usage.</li>\n</ul>\n<h2>2.0.1 (2016-02-27)</h2>\n<p>v2.0.0 shipped with docs which were slightly out of date for the cancel\nendpoint, this release corrects that.</p>\n<h2>2.0.0 (2016-02-27)</h2>\n<p>Why Version 2? This package has been production ready and stable in use\nfor over a year now, so it's ready  to commit to a stable API /w semver.\nVersion 1 has deliberately been skipped to make it absolutely clear that\nthis release contains a breaking change:</p>\n<p>Breaking changes:</p>\n<ul>\n<li>The cancel job endpoint now returns the previous state of the successfully\ncancelled spider rather than a simple boolean True/False. This change was\nmade because:\na) the boolean return was relatively useless and actually hiding data the\nscrapyd API passes us as part of the cancel endpoint response.\nb) before this change, the method would have returned <code>True</code> only if the\ncancelled job was previously running, and this resulted in us incorrectly\nreporting <code>False</code> when a <em>pending</em> job was cancelled.\nThis may require no changes to your codebase but nevertheless it is a change\nin a public API, thus the requirement for major version bumping.</li>\n</ul>\n<p>Other changes:</p>\n<ul>\n<li>The cancel job endpoint now accepts a <code>signal</code> keyword argument which is\nthe termination signal Scrapyd uses to cancel the spider job. If not\nspecified, the value is not sent to the scrapyd endpoint at all, therefore\nallows scrapyd control over which default signal gets used (currently <code>TERM</code>).</li>\n</ul>\n<h2>0.2.0 (2015-01-14)</h2>\n<ul>\n<li>Added the new <code>job_status</code> method which can retrieve the job status of a\nspecific job from a project. See docs for usage.</li>\n<li>Increased and improved test coverage.</li>\n</ul>\n<h2>0.1.0 (2014-09-16)</h2>\n<ul>\n<li>First release on PyPI.</li>\n</ul>\n\n          </div>"}, "last_serial": 3724708, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "e3a810aefe1c62c76ac34ba5b216e02e", "sha256": "bb5f11d461c930c5541c74e8c1fb5a0e7c86971773ed72727b2710e1d36b28f1"}, "downloads": -1, "filename": "python_scrapyd_api-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "e3a810aefe1c62c76ac34ba5b216e02e", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 9417, "upload_time": "2014-09-16T12:45:18", "upload_time_iso_8601": "2014-09-16T12:45:18.343729Z", "url": "https://files.pythonhosted.org/packages/cc/3d/872f6e2cba4354b599eb628f7fd9f2d53f10364bd4a4de499bc2b99d16b1/python_scrapyd_api-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cb45354a1545eddd4124c32ea798a52a", "sha256": "9998ccec6791648e5fc382478f48ba7b4a05725dc57aa1ca9136211c4583e3c6"}, "downloads": -1, "filename": "python-scrapyd-api-0.1.0.tar.gz", "has_sig": false, "md5_digest": "cb45354a1545eddd4124c32ea798a52a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19804, "upload_time": "2014-09-16T12:45:15", "upload_time_iso_8601": "2014-09-16T12:45:15.727494Z", "url": "https://files.pythonhosted.org/packages/db/30/a243f832bfd95db556ea494bb5372f5dbbfc9c26a4c38f7ab01dff57c4ba/python-scrapyd-api-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "75c4bf08f729d2afb5ca9f56cd1e79bc", "sha256": "65d689b28878682bfceb93cc4dcda26261fc6da91d8bb7683f71a0263b370adc"}, "downloads": -1, "filename": "python_scrapyd_api-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "75c4bf08f729d2afb5ca9f56cd1e79bc", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 10324, "upload_time": "2015-01-15T00:40:43", "upload_time_iso_8601": "2015-01-15T00:40:43.082663Z", "url": "https://files.pythonhosted.org/packages/7f/f8/6d7450dc0e6c5446776024096d2ba541ab9e1de2a1282b6345cfd1e5203d/python_scrapyd_api-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1d4dd817b9cb6408780285d84213713e", "sha256": "674093e2cd0e3d0802bf91d24379713ae4427d01642f25aee1b1c4f2710d187c"}, "downloads": -1, "filename": "python-scrapyd-api-0.2.0.tar.gz", "has_sig": false, "md5_digest": "1d4dd817b9cb6408780285d84213713e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23304, "upload_time": "2015-01-15T00:40:39", "upload_time_iso_8601": "2015-01-15T00:40:39.435794Z", "url": "https://files.pythonhosted.org/packages/8e/07/75da8d9b274592b05771be6753e647ba31cfd85f29896874865b0da66bb6/python-scrapyd-api-0.2.0.tar.gz", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "1a2262efd3e9b469eec434926f68e44b", "sha256": "ddd9098bdf1f5c69c1be186d68a7b3aed2c79f47fa51b7a22697e5da89f6b37f"}, "downloads": -1, "filename": "python_scrapyd_api-2.0.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "1a2262efd3e9b469eec434926f68e44b", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 11217, "upload_time": "2016-03-27T15:54:26", "upload_time_iso_8601": "2016-03-27T15:54:26.512596Z", "url": "https://files.pythonhosted.org/packages/d8/9b/d24fb7cf3db9bb0a10c64d6f56e3d634ea1e833ab5d3b34be62ddc63625d/python_scrapyd_api-2.0.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f2e19deab07fd35730cc1f1b4073bd16", "sha256": "c8cf0feed8d2aa16354ae2d452b09b4dc3d72366fa8923157cfc0f60f0ff9bad"}, "downloads": -1, "filename": "python-scrapyd-api-2.0.0.tar.gz", "has_sig": false, "md5_digest": "f2e19deab07fd35730cc1f1b4073bd16", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21863, "upload_time": "2016-03-27T15:54:16", "upload_time_iso_8601": "2016-03-27T15:54:16.280946Z", "url": "https://files.pythonhosted.org/packages/87/44/8da74fe399f146144f9d0b9a4415db40f97f7f49fac53e7d43fd0ea316fd/python-scrapyd-api-2.0.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "0407528e913e6910d34d5db5941bb4ab", "sha256": "536c05a2532a18f1511e0cf0eaa0e8539dda74dd88a7e5a1bdf8ba83265d4251"}, "downloads": -1, "filename": "python_scrapyd_api-2.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "0407528e913e6910d34d5db5941bb4ab", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 11327, "upload_time": "2016-03-27T16:15:25", "upload_time_iso_8601": "2016-03-27T16:15:25.883151Z", "url": "https://files.pythonhosted.org/packages/41/b6/9431b7c30f86a37348ef17669bf19b41290ca43223ed1d7dab29bf51d3d7/python_scrapyd_api-2.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "76641febbe1dd20f08373404922ed21a", "sha256": "5d09e4bb8665fec54dc240bebebfb80b5c99c0488df45fd4b1e6ad50e52599f9"}, "downloads": -1, "filename": "python-scrapyd-api-2.0.1.tar.gz", "has_sig": false, "md5_digest": "76641febbe1dd20f08373404922ed21a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21364, "upload_time": "2016-03-27T16:15:19", "upload_time_iso_8601": "2016-03-27T16:15:19.688598Z", "url": "https://files.pythonhosted.org/packages/5f/98/59bf3c3779aadc2ecfd228d6334d20c87f1dc969218941be281fa5072240/python-scrapyd-api-2.0.1.tar.gz", "yanked": false}], "2.1.0": [{"comment_text": "", "digests": {"md5": "c0de3cd3c163378bd89844fbdf99cf79", "sha256": "9fa09641b97363cdc753a299402ccb675f971a71c36a059e869f52314118c200"}, "downloads": -1, "filename": "python_scrapyd_api-2.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c0de3cd3c163378bd89844fbdf99cf79", "packagetype": "bdist_wheel", "python_version": "3.6", "requires_python": null, "size": 12122, "upload_time": "2018-03-31T21:16:16", "upload_time_iso_8601": "2018-03-31T21:16:16.767944Z", "url": "https://files.pythonhosted.org/packages/ad/27/2a1db74c985cd6251d651d2b37b7306683e9848eb75ac3f164b4f9884f55/python_scrapyd_api-2.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "814e6f39b5784d006d7fba228b49e728", "sha256": "4250f8fc3fe254210af5b003e6064c31f48b4560cd6565432a962a120dd41449"}, "downloads": -1, "filename": "python-scrapyd-api-2.1.0.tar.gz", "has_sig": false, "md5_digest": "814e6f39b5784d006d7fba228b49e728", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25214, "upload_time": "2018-03-31T21:16:15", "upload_time_iso_8601": "2018-03-31T21:16:15.134659Z", "url": "https://files.pythonhosted.org/packages/88/58/9499473061e5f31d40495807a22b8ff6b4cc14d37238b38722c1633793d8/python-scrapyd-api-2.1.0.tar.gz", "yanked": false}], "2.1.2": [{"comment_text": "", "digests": {"md5": "f5bc6b0c39b578f576be4705cf20f9ce", "sha256": "ab92d3461a81f46aaa6d82cc3de610642892c5454ddebb74d2b81fbc55e9c807"}, "downloads": -1, "filename": "python_scrapyd_api-2.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "f5bc6b0c39b578f576be4705cf20f9ce", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 12142, "upload_time": "2018-04-01T18:34:43", "upload_time_iso_8601": "2018-04-01T18:34:43.751930Z", "url": "https://files.pythonhosted.org/packages/13/13/cf8bbd7a6462a805c26bfd8eb92a0fc1f5e69a13fa2e5fbd87360943cacc/python_scrapyd_api-2.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cd62272e031d944d657e97ceebb837c6", "sha256": "e64f7caab23e541967fdf78926af5e6be6a863227f78d16b7ebb3a35acfc2099"}, "downloads": -1, "filename": "python-scrapyd-api-2.1.2.tar.gz", "has_sig": false, "md5_digest": "cd62272e031d944d657e97ceebb837c6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24717, "upload_time": "2018-04-01T18:34:34", "upload_time_iso_8601": "2018-04-01T18:34:34.672250Z", "url": "https://files.pythonhosted.org/packages/db/fb/7ba0c7242741c8352dab11a26106b23d8740ce98323633defd4656909bd0/python-scrapyd-api-2.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f5bc6b0c39b578f576be4705cf20f9ce", "sha256": "ab92d3461a81f46aaa6d82cc3de610642892c5454ddebb74d2b81fbc55e9c807"}, "downloads": -1, "filename": "python_scrapyd_api-2.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "f5bc6b0c39b578f576be4705cf20f9ce", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 12142, "upload_time": "2018-04-01T18:34:43", "upload_time_iso_8601": "2018-04-01T18:34:43.751930Z", "url": "https://files.pythonhosted.org/packages/13/13/cf8bbd7a6462a805c26bfd8eb92a0fc1f5e69a13fa2e5fbd87360943cacc/python_scrapyd_api-2.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cd62272e031d944d657e97ceebb837c6", "sha256": "e64f7caab23e541967fdf78926af5e6be6a863227f78d16b7ebb3a35acfc2099"}, "downloads": -1, "filename": "python-scrapyd-api-2.1.2.tar.gz", "has_sig": false, "md5_digest": "cd62272e031d944d657e97ceebb837c6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24717, "upload_time": "2018-04-01T18:34:34", "upload_time_iso_8601": "2018-04-01T18:34:34.672250Z", "url": "https://files.pythonhosted.org/packages/db/fb/7ba0c7242741c8352dab11a26106b23d8740ce98323633defd4656909bd0/python-scrapyd-api-2.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:15:29 2020"}