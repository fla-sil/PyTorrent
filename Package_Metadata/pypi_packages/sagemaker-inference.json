{"info": {"author": "Amazon Web Services", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "![SageMaker](https://github.com/aws/sagemaker-inference-toolkit/raw/master/branding/icon/sagemaker-banner.png)\n\n# SageMaker Inference Toolkit\n\n[![Latest Version](https://img.shields.io/pypi/v/sagemaker-inference.svg)](https://pypi.python.org/pypi/sagemaker-inference) [![Supported Python Versions](https://img.shields.io/pypi/pyversions/sagemaker-inference.svg)](https://pypi.python.org/pypi/sagemaker-inference) [![Code Style: Black](https://img.shields.io/badge/code_style-black-000000.svg)](https://github.com/python/black)\n\nServe machine learning models within a Docker container using Amazon\nSageMaker.\n\n\n## :books: Background\n\n[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service for data science and machine learning (ML) workflows.\nYou can use Amazon SageMaker to simplify the process of building, training, and deploying ML models.\n\nOnce you have a trained model, you can include it in a [Docker container](https://www.docker.com/resources/what-container) that runs your inference code.\nA container provides an effectively isolated environment, ensuring a consistent runtime regardless of where the container is deployed.\nContainerizing your model and code enables fast and reliable deployment of your model.\n\nThe **SageMaker Inference Toolkit** implements a model serving stack and can be easily added to any Docker container, making it [deployable to SageMaker](https://aws.amazon.com/sagemaker/deploy/).\nThis library's serving stack is built on [Multi Model Server](https://github.com/awslabs/mxnet-model-server), and it can serve your own models or those you trained on SageMaker using [machine learning frameworks with native SageMaker support](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html).\nIf you use a [prebuilt SageMaker Docker image for inference](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html), this library may already be included.\n\nFor more information, see the Amazon SageMaker Developer Guide sections on [building your own container with Multi Model Server](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html) and [using your own models](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html).\n\n## :hammer_and_wrench: Installation\n\nTo install this library in your Docker image, add the following line to your [Dockerfile](https://docs.docker.com/engine/reference/builder/):\n\n``` dockerfile\nRUN pip3 install multi-model-server sagemaker-inference-toolkit\n```\n\n[Here is an example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_bring_your_own/container/Dockerfile) of a Dockerfile that installs SageMaker Inference Toolkit.\n\n## :computer: Usage\n\n### Implementation Steps\n\nTo use the SageMaker Inference Toolkit, you need to do the following:\n\n1.  Implement an inference handler, which is responsible for loading the model and providing input, predict, and output functions.\n    ([Here is an example](https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/default_inference_handler.py) of an inference handler.)\n\n    ``` python\n    from sagemaker_inference import content_types, decoder, default_inference_handler, encoder, errors\n\n    class DefaultPytorchInferenceHandler(default_inference_handler.DefaultInferenceHandler):\n\n        def default_model_fn(self, model_dir):\n            \"\"\"Loads a model. For PyTorch, a default function to load a model cannot be provided.\n            Users should provide customized model_fn() in script.\n\n            Args:\n                model_dir: a directory where model is saved.\n\n            Returns: A PyTorch model.\n            \"\"\"\n            raise NotImplementedError(textwrap.dedent(\"\"\"\n            Please provide a model_fn implementation.\n            See documentation for model_fn at https://github.com/aws/sagemaker-python-sdk\n            \"\"\"))\n\n        def default_input_fn(self, input_data, content_type):\n            \"\"\"A default input_fn that can handle JSON, CSV and NPZ formats.\n\n            Args:\n                input_data: the request payload serialized in the content_type format\n                content_type: the request content_type\n\n            Returns: input_data deserialized into torch.FloatTensor or torch.cuda.FloatTensor depending if cuda is available.\n            \"\"\"\n            return decoder.decode(input_data, content_type)\n\n        def default_predict_fn(self, data, model):\n            \"\"\"A default predict_fn for PyTorch. Calls a model on data deserialized in input_fn.\n            Runs prediction on GPU if cuda is available.\n\n            Args:\n                data: input data (torch.Tensor) for prediction deserialized by input_fn\n                model: PyTorch model loaded in memory by model_fn\n\n            Returns: a prediction\n            \"\"\"\n            return model(input_data)\n\n        def default_output_fn(self, prediction, accept):\n            \"\"\"A default output_fn for PyTorch. Serializes predictions from predict_fn to JSON, CSV or NPY format.\n\n            Args:\n                prediction: a prediction result from predict_fn\n                accept: type which the output data needs to be serialized\n\n            Returns: output data serialized\n            \"\"\"\n            return encoder.encode(prediction, accept)\n    ```\n\n2.  Implement a handler service that is executed by the model server.\n    ([Here is an example](https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/handler_service.py) of a handler service.)\n    For more information on how to define your `HANDLER_SERVICE` file, see [the MMS custom service documentation](https://github.com/awslabs/mxnet-model-server/blob/master/docs/custom_service.md).\n\n    ``` python\n    from sagemaker_inference.default_handler_service import DefaultHandlerService\n    from sagemaker_inference.transformer import Transformer\n    from sagemaker_pytorch_serving_container.default_inference_handler import DefaultPytorchInferenceHandler\n\n\n    class HandlerService(DefaultHandlerService):\n        \"\"\"Handler service that is executed by the model server.\n        Determines specific default inference handlers to use based on model being used.\n        This class extends ``DefaultHandlerService``, which define the following:\n            - The ``handle`` method is invoked for all incoming inference requests to the model server.\n            - The ``initialize`` method is invoked at model server start up.\n        Based on: https://github.com/awslabs/mxnet-model-server/blob/master/docs/custom_service.md\n        \"\"\"\n        def __init__(self):\n            transformer = Transformer(default_inference_handler=DefaultPytorchInferenceHandler())\n            super(HandlerService, self).__init__(transformer=transformer)\n    ```\n\n3.  Implement a serving entrypoint, which starts the model server.\n    ([Here is an example](https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/serving.py) of a serving entrypoint.)\n\n    ``` python\n    from sagemaker_inference import model_server\n\n    model_server.start_model_server(handler_service=HANDLER_SERVICE)\n    ```\n\n4.  Define the location of the entrypoint in your Dockerfile.\n\n    ``` dockerfile\n    ENTRYPOINT [\"python\", \"/usr/local/bin/entrypoint.py\"]\n    ```\n\n### Complete Example\n\n[Here is a complete example](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_bring_your_own) demonstrating usage of the SageMaker Inference Toolkit in your own container for deployment to a multi-model endpoint.\n\n## :scroll: License\n\nThis library is licensed under the [Apache 2.0 License](http://aws.amazon.com/apache2.0/).\nFor more details, please take a look at the [LICENSE](https://github.com/aws-samples/sagemaker-inference-toolkit/blob/master/LICENSE) file.\n\n## :handshake: Contributing\n\nContributions are welcome!\nPlease read our [contributing guidelines](https://github.com/aws/sagemaker-inference-toolkit/blob/master/CONTRIBUTING.md)\nif you'd like to open an issue or submit a pull request.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/aws/sagemaker-inference-toolkit/", "keywords": "", "license": "Apache License 2.0", "maintainer": "", "maintainer_email": "", "name": "sagemaker-inference", "package_url": "https://pypi.org/project/sagemaker-inference/", "platform": "", "project_url": "https://pypi.org/project/sagemaker-inference/", "project_urls": {"Homepage": "https://github.com/aws/sagemaker-inference-toolkit/"}, "release_url": "https://pypi.org/project/sagemaker-inference/1.3.0/", "requires_dist": null, "requires_python": "", "summary": "Open source toolkit for helping create serving containers to run on Amazon SageMaker.", "version": "1.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"SageMaker\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2fd7abbc7d7f8bc2187f18add827761a809bca57/68747470733a2f2f6769746875622e636f6d2f6177732f736167656d616b65722d696e666572656e63652d746f6f6c6b69742f7261772f6d61737465722f6272616e64696e672f69636f6e2f736167656d616b65722d62616e6e65722e706e67\"></p>\n<h1>SageMaker Inference Toolkit</h1>\n<p><a href=\"https://pypi.python.org/pypi/sagemaker-inference\" rel=\"nofollow\"><img alt=\"Latest Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c21c608ed2add7d42cb0ccdf3edb3830bbae2e64/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f736167656d616b65722d696e666572656e63652e737667\"></a> <a href=\"https://pypi.python.org/pypi/sagemaker-inference\" rel=\"nofollow\"><img alt=\"Supported Python Versions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a82f87953ae5a92df92933f2b03f22ca58b9bd04/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f736167656d616b65722d696e666572656e63652e737667\"></a> <a href=\"https://github.com/python/black\" rel=\"nofollow\"><img alt=\"Code Style: Black\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/60c9766d3e4743ac1ccc37cf3755aa6609b5ce47/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64655f7374796c652d626c61636b2d3030303030302e737667\"></a></p>\n<p>Serve machine learning models within a Docker container using Amazon\nSageMaker.</p>\n<h2>:books: Background</h2>\n<p><a href=\"https://aws.amazon.com/sagemaker/\" rel=\"nofollow\">Amazon SageMaker</a> is a fully managed service for data science and machine learning (ML) workflows.\nYou can use Amazon SageMaker to simplify the process of building, training, and deploying ML models.</p>\n<p>Once you have a trained model, you can include it in a <a href=\"https://www.docker.com/resources/what-container\" rel=\"nofollow\">Docker container</a> that runs your inference code.\nA container provides an effectively isolated environment, ensuring a consistent runtime regardless of where the container is deployed.\nContainerizing your model and code enables fast and reliable deployment of your model.</p>\n<p>The <strong>SageMaker Inference Toolkit</strong> implements a model serving stack and can be easily added to any Docker container, making it <a href=\"https://aws.amazon.com/sagemaker/deploy/\" rel=\"nofollow\">deployable to SageMaker</a>.\nThis library's serving stack is built on <a href=\"https://github.com/awslabs/mxnet-model-server\" rel=\"nofollow\">Multi Model Server</a>, and it can serve your own models or those you trained on SageMaker using <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html\" rel=\"nofollow\">machine learning frameworks with native SageMaker support</a>.\nIf you use a <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\" rel=\"nofollow\">prebuilt SageMaker Docker image for inference</a>, this library may already be included.</p>\n<p>For more information, see the Amazon SageMaker Developer Guide sections on <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html\" rel=\"nofollow\">building your own container with Multi Model Server</a> and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html\" rel=\"nofollow\">using your own models</a>.</p>\n<h2>:hammer_and_wrench: Installation</h2>\n<p>To install this library in your Docker image, add the following line to your <a href=\"https://docs.docker.com/engine/reference/builder/\" rel=\"nofollow\">Dockerfile</a>:</p>\n<pre><span class=\"k\">RUN</span> pip3 install multi-model-server sagemaker-inference-toolkit\n</pre>\n<p><a href=\"https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_bring_your_own/container/Dockerfile\" rel=\"nofollow\">Here is an example</a> of a Dockerfile that installs SageMaker Inference Toolkit.</p>\n<h2>:computer: Usage</h2>\n<h3>Implementation Steps</h3>\n<p>To use the SageMaker Inference Toolkit, you need to do the following:</p>\n<ol>\n<li>\n<p>Implement an inference handler, which is responsible for loading the model and providing input, predict, and output functions.\n(<a href=\"https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/default_inference_handler.py\" rel=\"nofollow\">Here is an example</a> of an inference handler.)</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sagemaker_inference</span> <span class=\"kn\">import</span> <span class=\"n\">content_types</span><span class=\"p\">,</span> <span class=\"n\">decoder</span><span class=\"p\">,</span> <span class=\"n\">default_inference_handler</span><span class=\"p\">,</span> <span class=\"n\">encoder</span><span class=\"p\">,</span> <span class=\"n\">errors</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">DefaultPytorchInferenceHandler</span><span class=\"p\">(</span><span class=\"n\">default_inference_handler</span><span class=\"o\">.</span><span class=\"n\">DefaultInferenceHandler</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">default_model_fn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">model_dir</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Loads a model. For PyTorch, a default function to load a model cannot be provided.</span>\n<span class=\"sd\">        Users should provide customized model_fn() in script.</span>\n\n<span class=\"sd\">        Args:</span>\n<span class=\"sd\">            model_dir: a directory where model is saved.</span>\n\n<span class=\"sd\">        Returns: A PyTorch model.</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">raise</span> <span class=\"ne\">NotImplementedError</span><span class=\"p\">(</span><span class=\"n\">textwrap</span><span class=\"o\">.</span><span class=\"n\">dedent</span><span class=\"p\">(</span><span class=\"s2\">\"\"\"</span>\n<span class=\"s2\">        Please provide a model_fn implementation.</span>\n<span class=\"s2\">        See documentation for model_fn at https://github.com/aws/sagemaker-python-sdk</span>\n<span class=\"s2\">        \"\"\"</span><span class=\"p\">))</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">default_input_fn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">input_data</span><span class=\"p\">,</span> <span class=\"n\">content_type</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"A default input_fn that can handle JSON, CSV and NPZ formats.</span>\n\n<span class=\"sd\">        Args:</span>\n<span class=\"sd\">            input_data: the request payload serialized in the content_type format</span>\n<span class=\"sd\">            content_type: the request content_type</span>\n\n<span class=\"sd\">        Returns: input_data deserialized into torch.FloatTensor or torch.cuda.FloatTensor depending if cuda is available.</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">return</span> <span class=\"n\">decoder</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">input_data</span><span class=\"p\">,</span> <span class=\"n\">content_type</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">default_predict_fn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"A default predict_fn for PyTorch. Calls a model on data deserialized in input_fn.</span>\n<span class=\"sd\">        Runs prediction on GPU if cuda is available.</span>\n\n<span class=\"sd\">        Args:</span>\n<span class=\"sd\">            data: input data (torch.Tensor) for prediction deserialized by input_fn</span>\n<span class=\"sd\">            model: PyTorch model loaded in memory by model_fn</span>\n\n<span class=\"sd\">        Returns: a prediction</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">return</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_data</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">default_output_fn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">prediction</span><span class=\"p\">,</span> <span class=\"n\">accept</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"A default output_fn for PyTorch. Serializes predictions from predict_fn to JSON, CSV or NPY format.</span>\n\n<span class=\"sd\">        Args:</span>\n<span class=\"sd\">            prediction: a prediction result from predict_fn</span>\n<span class=\"sd\">            accept: type which the output data needs to be serialized</span>\n\n<span class=\"sd\">        Returns: output data serialized</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">return</span> <span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">prediction</span><span class=\"p\">,</span> <span class=\"n\">accept</span><span class=\"p\">)</span>\n</pre>\n</li>\n<li>\n<p>Implement a handler service that is executed by the model server.\n(<a href=\"https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/handler_service.py\" rel=\"nofollow\">Here is an example</a> of a handler service.)\nFor more information on how to define your <code>HANDLER_SERVICE</code> file, see <a href=\"https://github.com/awslabs/mxnet-model-server/blob/master/docs/custom_service.md\" rel=\"nofollow\">the MMS custom service documentation</a>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sagemaker_inference.default_handler_service</span> <span class=\"kn\">import</span> <span class=\"n\">DefaultHandlerService</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sagemaker_inference.transformer</span> <span class=\"kn\">import</span> <span class=\"n\">Transformer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sagemaker_pytorch_serving_container.default_inference_handler</span> <span class=\"kn\">import</span> <span class=\"n\">DefaultPytorchInferenceHandler</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">HandlerService</span><span class=\"p\">(</span><span class=\"n\">DefaultHandlerService</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"Handler service that is executed by the model server.</span>\n<span class=\"sd\">    Determines specific default inference handlers to use based on model being used.</span>\n<span class=\"sd\">    This class extends ``DefaultHandlerService``, which define the following:</span>\n<span class=\"sd\">        - The ``handle`` method is invoked for all incoming inference requests to the model server.</span>\n<span class=\"sd\">        - The ``initialize`` method is invoked at model server start up.</span>\n<span class=\"sd\">    Based on: https://github.com/awslabs/mxnet-model-server/blob/master/docs/custom_service.md</span>\n<span class=\"sd\">    \"\"\"</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"n\">transformer</span> <span class=\"o\">=</span> <span class=\"n\">Transformer</span><span class=\"p\">(</span><span class=\"n\">default_inference_handler</span><span class=\"o\">=</span><span class=\"n\">DefaultPytorchInferenceHandler</span><span class=\"p\">())</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">HandlerService</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">transformer</span><span class=\"o\">=</span><span class=\"n\">transformer</span><span class=\"p\">)</span>\n</pre>\n</li>\n<li>\n<p>Implement a serving entrypoint, which starts the model server.\n(<a href=\"https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/serving.py\" rel=\"nofollow\">Here is an example</a> of a serving entrypoint.)</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sagemaker_inference</span> <span class=\"kn\">import</span> <span class=\"n\">model_server</span>\n\n<span class=\"n\">model_server</span><span class=\"o\">.</span><span class=\"n\">start_model_server</span><span class=\"p\">(</span><span class=\"n\">handler_service</span><span class=\"o\">=</span><span class=\"n\">HANDLER_SERVICE</span><span class=\"p\">)</span>\n</pre>\n</li>\n<li>\n<p>Define the location of the entrypoint in your Dockerfile.</p>\n<pre><span class=\"k\">ENTRYPOINT</span> <span class=\"p\">[</span><span class=\"s2\">\"python\"</span><span class=\"p\">,</span> <span class=\"s2\">\"/usr/local/bin/entrypoint.py\"</span><span class=\"p\">]</span>\n</pre>\n</li>\n</ol>\n<h3>Complete Example</h3>\n<p><a href=\"https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_bring_your_own\" rel=\"nofollow\">Here is a complete example</a> demonstrating usage of the SageMaker Inference Toolkit in your own container for deployment to a multi-model endpoint.</p>\n<h2>:scroll: License</h2>\n<p>This library is licensed under the <a href=\"http://aws.amazon.com/apache2.0/\" rel=\"nofollow\">Apache 2.0 License</a>.\nFor more details, please take a look at the <a href=\"https://github.com/aws-samples/sagemaker-inference-toolkit/blob/master/LICENSE\" rel=\"nofollow\">LICENSE</a> file.</p>\n<h2>:handshake: Contributing</h2>\n<p>Contributions are welcome!\nPlease read our <a href=\"https://github.com/aws/sagemaker-inference-toolkit/blob/master/CONTRIBUTING.md\" rel=\"nofollow\">contributing guidelines</a>\nif you'd like to open an issue or submit a pull request.</p>\n\n          </div>"}, "last_serial": 7191848, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "73150540ba05e133659def2d5c4e4307", "sha256": "1abcadf16c32f5c7322ee9764970acaf2552c2fce2c64ef74701721df801bdba"}, "downloads": -1, "filename": "sagemaker_inference-1.0.0.tar.gz", "has_sig": true, "md5_digest": "73150540ba05e133659def2d5c4e4307", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13578, "upload_time": "2019-05-23T00:16:18", "upload_time_iso_8601": "2019-05-23T00:16:18.426120Z", "url": "https://files.pythonhosted.org/packages/b0/e4/ef45abbaa2c522b7779a7927e8e7a0a413b501a6733dbe65e8e0f5512bab/sagemaker_inference-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "35d895c1a6b726dda537cd0e6231dbac", "sha256": "47e65ba542cef323ab0a9bea5b3db06e99f36963949ce7eff33f0ee45a83ea34"}, "downloads": -1, "filename": "sagemaker_inference-1.0.1.tar.gz", "has_sig": true, "md5_digest": "35d895c1a6b726dda537cd0e6231dbac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13540, "upload_time": "2019-06-27T16:41:42", "upload_time_iso_8601": "2019-06-27T16:41:42.691345Z", "url": "https://files.pythonhosted.org/packages/ce/2a/7bc884e97a5a0e2d66a5e7dabed4664621065b1a9d1fba30781fff13df93/sagemaker_inference-1.0.1.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "8c628ec20dc3ef3e68ea322ad9a988d8", "sha256": "e287b20a5feaf8cb66db210c460e4610898f9c4b950ac0c93903f24cb256e6a9"}, "downloads": -1, "filename": "sagemaker_inference-1.0.3.tar.gz", "has_sig": true, "md5_digest": "8c628ec20dc3ef3e68ea322ad9a988d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14305, "upload_time": "2019-09-06T22:18:09", "upload_time_iso_8601": "2019-09-06T22:18:09.411324Z", "url": "https://files.pythonhosted.org/packages/2e/f8/419a0b876036e87362a460929c8f162127edf172f4f7dc02a261bd63a4a6/sagemaker_inference-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "7e5529a8ce61809891fffe73a2c39870", "sha256": "266fd01ff4aea5c3b0d30c7a65c988b114e848e913e8ba9c60837855d3dfa15f"}, "downloads": -1, "filename": "sagemaker_inference-1.0.4.tar.gz", "has_sig": true, "md5_digest": "7e5529a8ce61809891fffe73a2c39870", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16576, "upload_time": "2019-10-07T09:18:53", "upload_time_iso_8601": "2019-10-07T09:18:53.486436Z", "url": "https://files.pythonhosted.org/packages/e0/4d/a1318d07b1ea3cbcbe7be92f6b75498e8bdc4a88e5af79a6bfd7cf11623e/sagemaker_inference-1.0.4.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "0b3ee163918e7d01c80754e3bef03138", "sha256": "e6f881ac04417e0a6c9f953882a145c79a45ab4e0c82de9cfc64dee31669d084"}, "downloads": -1, "filename": "sagemaker_inference-1.1.0.tar.gz", "has_sig": true, "md5_digest": "0b3ee163918e7d01c80754e3bef03138", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17661, "upload_time": "2019-10-23T09:18:55", "upload_time_iso_8601": "2019-10-23T09:18:55.920767Z", "url": "https://files.pythonhosted.org/packages/fa/85/1e5c065f5b5cf76d1a535b947d0a9fde3d6683402f84f2835e07ff5c9b07/sagemaker_inference-1.1.0.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "aab09f851bc24409d9cac513206cd4aa", "sha256": "7716f654c873ca86fae76bf53421dd00ca0ce7216d7cf4bff3d1011d83f2a49f"}, "downloads": -1, "filename": "sagemaker_inference-1.1.1.tar.gz", "has_sig": true, "md5_digest": "aab09f851bc24409d9cac513206cd4aa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17918, "upload_time": "2019-11-06T00:06:33", "upload_time_iso_8601": "2019-11-06T00:06:33.787078Z", "url": "https://files.pythonhosted.org/packages/16/24/d64d45b4d8264c6d17fb473e93668cbd5c2285f2365d47444c5ab2063a5f/sagemaker_inference-1.1.1.tar.gz", "yanked": false}], "1.1.2": [{"comment_text": "", "digests": {"md5": "78e952221ff91149c91f2d1e609dc5ec", "sha256": "02c485f48f106679d5a974cc2a851ab643433e31eb2df2ae8afb3031a2054fc8"}, "downloads": -1, "filename": "sagemaker_inference-1.1.2.tar.gz", "has_sig": true, "md5_digest": "78e952221ff91149c91f2d1e609dc5ec", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17950, "upload_time": "2019-11-14T23:54:27", "upload_time_iso_8601": "2019-11-14T23:54:27.773564Z", "url": "https://files.pythonhosted.org/packages/92/14/6ff887ccdd2dd39671d2a4be6a1db7fb82b298862a0d815007c49791d5de/sagemaker_inference-1.1.2.tar.gz", "yanked": false}], "1.1.3": [{"comment_text": "", "digests": {"md5": "43a7636532a748c490bc511ba71f3de7", "sha256": "72434330e4901c818a26093b8225d53832d36e6e6a47b228e30b9912e3678a19"}, "downloads": -1, "filename": "sagemaker_inference-1.1.3.tar.gz", "has_sig": true, "md5_digest": "43a7636532a748c490bc511ba71f3de7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17982, "upload_time": "2020-01-08T09:19:48", "upload_time_iso_8601": "2020-01-08T09:19:48.213859Z", "url": "https://files.pythonhosted.org/packages/22/2a/fa556102f8aa787ad3a292ca63f06389ee742af1ccb31614502221638686/sagemaker_inference-1.1.3.tar.gz", "yanked": false}], "1.1.4": [{"comment_text": "", "digests": {"md5": "7a7723b26b18b6d9a35e131bc97ba213", "sha256": "73ad1d718dd141a9ab77dc1b4dbbee22485cd1c8003ad033eda9d364c4536ea3"}, "downloads": -1, "filename": "sagemaker_inference-1.1.4.tar.gz", "has_sig": true, "md5_digest": "7a7723b26b18b6d9a35e131bc97ba213", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18359, "upload_time": "2020-01-13T09:20:19", "upload_time_iso_8601": "2020-01-13T09:20:19.399361Z", "url": "https://files.pythonhosted.org/packages/1a/59/9a85f9665cc55321f97f9eda21ac788fb232d7d490cb7e0c5b8992cea86b/sagemaker_inference-1.1.4.tar.gz", "yanked": false}], "1.1.4.post0": [{"comment_text": "", "digests": {"md5": "d74cfde48d0ac134a8bdafac5e440cb7", "sha256": "dd0aa790b822960a51bb0ae2eed6f6b041226b46a33569f0cfd61ad150852bd5"}, "downloads": -1, "filename": "sagemaker_inference-1.1.4.post0.tar.gz", "has_sig": true, "md5_digest": "d74cfde48d0ac134a8bdafac5e440cb7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18373, "upload_time": "2020-01-14T09:19:47", "upload_time_iso_8601": "2020-01-14T09:19:47.586783Z", "url": "https://files.pythonhosted.org/packages/07/78/a39d730126244fdc7a6260c0140a791dd195c5e83db172a41ff22fda2ebb/sagemaker_inference-1.1.4.post0.tar.gz", "yanked": false}], "1.1.4.post1": [{"comment_text": "", "digests": {"md5": "22229cbe8bd291226fc59e9a32f66bfb", "sha256": "8fe0d3892a9ef90d70395928c48186c2cc32f8bbe3851fd3158f69d57ae167d1"}, "downloads": -1, "filename": "sagemaker_inference-1.1.4.post1.tar.gz", "has_sig": true, "md5_digest": "22229cbe8bd291226fc59e9a32f66bfb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18368, "upload_time": "2020-01-16T09:19:49", "upload_time_iso_8601": "2020-01-16T09:19:49.802486Z", "url": "https://files.pythonhosted.org/packages/93/11/c7c81663e68d9c0cc95f46f44cb0d5fd90819cb52226839f643693db0cb8/sagemaker_inference-1.1.4.post1.tar.gz", "yanked": false}], "1.1.5": [{"comment_text": "", "digests": {"md5": "fd63926c9155217ad02e7e637feb9071", "sha256": "688feeefe28825253b1aea6f9228b4bdb3c08e3517583437415be21d48156f83"}, "downloads": -1, "filename": "sagemaker_inference-1.1.5.tar.gz", "has_sig": true, "md5_digest": "fd63926c9155217ad02e7e637feb9071", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19930, "upload_time": "2020-01-31T19:29:54", "upload_time_iso_8601": "2020-01-31T19:29:54.349334Z", "url": "https://files.pythonhosted.org/packages/27/29/5cb93c8f40c41c95d1e04db4aadb63a8d806004d7810c29473b80095f430/sagemaker_inference-1.1.5.tar.gz", "yanked": false}], "1.1.5.post0": [{"comment_text": "", "digests": {"md5": "9d65f50b485eaab7f28479a5f32348fe", "sha256": "d1b49c3e859c40046e5ee23f28e8d2221368d4ae627b5bc43333da5aa019ec39"}, "downloads": -1, "filename": "sagemaker_inference-1.1.5.post0.tar.gz", "has_sig": true, "md5_digest": "9d65f50b485eaab7f28479a5f32348fe", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19944, "upload_time": "2020-02-06T09:25:37", "upload_time_iso_8601": "2020-02-06T09:25:37.439161Z", "url": "https://files.pythonhosted.org/packages/85/fa/ff5bf8eb870b5861b9ed273650b9956ffde7f376dae6cd6fafafd219c249/sagemaker_inference-1.1.5.post0.tar.gz", "yanked": false}], "1.1.5.post1": [{"comment_text": "", "digests": {"md5": "09a7eac0c7a90ee6c9d05dd125fe61ab", "sha256": "cdf912674c97b611f9a000029a995770d7d89d805292c49ac9440c10f8d5c215"}, "downloads": -1, "filename": "sagemaker_inference-1.1.5.post1.tar.gz", "has_sig": true, "md5_digest": "09a7eac0c7a90ee6c9d05dd125fe61ab", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19817, "upload_time": "2020-02-17T09:26:05", "upload_time_iso_8601": "2020-02-17T09:26:05.214783Z", "url": "https://files.pythonhosted.org/packages/b2/f1/c47275ef275952f55492fd422525a913fda79331f51de1c783e18072cf19/sagemaker_inference-1.1.5.post1.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "b275cf5fb3e271eb32921a2ee74e72f6", "sha256": "9c13e4e1b2bf78a40babb8a673dfb0f34f562dfc90243a07b83e064fa6386e7d"}, "downloads": -1, "filename": "sagemaker_inference-1.2.0.tar.gz", "has_sig": true, "md5_digest": "b275cf5fb3e271eb32921a2ee74e72f6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19886, "upload_time": "2020-03-04T23:30:04", "upload_time_iso_8601": "2020-03-04T23:30:04.812009Z", "url": "https://files.pythonhosted.org/packages/d1/04/2ee63e7217c89b3e0f2a2c82016fad930b3c335371dcceb2052d24438e30/sagemaker_inference-1.2.0.tar.gz", "yanked": false}], "1.2.1": [{"comment_text": "", "digests": {"md5": "3f7a8c18af1851ed92fcfc6c7230208a", "sha256": "b38684afba20009a0a282ca4ce1a23b175d6b550596f867de875d04ae522f322"}, "downloads": -1, "filename": "sagemaker_inference-1.2.1.tar.gz", "has_sig": true, "md5_digest": "3f7a8c18af1851ed92fcfc6c7230208a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17440, "upload_time": "2020-03-23T09:25:44", "upload_time_iso_8601": "2020-03-23T09:25:44.242648Z", "url": "https://files.pythonhosted.org/packages/00/eb/449d49ac718e7c73ee6ba1b332e18899cfd37d2e43397b22bf4b8598c250/sagemaker_inference-1.2.1.tar.gz", "yanked": false}], "1.2.2": [{"comment_text": "", "digests": {"md5": "8f70c085133459f2a7548ae3b45e3283", "sha256": "b697d9cdf143f44cd4267bf2b89623638cd8d8b173c441eecf53cb232bc7f55e"}, "downloads": -1, "filename": "sagemaker_inference-1.2.2.tar.gz", "has_sig": true, "md5_digest": "8f70c085133459f2a7548ae3b45e3283", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17491, "upload_time": "2020-04-01T09:25:39", "upload_time_iso_8601": "2020-04-01T09:25:39.467775Z", "url": "https://files.pythonhosted.org/packages/a4/bb/58bde5f38f7cc4a068f69803139578d284d333bc010233d887174493dfca/sagemaker_inference-1.2.2.tar.gz", "yanked": false}], "1.3.0": [{"comment_text": "", "digests": {"md5": "26775ec4134f92d0a66319287d34cfd8", "sha256": "e6657e36ce3c6306592feca88a91e012642df17f11d805ecd5e1b0a97561296b"}, "downloads": -1, "filename": "sagemaker_inference-1.3.0.tar.gz", "has_sig": true, "md5_digest": "26775ec4134f92d0a66319287d34cfd8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17488, "upload_time": "2020-05-07T21:07:03", "upload_time_iso_8601": "2020-05-07T21:07:03.531355Z", "url": "https://files.pythonhosted.org/packages/b5/97/e72805f632899cee2fce26efd4b7a809d0cb7605b0bbfc2221eb111060d7/sagemaker_inference-1.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "26775ec4134f92d0a66319287d34cfd8", "sha256": "e6657e36ce3c6306592feca88a91e012642df17f11d805ecd5e1b0a97561296b"}, "downloads": -1, "filename": "sagemaker_inference-1.3.0.tar.gz", "has_sig": true, "md5_digest": "26775ec4134f92d0a66319287d34cfd8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17488, "upload_time": "2020-05-07T21:07:03", "upload_time_iso_8601": "2020-05-07T21:07:03.531355Z", "url": "https://files.pythonhosted.org/packages/b5/97/e72805f632899cee2fce26efd4b7a809d0cb7605b0bbfc2221eb111060d7/sagemaker_inference-1.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:59:17 2020"}