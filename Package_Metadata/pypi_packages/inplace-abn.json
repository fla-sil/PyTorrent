{"info": {"author": "Lorenzo Porzi", "author_email": "lorenzo@mapillary.com", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# In-Place Activated BatchNorm\n\n[**In-Place Activated BatchNorm for Memory-Optimized Training of DNNs**](https://arxiv.org/abs/1712.02616)\n\nIn-Place Activated BatchNorm (InPlace-ABN) is a novel approach to reduce the memory required for training deep networks.\nIt allows for up to 50% memory savings in modern architectures such as ResNet, ResNeXt and Wider ResNet by redefining\nBN + non linear activation as a single in-place operation, while smartly dropping or recomputing intermediate buffers as\nneeded.\n\nThis repository contains a [PyTorch](http://pytorch.org/) implementation of the InPlace-ABN layer, as well as some\ntraining scripts to reproduce the ImageNet classification results reported in our paper.\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Training on ImageNet](#training-on-imagenet)\n\nWe have now also released the inference code for semantic segmentation, together with the Mapillary Vistas trained model leading to [#1 position on the Mapillary Vistas Semantic Segmentation leaderboard](https://eval-vistas.mapillary.com/featured-challenges/1/leaderboard/1). More information can be found at the bottom of this page.\n\n## Citation\n\nIf you use In-Place Activated BatchNorm in your research, please cite:\n```bibtex\n@inproceedings{rotabulo2017place,\n  title={In-Place Activated BatchNorm for Memory-Optimized Training of DNNs},\n  author={Rota Bul\\`o, Samuel and Porzi, Lorenzo and Kontschieder, Peter},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2018}\n}\n```\n\n## Overview\n\n<p align=\"center\"><img width=\"70%\" src=\"inplace_abn.png\" /></p>\n\nWhen processing a BN-Activation-Convolution sequence in the forward pass, most deep learning frameworks need to store\ntwo big buffers, _i.e._ the input `x` of BN and the input `z` of Conv.\nThis is necessary because the standard implementations of the backward passes of BN and Conv depend on their inputs to\ncalculate the gradients.\nUsing Inplace-ABN to replace the BN-Activation sequence, we can safely discard `x`, thus saving up to 50% GPU memory at\ntraining time.\nTo achieve this, we rewrite the backward pass of BN in terms of its output `y`, which is in turn reconstructed from `z`\nby inverting the activation function.\n\nThe parametrization for the scaling factor of BN changed compared to standard BN, in order to ensure an invertible transformation. Specifically, the scaling factor becomes\n<img src=\"./equation.svg\">.\n\n## Requirements\n\nTo install PyTorch, please refer to https://github.com/pytorch/pytorch#installation.\n\n**NOTE 1: our code _requires_ PyTorch v1.1 or later**\n\n**NOTE 2: we are only able to provide support for Linux platforms and CUDA versions >= 10.0**\n\n**NOTE 3: in general, it is not possible to load weights from a network trained with standard BN into an InPlace-ABN network without severe performance degradation, due to the different handling of BN scaling parameters**\n\nTo install the package containing the iABN layers:\n```bash\npip install git+https://github.com/mapillary/inplace_abn.git@v1.0.12\n```\nNote that some parts of InPlace-ABN have native C++/CUDA implementations, meaning that the command above will need to\ncompile them.\n\nAlternatively, to download and install the latest version of our library, also obtaining a copy of the Imagenet / Vistas\nscripts:\n```bash\ngit clone https://github.com/mapillary/inplace_abn.git\ncd inplace_abn\npython setup.py install\ncd scripts\npip install -r requirements.txt\n```\nThe last of the commands above will install some additional libraries required by the Imagenet / Vistas scripts.\n\n## Training on ImageNet-1k\n\nHere you can find the results from our arXiv paper (top-1 / top-5 scores) with corresponding, trained models and md5 checksums, respectively. The model files provided below are made available under the [license attached to ImageNet](http://www.image-net.org/download-faq). \n\n| Network                           | Batch | 224            | 224, 10-crops  | 320           |       Trained models (+md5)      |\n|-----------------------------------|-------|----------------|----------------|---------------|----------------------------------|\n| [ResNeXt101, Std-BN][1]           | 256   | 77.04 / 93.50  | 78.72 / 94.47  | 77.92 / 94.28 | [`448438885986d14db5e870b95f814f91`][6] |\n| [ResNeXt101, InPlace-ABN][2]      | 512   | 78.08 / 93.79  | 79.52 / 94.66  | 79.38 / 94.67 | [`3b7a221cbc076410eb12c8dd361b7e4e`][7] |\n| [ResNeXt152, InPlace-ABN][3]      | 256   | 78.28 / 94.04  | 79.73 / 94.82  | 79.56 / 94.67 | [`2c8d572587961ed74611d534c5b2e9ce`][8] |\n| [WideResNet38, InPlace-ABN][4]    | 256   | 79.72 / 94.78  | 81.03 / 95.43  | 80.69 / 95.27 | [`1c085ab70b789cc1d6c1594f7a761007`][9] |\n| [ResNeXt101, InPlace-ABN sync][5] | 256   | 77.70 / 93.78  | 79.18 / 94.60  | 78.98 / 94.56 | [`0a85a21847b15e5a242e17bf3b753849`][10] |\n| [DenseNet264, InPlace-ABN][11]    | 256   | 78.57 / 94.17  | 79.72 / 94.93  | 79.49 / 94.89 | [`0b413d67b725619441d0646d663865bf`][12] |\n| [ResNet50v1, InPlace-ABN sync][13]  | 512   | 75.53 / 92.59  | 77.04 / 93.57  | 76.60 / 93.49 | [`2522ca639f7fdfd7c0089ba1f5f6c2e8`][14] |\n| [ResNet34v1, InPlace-ABN sync][15]  | 512   | 73.27 / 91.34  | 75.19 / 92.66  | 74.87 / 92.42 | [`61515c1484911c3cc753d405131e1dda`][16] |\n| [ResNet101v1, InPlace-ABN sync][17]  | 512   | 77.07 / 93.45  | 78.58 / 94.40  | 78.25 / 94.19 | [`1552ae0f3d610108df702135f56bd27b`][18] |\n  \n[1]: scripts/experiments/resnext101_stdbn_lr_256.json\n[2]: scripts/experiments/resnext101_ipabn_lr_512.json\n[3]: scripts/experiments/resnext152_ipabn_lr_256.json\n[4]: scripts/experiments/wider_resnet38_ipabn_lr_256.json\n[5]: scripts/experiments/resnext101_ipabn-sync_lr_256.json\n[6]: https://drive.google.com/file/d/1qT8qCSZzUHorai1EP6Liywa28ASac_G_/view\n[7]: https://drive.google.com/file/d/1rQd-NoZuCsGZ7_l_X9GO1GGiXeXHE8CT/view\n[8]: https://drive.google.com/file/d/1RmHK3tdVTVsHiyNO14bYLkMC0XUjenIn/view\n[9]: https://drive.google.com/file/d/1Y0McSz9InDSxMEcBylAbCv1gvyeaz8Ij/view\n[10]: https://drive.google.com/file/d/1v2gmUPBMDKf0wZm9r1JwCQLGAig0DdXJ/view\n[11]: scripts/experiments/densenet264_ipabn_lr_256.json\n[12]: https://drive.google.com/file/d/1J2wp59bzzEd6zttM6oMa1KgbmCL1MS0k/view\n[13]: scripts/experiments/resnet50_ipabn-sync_lr_512.json\n[14]: https://drive.google.com/file/d/1N7kjWrnUbD_aBOUNi9ZLGnI3E_1ATH8U/view\n[15]: scripts/experiments/resnet34_ipabn-sync_lr_512.json\n[16]: https://drive.google.com/file/d/1V5dCIZeRCfnZi9krNaQNhXNDHyXz9JR8/view\n[17]: scripts/experiments/resnet101_ipabn-sync_lr_512.json\n[18]: https://drive.google.com/file/d/1oFVSIUYAxa_uNDq2OLkbhyiFmKwnYzpt/view\n\n### Data preparation\n\nOur script uses [torchvision.datasets.ImageFolder](http://pytorch.org/docs/master/torchvision/datasets.html#torchvision.datasets.ImageFolder)\nfor loading ImageNet data, which expects folders organized as follows:\n```\nroot/train/[class_id1]/xxx.{jpg,png,jpeg}\nroot/train/[class_id1]/xxy.{jpg,png,jpeg}\nroot/train/[class_id2]/xxz.{jpg,png,jpeg}\n...\n\nroot/val/[class_id1]/asdas.{jpg,png,jpeg}\nroot/val/[class_id1]/123456.{jpg,png,jpeg}\nroot/val/[class_id2]/__32_.{jpg,png,jpeg}\n...\n```\nImages can have any name, as long as the extension is that of a recognized image format.\nClass ids are also free-form, but they are expected to match between train and validation data.\nNote that the training data in the standard ImageNet distribution is already given in the required format, while\nvalidation images need to be split into class sub-folders as described above.  \n\n### Training\n\nThe main training script is `scripts/train_imagenet.py`: this supports training on ImageNet, or any other dataset\nformatted as described above, while keeping a log of relevant metrics in Tensorboard format and periodically saving\nsnapshots.\nMost training parameters can be specified as a `json`-formatted configuration file (look\n[here](scripts/imagenet/config.py) for a complete list of configurable parameters).\nAll parameters not explicitly specified in the configuration file are set to their defaults, also available in\n[scripts/imagenet/config.py](scripts/imagenet/config.py).\n\nOur arXiv results can be reproduced by running `scripts/train_imagenet.py` with the configuration files in\n`scripts/experiments`.\nAs an example, the command to train `ResNeXt101` with InPlace-ABN, Leaky ReLU and `batch_size = 512` is:\n```bash\ncd scripts\npython -m torch.distributed.launch --nproc_per_node <n. GPUs per node> train_imagenet.py --log-dir /path/to/tensorboard/logs experiments/resnext101_ipabn_lr_512.json /path/to/imagenet/root\n```\n\n### Validation\n\nValidation is run by `scripts/train_imagenet.py` at the end of every training epoch.\nTo validate a trained model, you can use the `scripts/test_imagenet.py` script, which allows for 10-crops validation and\ntransferring weights across compatible networks (_e.g._ from `ResNeXt101` with ReLU to `ResNeXt101` with Leaky\nReLU).\nThis script accepts the same configuration files as `scripts/train_imagenet.py`, but note that the `scale_val` and\n`crop_val` parameters are ignored in favour of the `--scale` and `--crop` command-line arguments.\n\nAs an example, to validate the `ResNeXt101` trained above using 10-crops of size `224` from images scaled to `256`\npixels, you can run:\n```bash\ncd scripts\npython -m torch.distributed.launch --nproc_per_node <n. GPUs per node> test_imagenet.py --crop 224 --scale 256 --ten_crops experiments/resnext101_ipabn_lr_512.json /path/to/checkpoint /path/to/imagenet/root\n```\n\n## Usage for Semantic Segmentation on Cityscapes and Mapillary Vistas\n\nWe have successfully used InPlace-ABN with a DeepLab3 segmentation head that was trained on top of the WideResNet38\nmodel above.\nDue to InPlace-ABN, we can significantly increase the amount of input data to this model, which eventually allowed us to\nobtain #1 positions on [Cityscapes](https://www.cityscapes-dataset.com/benchmarks/#scene-labeling-task),\n[Mapillary Vistas](https://eval-vistas.mapillary.com/featured-challenges/1/leaderboard/1), [AutoNUE](http://cvit.iiit.ac.in/scene-understanding-challenge-2018/benchmarks.php), \n[Kitti](http://www.cvlibs.net/datasets/kitti/eval_semseg.php?benchmark=semantics2015) and\n[ScanNet](http://dovahkiin.stanford.edu/adai/semantic_label) segmentation leaderboards.\nThe training settings mostly follow the description in our [paper](https://arxiv.org/abs/1712.02616).\n\n### Mapillary Vistas pre-trained model\n\nWe release our WideResNet38 + DeepLab3 segmentation model trained on the Mapillary Vistas research set.\nThis is the model used to reach #1 position on the MVD semantic segmentation leaderboard.\nThe segmentation model file provided below is made available under a\n[CC BY-NC-SA 4.0 license](https://creativecommons.org/licenses/by-nc-sa/4.0/).\n\n| Network                       | mIOU  | Trained model (+md5)                   |\n|-------------------------------|-------|----------------------------------------|\n| [WideResNet38 + DeepLab3][19] | 53.42 | [913f78486a34aa1577a7cd295e8a33bb][20] |\n\n[19]: scripts/test_vistas.py\n[20]: https://drive.google.com/file/d/1SJJx5-LFG3J3M99TrPMU-z6ZmgWynxo-/view\n\nTo use this, please download the `.pth.tar` model file linked above and run the `test_vistas.py` script as follows:\n```bash\ncd scripts\npython test_vistas.py /path/to/model.pth.tar /path/to/input/folder /path/to/output/folder\n```\n\nThe script will process all `.png`, `.jpg` and `.jpeg` images from the input folder and write the predictions in the\noutput folder as `.png` images.\nFor additional options, _e.g._ test time augmentation, please consult the script's help message.\n\nThe results on the test data written above were obtained by employing only scale 1.0 + flipping. \n\n## Changelog\n\n**Update 04 Jul. 2019: version 1.0.0**\n- Complete rewrite of the CUDA code following the most recent native BN implementation from Pytorch\n- Improved synchronized BN implementation, correctly handling different per-GPU batch sizes and Pytorch distributed groups\n- The iABN layers are now packaged in an installable python library to simplify use in other projects\n- The Imagenet / Vistas scripts are still available in the `scripts` folder\n- Requires now PyTorch 1.1\n\n**Update 08 Jan. 2019:**\n- Enabled multiprocessing and inplace ABN synchronization over multiple processes (previously using threads). It now requires to use DistributedDataParallel instead of DataParallel\n- Added compatibility with fp16 (currently allows fp16 input but requires the module to stay in fp32 mode)\n- Requires now PyTorch 1.0\n\n**Update Feb. 2019:**\n- Added ResNet34v1, ResNet50v1 and ResNet101v1 ImageNet-1k pre-trained models\n\nWe have modified the imagenet training code and BN synchronization in order to work with multiple processes. We have also added compatibility of our Inplace ABN module with fp16.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/mapillary/inplace_abn", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "inplace-abn", "package_url": "https://pypi.org/project/inplace-abn/", "platform": "", "project_url": "https://pypi.org/project/inplace-abn/", "project_urls": {"Homepage": "https://github.com/mapillary/inplace_abn"}, "release_url": "https://pypi.org/project/inplace-abn/1.0.12/", "requires_dist": null, "requires_python": ">=3, <4", "summary": "In-Place Activate BatchNorm for Pytorch", "version": "1.0.12", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>In-Place Activated BatchNorm</h1>\n<p><a href=\"https://arxiv.org/abs/1712.02616\" rel=\"nofollow\"><strong>In-Place Activated BatchNorm for Memory-Optimized Training of DNNs</strong></a></p>\n<p>In-Place Activated BatchNorm (InPlace-ABN) is a novel approach to reduce the memory required for training deep networks.\nIt allows for up to 50% memory savings in modern architectures such as ResNet, ResNeXt and Wider ResNet by redefining\nBN + non linear activation as a single in-place operation, while smartly dropping or recomputing intermediate buffers as\nneeded.</p>\n<p>This repository contains a <a href=\"http://pytorch.org/\" rel=\"nofollow\">PyTorch</a> implementation of the InPlace-ABN layer, as well as some\ntraining scripts to reproduce the ImageNet classification results reported in our paper.</p>\n<ul>\n<li><a href=\"#overview\" rel=\"nofollow\">Overview</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#training-on-imagenet\" rel=\"nofollow\">Training on ImageNet</a></li>\n</ul>\n<p>We have now also released the inference code for semantic segmentation, together with the Mapillary Vistas trained model leading to <a href=\"https://eval-vistas.mapillary.com/featured-challenges/1/leaderboard/1\" rel=\"nofollow\">#1 position on the Mapillary Vistas Semantic Segmentation leaderboard</a>. More information can be found at the bottom of this page.</p>\n<h2>Citation</h2>\n<p>If you use In-Place Activated BatchNorm in your research, please cite:</p>\n<pre><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">rotabulo2017place</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span><span class=\"p\">=</span><span class=\"s\">{In-Place Activated BatchNorm for Memory-Optimized Training of DNNs}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span><span class=\"p\">=</span><span class=\"s\">{Rota Bul\\`o, Samuel and Porzi, Lorenzo and Kontschieder, Peter}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span><span class=\"p\">=</span><span class=\"s\">{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span><span class=\"p\">=</span><span class=\"s\">{2018}</span>\n<span class=\"p\">}</span>\n</pre>\n<h2>Overview</h2>\n<p align=\"center\"><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/aeb9325696249e42ffd59029cd0f1703e70462a8/696e706c6163655f61626e2e706e67\" width=\"70%\"></p>\n<p>When processing a BN-Activation-Convolution sequence in the forward pass, most deep learning frameworks need to store\ntwo big buffers, <em>i.e.</em> the input <code>x</code> of BN and the input <code>z</code> of Conv.\nThis is necessary because the standard implementations of the backward passes of BN and Conv depend on their inputs to\ncalculate the gradients.\nUsing Inplace-ABN to replace the BN-Activation sequence, we can safely discard <code>x</code>, thus saving up to 50% GPU memory at\ntraining time.\nTo achieve this, we rewrite the backward pass of BN in terms of its output <code>y</code>, which is in turn reconstructed from <code>z</code>\nby inverting the activation function.</p>\n<p>The parametrization for the scaling factor of BN changed compared to standard BN, in order to ensure an invertible transformation. Specifically, the scaling factor becomes\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cfc1ce145a04fca2a6cb1af45f689c7d52414a5e/2e2f6571756174696f6e2e737667\">.</p>\n<h2>Requirements</h2>\n<p>To install PyTorch, please refer to <a href=\"https://github.com/pytorch/pytorch#installation\" rel=\"nofollow\">https://github.com/pytorch/pytorch#installation</a>.</p>\n<p><strong>NOTE 1: our code <em>requires</em> PyTorch v1.1 or later</strong></p>\n<p><strong>NOTE 2: we are only able to provide support for Linux platforms and CUDA versions &gt;= 10.0</strong></p>\n<p><strong>NOTE 3: in general, it is not possible to load weights from a network trained with standard BN into an InPlace-ABN network without severe performance degradation, due to the different handling of BN scaling parameters</strong></p>\n<p>To install the package containing the iABN layers:</p>\n<pre>pip install git+https://github.com/mapillary/inplace_abn.git@v1.0.12\n</pre>\n<p>Note that some parts of InPlace-ABN have native C++/CUDA implementations, meaning that the command above will need to\ncompile them.</p>\n<p>Alternatively, to download and install the latest version of our library, also obtaining a copy of the Imagenet / Vistas\nscripts:</p>\n<pre>git clone https://github.com/mapillary/inplace_abn.git\n<span class=\"nb\">cd</span> inplace_abn\npython setup.py install\n<span class=\"nb\">cd</span> scripts\npip install -r requirements.txt\n</pre>\n<p>The last of the commands above will install some additional libraries required by the Imagenet / Vistas scripts.</p>\n<h2>Training on ImageNet-1k</h2>\n<p>Here you can find the results from our arXiv paper (top-1 / top-5 scores) with corresponding, trained models and md5 checksums, respectively. The model files provided below are made available under the <a href=\"http://www.image-net.org/download-faq\" rel=\"nofollow\">license attached to ImageNet</a>.</p>\n<table>\n<thead>\n<tr>\n<th>Network</th>\n<th>Batch</th>\n<th>224</th>\n<th>224, 10-crops</th>\n<th>320</th>\n<th>Trained models (+md5)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"scripts/experiments/resnext101_stdbn_lr_256.json\" rel=\"nofollow\">ResNeXt101, Std-BN</a></td>\n<td>256</td>\n<td>77.04 / 93.50</td>\n<td>78.72 / 94.47</td>\n<td>77.92 / 94.28</td>\n<td><a href=\"https://drive.google.com/file/d/1qT8qCSZzUHorai1EP6Liywa28ASac_G_/view\" rel=\"nofollow\"><code>448438885986d14db5e870b95f814f91</code></a></td>\n</tr>\n<tr>\n<td><a href=\"scripts/experiments/resnext101_ipabn_lr_512.json\" rel=\"nofollow\">ResNeXt101, InPlace-ABN</a></td>\n<td>512</td>\n<td>78.08 / 93.79</td>\n<td>79.52 / 94.66</td>\n<td>79.38 / 94.67</td>\n<td><a href=\"https://drive.google.com/file/d/1rQd-NoZuCsGZ7_l_X9GO1GGiXeXHE8CT/view\" rel=\"nofollow\"><code>3b7a221cbc076410eb12c8dd361b7e4e</code></a></td>\n</tr>\n<tr>\n<td><a href=\"scripts/experiments/resnext152_ipabn_lr_256.json\" rel=\"nofollow\">ResNeXt152, InPlace-ABN</a></td>\n<td>256</td>\n<td>78.28 / 94.04</td>\n<td>79.73 / 94.82</td>\n<td>79.56 / 94.67</td>\n<td><a href=\"https://drive.google.com/file/d/1RmHK3tdVTVsHiyNO14bYLkMC0XUjenIn/view\" rel=\"nofollow\"><code>2c8d572587961ed74611d534c5b2e9ce</code></a></td>\n</tr>\n<tr>\n<td><a href=\"scripts/experiments/wider_resnet38_ipabn_lr_256.json\" rel=\"nofollow\">WideResNet38, InPlace-ABN</a></td>\n<td>256</td>\n<td>79.72 / 94.78</td>\n<td>81.03 / 95.43</td>\n<td>80.69 / 95.27</td>\n<td><a href=\"https://drive.google.com/file/d/1Y0McSz9InDSxMEcBylAbCv1gvyeaz8Ij/view\" rel=\"nofollow\"><code>1c085ab70b789cc1d6c1594f7a761007</code></a></td>\n</tr>\n<tr>\n<td><a href=\"scripts/experiments/resnext101_ipabn-sync_lr_256.json\" rel=\"nofollow\">ResNeXt101, InPlace-ABN sync</a></td>\n<td>256</td>\n<td>77.70 / 93.78</td>\n<td>79.18 / 94.60</td>\n<td>78.98 / 94.56</td>\n<td><a href=\"https://drive.google.com/file/d/1v2gmUPBMDKf0wZm9r1JwCQLGAig0DdXJ/view\" rel=\"nofollow\"><code>0a85a21847b15e5a242e17bf3b753849</code></a></td>\n</tr>\n<tr>\n<td><a href=\"scripts/experiments/densenet264_ipabn_lr_256.json\" rel=\"nofollow\">DenseNet264, InPlace-ABN</a></td>\n<td>256</td>\n<td>78.57 / 94.17</td>\n<td>79.72 / 94.93</td>\n<td>79.49 / 94.89</td>\n<td><a href=\"https://drive.google.com/file/d/1J2wp59bzzEd6zttM6oMa1KgbmCL1MS0k/view\" rel=\"nofollow\"><code>0b413d67b725619441d0646d663865bf</code></a></td>\n</tr>\n<tr>\n<td><a href=\"scripts/experiments/resnet50_ipabn-sync_lr_512.json\" rel=\"nofollow\">ResNet50v1, InPlace-ABN sync</a></td>\n<td>512</td>\n<td>75.53 / 92.59</td>\n<td>77.04 / 93.57</td>\n<td>76.60 / 93.49</td>\n<td><a href=\"https://drive.google.com/file/d/1N7kjWrnUbD_aBOUNi9ZLGnI3E_1ATH8U/view\" rel=\"nofollow\"><code>2522ca639f7fdfd7c0089ba1f5f6c2e8</code></a></td>\n</tr>\n<tr>\n<td><a href=\"scripts/experiments/resnet34_ipabn-sync_lr_512.json\" rel=\"nofollow\">ResNet34v1, InPlace-ABN sync</a></td>\n<td>512</td>\n<td>73.27 / 91.34</td>\n<td>75.19 / 92.66</td>\n<td>74.87 / 92.42</td>\n<td><a href=\"https://drive.google.com/file/d/1V5dCIZeRCfnZi9krNaQNhXNDHyXz9JR8/view\" rel=\"nofollow\"><code>61515c1484911c3cc753d405131e1dda</code></a></td>\n</tr>\n<tr>\n<td><a href=\"scripts/experiments/resnet101_ipabn-sync_lr_512.json\" rel=\"nofollow\">ResNet101v1, InPlace-ABN sync</a></td>\n<td>512</td>\n<td>77.07 / 93.45</td>\n<td>78.58 / 94.40</td>\n<td>78.25 / 94.19</td>\n<td><a href=\"https://drive.google.com/file/d/1oFVSIUYAxa_uNDq2OLkbhyiFmKwnYzpt/view\" rel=\"nofollow\"><code>1552ae0f3d610108df702135f56bd27b</code></a></td>\n</tr></tbody></table>\n<h3>Data preparation</h3>\n<p>Our script uses <a href=\"http://pytorch.org/docs/master/torchvision/datasets.html#torchvision.datasets.ImageFolder\" rel=\"nofollow\">torchvision.datasets.ImageFolder</a>\nfor loading ImageNet data, which expects folders organized as follows:</p>\n<pre><code>root/train/[class_id1]/xxx.{jpg,png,jpeg}\nroot/train/[class_id1]/xxy.{jpg,png,jpeg}\nroot/train/[class_id2]/xxz.{jpg,png,jpeg}\n...\n\nroot/val/[class_id1]/asdas.{jpg,png,jpeg}\nroot/val/[class_id1]/123456.{jpg,png,jpeg}\nroot/val/[class_id2]/__32_.{jpg,png,jpeg}\n...\n</code></pre>\n<p>Images can have any name, as long as the extension is that of a recognized image format.\nClass ids are also free-form, but they are expected to match between train and validation data.\nNote that the training data in the standard ImageNet distribution is already given in the required format, while\nvalidation images need to be split into class sub-folders as described above.</p>\n<h3>Training</h3>\n<p>The main training script is <code>scripts/train_imagenet.py</code>: this supports training on ImageNet, or any other dataset\nformatted as described above, while keeping a log of relevant metrics in Tensorboard format and periodically saving\nsnapshots.\nMost training parameters can be specified as a <code>json</code>-formatted configuration file (look\n<a href=\"scripts/imagenet/config.py\" rel=\"nofollow\">here</a> for a complete list of configurable parameters).\nAll parameters not explicitly specified in the configuration file are set to their defaults, also available in\n<a href=\"scripts/imagenet/config.py\" rel=\"nofollow\">scripts/imagenet/config.py</a>.</p>\n<p>Our arXiv results can be reproduced by running <code>scripts/train_imagenet.py</code> with the configuration files in\n<code>scripts/experiments</code>.\nAs an example, the command to train <code>ResNeXt101</code> with InPlace-ABN, Leaky ReLU and <code>batch_size = 512</code> is:</p>\n<pre><span class=\"nb\">cd</span> scripts\npython -m torch.distributed.launch --nproc_per_node &lt;n. GPUs per node&gt; train_imagenet.py --log-dir /path/to/tensorboard/logs experiments/resnext101_ipabn_lr_512.json /path/to/imagenet/root\n</pre>\n<h3>Validation</h3>\n<p>Validation is run by <code>scripts/train_imagenet.py</code> at the end of every training epoch.\nTo validate a trained model, you can use the <code>scripts/test_imagenet.py</code> script, which allows for 10-crops validation and\ntransferring weights across compatible networks (<em>e.g.</em> from <code>ResNeXt101</code> with ReLU to <code>ResNeXt101</code> with Leaky\nReLU).\nThis script accepts the same configuration files as <code>scripts/train_imagenet.py</code>, but note that the <code>scale_val</code> and\n<code>crop_val</code> parameters are ignored in favour of the <code>--scale</code> and <code>--crop</code> command-line arguments.</p>\n<p>As an example, to validate the <code>ResNeXt101</code> trained above using 10-crops of size <code>224</code> from images scaled to <code>256</code>\npixels, you can run:</p>\n<pre><span class=\"nb\">cd</span> scripts\npython -m torch.distributed.launch --nproc_per_node &lt;n. GPUs per node&gt; test_imagenet.py --crop <span class=\"m\">224</span> --scale <span class=\"m\">256</span> --ten_crops experiments/resnext101_ipabn_lr_512.json /path/to/checkpoint /path/to/imagenet/root\n</pre>\n<h2>Usage for Semantic Segmentation on Cityscapes and Mapillary Vistas</h2>\n<p>We have successfully used InPlace-ABN with a DeepLab3 segmentation head that was trained on top of the WideResNet38\nmodel above.\nDue to InPlace-ABN, we can significantly increase the amount of input data to this model, which eventually allowed us to\nobtain #1 positions on <a href=\"https://www.cityscapes-dataset.com/benchmarks/#scene-labeling-task\" rel=\"nofollow\">Cityscapes</a>,\n<a href=\"https://eval-vistas.mapillary.com/featured-challenges/1/leaderboard/1\" rel=\"nofollow\">Mapillary Vistas</a>, <a href=\"http://cvit.iiit.ac.in/scene-understanding-challenge-2018/benchmarks.php\" rel=\"nofollow\">AutoNUE</a>,\n<a href=\"http://www.cvlibs.net/datasets/kitti/eval_semseg.php?benchmark=semantics2015\" rel=\"nofollow\">Kitti</a> and\n<a href=\"http://dovahkiin.stanford.edu/adai/semantic_label\" rel=\"nofollow\">ScanNet</a> segmentation leaderboards.\nThe training settings mostly follow the description in our <a href=\"https://arxiv.org/abs/1712.02616\" rel=\"nofollow\">paper</a>.</p>\n<h3>Mapillary Vistas pre-trained model</h3>\n<p>We release our WideResNet38 + DeepLab3 segmentation model trained on the Mapillary Vistas research set.\nThis is the model used to reach #1 position on the MVD semantic segmentation leaderboard.\nThe segmentation model file provided below is made available under a\n<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"nofollow\">CC BY-NC-SA 4.0 license</a>.</p>\n<table>\n<thead>\n<tr>\n<th>Network</th>\n<th>mIOU</th>\n<th>Trained model (+md5)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"scripts/test_vistas.py\" rel=\"nofollow\">WideResNet38 + DeepLab3</a></td>\n<td>53.42</td>\n<td><a href=\"https://drive.google.com/file/d/1SJJx5-LFG3J3M99TrPMU-z6ZmgWynxo-/view\" rel=\"nofollow\">913f78486a34aa1577a7cd295e8a33bb</a></td>\n</tr></tbody></table>\n<p>To use this, please download the <code>.pth.tar</code> model file linked above and run the <code>test_vistas.py</code> script as follows:</p>\n<pre><span class=\"nb\">cd</span> scripts\npython test_vistas.py /path/to/model.pth.tar /path/to/input/folder /path/to/output/folder\n</pre>\n<p>The script will process all <code>.png</code>, <code>.jpg</code> and <code>.jpeg</code> images from the input folder and write the predictions in the\noutput folder as <code>.png</code> images.\nFor additional options, <em>e.g.</em> test time augmentation, please consult the script's help message.</p>\n<p>The results on the test data written above were obtained by employing only scale 1.0 + flipping.</p>\n<h2>Changelog</h2>\n<p><strong>Update 04 Jul. 2019: version 1.0.0</strong></p>\n<ul>\n<li>Complete rewrite of the CUDA code following the most recent native BN implementation from Pytorch</li>\n<li>Improved synchronized BN implementation, correctly handling different per-GPU batch sizes and Pytorch distributed groups</li>\n<li>The iABN layers are now packaged in an installable python library to simplify use in other projects</li>\n<li>The Imagenet / Vistas scripts are still available in the <code>scripts</code> folder</li>\n<li>Requires now PyTorch 1.1</li>\n</ul>\n<p><strong>Update 08 Jan. 2019:</strong></p>\n<ul>\n<li>Enabled multiprocessing and inplace ABN synchronization over multiple processes (previously using threads). It now requires to use DistributedDataParallel instead of DataParallel</li>\n<li>Added compatibility with fp16 (currently allows fp16 input but requires the module to stay in fp32 mode)</li>\n<li>Requires now PyTorch 1.0</li>\n</ul>\n<p><strong>Update Feb. 2019:</strong></p>\n<ul>\n<li>Added ResNet34v1, ResNet50v1 and ResNet101v1 ImageNet-1k pre-trained models</li>\n</ul>\n<p>We have modified the imagenet training code and BN synchronization in order to work with multiple processes. We have also added compatibility of our Inplace ABN module with fp16.</p>\n\n          </div>"}, "last_serial": 7137913, "releases": {"1.0.11": [{"comment_text": "", "digests": {"md5": "74005df130e0935d7bb356172d00535b", "sha256": "e6a8b2447b6de1b650557d22c6d95ef5bdc894b472e8de8d7eb3a3ce40ab369e"}, "downloads": -1, "filename": "inplace-abn-1.0.11.tar.gz", "has_sig": false, "md5_digest": "74005df130e0935d7bb356172d00535b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 136788, "upload_time": "2020-02-13T18:59:10", "upload_time_iso_8601": "2020-02-13T18:59:10.870869Z", "url": "https://files.pythonhosted.org/packages/7b/1d/64d461867f685b339248e1fbd30bcb55af9fbc50cad4ec14f30df7bb3a71/inplace-abn-1.0.11.tar.gz", "yanked": false}], "1.0.12": [{"comment_text": "", "digests": {"md5": "74a699459ac8b22054614ee4601f55ba", "sha256": "c0d83d50d59b247240d231846a361b1bb4c07915e27359264a41f5e26a6cb8e0"}, "downloads": -1, "filename": "inplace-abn-1.0.12.tar.gz", "has_sig": false, "md5_digest": "74a699459ac8b22054614ee4601f55ba", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 137013, "upload_time": "2020-04-30T14:42:14", "upload_time_iso_8601": "2020-04-30T14:42:14.912230Z", "url": "https://files.pythonhosted.org/packages/a6/63/2aea9083ce5349ec2c3eff3b23f784c4204714dfb6ea75c9f4a460a3545d/inplace-abn-1.0.12.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "74a699459ac8b22054614ee4601f55ba", "sha256": "c0d83d50d59b247240d231846a361b1bb4c07915e27359264a41f5e26a6cb8e0"}, "downloads": -1, "filename": "inplace-abn-1.0.12.tar.gz", "has_sig": false, "md5_digest": "74a699459ac8b22054614ee4601f55ba", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 137013, "upload_time": "2020-04-30T14:42:14", "upload_time_iso_8601": "2020-04-30T14:42:14.912230Z", "url": "https://files.pythonhosted.org/packages/a6/63/2aea9083ce5349ec2c3eff3b23f784c4204714dfb6ea75c9f4a460a3545d/inplace-abn-1.0.12.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:55:52 2020"}