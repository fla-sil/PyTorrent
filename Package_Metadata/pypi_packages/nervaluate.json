{"info": {"author": "David Batista, Matthew Upson", "author_email": "david.batista@gmail.com,matthew.a.upson@gmail.com", "bugtrack_url": null, "classifiers": ["Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# nervaluate\n\nnervaluate is a python module for evaluating Named Entity Recognition (NER) models as defined in the SemEval 2013 - 9.1 task.\n\nThe evaluation metrics output by nervaluate go beyond a simple token/tag based schema, and consider diferent scenarios based on wether all the tokens that belong to a named entity were classified or not, and also wether the correct entity type was assigned.\n\nThis problem is described in detail in the [original blog](http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/) post by [David Batista](https://github.com/davidsbatista), and extends the code in the [original repository](https://github.com/davidsbatista/NER-Evaluation) which accompanied the blog post.\n\nThe code draws heavily on:\n\n* Segura-bedmar, I., & Mart, P. (2013). 2013 SemEval-2013 Task 9 Extraction of Drug-Drug Interactions from. Semeval, 2(DDIExtraction), 341\u2013350. [link](https://www.aclweb.org/anthology/S13-2056)\n* https://www.cs.york.ac.uk/semeval-2013/task9/data/uploads/semeval_2013-task-9_1-evaluation-metrics.pdf\n\n## Notes:\n\nIn scenarios IV and VI the entity type of the `true` and `pred` does not match, in both cases we only scored against the true entity, not the predicted one. You can argue that the predicted entity could also be scored as spurious, but according to the definition of `spurius`:\n\n* Spurius (SPU) : system produces a response which doesn\u2019t exist in the golden annotation;\n\nIn this case there exists an annotation, but with a different entity type, so we assume it's only incorrect.\n\n## Installation\n\nTo install the package:\n\n```\npip install nervaluate\n```\n\nTo create a virtual environment for development:\n\n```\nmake virtualenv\n\n# Then to activate the virtualenv:\n\nsource /build/virtualenv/bin/activate\n```\n\nAlternatively you can use your own virtualenv manager and simply `make reqs` to install requirements.\n\nTo run tests:\n\n```\n# Will run tox\n\nmake test\n```\n\n## Example:\n\nThe main `Evaluator` class will accept a number of formats:\n\n* [prodi.gy](https://prodi.gy) style lists of spans.\n* Nested lists containing NER labels.\n* CoNLL style tab delimited strings.\n\n### Prodigy spans\n\n```\ntrue = [\n    [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n    [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n     {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n]\n\npred = [\n    [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n    [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n     {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n]\n\nfrom nervaluate import Evaluator\n\nevaluator = Evaluator(true, pred, tags=['LOC', 'PER'])\n\n# Returns overall metrics and metrics for each tag\n\nresults, results_per_tag = evaluator.evaluate()\n\nprint(results)\n```\n\n```\n{\n    'ent_type':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    },\n    'partial':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    },\n    'strict':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    },\n    'exact':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    }\n}\n```\n\n```\nprint(results_by_tag)\n```\n\n```\n{\n    'LOC':{\n        'ent_type':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'partial':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'strict':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'exact':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        }\n    },\n    'PER':{\n        'ent_type':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'partial':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'strict':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'exact':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        }\n    }\n}\n```\n\n### Nested lists\n\n```\ntrue = [\n    ['O', 'O', 'B-PER', 'I-PER', 'O'],\n    ['O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'O'],\n]\n\npred = [\n    ['O', 'O', 'B-PER', 'I-PER', 'O'],\n    ['O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'O'],\n]\n\nevaluator = Evaluator(true, pred, tags=['LOC', 'PER'], loader=\"list\")\n\nresults, results_by_tag = evaluator.evaluate()\n```\n\n### CoNLL style tab delimited\n\n```\n\ntrue = \"word\\tO\\nword\\tO\\B-PER\\nword\\tI-PER\\n\"\n\npred = \"word\\tO\\nword\\tO\\B-PER\\nword\\tI-PER\\n\"\n\nevaluator = Evaluator(true, pred, tags=['PER'], loader=\"conll\")\n\nresults, results_by_tag = evaluator.evaluate()\n\n```\n\n## Extending the package to accept more formats\n\nAdditional formats can easily be added to the module by creating a converstion function in `nervaluate/utils.py`, for example `conll_to_spans()`. This function must return the spans in the prodigy style dicts shown in the prodigy example above.\n\nThe new function can then be added to the list of loaders in `nervaluate/nervaluate.py`, and can then be selection with the `loader` argument when instantiating the `Evaluator` class.\n\nA list of formats we intend to include is included in https://github.com/ivyleavedtoadflax/nervaluate/issues/3.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://pypi.org/project/nervaluate", "keywords": "", "license": "GNU GPL v3", "maintainer": "", "maintainer_email": "", "name": "nervaluate", "package_url": "https://pypi.org/project/nervaluate/", "platform": "", "project_url": "https://pypi.org/project/nervaluate/", "project_urls": {"Homepage": "https://pypi.org/project/nervaluate"}, "release_url": "https://pypi.org/project/nervaluate/0.1.7/", "requires_dist": null, "requires_python": "", "summary": "NER evaluation done right", "version": "0.1.7", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>nervaluate</h1>\n<p>nervaluate is a python module for evaluating Named Entity Recognition (NER) models as defined in the SemEval 2013 - 9.1 task.</p>\n<p>The evaluation metrics output by nervaluate go beyond a simple token/tag based schema, and consider diferent scenarios based on wether all the tokens that belong to a named entity were classified or not, and also wether the correct entity type was assigned.</p>\n<p>This problem is described in detail in the <a href=\"http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\" rel=\"nofollow\">original blog</a> post by <a href=\"https://github.com/davidsbatista\" rel=\"nofollow\">David Batista</a>, and extends the code in the <a href=\"https://github.com/davidsbatista/NER-Evaluation\" rel=\"nofollow\">original repository</a> which accompanied the blog post.</p>\n<p>The code draws heavily on:</p>\n<ul>\n<li>Segura-bedmar, I., &amp; Mart, P. (2013). 2013 SemEval-2013 Task 9 Extraction of Drug-Drug Interactions from. Semeval, 2(DDIExtraction), 341\u2013350. <a href=\"https://www.aclweb.org/anthology/S13-2056\" rel=\"nofollow\">link</a></li>\n<li><a href=\"https://www.cs.york.ac.uk/semeval-2013/task9/data/uploads/semeval_2013-task-9_1-evaluation-metrics.pdf\" rel=\"nofollow\">https://www.cs.york.ac.uk/semeval-2013/task9/data/uploads/semeval_2013-task-9_1-evaluation-metrics.pdf</a></li>\n</ul>\n<h2>Notes:</h2>\n<p>In scenarios IV and VI the entity type of the <code>true</code> and <code>pred</code> does not match, in both cases we only scored against the true entity, not the predicted one. You can argue that the predicted entity could also be scored as spurious, but according to the definition of <code>spurius</code>:</p>\n<ul>\n<li>Spurius (SPU) : system produces a response which doesn\u2019t exist in the golden annotation;</li>\n</ul>\n<p>In this case there exists an annotation, but with a different entity type, so we assume it's only incorrect.</p>\n<h2>Installation</h2>\n<p>To install the package:</p>\n<pre><code>pip install nervaluate\n</code></pre>\n<p>To create a virtual environment for development:</p>\n<pre><code>make virtualenv\n\n# Then to activate the virtualenv:\n\nsource /build/virtualenv/bin/activate\n</code></pre>\n<p>Alternatively you can use your own virtualenv manager and simply <code>make reqs</code> to install requirements.</p>\n<p>To run tests:</p>\n<pre><code># Will run tox\n\nmake test\n</code></pre>\n<h2>Example:</h2>\n<p>The main <code>Evaluator</code> class will accept a number of formats:</p>\n<ul>\n<li><a href=\"https://prodi.gy\" rel=\"nofollow\">prodi.gy</a> style lists of spans.</li>\n<li>Nested lists containing NER labels.</li>\n<li>CoNLL style tab delimited strings.</li>\n</ul>\n<h3>Prodigy spans</h3>\n<pre><code>true = [\n    [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n    [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n     {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n]\n\npred = [\n    [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n    [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n     {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n]\n\nfrom nervaluate import Evaluator\n\nevaluator = Evaluator(true, pred, tags=['LOC', 'PER'])\n\n# Returns overall metrics and metrics for each tag\n\nresults, results_per_tag = evaluator.evaluate()\n\nprint(results)\n</code></pre>\n<pre><code>{\n    'ent_type':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    },\n    'partial':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    },\n    'strict':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    },\n    'exact':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    }\n}\n</code></pre>\n<pre><code>print(results_by_tag)\n</code></pre>\n<pre><code>{\n    'LOC':{\n        'ent_type':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'partial':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'strict':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'exact':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        }\n    },\n    'PER':{\n        'ent_type':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'partial':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'strict':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'exact':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        }\n    }\n}\n</code></pre>\n<h3>Nested lists</h3>\n<pre><code>true = [\n    ['O', 'O', 'B-PER', 'I-PER', 'O'],\n    ['O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'O'],\n]\n\npred = [\n    ['O', 'O', 'B-PER', 'I-PER', 'O'],\n    ['O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'O'],\n]\n\nevaluator = Evaluator(true, pred, tags=['LOC', 'PER'], loader=\"list\")\n\nresults, results_by_tag = evaluator.evaluate()\n</code></pre>\n<h3>CoNLL style tab delimited</h3>\n<pre><code>\ntrue = \"word\\tO\\nword\\tO\\B-PER\\nword\\tI-PER\\n\"\n\npred = \"word\\tO\\nword\\tO\\B-PER\\nword\\tI-PER\\n\"\n\nevaluator = Evaluator(true, pred, tags=['PER'], loader=\"conll\")\n\nresults, results_by_tag = evaluator.evaluate()\n\n</code></pre>\n<h2>Extending the package to accept more formats</h2>\n<p>Additional formats can easily be added to the module by creating a converstion function in <code>nervaluate/utils.py</code>, for example <code>conll_to_spans()</code>. This function must return the spans in the prodigy style dicts shown in the prodigy example above.</p>\n<p>The new function can then be added to the list of loaders in <code>nervaluate/nervaluate.py</code>, and can then be selection with the <code>loader</code> argument when instantiating the <code>Evaluator</code> class.</p>\n<p>A list of formats we intend to include is included in <a href=\"https://github.com/ivyleavedtoadflax/nervaluate/issues/3\" rel=\"nofollow\">https://github.com/ivyleavedtoadflax/nervaluate/issues/3</a>.</p>\n\n          </div>"}, "last_serial": 6259285, "releases": {"0.1.3": [{"comment_text": "", "digests": {"md5": "db77e30601c0adc0b9fdefa2ab7b715c", "sha256": "71e4ddb2e1ca4c785ea1c797e180fa911d2654db68e24b4576c650d43388d360"}, "downloads": -1, "filename": "nervaluate-0.1.3.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "db77e30601c0adc0b9fdefa2ab7b715c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9529, "upload_time": "2019-12-06T21:14:04", "upload_time_iso_8601": "2019-12-06T21:14:04.321335Z", "url": "https://files.pythonhosted.org/packages/91/88/f310a1b16da368ffba86717f83b2fe102e1f67bd139f23c071c79f05c37b/nervaluate-0.1.3.linux-x86_64.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "4b2ced0907ef4c0290ec842e7d5519f6", "sha256": "ce2eb1831770ed4979b2fb840b9875787aec98f6ad4f5ab469a76236ab0b29e9"}, "downloads": -1, "filename": "nervaluate-0.1.4.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "4b2ced0907ef4c0290ec842e7d5519f6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10998, "upload_time": "2019-12-06T21:27:17", "upload_time_iso_8601": "2019-12-06T21:27:17.253241Z", "url": "https://files.pythonhosted.org/packages/b0/bb/351cbcf6533d6ce9498ebcfabdb95a8b5869fe18a961b98806b6fea5166f/nervaluate-0.1.4.linux-x86_64.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "b9ac68a9e1c079799b4a99aeb6cb5d30", "sha256": "2a0f4075769472d9c0603277e28214cde6918df35aba47997fd6296b4568ad88"}, "downloads": -1, "filename": "nervaluate-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "b9ac68a9e1c079799b4a99aeb6cb5d30", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 21205, "upload_time": "2019-12-06T21:46:05", "upload_time_iso_8601": "2019-12-06T21:46:05.276035Z", "url": "https://files.pythonhosted.org/packages/25/30/e329227b16914405deab65dee84491c49e9e767f7c948f2a1cc8f9cf5f90/nervaluate-0.1.5-py3-none-any.whl", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "15f68c43bc683a210ac8a407182cdb03", "sha256": "95227f7f441e94be08936f6d6d43a5321fe08797d6c47f55f6890b16da7f65c1"}, "downloads": -1, "filename": "nervaluate-0.1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "15f68c43bc683a210ac8a407182cdb03", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22464, "upload_time": "2019-12-07T18:59:50", "upload_time_iso_8601": "2019-12-07T18:59:50.891869Z", "url": "https://files.pythonhosted.org/packages/b0/c3/936e68696d5af52445c10626eca9fd50738cd6828203fe17d7255e79bf80/nervaluate-0.1.6-py3-none-any.whl", "yanked": false}], "0.1.7": [{"comment_text": "", "digests": {"md5": "fde6ca775a1899d8617c751c5067d46d", "sha256": "6108535160ce52a3549259befc82e52916648befc11aec0581b8c485c72069d3"}, "downloads": -1, "filename": "nervaluate-0.1.7-py3-none-any.whl", "has_sig": false, "md5_digest": "fde6ca775a1899d8617c751c5067d46d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22563, "upload_time": "2019-12-07T19:15:35", "upload_time_iso_8601": "2019-12-07T19:15:35.053003Z", "url": "https://files.pythonhosted.org/packages/33/66/ee4edaa89f9ee78406207785dd7ffb25c1dac75c6bdcf4808939fe203526/nervaluate-0.1.7-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fde6ca775a1899d8617c751c5067d46d", "sha256": "6108535160ce52a3549259befc82e52916648befc11aec0581b8c485c72069d3"}, "downloads": -1, "filename": "nervaluate-0.1.7-py3-none-any.whl", "has_sig": false, "md5_digest": "fde6ca775a1899d8617c751c5067d46d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22563, "upload_time": "2019-12-07T19:15:35", "upload_time_iso_8601": "2019-12-07T19:15:35.053003Z", "url": "https://files.pythonhosted.org/packages/33/66/ee4edaa89f9ee78406207785dd7ffb25c1dac75c6bdcf4808939fe203526/nervaluate-0.1.7-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:46:25 2020"}