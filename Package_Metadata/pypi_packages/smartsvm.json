{"info": {"author": "G.J.J. van den Burg", "author_email": "gertjanvandenburg@gmail.com", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3"], "description": "SmartSVM\n========\n\nSmartSVM is a Python package that implements the methods from `Fast \nMeta-Learning for Adaptive Hierarchical Classifier Design \n<https://arxiv.org/abs/1711.03512>`_ by `Gerrit J.J. van den Burg \n<https://gertjanvandenburg.com/research>`_ and `Alfred O. Hero \n<https://web.eecs.umich.edu/~hero/>`_. The package contains functions for \nestimating the Bayes error rate (BER) using the Henze-Penrose divergence and a \nhierarchical classifier called SmartSVM. See the Usage documentation below for \nmore details.\n\nInstallation\n============\n\nSmartSVM is available on PyPI and can be installed easily with:\n\n.. code:: bash\n\n    pip install smartsvm\n\n\nUsage\n=====\n\nIn the paper the main focus is on the accurate Bayes error estimates and the \nhierarchical classifier SmartSVM. These will therefore be of most interest to \nusers of the SmartSVM package. Below we briefly explain how to use these \nfunctions.\n\nCiting\n------\n\nIf you use this package in your research, please cite the paper using the \nfollowing BibTex entry::\n\n    @article{van2017fast,\n      title={Fast Meta-Learning for Adaptive Hierarchical Classifier Design},\n      author={Gerrit J.J. van den Burg and Alfred O. Hero},\n      journal={arXiv preprint arXiv:1711.03512},\n      archiveprefix={arXiv},\n      year={2017},\n      eprint={1711.03512},\n      url={https://arxiv.org/abs/1711.03512},\n      primaryclass={cs.LG}\n    }\n\nBayes error estimates\n---------------------\n\nError estimation is implemented three functions:\n\n* ``hp_estimate`` for the Henze-Penrose estimator of the Bayes error rate.  \n  This can be used as:\n\n  .. code:: python\n\n    >>> import numpy as np\n    >>> from smartsvm import hp_estimate\n    >>> X1 = np.random.multivariate_normal([-1, 0], [[1, 0], [0, 1]], 100)\n    >>> X2 = np.random.multivariate_normal([1, 0], [[1, 0], [0, 1]], 100)\n    >>> hp_estimate(X1, X2) # with normalization\n    >>> hp_estimate(X1, X2, normalize=False) # without normalization\n\n* ``compute_error_graph`` and ``compute_ovr_error`` respectively compute the \n  complete weighted graph of pairwise BER estimates or the One-vs-Rest BER for \n  each class. They have a similar interface:\n\n  .. code:: python\n\n    >>> import numpy as np\n    >>> from smartsvm import compute_error_graph, compute_ovr_error\n    >>> from sklearn.datasets import load_digits\n    >>> digits = load_digits(5)\n    >>> n_samples = len(digits.images)\n    >>> X = digits.images.reshape((n_samples, -1))\n    >>> y = digits.target\n    >>> G = compute_error_graph(X, y, n_jobs=2, normalize=True)\n    >>> d = compute_ovr_error(X, y, normalize=True)\n\n\nSmartSVM Classifier\n-------------------\n\nSmartSVM is an adaptive hierarchical classifier which constructs a \nclassification hierarchy based on the Henze-Penrose estimates of the Bayes \nerror between each pair of classes. The classifier is build on top of \nScikit-Learn and can be used in the exact same way as other sklearn \nclassifiers:\n\n.. code:: python\n\n    >>> import numpy as np\n    >>> from smartsvm import SmartSVM\n    >>> from sklearn.datasets import load_digits\n    >>> digits = load_digits(10)\n    >>> n_samples = len(digits.images)\n    >>> X = digits.images.reshape((n_samples, -1))\n    >>> y = digits.target\n    >>> clf = SmartSVM()\n    >>> clf.fit(X, y)\n    >>> clf.predict(X)\n\nBy default, the SmartSVM classifier uses the Linear Support Vector Machine \n(``LinearSVC``) as the underlying binary classifier for each binary subproblem \nin the hierarchy.  This can easily be changed with the ``binary_clf`` \nparameter to the class constructor, for instance:\n\n.. code:: python\n\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> clf = SmartSVM(binary_clf=DecisionTreeClassifier)\n    >>> clf.fit(X, y)\n    >>> clf._get_binary()\n    DecisionTreeClassifier(class_weight=None, criterion='gini',\n            max_depth=None, max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')\n\nYou may optionally add parameters for the classifier through the \n``clf_params`` parameter. This should be a dict with the parameters to the \nbinary classifier, as follows:\n\n.. code:: python\n\n    >>> clf = SmartSVM(binary_clf=DecisionTreeClassifier, clf_params={'criterion': 'entropy'})\n    >>> clf.fit(X, y)\n    >>> clf._get_binary()\n    DecisionTreeClassifier(class_weight=None, criterion='entropy',\n            max_depth=None, max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')\n\nFinally, it's possible to retrieve probability estimates for the classes if \nthe underlying classifier supports the ``predict_proba`` method:\n\n.. code:: python\n\n    >>> from sklearn.svm import SVC\n    >>> clf = SmartSVM(binary_clf=SVC, clf_params={\"probabilities\": True})\n    >>> clf.fit(X, y)\n    >>> prob = clf.predict_proba(X)\n    >>> import pandas as pd\n    >>> df = pd.DataFrame(prob)\n    >>> df\n                     0             1             2      ...\n    0     9.999997e-01  1.716831e-18  2.677824e-13      ...\n    1     1.000000e-07  9.956408e-01  1.035589e-09      ...\n    2     2.595652e-05  1.452011e-02  9.722321e-01      ...\n\nFor more information about parameters to SmartSVM, see the API documentation \n`here <https://smartsvm.readthedocs.io/en/latest/#api-documentation>`_.\n\nKnown Limitations\n=================\n\nThe Henze-Penrose estimator of the Bayes error rate is based on construction \nof the Euclidean minimal spanning tree. The current algorithm for this in the \nSmartSVM package uses an adaptation of `Whitney's algorithm \n<https://dl.acm.org/citation.cfm?id=361299>`_. This is not the fastest way to \nconstruct a minimal spanning tree. The `Fast Euclidean Minimal Spanning Tree \nalgorithm by March et al. <http://www.mlpack.org/papers/emst.pdf>`_, would be \na faster option but this makes it more difficult to construct orthogonal MSTs.  \nIncorporating this algorithm into the SmartSVM package is considered a topic \nfor future work.\n\nReferences\n==========\n\nThe main reference for this package is:\n\n* `G.J.J. van den Burg and A.O. Hero - Fast Meta-Learning for Adaptive \n  Hierarchical Classifier Design (2017) <https://arxiv.org/abs/1711.03512>`_.\n\nThe theory of the Henze-Penrose estimator is developed in:\n\n* `V. Berisha, A. Wisler, A.O. Hero, A. Spanias - Empirically Estimable \n  Classification Bounds Based on a Nonparametric Divergence Measure (2016) \n  <http://ieeexplore.ieee.org/abstract/document/7254229/>`_.\n* `V. Berisha, A.O. Hero -  Empirical Non-Parametric Estimation of the Fisher \n  Information (2015) \n  <http://ieeexplore.ieee.org/abstract/document/6975144/>`_.", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/HeroResearchGroup/SmartSVM", "keywords": "", "license": "GPL v2", "maintainer": "", "maintainer_email": "", "name": "smartsvm", "package_url": "https://pypi.org/project/smartsvm/", "platform": "", "project_url": "https://pypi.org/project/smartsvm/", "project_urls": {"Homepage": "https://github.com/HeroResearchGroup/SmartSVM"}, "release_url": "https://pypi.org/project/smartsvm/1.1.0/", "requires_dist": null, "requires_python": "", "summary": "Python package for Meta-Learning and Adaptive Hierarchical Classifier Design", "version": "1.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"smartsvm\">\n<h2>SmartSVM</h2>\n<p>SmartSVM is a Python package that implements the methods from <a href=\"https://arxiv.org/abs/1711.03512\" rel=\"nofollow\">Fast\nMeta-Learning for Adaptive Hierarchical Classifier Design</a> by <a href=\"https://gertjanvandenburg.com/research\" rel=\"nofollow\">Gerrit J.J. van den Burg</a> and <a href=\"https://web.eecs.umich.edu/~hero/\" rel=\"nofollow\">Alfred O. Hero</a>. The package contains functions for\nestimating the Bayes error rate (BER) using the Henze-Penrose divergence and a\nhierarchical classifier called SmartSVM. See the Usage documentation below for\nmore details.</p>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>SmartSVM is available on PyPI and can be installed easily with:</p>\n<pre>pip install smartsvm\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>In the paper the main focus is on the accurate Bayes error estimates and the\nhierarchical classifier SmartSVM. These will therefore be of most interest to\nusers of the SmartSVM package. Below we briefly explain how to use these\nfunctions.</p>\n<div id=\"citing\">\n<h3>Citing</h3>\n<p>If you use this package in your research, please cite the paper using the\nfollowing BibTex entry:</p>\n<pre>@article{van2017fast,\n  title={Fast Meta-Learning for Adaptive Hierarchical Classifier Design},\n  author={Gerrit J.J. van den Burg and Alfred O. Hero},\n  journal={arXiv preprint arXiv:1711.03512},\n  archiveprefix={arXiv},\n  year={2017},\n  eprint={1711.03512},\n  url={https://arxiv.org/abs/1711.03512},\n  primaryclass={cs.LG}\n}\n</pre>\n</div>\n<div id=\"bayes-error-estimates\">\n<h3>Bayes error estimates</h3>\n<p>Error estimation is implemented three functions:</p>\n<ul>\n<li><p><tt>hp_estimate</tt> for the Henze-Penrose estimator of the Bayes error rate.\nThis can be used as:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">smartsvm</span> <span class=\"kn\">import</span> <span class=\"n\">hp_estimate</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">X1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">multivariate_normal</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]],</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">X2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">multivariate_normal</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]],</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">hp_estimate</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">,</span> <span class=\"n\">X2</span><span class=\"p\">)</span> <span class=\"c1\"># with normalization</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">hp_estimate</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">,</span> <span class=\"n\">X2</span><span class=\"p\">,</span> <span class=\"n\">normalize</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span> <span class=\"c1\"># without normalization</span>\n</pre>\n</li>\n<li><p><tt>compute_error_graph</tt> and <tt>compute_ovr_error</tt> respectively compute the\ncomplete weighted graph of pairwise BER estimates or the One-vs-Rest BER for\neach class. They have a similar interface:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">smartsvm</span> <span class=\"kn\">import</span> <span class=\"n\">compute_error_graph</span><span class=\"p\">,</span> <span class=\"n\">compute_ovr_error</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_digits</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">digits</span> <span class=\"o\">=</span> <span class=\"n\">load_digits</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">n_samples</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">images</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">images</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">target</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">G</span> <span class=\"o\">=</span> <span class=\"n\">compute_error_graph</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">compute_ovr_error</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n</li>\n</ul>\n</div>\n<div id=\"smartsvm-classifier\">\n<h3>SmartSVM Classifier</h3>\n<p>SmartSVM is an adaptive hierarchical classifier which constructs a\nclassification hierarchy based on the Henze-Penrose estimates of the Bayes\nerror between each pair of classes. The classifier is build on top of\nScikit-Learn and can be used in the exact same way as other sklearn\nclassifiers:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">smartsvm</span> <span class=\"kn\">import</span> <span class=\"n\">SmartSVM</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_digits</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">digits</span> <span class=\"o\">=</span> <span class=\"n\">load_digits</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">n_samples</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">images</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">images</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">target</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">SmartSVM</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n</pre>\n<p>By default, the SmartSVM classifier uses the Linear Support Vector Machine\n(<tt>LinearSVC</tt>) as the underlying binary classifier for each binary subproblem\nin the hierarchy.  This can easily be changed with the <tt>binary_clf</tt>\nparameter to the class constructor, for instance:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">sklearn.tree</span> <span class=\"kn\">import</span> <span class=\"n\">DecisionTreeClassifier</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">SmartSVM</span><span class=\"p\">(</span><span class=\"n\">binary_clf</span><span class=\"o\">=</span><span class=\"n\">DecisionTreeClassifier</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">_get_binary</span><span class=\"p\">()</span>\n<span class=\"n\">DecisionTreeClassifier</span><span class=\"p\">(</span><span class=\"n\">class_weight</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">criterion</span><span class=\"o\">=</span><span class=\"s1\">'gini'</span><span class=\"p\">,</span>\n        <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_features</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_leaf_nodes</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">min_impurity_decrease</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"n\">min_impurity_split</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">min_samples_leaf</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">min_samples_split</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n        <span class=\"n\">min_weight_fraction_leaf</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"n\">presort</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">splitter</span><span class=\"o\">=</span><span class=\"s1\">'best'</span><span class=\"p\">)</span>\n</pre>\n<p>You may optionally add parameters for the classifier through the\n<tt>clf_params</tt> parameter. This should be a dict with the parameters to the\nbinary classifier, as follows:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">SmartSVM</span><span class=\"p\">(</span><span class=\"n\">binary_clf</span><span class=\"o\">=</span><span class=\"n\">DecisionTreeClassifier</span><span class=\"p\">,</span> <span class=\"n\">clf_params</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'criterion'</span><span class=\"p\">:</span> <span class=\"s1\">'entropy'</span><span class=\"p\">})</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">_get_binary</span><span class=\"p\">()</span>\n<span class=\"n\">DecisionTreeClassifier</span><span class=\"p\">(</span><span class=\"n\">class_weight</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">criterion</span><span class=\"o\">=</span><span class=\"s1\">'entropy'</span><span class=\"p\">,</span>\n        <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_features</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_leaf_nodes</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">min_impurity_decrease</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"n\">min_impurity_split</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">min_samples_leaf</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">min_samples_split</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n        <span class=\"n\">min_weight_fraction_leaf</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"n\">presort</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">splitter</span><span class=\"o\">=</span><span class=\"s1\">'best'</span><span class=\"p\">)</span>\n</pre>\n<p>Finally, it\u2019s possible to retrieve probability estimates for the classes if\nthe underlying classifier supports the <tt>predict_proba</tt> method:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">sklearn.svm</span> <span class=\"kn\">import</span> <span class=\"n\">SVC</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">SmartSVM</span><span class=\"p\">(</span><span class=\"n\">binary_clf</span><span class=\"o\">=</span><span class=\"n\">SVC</span><span class=\"p\">,</span> <span class=\"n\">clf_params</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"probabilities\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">predict_proba</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">prob</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span>\n                 <span class=\"mi\">0</span>             <span class=\"mi\">1</span>             <span class=\"mi\">2</span>      <span class=\"o\">...</span>\n<span class=\"mi\">0</span>     <span class=\"mf\">9.999997e-01</span>  <span class=\"mf\">1.716831e-18</span>  <span class=\"mf\">2.677824e-13</span>      <span class=\"o\">...</span>\n<span class=\"mi\">1</span>     <span class=\"mf\">1.000000e-07</span>  <span class=\"mf\">9.956408e-01</span>  <span class=\"mf\">1.035589e-09</span>      <span class=\"o\">...</span>\n<span class=\"mi\">2</span>     <span class=\"mf\">2.595652e-05</span>  <span class=\"mf\">1.452011e-02</span>  <span class=\"mf\">9.722321e-01</span>      <span class=\"o\">...</span>\n</pre>\n<p>For more information about parameters to SmartSVM, see the API documentation\n<a href=\"https://smartsvm.readthedocs.io/en/latest/#api-documentation\" rel=\"nofollow\">here</a>.</p>\n</div>\n</div>\n<div id=\"known-limitations\">\n<h2>Known Limitations</h2>\n<p>The Henze-Penrose estimator of the Bayes error rate is based on construction\nof the Euclidean minimal spanning tree. The current algorithm for this in the\nSmartSVM package uses an adaptation of <a href=\"https://dl.acm.org/citation.cfm?id=361299\" rel=\"nofollow\">Whitney\u2019s algorithm</a>. This is not the fastest way to\nconstruct a minimal spanning tree. The <a href=\"http://www.mlpack.org/papers/emst.pdf\" rel=\"nofollow\">Fast Euclidean Minimal Spanning Tree\nalgorithm by March et al.</a>, would be\na faster option but this makes it more difficult to construct orthogonal MSTs.\nIncorporating this algorithm into the SmartSVM package is considered a topic\nfor future work.</p>\n</div>\n<div id=\"references\">\n<h2>References</h2>\n<p>The main reference for this package is:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1711.03512\" rel=\"nofollow\">G.J.J. van den Burg and A.O. Hero - Fast Meta-Learning for Adaptive\nHierarchical Classifier Design (2017)</a>.</li>\n</ul>\n<p>The theory of the Henze-Penrose estimator is developed in:</p>\n<ul>\n<li><a href=\"http://ieeexplore.ieee.org/abstract/document/7254229/\" rel=\"nofollow\">V. Berisha, A. Wisler, A.O. Hero, A. Spanias - Empirically Estimable\nClassification Bounds Based on a Nonparametric Divergence Measure (2016)</a>.</li>\n<li><a href=\"http://ieeexplore.ieee.org/abstract/document/6975144/\" rel=\"nofollow\">V. Berisha, A.O. Hero -  Empirical Non-Parametric Estimation of the Fisher\nInformation (2015)</a>.</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 5385946, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "8705362b72125a62e2c4cbda095a6ad6", "sha256": "933a3fc451485c5fd1c27b702e01f3b191aa07c339b49d771e16bdb16f0a56bf"}, "downloads": -1, "filename": "smartsvm-1.0.0.tar.gz", "has_sig": false, "md5_digest": "8705362b72125a62e2c4cbda095a6ad6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 38108, "upload_time": "2017-11-10T07:24:08", "upload_time_iso_8601": "2017-11-10T07:24:08.276374Z", "url": "https://files.pythonhosted.org/packages/45/33/845dbbdf2cfa5107521be2acd8a3d1cc6af4ea738ea9c9fb3934382ae5bf/smartsvm-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "fa95247e2841218ab464ac7dffb56bde", "sha256": "2a2b2c5255b7f1630baa0ecadde3c3c0df989c7905d422f3bbf07429fe27f602"}, "downloads": -1, "filename": "smartsvm-1.0.1.tar.gz", "has_sig": false, "md5_digest": "fa95247e2841218ab464ac7dffb56bde", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 122903, "upload_time": "2017-11-10T07:58:43", "upload_time_iso_8601": "2017-11-10T07:58:43.081053Z", "url": "https://files.pythonhosted.org/packages/01/c0/0b2938508d8fbea8fd2ab1844aa14c57eb1f7d73230a62d827aea62fdac8/smartsvm-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "f844fe3cff8676499ca34ff374d2a772", "sha256": "ade90e50118ab6b36c28aefb322db62cec49fdacb039805bf1716a70680e1c9a"}, "downloads": -1, "filename": "smartsvm-1.0.2.tar.gz", "has_sig": false, "md5_digest": "f844fe3cff8676499ca34ff374d2a772", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 122895, "upload_time": "2017-11-10T08:19:33", "upload_time_iso_8601": "2017-11-10T08:19:33.761615Z", "url": "https://files.pythonhosted.org/packages/a8/f5/34e733894ae3ccb885c2f46c08499ce5f0f05c63c2ea83e4dc505dfd5f55/smartsvm-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "b1df2754215415ef3da6cc834abf5f1c", "sha256": "3c9be4e26d7adb90bf0a55ea246c3bd73dfa39aee3ccef678ae88b157a27d604"}, "downloads": -1, "filename": "smartsvm-1.0.3.tar.gz", "has_sig": false, "md5_digest": "b1df2754215415ef3da6cc834abf5f1c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 135176, "upload_time": "2019-01-18T08:33:24", "upload_time_iso_8601": "2019-01-18T08:33:24.559981Z", "url": "https://files.pythonhosted.org/packages/a2/73/ec92b3e476f9b1de68c5d3e3bf903ad3a6528e405194acc7118d2401fae5/smartsvm-1.0.3.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "f63777ebb52b2db52b9e5f8c4fd210a1", "sha256": "31531d75d5d4536673f0e696d9faab0699b09449c2fb175ddd7d9d9def1f6f46"}, "downloads": -1, "filename": "smartsvm-1.1.0.tar.gz", "has_sig": false, "md5_digest": "f63777ebb52b2db52b9e5f8c4fd210a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 136426, "upload_time": "2019-06-11T10:20:12", "upload_time_iso_8601": "2019-06-11T10:20:12.520661Z", "url": "https://files.pythonhosted.org/packages/cf/a3/5110d74428c074a4c0cf81f97058f67387365ed31fd3f3e8ae684ae09502/smartsvm-1.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f63777ebb52b2db52b9e5f8c4fd210a1", "sha256": "31531d75d5d4536673f0e696d9faab0699b09449c2fb175ddd7d9d9def1f6f46"}, "downloads": -1, "filename": "smartsvm-1.1.0.tar.gz", "has_sig": false, "md5_digest": "f63777ebb52b2db52b9e5f8c4fd210a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 136426, "upload_time": "2019-06-11T10:20:12", "upload_time_iso_8601": "2019-06-11T10:20:12.520661Z", "url": "https://files.pythonhosted.org/packages/cf/a3/5110d74428c074a4c0cf81f97058f67387365ed31fd3f3e8ae684ae09502/smartsvm-1.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:07:59 2020"}