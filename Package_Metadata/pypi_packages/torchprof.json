{"info": {"author": "Alexander Wong", "author_email": "alex@udia.ca", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "# torchprof\n\n[![PyPI version](https://badge.fury.io/py/torchprof.svg)](https://pypi.org/project/torchprof/)\n\nA minimal dependency library for layer-by-layer profiling of Pytorch models.\n\nAll metrics are derived using the PyTorch autograd profiler.\n\n## Quickstart\n\n`pip install torchprof`\n\n```python\nimport torch\nimport torchvision\nimport torchprof\n\nmodel = torchvision.models.alexnet(pretrained=False).cuda()\nx = torch.rand([1, 3, 224, 224]).cuda()\n\nwith torchprof.Profile(model, use_cuda=True) as prof:\n    model(x)\n\nprint(prof.display(show_events=False)) # equivalent to `print(prof)` and `print(prof.display())`\n```\n```text\nModule         | Self CPU total | CPU total | CUDA total | Occurrences\n---------------|----------------|-----------|------------|------------\nAlexNet        |                |           |            |\n\u251c\u2500\u2500 features   |                |           |            |\n\u2502\u251c\u2500\u2500 0         |        1.671ms |   6.589ms |    6.701ms |           1\n\u2502\u251c\u2500\u2500 1         |       62.430us |  62.430us |   63.264us |           1\n\u2502\u251c\u2500\u2500 2         |       62.909us | 109.948us |  112.640us |           1\n\u2502\u251c\u2500\u2500 3         |      225.389us | 858.376us |    1.814ms |           1\n\u2502\u251c\u2500\u2500 4         |       18.999us |  18.999us |   19.456us |           1\n\u2502\u251c\u2500\u2500 5         |       29.560us |  52.720us |   54.272us |           1\n\u2502\u251c\u2500\u2500 6         |      136.959us | 511.216us |  707.360us |           1\n\u2502\u251c\u2500\u2500 7         |       18.480us |  18.480us |   18.624us |           1\n\u2502\u251c\u2500\u2500 8         |       84.380us | 300.700us |  590.688us |           1\n\u2502\u251c\u2500\u2500 9         |       18.249us |  18.249us |   17.632us |           1\n\u2502\u251c\u2500\u2500 10        |       81.289us | 289.946us |  470.016us |           1\n\u2502\u251c\u2500\u2500 11        |       17.850us |  17.850us |   18.432us |           1\n\u2502\u2514\u2500\u2500 12        |       29.350us |  52.260us |   52.288us |           1\n\u251c\u2500\u2500 avgpool    |       41.840us |  70.840us |   76.832us |           1\n\u2514\u2500\u2500 classifier |                |           |            |\n \u251c\u2500\u2500 0         |       66.400us | 122.110us |  125.920us |           1\n \u251c\u2500\u2500 1         |      293.658us | 293.658us |  664.704us |           1\n \u251c\u2500\u2500 2         |       17.600us |  17.600us |   18.432us |           1\n \u251c\u2500\u2500 3         |       27.920us |  49.030us |   51.168us |           1\n \u251c\u2500\u2500 4         |       40.590us |  40.590us |  208.672us |           1\n \u251c\u2500\u2500 5         |       17.570us |  17.570us |   18.432us |           1\n \u2514\u2500\u2500 6         |       40.489us |  40.489us |   81.920us |           1\n```\n\nTo see the low level operations that occur within each layer, print the contents of  `prof.display(show_events=True)`.\n\n```text\nModule                        | Self CPU total | CPU total | CUDA total | Occurrences\n------------------------------|----------------|-----------|------------|------------\nAlexNet                       |                |           |            |\n\u251c\u2500\u2500 features                  |                |           |            |\n\u2502\u251c\u2500\u2500 0                        |                |           |            |\n\u2502\u2502\u251c\u2500\u2500 conv2d                  |       13.370us |   1.671ms |    1.698ms |           1\n\u2502\u2502\u251c\u2500\u2500 convolution             |       12.730us |   1.658ms |    1.685ms |           1\n\u2502\u2502\u251c\u2500\u2500 _convolution            |       30.660us |   1.645ms |    1.673ms |           1\n\u2502\u2502\u251c\u2500\u2500 contiguous              |        6.970us |   6.970us |    7.136us |           1\n\u2502\u2502\u2514\u2500\u2500 cudnn_convolution       |        1.608ms |   1.608ms |    1.638ms |           1\n\u2502\u251c\u2500\u2500 1                        |                |           |            |\n\u2502\u2502\u2514\u2500\u2500 relu_                   |       62.430us |  62.430us |   63.264us |           1\n\u2502\u251c\u2500\u2500 2                        |                |           |            |\n\u2502\u2502\u251c\u2500\u2500 max_pool2d              |       15.870us |  62.909us |   63.488us |           1\n\u2502\u2502\u2514\u2500\u2500 max_pool2d_with_indices |       47.039us |  47.039us |   49.152us |           1\n...\n```\n\n\nThe original [Pytorch EventList](https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile) can be returned by calling `raw()` on the profile instance.\n\n```python\ntrace, event_lists_dict = prof.raw()\nprint(trace[2])\n# Trace(path=('AlexNet', 'features', '0'), leaf=True, module=Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)))\n\nprint(event_lists_dict[trace[2].path][0])\n```\n```text\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nName                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nconv2d                 0.80%            13.370us         100.00%          1.671ms          1.671ms          25.34%           1.698ms          1.698ms          1                []\nconvolution            0.76%            12.730us         99.20%           1.658ms          1.658ms          25.15%           1.685ms          1.685ms          1                []\n_convolution           1.83%            30.660us         98.44%           1.645ms          1.645ms          24.97%           1.673ms          1.673ms          1                []\ncontiguous             0.42%            6.970us          0.42%            6.970us          6.970us          0.11%            7.136us          7.136us          1                []\ncudnn_convolution      96.19%           1.608ms          96.19%           1.608ms          1.608ms          24.44%           1.638ms          1.638ms          1                []\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nSelf CPU time total: 1.671ms\nCUDA time total: 6.701ms\n\n```\n\nLayers can be selected for individually using the optional `paths` kwarg. Profiling is ignored for all other layers.\n\n```python\nmodel = torchvision.models.alexnet(pretrained=False)\nx = torch.rand([1, 3, 224, 224])\n\n# Layer does not have to be a leaf layer\npaths = [(\"AlexNet\", \"features\", \"3\"), (\"AlexNet\", \"classifier\")]\n\nwith torchprof.Profile(model, paths=paths) as prof:\n    model(x)\n\nprint(prof)\n```\n\n```text\nModule         | Self CPU total | CPU total | CUDA total | Occurrences\n---------------|----------------|-----------|------------|------------\nAlexNet        |                |           |            |\n\u251c\u2500\u2500 features   |                |           |            |\n\u2502\u251c\u2500\u2500 0         |                |           |            |\n\u2502\u251c\u2500\u2500 1         |                |           |            |\n\u2502\u251c\u2500\u2500 2         |                |           |            |\n\u2502\u251c\u2500\u2500 3         |        3.189ms |  12.717ms |    0.000us |           1\n\u2502\u251c\u2500\u2500 4         |                |           |            |\n\u2502\u251c\u2500\u2500 5         |                |           |            |\n\u2502\u251c\u2500\u2500 6         |                |           |            |\n\u2502\u251c\u2500\u2500 7         |                |           |            |\n\u2502\u251c\u2500\u2500 8         |                |           |            |\n\u2502\u251c\u2500\u2500 9         |                |           |            |\n\u2502\u251c\u2500\u2500 10        |                |           |            |\n\u2502\u251c\u2500\u2500 11        |                |           |            |\n\u2502\u2514\u2500\u2500 12        |                |           |            |\n\u251c\u2500\u2500 avgpool    |                |           |            |\n\u2514\u2500\u2500 classifier |       13.403ms |  14.011ms |    0.000us |           1\n \u251c\u2500\u2500 0         |                |           |            |\n \u251c\u2500\u2500 1         |                |           |            |\n \u251c\u2500\u2500 2         |                |           |            |\n \u251c\u2500\u2500 3         |                |           |            |\n \u251c\u2500\u2500 4         |                |           |            |\n \u251c\u2500\u2500 5         |                |           |            |\n \u2514\u2500\u2500 6         |                |           |            |\n\n```\n\n* [Self CPU Time vs CPU Time](https://software.intel.com/en-us/vtune-amplifier-help-self-time-and-total-time)\n\n## Citation\n\nIf this software is useful to your research, I would greatly appreciate a citation in your work.\n\n```tex\n@misc{torchprof,\n  author       = {Alexander William Wong}, \n  title        = {torchprof},\n  howpublished = {github.com},\n  month        = 4,\n  year         = 2020,\n  note         = {A minimal dependency library for layer-by-layer profiling of Pytorch models.}\n}\n```\n\n## LICENSE\n[MIT](LICENSE)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/awwong1/torchprof", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "torchprof", "package_url": "https://pypi.org/project/torchprof/", "platform": "", "project_url": "https://pypi.org/project/torchprof/", "project_urls": {"Homepage": "https://github.com/awwong1/torchprof"}, "release_url": "https://pypi.org/project/torchprof/1.1.1/", "requires_dist": ["torch (<2,>=1.1.0)"], "requires_python": "", "summary": "Measure neural network device specific metrics (latency, flops, etc.)", "version": "1.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>torchprof</h1>\n<p><a href=\"https://pypi.org/project/torchprof/\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f19b33ac3abb662c101a60a9616a44ab720430db/68747470733a2f2f62616467652e667572792e696f2f70792f746f72636870726f662e737667\"></a></p>\n<p>A minimal dependency library for layer-by-layer profiling of Pytorch models.</p>\n<p>All metrics are derived using the PyTorch autograd profiler.</p>\n<h2>Quickstart</h2>\n<p><code>pip install torchprof</code></p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchprof</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">alexnet</span><span class=\"p\">(</span><span class=\"n\">pretrained</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n\n<span class=\"k\">with</span> <span class=\"n\">torchprof</span><span class=\"o\">.</span><span class=\"n\">Profile</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">use_cuda</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">prof</span><span class=\"p\">:</span>\n    <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">prof</span><span class=\"o\">.</span><span class=\"n\">display</span><span class=\"p\">(</span><span class=\"n\">show_events</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">))</span> <span class=\"c1\"># equivalent to `print(prof)` and `print(prof.display())`</span>\n</pre>\n<pre>Module         | Self CPU total | CPU total | CUDA total | Occurrences\n---------------|----------------|-----------|------------|------------\nAlexNet        |                |           |            |\n\u251c\u2500\u2500 features   |                |           |            |\n\u2502\u251c\u2500\u2500 0         |        1.671ms |   6.589ms |    6.701ms |           1\n\u2502\u251c\u2500\u2500 1         |       62.430us |  62.430us |   63.264us |           1\n\u2502\u251c\u2500\u2500 2         |       62.909us | 109.948us |  112.640us |           1\n\u2502\u251c\u2500\u2500 3         |      225.389us | 858.376us |    1.814ms |           1\n\u2502\u251c\u2500\u2500 4         |       18.999us |  18.999us |   19.456us |           1\n\u2502\u251c\u2500\u2500 5         |       29.560us |  52.720us |   54.272us |           1\n\u2502\u251c\u2500\u2500 6         |      136.959us | 511.216us |  707.360us |           1\n\u2502\u251c\u2500\u2500 7         |       18.480us |  18.480us |   18.624us |           1\n\u2502\u251c\u2500\u2500 8         |       84.380us | 300.700us |  590.688us |           1\n\u2502\u251c\u2500\u2500 9         |       18.249us |  18.249us |   17.632us |           1\n\u2502\u251c\u2500\u2500 10        |       81.289us | 289.946us |  470.016us |           1\n\u2502\u251c\u2500\u2500 11        |       17.850us |  17.850us |   18.432us |           1\n\u2502\u2514\u2500\u2500 12        |       29.350us |  52.260us |   52.288us |           1\n\u251c\u2500\u2500 avgpool    |       41.840us |  70.840us |   76.832us |           1\n\u2514\u2500\u2500 classifier |                |           |            |\n \u251c\u2500\u2500 0         |       66.400us | 122.110us |  125.920us |           1\n \u251c\u2500\u2500 1         |      293.658us | 293.658us |  664.704us |           1\n \u251c\u2500\u2500 2         |       17.600us |  17.600us |   18.432us |           1\n \u251c\u2500\u2500 3         |       27.920us |  49.030us |   51.168us |           1\n \u251c\u2500\u2500 4         |       40.590us |  40.590us |  208.672us |           1\n \u251c\u2500\u2500 5         |       17.570us |  17.570us |   18.432us |           1\n \u2514\u2500\u2500 6         |       40.489us |  40.489us |   81.920us |           1\n</pre>\n<p>To see the low level operations that occur within each layer, print the contents of  <code>prof.display(show_events=True)</code>.</p>\n<pre>Module                        | Self CPU total | CPU total | CUDA total | Occurrences\n------------------------------|----------------|-----------|------------|------------\nAlexNet                       |                |           |            |\n\u251c\u2500\u2500 features                  |                |           |            |\n\u2502\u251c\u2500\u2500 0                        |                |           |            |\n\u2502\u2502\u251c\u2500\u2500 conv2d                  |       13.370us |   1.671ms |    1.698ms |           1\n\u2502\u2502\u251c\u2500\u2500 convolution             |       12.730us |   1.658ms |    1.685ms |           1\n\u2502\u2502\u251c\u2500\u2500 _convolution            |       30.660us |   1.645ms |    1.673ms |           1\n\u2502\u2502\u251c\u2500\u2500 contiguous              |        6.970us |   6.970us |    7.136us |           1\n\u2502\u2502\u2514\u2500\u2500 cudnn_convolution       |        1.608ms |   1.608ms |    1.638ms |           1\n\u2502\u251c\u2500\u2500 1                        |                |           |            |\n\u2502\u2502\u2514\u2500\u2500 relu_                   |       62.430us |  62.430us |   63.264us |           1\n\u2502\u251c\u2500\u2500 2                        |                |           |            |\n\u2502\u2502\u251c\u2500\u2500 max_pool2d              |       15.870us |  62.909us |   63.488us |           1\n\u2502\u2502\u2514\u2500\u2500 max_pool2d_with_indices |       47.039us |  47.039us |   49.152us |           1\n...\n</pre>\n<p>The original <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile\" rel=\"nofollow\">Pytorch EventList</a> can be returned by calling <code>raw()</code> on the profile instance.</p>\n<pre><span class=\"n\">trace</span><span class=\"p\">,</span> <span class=\"n\">event_lists_dict</span> <span class=\"o\">=</span> <span class=\"n\">prof</span><span class=\"o\">.</span><span class=\"n\">raw</span><span class=\"p\">()</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">trace</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">])</span>\n<span class=\"c1\"># Trace(path=('AlexNet', 'features', '0'), leaf=True, module=Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)))</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">event_lists_dict</span><span class=\"p\">[</span><span class=\"n\">trace</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n</pre>\n<pre>---------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nName                   Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nconv2d                 0.80%            13.370us         100.00%          1.671ms          1.671ms          25.34%           1.698ms          1.698ms          1                []\nconvolution            0.76%            12.730us         99.20%           1.658ms          1.658ms          25.15%           1.685ms          1.685ms          1                []\n_convolution           1.83%            30.660us         98.44%           1.645ms          1.645ms          24.97%           1.673ms          1.673ms          1                []\ncontiguous             0.42%            6.970us          0.42%            6.970us          6.970us          0.11%            7.136us          7.136us          1                []\ncudnn_convolution      96.19%           1.608ms          96.19%           1.608ms          1.608ms          24.44%           1.638ms          1.638ms          1                []\n---------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nSelf CPU time total: 1.671ms\nCUDA time total: 6.701ms\n</pre>\n<p>Layers can be selected for individually using the optional <code>paths</code> kwarg. Profiling is ignored for all other layers.</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">alexnet</span><span class=\"p\">(</span><span class=\"n\">pretrained</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Layer does not have to be a leaf layer</span>\n<span class=\"n\">paths</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"s2\">\"AlexNet\"</span><span class=\"p\">,</span> <span class=\"s2\">\"features\"</span><span class=\"p\">,</span> <span class=\"s2\">\"3\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s2\">\"AlexNet\"</span><span class=\"p\">,</span> <span class=\"s2\">\"classifier\"</span><span class=\"p\">)]</span>\n\n<span class=\"k\">with</span> <span class=\"n\">torchprof</span><span class=\"o\">.</span><span class=\"n\">Profile</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">paths</span><span class=\"o\">=</span><span class=\"n\">paths</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">prof</span><span class=\"p\">:</span>\n    <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">prof</span><span class=\"p\">)</span>\n</pre>\n<pre>Module         | Self CPU total | CPU total | CUDA total | Occurrences\n---------------|----------------|-----------|------------|------------\nAlexNet        |                |           |            |\n\u251c\u2500\u2500 features   |                |           |            |\n\u2502\u251c\u2500\u2500 0         |                |           |            |\n\u2502\u251c\u2500\u2500 1         |                |           |            |\n\u2502\u251c\u2500\u2500 2         |                |           |            |\n\u2502\u251c\u2500\u2500 3         |        3.189ms |  12.717ms |    0.000us |           1\n\u2502\u251c\u2500\u2500 4         |                |           |            |\n\u2502\u251c\u2500\u2500 5         |                |           |            |\n\u2502\u251c\u2500\u2500 6         |                |           |            |\n\u2502\u251c\u2500\u2500 7         |                |           |            |\n\u2502\u251c\u2500\u2500 8         |                |           |            |\n\u2502\u251c\u2500\u2500 9         |                |           |            |\n\u2502\u251c\u2500\u2500 10        |                |           |            |\n\u2502\u251c\u2500\u2500 11        |                |           |            |\n\u2502\u2514\u2500\u2500 12        |                |           |            |\n\u251c\u2500\u2500 avgpool    |                |           |            |\n\u2514\u2500\u2500 classifier |       13.403ms |  14.011ms |    0.000us |           1\n \u251c\u2500\u2500 0         |                |           |            |\n \u251c\u2500\u2500 1         |                |           |            |\n \u251c\u2500\u2500 2         |                |           |            |\n \u251c\u2500\u2500 3         |                |           |            |\n \u251c\u2500\u2500 4         |                |           |            |\n \u251c\u2500\u2500 5         |                |           |            |\n \u2514\u2500\u2500 6         |                |           |            |\n</pre>\n<ul>\n<li><a href=\"https://software.intel.com/en-us/vtune-amplifier-help-self-time-and-total-time\" rel=\"nofollow\">Self CPU Time vs CPU Time</a></li>\n</ul>\n<h2>Citation</h2>\n<p>If this software is useful to your research, I would greatly appreciate a citation in your work.</p>\n<pre>@misc<span class=\"nb\">{</span>torchprof,\n  author       = <span class=\"nb\">{</span>Alexander William Wong<span class=\"nb\">}</span>, \n  title        = <span class=\"nb\">{</span>torchprof<span class=\"nb\">}</span>,\n  howpublished = <span class=\"nb\">{</span>github.com<span class=\"nb\">}</span>,\n  month        = 4,\n  year         = 2020,\n  note         = <span class=\"nb\">{</span>A minimal dependency library for layer-by-layer profiling of Pytorch models.<span class=\"nb\">}</span>\n<span class=\"nb\">}</span>\n</pre>\n<h2>LICENSE</h2>\n<p><a href=\"LICENSE\" rel=\"nofollow\">MIT</a></p>\n\n          </div>"}, "last_serial": 6970862, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "a483f050401f5d29e74e7b8a9bb1b6fa", "sha256": "1e316a0812a873058cf60c1a9f1d506023adf75c9776982e84caea0b20276f9f"}, "downloads": -1, "filename": "torchprof-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "a483f050401f5d29e74e7b8a9bb1b6fa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5485, "upload_time": "2019-07-05T18:20:16", "upload_time_iso_8601": "2019-07-05T18:20:16.545798Z", "url": "https://files.pythonhosted.org/packages/e1/1a/149dfb97b8fbae3c1df4e493b52c68e9a052e7cde4d5d579bef6e965b053/torchprof-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a4a4cbe385c4bd24d51c7cba2205bb3f", "sha256": "c95acb68ffcd5ad2ebd636ed52dca853b828e83091c616805d41b1c5f2b8d7a6"}, "downloads": -1, "filename": "torchprof-0.0.1.tar.gz", "has_sig": false, "md5_digest": "a4a4cbe385c4bd24d51c7cba2205bb3f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3867, "upload_time": "2019-07-05T18:20:18", "upload_time_iso_8601": "2019-07-05T18:20:18.408711Z", "url": "https://files.pythonhosted.org/packages/1e/37/5bb675dbf72b91f9882b85051fc2370f8047b2e79e4f0f2994c1d3390d4b/torchprof-0.0.1.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "4d6f941bafe3db491511a382a5c22384", "sha256": "b1ba29b5b9d47c7cb48dcc8a3e446c4b8dc0d42c75d5f65e1d8288d084eeaf0f"}, "downloads": -1, "filename": "torchprof-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "4d6f941bafe3db491511a382a5c22384", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5943, "upload_time": "2019-07-05T19:55:51", "upload_time_iso_8601": "2019-07-05T19:55:51.307093Z", "url": "https://files.pythonhosted.org/packages/f6/23/9c99fd367261cd9bc1c30f5ed0c070813d6611d1f012e5d724f05cbcceab/torchprof-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "46f6c80c3aa01a90ad3a1cc902ecc98e", "sha256": "130d03da9c006936c633869944addf0aa9cdd41c2c8cc586d9c73609dc8b692f"}, "downloads": -1, "filename": "torchprof-0.1.0.tar.gz", "has_sig": false, "md5_digest": "46f6c80c3aa01a90ad3a1cc902ecc98e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4328, "upload_time": "2019-07-05T19:55:52", "upload_time_iso_8601": "2019-07-05T19:55:52.679748Z", "url": "https://files.pythonhosted.org/packages/84/34/d2332f7dad0321eb4eb7e4d23d691ddb5b22dc06d374bc06b359d8092e4b/torchprof-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "87480cff76257d68c77a2ccac0cc0784", "sha256": "8d60f307f9e1cd688df44defc66b9b79871bf983eea04ac77e242d2fae6f1082"}, "downloads": -1, "filename": "torchprof-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "87480cff76257d68c77a2ccac0cc0784", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5935, "upload_time": "2019-07-10T02:37:48", "upload_time_iso_8601": "2019-07-10T02:37:48.189789Z", "url": "https://files.pythonhosted.org/packages/92/7a/52fd3fb3c7d3bccbec09e8e7445944d479567a942658fe3adc9e32ee9181/torchprof-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "35b1fd724aaf08dc141707e4c2ce225b", "sha256": "e8c29214efb07f166d8d53ad6d3c0f800a61f66fe4ae43d8cae38dbefc831e5f"}, "downloads": -1, "filename": "torchprof-0.1.1.tar.gz", "has_sig": false, "md5_digest": "35b1fd724aaf08dc141707e4c2ce225b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4324, "upload_time": "2019-07-10T02:37:49", "upload_time_iso_8601": "2019-07-10T02:37:49.888246Z", "url": "https://files.pythonhosted.org/packages/42/19/749b133f234e8ef19d288dd8a1b5277172f69f4751ff220a8acdc947bd77/torchprof-0.1.1.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "a0cd834c3263b29230b9fca0d3d9ab3f", "sha256": "22d39a02d65ddf0da6e0feb155ceb07b11b82dc69b652e5512fd56369db16ac9"}, "downloads": -1, "filename": "torchprof-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "a0cd834c3263b29230b9fca0d3d9ab3f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8734, "upload_time": "2019-07-12T04:06:07", "upload_time_iso_8601": "2019-07-12T04:06:07.036381Z", "url": "https://files.pythonhosted.org/packages/bc/90/d66ec3f982d86ac4bf2172676336735826ccb7caeea5ce31225b70fcefa5/torchprof-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cb34bce47aa408a46fba6848c5066f61", "sha256": "a7ee9e5beb21f67dc8115aff4b3d969fb68cce058b759628da49edaa96c8519d"}, "downloads": -1, "filename": "torchprof-0.2.0.tar.gz", "has_sig": false, "md5_digest": "cb34bce47aa408a46fba6848c5066f61", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7130, "upload_time": "2019-07-12T04:06:09", "upload_time_iso_8601": "2019-07-12T04:06:09.078462Z", "url": "https://files.pythonhosted.org/packages/51/7d/230e10d1bfab2024efdc8fd92c3c3c50ccd736e3c1146ff7dbf2393c28ee/torchprof-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "d5520468a1c972ab2054bc6055328cd1", "sha256": "0dc959be125e53bd6596bd317aa1f8f485f6ae4b9cf4f21bf70bb96629c8d932"}, "downloads": -1, "filename": "torchprof-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d5520468a1c972ab2054bc6055328cd1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8862, "upload_time": "2019-07-12T04:14:53", "upload_time_iso_8601": "2019-07-12T04:14:53.307978Z", "url": "https://files.pythonhosted.org/packages/0c/61/32b27cc00e12acef06df2494bf517a38d72efa7054b52d372c446771705e/torchprof-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "da80392668073f44264be4294e230436", "sha256": "1a44af5a752f33054fea60c07351c07d05edddb5b2b2522da5f44b02ae1e57e0"}, "downloads": -1, "filename": "torchprof-0.2.1.tar.gz", "has_sig": false, "md5_digest": "da80392668073f44264be4294e230436", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7317, "upload_time": "2019-07-12T04:14:55", "upload_time_iso_8601": "2019-07-12T04:14:55.186989Z", "url": "https://files.pythonhosted.org/packages/06/50/4213d9d8006ff8fde082569b900ce8d8ef095888d10d0e70140f41fc4804/torchprof-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "db1740e299697fa43d186eeabf3f7d03", "sha256": "c7325d57e0a67792f436ca94f2eb49b5f91dca87137b8ecb5694f6acbba5fbf0"}, "downloads": -1, "filename": "torchprof-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "db1740e299697fa43d186eeabf3f7d03", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9446, "upload_time": "2019-07-12T15:14:09", "upload_time_iso_8601": "2019-07-12T15:14:09.829695Z", "url": "https://files.pythonhosted.org/packages/d4/be/40b8878e06cf0f22b7d464ba5a88fd1e7f0fe6d228d00c87e2196c408d82/torchprof-0.2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c2a6b8cb8b41506e06b394acbc5d54db", "sha256": "e1b283f211f224add0e73347727acacdf23576a4c76b955bb2a95bd5c9c504c5"}, "downloads": -1, "filename": "torchprof-0.2.2.tar.gz", "has_sig": false, "md5_digest": "c2a6b8cb8b41506e06b394acbc5d54db", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8742, "upload_time": "2019-07-12T15:14:11", "upload_time_iso_8601": "2019-07-12T15:14:11.426587Z", "url": "https://files.pythonhosted.org/packages/dd/9b/216b9d2f62e8831c94be9f53490b0010d4fdce68833b7451eb1af697ac50/torchprof-0.2.2.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "16fe73b8278d3f0584901b804ed1e0d5", "sha256": "a9ba3fb4840ba6962d048ef39a86d8f68efaa317e7d741c349beb6637c944437"}, "downloads": -1, "filename": "torchprof-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "16fe73b8278d3f0584901b804ed1e0d5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9967, "upload_time": "2019-07-16T21:40:00", "upload_time_iso_8601": "2019-07-16T21:40:00.739852Z", "url": "https://files.pythonhosted.org/packages/4f/7a/1304b6cd588636a6cf8f2c078e9b1a1889ad0d373d148e07d5ae5e35eca7/torchprof-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "959194581e9fc8be883b645e5ad2acc5", "sha256": "fdbaee9177bb7acf88d877707d790bdbc589ca54df972e8e902954865a5d4b07"}, "downloads": -1, "filename": "torchprof-0.3.0.tar.gz", "has_sig": false, "md5_digest": "959194581e9fc8be883b645e5ad2acc5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9004, "upload_time": "2019-07-16T21:40:02", "upload_time_iso_8601": "2019-07-16T21:40:02.017087Z", "url": "https://files.pythonhosted.org/packages/39/06/2e76c3400f56aa6bfbe035f64bc256d7cd8ed1e280b861162f580970ec5c/torchprof-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "c926899d47b5aaea660a98d85b660fd8", "sha256": "68a1695318d2520d78562a158db75762089413c7906a6fa7ac784d1f96c0c531"}, "downloads": -1, "filename": "torchprof-0.3.1-py3-none-any.whl", "has_sig": false, "md5_digest": "c926899d47b5aaea660a98d85b660fd8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10298, "upload_time": "2019-07-22T15:24:58", "upload_time_iso_8601": "2019-07-22T15:24:58.822860Z", "url": "https://files.pythonhosted.org/packages/47/6c/a4faf0401ef2c06c9d98fa60987528079902e66a4dfb06d8262fb1a1dadf/torchprof-0.3.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d85a909bd8a408d0fa7eb97345fc6d48", "sha256": "2d57180cbc8fcc9afc9fed46fc315b705f377d4f4c82c2f4d87d13b153430ab2"}, "downloads": -1, "filename": "torchprof-0.3.1.tar.gz", "has_sig": false, "md5_digest": "d85a909bd8a408d0fa7eb97345fc6d48", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9675, "upload_time": "2019-07-22T15:25:00", "upload_time_iso_8601": "2019-07-22T15:25:00.521164Z", "url": "https://files.pythonhosted.org/packages/10/83/56f60686cf0177d9c99cf472e7225d3d757a204d4a45dfdbdacbf6881d9c/torchprof-0.3.1.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "f2fdb8a86724096c33b424cfd1d21648", "sha256": "7a7076e639edc4bda2544be2bf99aa3a5d95d416b8ca18e69b7f87b9d94236ae"}, "downloads": -1, "filename": "torchprof-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f2fdb8a86724096c33b424cfd1d21648", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8307, "upload_time": "2019-10-26T19:03:21", "upload_time_iso_8601": "2019-10-26T19:03:21.016209Z", "url": "https://files.pythonhosted.org/packages/fc/f1/82e7de460ac2bb0b6cd220b682168d896e9a9048cb7c0bfd8815118933d9/torchprof-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3c7a6db3d4c0bb3b984a3ff3178f6877", "sha256": "bf48fe5683326f019f8a6f33674f25672b90b029848aecfd5a2381e0e3c83a78"}, "downloads": -1, "filename": "torchprof-1.0.0.tar.gz", "has_sig": false, "md5_digest": "3c7a6db3d4c0bb3b984a3ff3178f6877", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7431, "upload_time": "2019-10-26T19:03:22", "upload_time_iso_8601": "2019-10-26T19:03:22.660881Z", "url": "https://files.pythonhosted.org/packages/86/53/346b18903175669095ff2dcc72bef285f133c2af7c4bfdcefda1050f75a2/torchprof-1.0.0.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "40a862b5fbeeb4f2e59f04ebf6b08c97", "sha256": "c87369f127f36ddb71bbb9c7014af8e02bf9f5073d164d774253975917285694"}, "downloads": -1, "filename": "torchprof-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "40a862b5fbeeb4f2e59f04ebf6b08c97", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8412, "upload_time": "2020-04-05T18:04:17", "upload_time_iso_8601": "2020-04-05T18:04:17.273280Z", "url": "https://files.pythonhosted.org/packages/db/08/25b6821a390cd70bbce986c503ede19ca3aff47f1a581738f37bd710ce51/torchprof-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c4903dca5368c37860beaa1f030c73d8", "sha256": "3128440776a0501fc265608adb330c7e8f535924f32b42122efc6c05a21ce7c2"}, "downloads": -1, "filename": "torchprof-1.1.0.tar.gz", "has_sig": false, "md5_digest": "c4903dca5368c37860beaa1f030c73d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8503, "upload_time": "2020-04-05T18:04:18", "upload_time_iso_8601": "2020-04-05T18:04:18.602716Z", "url": "https://files.pythonhosted.org/packages/a9/fa/f3eac84565c614d0db00f2db0f04b24c5ae27ee96948c9dafed98fca32ec/torchprof-1.1.0.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "86ddc70f031a19f05eab74a19b660eba", "sha256": "65b67301815a65341b8669f230d4962c9f62a80dc9151dcd0908b7383c5693b8"}, "downloads": -1, "filename": "torchprof-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "86ddc70f031a19f05eab74a19b660eba", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8574, "upload_time": "2020-04-07T15:35:37", "upload_time_iso_8601": "2020-04-07T15:35:37.933226Z", "url": "https://files.pythonhosted.org/packages/ce/cf/daf7a49c33c7d5d70aa22fc80be9545cdfe3785409c4966f016da75b93e1/torchprof-1.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "90563ce388e353a8e6c8abe83f31d4b7", "sha256": "a42cc12a83c66da59c960cf72a3c7306b38cc0d111ae05be92b8c1ba6d761b58"}, "downloads": -1, "filename": "torchprof-1.1.1.tar.gz", "has_sig": false, "md5_digest": "90563ce388e353a8e6c8abe83f31d4b7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8904, "upload_time": "2020-04-07T15:35:38", "upload_time_iso_8601": "2020-04-07T15:35:38.987811Z", "url": "https://files.pythonhosted.org/packages/61/2a/c0cbaf1edeaa39d1d31cf9cf4b8f5b16183f48e5885545ce0f6d9c44a203/torchprof-1.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "86ddc70f031a19f05eab74a19b660eba", "sha256": "65b67301815a65341b8669f230d4962c9f62a80dc9151dcd0908b7383c5693b8"}, "downloads": -1, "filename": "torchprof-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "86ddc70f031a19f05eab74a19b660eba", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8574, "upload_time": "2020-04-07T15:35:37", "upload_time_iso_8601": "2020-04-07T15:35:37.933226Z", "url": "https://files.pythonhosted.org/packages/ce/cf/daf7a49c33c7d5d70aa22fc80be9545cdfe3785409c4966f016da75b93e1/torchprof-1.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "90563ce388e353a8e6c8abe83f31d4b7", "sha256": "a42cc12a83c66da59c960cf72a3c7306b38cc0d111ae05be92b8c1ba6d761b58"}, "downloads": -1, "filename": "torchprof-1.1.1.tar.gz", "has_sig": false, "md5_digest": "90563ce388e353a8e6c8abe83f31d4b7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8904, "upload_time": "2020-04-07T15:35:38", "upload_time_iso_8601": "2020-04-07T15:35:38.987811Z", "url": "https://files.pythonhosted.org/packages/61/2a/c0cbaf1edeaa39d1d31cf9cf4b8f5b16183f48e5885545ce0f6d9c44a203/torchprof-1.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:13 2020"}