{"info": {"author": "Wang Qin", "author_email": "danielqin7@outlook.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# catd\nA Chinese co-word analysis with topic discovery package.\n\n# Overview\nThe catd co-word analysis with topic discovery package is intend for Chinese corpus analysis.\n\n## Use case\nFor better experience, you can run this script (with your corpus which have list of documents separated by `'\\n'`.)\n\nCorpus('$ProjectRoot/data/original_data/tianya_posts_test_set_10.txt):\n```text\ndocuments1\ndocuments2\n...\n```\n\nProgram: \n```python\nimport catd\nimport os\n\ncorpus = []\nwith open(os.path.join('data', 'original_data', 'tianya_posts_test_set_10.txt'), encoding='utf-8') as f:\n    for line in f:\n        corpus.append(line)\n\nstop_words_set = catd.util.collect_all_words_to_set_from_dir(os.path.join('data', 'stop_words'))\n\ncut_corpus = catd.util.word_cut(corpus, stop_words_set)\n\nword_net = catd.WordNet()\ncoded_corpus = word_net.generate_nodes_hash_and_edge(cut_corpus)\nword_net.add_cut_corpus(coded_corpus)\n```\n## Note\n\nNow I am working on the efficient visualization for big graph (hundreds of millions of edges).\n\nIf you have any question or suggestion, feel free to contact [the Author](mailto:danielqin7@outlook.com) in English or Chinese. But for the benefit of all users, please make communicate in English when it is public.  \n\n\n## Data Structure\n \n```\n* WordNet\n    * nodes   list[WordNode1, WordNode2, ...])\n    * edges   dict[word][neighbors] -> weight)\n    * docs    list[Doc1, Doc2, ...]\n    * get_node_by_str dict[word] -> WordNode\n\n* WordNode\n    * id\n    * name\n    * doc_count\n    * word_count\n    * inverse_document_frequency\n\n* Doc\n    * id\n    * word_count_in_doc\n    * word_tf_in_doc\n    * word_tf_idf\n    * num_of_words\n```\n\n## log\n\n### 0.3.0\n\nAdd support for lda model and topic information aggregation from words. \n\n## License\n\nMIT License", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/dqwerter/catd/archive/0.3.0.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/dqwerter/catd", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "catd", "package_url": "https://pypi.org/project/catd/", "platform": "", "project_url": "https://pypi.org/project/catd/", "project_urls": {"Download": "https://github.com/dqwerter/catd/archive/0.3.0.tar.gz", "Homepage": "https://github.com/dqwerter/catd"}, "release_url": "https://pypi.org/project/catd/0.3.0/", "requires_dist": null, "requires_python": ">=3.6", "summary": "A Chinese co-word analysis with topic discovery package", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>catd</h1>\n<p>A Chinese co-word analysis with topic discovery package.</p>\n<h1>Overview</h1>\n<p>The catd co-word analysis with topic discovery package is intend for Chinese corpus analysis.</p>\n<h2>Use case</h2>\n<p>For better experience, you can run this script (with your corpus which have list of documents separated by <code>'\\n'</code>.)</p>\n<p>Corpus('$ProjectRoot/data/original_data/tianya_posts_test_set_10.txt):</p>\n<pre>documents1\ndocuments2\n...\n</pre>\n<p>Program:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">catd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n\n<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"s1\">'data'</span><span class=\"p\">,</span> <span class=\"s1\">'original_data'</span><span class=\"p\">,</span> <span class=\"s1\">'tianya_posts_test_set_10.txt'</span><span class=\"p\">),</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">'utf-8'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n\n<span class=\"n\">stop_words_set</span> <span class=\"o\">=</span> <span class=\"n\">catd</span><span class=\"o\">.</span><span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">collect_all_words_to_set_from_dir</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"s1\">'data'</span><span class=\"p\">,</span> <span class=\"s1\">'stop_words'</span><span class=\"p\">))</span>\n\n<span class=\"n\">cut_corpus</span> <span class=\"o\">=</span> <span class=\"n\">catd</span><span class=\"o\">.</span><span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">word_cut</span><span class=\"p\">(</span><span class=\"n\">corpus</span><span class=\"p\">,</span> <span class=\"n\">stop_words_set</span><span class=\"p\">)</span>\n\n<span class=\"n\">word_net</span> <span class=\"o\">=</span> <span class=\"n\">catd</span><span class=\"o\">.</span><span class=\"n\">WordNet</span><span class=\"p\">()</span>\n<span class=\"n\">coded_corpus</span> <span class=\"o\">=</span> <span class=\"n\">word_net</span><span class=\"o\">.</span><span class=\"n\">generate_nodes_hash_and_edge</span><span class=\"p\">(</span><span class=\"n\">cut_corpus</span><span class=\"p\">)</span>\n<span class=\"n\">word_net</span><span class=\"o\">.</span><span class=\"n\">add_cut_corpus</span><span class=\"p\">(</span><span class=\"n\">coded_corpus</span><span class=\"p\">)</span>\n</pre>\n<h2>Note</h2>\n<p>Now I am working on the efficient visualization for big graph (hundreds of millions of edges).</p>\n<p>If you have any question or suggestion, feel free to contact <a href=\"mailto:danielqin7@outlook.com\">the Author</a> in English or Chinese. But for the benefit of all users, please make communicate in English when it is public.</p>\n<h2>Data Structure</h2>\n<pre><code>* WordNet\n    * nodes   list[WordNode1, WordNode2, ...])\n    * edges   dict[word][neighbors] -&gt; weight)\n    * docs    list[Doc1, Doc2, ...]\n    * get_node_by_str dict[word] -&gt; WordNode\n\n* WordNode\n    * id\n    * name\n    * doc_count\n    * word_count\n    * inverse_document_frequency\n\n* Doc\n    * id\n    * word_count_in_doc\n    * word_tf_in_doc\n    * word_tf_idf\n    * num_of_words\n</code></pre>\n<h2>log</h2>\n<h3>0.3.0</h3>\n<p>Add support for lda model and topic information aggregation from words.</p>\n<h2>License</h2>\n<p>MIT License</p>\n\n          </div>"}, "last_serial": 7091211, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "76c87f3408342a01a4be88d4fbeb1007", "sha256": "eca218cdc36757f87c721080e4895e8068cf139a11274855836bdf1725a7fcdf"}, "downloads": -1, "filename": "catd-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "76c87f3408342a01a4be88d4fbeb1007", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 7912, "upload_time": "2020-03-17T05:32:03", "upload_time_iso_8601": "2020-03-17T05:32:03.632361Z", "url": "https://files.pythonhosted.org/packages/3f/99/7679a4abd8a61c24fd0a6ba50fdde893683fa02c6006cdd3c0f2ce012abb/catd-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "492f71bfdc53d2ac1ad38e124261af59", "sha256": "2f110abb0ada274c28636ed51e2f8f0f03ff9092110e59cc412702e7be172b90"}, "downloads": -1, "filename": "catd-0.1.0.tar.gz", "has_sig": false, "md5_digest": "492f71bfdc53d2ac1ad38e124261af59", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 5851, "upload_time": "2020-03-17T05:32:08", "upload_time_iso_8601": "2020-03-17T05:32:08.483013Z", "url": "https://files.pythonhosted.org/packages/3a/16/645c87486f39a17425265bcbf0f64444cc79ce679f5db4f1b8df6635796f/catd-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "2569f17b296b824658b81879e2fcc462", "sha256": "0572814e6e4f34ea5d732ff909abef37d4680f5515b830573be4509e3080328c"}, "downloads": -1, "filename": "catd-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "2569f17b296b824658b81879e2fcc462", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 8386, "upload_time": "2020-03-18T03:36:02", "upload_time_iso_8601": "2020-03-18T03:36:02.457009Z", "url": "https://files.pythonhosted.org/packages/66/a1/91fc8be607f09ac80125cd083250af31d8461106acb1f451477a0f09bae6/catd-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f31bc958189c17fdd4ec51c84abe8699", "sha256": "041160471aa22693d0d0b49ca8d646e16ec32acb77813410a25375f4bd81ffe3"}, "downloads": -1, "filename": "catd-0.2.0.tar.gz", "has_sig": false, "md5_digest": "f31bc958189c17fdd4ec51c84abe8699", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7596, "upload_time": "2020-03-18T03:36:04", "upload_time_iso_8601": "2020-03-18T03:36:04.243928Z", "url": "https://files.pythonhosted.org/packages/2f/08/65ca1206d82352c95ec31a8a260eb38459524516477ee633cdf0a785c6a7/catd-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "fc0bf4b6c765111e454e44a49ba3556b", "sha256": "b5359319fb8b861b619b2f1238dde42e097f69be4e1c606b01d728c68e0fe23e"}, "downloads": -1, "filename": "catd-0.3.0.tar.gz", "has_sig": false, "md5_digest": "fc0bf4b6c765111e454e44a49ba3556b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 9366, "upload_time": "2020-04-24T09:14:00", "upload_time_iso_8601": "2020-04-24T09:14:00.486780Z", "url": "https://files.pythonhosted.org/packages/09/a0/e4e2bd5f7bda23b25d9ed1fb6bc0bcee2c029b1d3cd4d9364bb17246bd5c/catd-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fc0bf4b6c765111e454e44a49ba3556b", "sha256": "b5359319fb8b861b619b2f1238dde42e097f69be4e1c606b01d728c68e0fe23e"}, "downloads": -1, "filename": "catd-0.3.0.tar.gz", "has_sig": false, "md5_digest": "fc0bf4b6c765111e454e44a49ba3556b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 9366, "upload_time": "2020-04-24T09:14:00", "upload_time_iso_8601": "2020-04-24T09:14:00.486780Z", "url": "https://files.pythonhosted.org/packages/09/a0/e4e2bd5f7bda23b25d9ed1fb6bc0bcee2c029b1d3cd4d9364bb17246bd5c/catd-0.3.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:35:14 2020"}