{"info": {"author": "Koala Yeung", "author_email": "koalay@gmail.com", "bugtrack_url": null, "classifiers": ["Environment :: Console", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3 :: Only", "Topic :: Office/Business :: Financial :: Spreadsheet", "Topic :: Utilities"], "description": "# Scraparser\n\nA generic PDF table scraper and parser for data analysis.\n\n[![CI Status][ci-image]][ci-link] [![PyPi][pypi-image]][pypi-link] [![Pyversion][pyversion-image]][pypi-link]\n\n\nOriginally written for scraping and parsing Hong Kong government\n[COVID-19](https://en.wikipedia.org/wiki/Coronavirus_disease_2019)\nrelated public data. Now generalize for hopefully other research\npurposes as well.\n\nPackage is available on [pypi.org][pypi-link]. The development is\non [GitLab](https://gitlab.com/yookoala/scraparser). You are welcome\nto submit\n[issue](https://gitlab.com/yookoala/scraparser/-/issues) and\n[merge request](https://gitlab.com/yookoala/scraparser/-/merge_requests).\nAnd should you want to contribute, please read the\n[Development](https://gitlab.com/yookoala/scraparser/-/blob/master/#development) section.\n\n[ci-image]: https://gitlab.com/yookoala/scraparser/badges/master/pipeline.svg\n[ci-link]: https://gitlab.com/yookoala/scraparser/pipelines\n[pypi-image]: https://img.shields.io/pypi/v/scraparser.svg?label=pypi%20version\n[pyversion-image]: https://img.shields.io/pypi/pyversions/scraparser.svg\n[pypi-link]: https://pypi.org/project/scraparser/\n\n\n## Prerequisites\n\nTo use scraparser, you need [Python 3](https://www.python.org/)\ninstalled to your system. You will also need to know how to use\nterminal commands on your system.\n\nThe instructions below assumes that Python 3 is available to\nyou through the command `python3`. If it is only available to\nyou as command `python` or other name, simply use `python` or\nthe available name for the commands `python3` described below.\n\n\n## Install\n\nThe recommended way is to install\n[the PyPi package](https://pypi.org/project/scraparser/) with\nthe [pip] module:\n\n```\npython3 -m pip install --upgrade scraparser\n```\n\n\n## Example Use\n\n### Basic Scraping\n\nTo scrap the latest location situation report:\n\n```\npython3 -m scraparser scrap \"https://www.chp.gov.hk/files/pdf/local_situation_covid19_tc.pdf\" \\\n| python3 -m scraparser parse-pdf-to-csv --headers=\"\u500b\u6848\u7de8\u865f,\u5831\u544a\u65e5\u671f,\u767c\u75c5\u65e5\u671f,\u6027\u5225,\u5e74\u9f61,\u5165\u4f4f\u91ab\u9662\u540d\u7a31,\u4f4f\u9662/\u51fa\u9662/\u6b7b\u4ea1,\u9999\u6e2f/\u975e\u9999\u6e2f\u5c45\u6c11,\u500b\u6848\u5206\u985e,\u78ba\u8a3a/\u7591\u4f3c\u500b\u6848\"\n```\n\nThe downloaded PDF file and the parsed CSV file will be stored in:\n\n```\n./data/local_situation_covid19_tc.<time-string>.pdf\n./data/local_situation_covid19_tc.<time-string>.csv\n```\n\nThe `time-string` will be formated as `YYYY-MM-DD-HHmmss`.\n\n\n### Parse Previously Downloaded PDF Report\n\nTo parse pre-exist PDF file from your local computer:\n\n```\npython3 -m scraparser scrap-location-situation-pdf --file=path/to/somename.pdf\n```\n\nThe parsed CSV file will be stored to \"`path/to/somename.csv`\"\n\n\n### Utility to Fix or Modify Parsed CSV\n\nIts highly difficult to correctly read tables from PDF files. Common errors include:\n\n* **Column underflow / overflow**\n\n  The content of a cell spilled over to the last or next cell\n\n* **Row overflow**\n\n  The content of a cell (usually with line wraped into multiple lines), spilled over\n  to create a phantom row with only 1 content-filled cell.\n\nTo fix these issue, please use the following subcommands:\n\n#### `sort`\n\nThe command takes CSV filenames either from arguments or from `STDIN` (one filename)\nper line:\n\n```\npython3 -m scraparser sort --column=0 --sort-as-number --in-place ./data/local_situation_covid19_tc.<time-string>.csv\n```\n\nThis command will:\n\n1. Read the file and parse 1st column (parameter `--column`\n   accepts column definition start with 0, like in Python list index)\n2. Sort all rows by the 1st column.\n3. Save the fix result back to the input file.\n\n\n#### `fix column-underflow`\n\nThe command takes CSV filenames either from arguments or from `STDIN` (one filename)\nper line:\n\n```\npython3 -m scraparser fix column-underflow --column=5 --in-place ./data/local_situation_covid19_tc.<time-string>.csv\n```\n\nThis command will:\n\n1. Automatically read all the valid contents in the 6th column (parameter `--column`\n   accepts column definition start with 0, like in Python list index).\n2. Read every row and check if a cell in that column is empty (`math.isnan()`).\n3. If so, check the column before it (6th column for our case) and see if it is \n   suffixed by any valid content found in step (1).\n4. Split the content correctly for the 6th and 7th column.\n5. Save the fix result back to the input file.\n\n#### `fix date-column-underflow`\n\nThe command takes CSV filenames either from arguments or from `STDIN` (one filename)\nper line:\n\n```\npython3 -m scraparser fix date-column-underflow --column=1 --format=DD/MM/YYYY --in-place ./data/local_situation_covid19_tc.<time-string>.csv\n```\n\nThis command will:\n\n1. Read every row and check if a cell in the 2nd column is empty (`math.isnan()`).\n2. If so, check the column before it (1st column for our case) and see if it is \n   suffixed by string that matches our specified date format.\n3. Split the content correctly for the 1st and 2nd column.\n4. Save the fix result back to the input file.\n\n#### `fix date-column-overflow`\n\nThe command takes CSV filenames either from arguments or from `STDIN` (one filename)\nper line:\n\n```\npython3 -m scraparser fix date-column-overflow --column=1 --format=DD/MM/YYYY --in-place ./data/local_situation_covid19_tc.<time-string>.csv\n```\n\nThis command will:\n\n1. Read every row and check if a cell in the column before target column\n   (i.e. 1st column in our case) to see is empty (`math.isnan()`).\n2. If so, check the target column and see if it is some string\n   suffixed by date string of specified format.\n3. Split the content correctly for the 1st and 2nd column.\n4. Save the fix result back to the input file.\n\n#### `fix empty-rows`\n\nThe command takes CSV filenames either from arguments or from `STDIN` (one filename)\nper line:\n\n```\npython3 -m scraparser fix empty-rows --in-place ./data/local_situation_covid19_tc.<time-string>.csv\n```\n\nThis command will:\n\n1. Read every row and find all rows with all but 1 cell empty (`math.isnan()`).\n2. If so, append the content of that 1 cell to the cell directly above it.\n3. Drop all \"phantom rows\" found in step (1).\n4. Save the fix result back to the input file.\n\n## Advanced Piping usage\n\n### Parse and Show Result Data\n\nTo correctly fix all the issue created from the parsed CSV file in local situation report:\n\n**Linux**\n\n```\npython3 -m scraparser scrap \"https://www.chp.gov.hk/files/pdf/local_situation_covid19_tc.pdf\" \\\n| python3 -m scraparser parse-pdf-to-csv --headers=\"\u500b\u6848\u7de8\u865f,\u5831\u544a\u65e5\u671f,\u767c\u75c5\u65e5\u671f,\u6027\u5225,\u5e74\u9f61,\u5165\u4f4f\u91ab\u9662\u540d\u7a31,\u4f4f\u9662/\u51fa\u9662/\u6b7b\u4ea1,\u9999\u6e2f/\u975e\u9999\u6e2f\u5c45\u6c11,\u500b\u6848\u5206\u985e,\u78ba\u8a3a/\u7591\u4f3c\u500b\u6848\" \\\n| python3 -m scraparser fix date-column-underflow --column=1 --in-place \\\n| python3 -m scraparser fix date-column-overerflow --column=1 --in-place \\\n| python3 -m scraparser fix column-underflow --column=6 --in-place \\\n| python3 -m scraparser fix column-underflow --column=5 --in-place \\\n| python3 -m scraparser fix empty-rows --in-place \\\n| python3 -m scraparser sort --in-place \\\n| xargs -i xdg-open \"{}\"\n```\n\n**macos**\n\n```\npython3 -m scraparser scrap \"https://www.chp.gov.hk/files/pdf/local_situation_covid19_tc.pdf\" \\\n| python3 -m scraparser parse-pdf-to-csv --headers=\"\u500b\u6848\u7de8\u865f,\u5831\u544a\u65e5\u671f,\u767c\u75c5\u65e5\u671f,\u6027\u5225,\u5e74\u9f61,\u5165\u4f4f\u91ab\u9662\u540d\u7a31,\u4f4f\u9662/\u51fa\u9662/\u6b7b\u4ea1,\u9999\u6e2f/\u975e\u9999\u6e2f\u5c45\u6c11,\u500b\u6848\u5206\u985e,\u78ba\u8a3a/\u7591\u4f3c\u500b\u6848\" \\\n| python3 -m scraparser fix date-column-underflow --column=1 --in-place \\\n| python3 -m scraparser fix date-column-overerflow --column=1 --in-place \\\n| python3 -m scraparser fix column-underflow --column=6 --in-place \\\n| python3 -m scraparser fix column-underflow --column=5 --in-place \\\n| python3 -m scraparser fix empty-rows --in-place \\\n| python3 -m scraparser sort --in-place\n| xargs -I{} open \"{}\"\n```\n\n### Parse Data then Update Google Sheet\n\nThis will overwrite the current data specified in the range. If there are not enough rows in\nthe Google Sheet, the file will be expanded automatically.\n\nPresume you have defined the string `$GOOGLE_SHEET_ID` and the target sheet\n'CHP/DH Local Situation Input' exists:\n\n```\npython3 -m scraparser scrap \"https://www.chp.gov.hk/files/pdf/local_situation_covid19_tc.pdf\" \\\n| python3 -m scraparser parse-pdf-to-csv --headers=\"\u500b\u6848\u7de8\u865f,\u5831\u544a\u65e5\u671f,\u767c\u75c5\u65e5\u671f,\u6027\u5225,\u5e74\u9f61,\u5165\u4f4f\u91ab\u9662\u540d\u7a31,\u4f4f\u9662/\u51fa\u9662/\u6b7b\u4ea1,\u9999\u6e2f/\u975e\u9999\u6e2f\u5c45\u6c11,\u500b\u6848\u5206\u985e,\u78ba\u8a3a/\u7591\u4f3c\u500b\u6848\" \\\n| python3 -m scraparser fix date-column-underflow --column=1 --in-place \\\n| python3 -m scraparser fix date-column-overerflow --column=1 --in-place \\\n| python3 -m scraparser fix column-underflow --column=6 --in-place \\\n| python3 -m scraparser fix column-underflow --column=5 --in-place \\\n| python3 -m scraparser fix empty-rows --in-place \\\n| python3 -m scraparser sort --in-place \\\n| python3 -m scraparser googlesheet \"$GOOGLE_SHEET_ID\" update --range=\"'CHP/DH Local Situation Input'!A2:Z\" \n```\n\n## Development\n\nFirst clone this repository by:\n\n```\ngit clone https://gitlab.com/yookoala/scraparser.git\ncd scraparser\n```\n\nYou are recommended to use [venv](https://docs.python.org/3/library/venv.html)\nfor the development environment.\n\nFirst you would need to initialize venv and install all the packages specified\nin the [requirements.txt](https://gitlab.com/yookoala/scraparser/-/blob/master/requirements.txt):\n\n```\npip -m venv .venv\n. ./bin/activate.sh\npip install -r requirements.txt\n```\n\nOnce this is done, you are ready to run the package in the repository folder\nas if the module was installed locally:\n```\npython3 -m scraparser <command>\n```\n\nYou can change the [scraparser](https://gitlab.com/yookoala/scraparser/-/blob/master/scraparser) folder in this repository and this\ncommand will run correctly.\n\n### Build and Submit\n\nShould you want to fork and create your own `scraparser` package on Python Package index,\nyou may build and release your package (requires\n[make](https://www.gnu.org/software/make/manual/make.html)) with commands.\n\n#### Building\n\nTo build the package for upload, you need to rename the package to something other\nthan `scraparser`. Let's say you would suffix the package with `YOURNAME`:\n\n```\nPYPI_PKG_NAME=scraparser-YOURNAME make clean dist\n```\n\nThe default version is specified by git commands. If it fails to work, you may\nforce a version string on it:\n\n```\nPYPI_PKG_VERSION=0.5.0 PYPI_PKG_NAME=scraparser-YOURNAME make clean dist\n```\n\nPlease note that the version string **MUST** follow the\n[PEP 440](https://www.python.org/dev/peps/pep-0440/#version-scheme) convension\nor it cannot be submitted.\n\n#### Submitting to test.pypi.org\n\n```\nPYPI_TEST_PASSWORD=<your-pypi-test-token> make upload-test\n```\n\n#### Submitting to pypi.org\n\n```\nPYPI_PASSWORD=<your-pypi-test-token> make upload\n```\n\n## License\n\nLicense under [the MIT License](https://gitlab.com/yookoala/scraparser/-/blob/master/LICENSE). You may obtain the license in this repository.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://gitlab.com/yookoala/scraparser", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "scraparser", "package_url": "https://pypi.org/project/scraparser/", "platform": "", "project_url": "https://pypi.org/project/scraparser/", "project_urls": {"Code": "https://gitlab.com/yookoala/scraparser", "Homepage": "https://gitlab.com/yookoala/scraparser", "Issue tracker": "https://gitlab.com/yookoala/scraparser/-/issues"}, "release_url": "https://pypi.org/project/scraparser/0.3.1/", "requires_dist": ["camelot-py (>=0.7click>=7.1google-api-python-client>=1.8google-auth-httplib2>=0.0.3google-auth-oauthlib>=0.4opencv-python>=4.2python-magic>=0.4validators>=0.14)"], "requires_python": ">=3.6", "summary": "A simplified PDF table scraping and parsing tool", "version": "0.3.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Scraparser</h1>\n<p>A generic PDF table scraper and parser for data analysis.</p>\n<p><a href=\"https://gitlab.com/yookoala/scraparser/pipelines\" rel=\"nofollow\"><img alt=\"CI Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8d84dc73484b3a88924bdaaba162d81151b0eb6b/68747470733a2f2f6769746c61622e636f6d2f796f6f6b6f616c612f736372617061727365722f6261646765732f6d61737465722f706970656c696e652e737667\"></a> <a href=\"https://pypi.org/project/scraparser/\" rel=\"nofollow\"><img alt=\"PyPi\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7ecce33cfa4e5b05530f7cf5c0d7fd324ff2a228/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f736372617061727365722e7376673f6c6162656c3d7079706925323076657273696f6e\"></a> <a href=\"https://pypi.org/project/scraparser/\" rel=\"nofollow\"><img alt=\"Pyversion\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d4f5d320ce6632d9c80e55996ffac072a48ff9f4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f736372617061727365722e737667\"></a></p>\n<p>Originally written for scraping and parsing Hong Kong government\n<a href=\"https://en.wikipedia.org/wiki/Coronavirus_disease_2019\" rel=\"nofollow\">COVID-19</a>\nrelated public data. Now generalize for hopefully other research\npurposes as well.</p>\n<p>Package is available on <a href=\"https://pypi.org/project/scraparser/\" rel=\"nofollow\">pypi.org</a>. The development is\non <a href=\"https://gitlab.com/yookoala/scraparser\" rel=\"nofollow\">GitLab</a>. You are welcome\nto submit\n<a href=\"https://gitlab.com/yookoala/scraparser/-/issues\" rel=\"nofollow\">issue</a> and\n<a href=\"https://gitlab.com/yookoala/scraparser/-/merge_requests\" rel=\"nofollow\">merge request</a>.\nAnd should you want to contribute, please read the\n<a href=\"https://gitlab.com/yookoala/scraparser/-/blob/master/#development\" rel=\"nofollow\">Development</a> section.</p>\n<h2>Prerequisites</h2>\n<p>To use scraparser, you need <a href=\"https://www.python.org/\" rel=\"nofollow\">Python 3</a>\ninstalled to your system. You will also need to know how to use\nterminal commands on your system.</p>\n<p>The instructions below assumes that Python 3 is available to\nyou through the command <code>python3</code>. If it is only available to\nyou as command <code>python</code> or other name, simply use <code>python</code> or\nthe available name for the commands <code>python3</code> described below.</p>\n<h2>Install</h2>\n<p>The recommended way is to install\n<a href=\"https://pypi.org/project/scraparser/\" rel=\"nofollow\">the PyPi package</a> with\nthe [pip] module:</p>\n<pre><code>python3 -m pip install --upgrade scraparser\n</code></pre>\n<h2>Example Use</h2>\n<h3>Basic Scraping</h3>\n<p>To scrap the latest location situation report:</p>\n<pre><code>python3 -m scraparser scrap \"https://www.chp.gov.hk/files/pdf/local_situation_covid19_tc.pdf\" \\\n| python3 -m scraparser parse-pdf-to-csv --headers=\"\u500b\u6848\u7de8\u865f,\u5831\u544a\u65e5\u671f,\u767c\u75c5\u65e5\u671f,\u6027\u5225,\u5e74\u9f61,\u5165\u4f4f\u91ab\u9662\u540d\u7a31,\u4f4f\u9662/\u51fa\u9662/\u6b7b\u4ea1,\u9999\u6e2f/\u975e\u9999\u6e2f\u5c45\u6c11,\u500b\u6848\u5206\u985e,\u78ba\u8a3a/\u7591\u4f3c\u500b\u6848\"\n</code></pre>\n<p>The downloaded PDF file and the parsed CSV file will be stored in:</p>\n<pre><code>./data/local_situation_covid19_tc.&lt;time-string&gt;.pdf\n./data/local_situation_covid19_tc.&lt;time-string&gt;.csv\n</code></pre>\n<p>The <code>time-string</code> will be formated as <code>YYYY-MM-DD-HHmmss</code>.</p>\n<h3>Parse Previously Downloaded PDF Report</h3>\n<p>To parse pre-exist PDF file from your local computer:</p>\n<pre><code>python3 -m scraparser scrap-location-situation-pdf --file=path/to/somename.pdf\n</code></pre>\n<p>The parsed CSV file will be stored to \"<code>path/to/somename.csv</code>\"</p>\n<h3>Utility to Fix or Modify Parsed CSV</h3>\n<p>Its highly difficult to correctly read tables from PDF files. Common errors include:</p>\n<ul>\n<li>\n<p><strong>Column underflow / overflow</strong></p>\n<p>The content of a cell spilled over to the last or next cell</p>\n</li>\n<li>\n<p><strong>Row overflow</strong></p>\n<p>The content of a cell (usually with line wraped into multiple lines), spilled over\nto create a phantom row with only 1 content-filled cell.</p>\n</li>\n</ul>\n<p>To fix these issue, please use the following subcommands:</p>\n<h4><code>sort</code></h4>\n<p>The command takes CSV filenames either from arguments or from <code>STDIN</code> (one filename)\nper line:</p>\n<pre><code>python3 -m scraparser sort --column=0 --sort-as-number --in-place ./data/local_situation_covid19_tc.&lt;time-string&gt;.csv\n</code></pre>\n<p>This command will:</p>\n<ol>\n<li>Read the file and parse 1st column (parameter <code>--column</code>\naccepts column definition start with 0, like in Python list index)</li>\n<li>Sort all rows by the 1st column.</li>\n<li>Save the fix result back to the input file.</li>\n</ol>\n<h4><code>fix column-underflow</code></h4>\n<p>The command takes CSV filenames either from arguments or from <code>STDIN</code> (one filename)\nper line:</p>\n<pre><code>python3 -m scraparser fix column-underflow --column=5 --in-place ./data/local_situation_covid19_tc.&lt;time-string&gt;.csv\n</code></pre>\n<p>This command will:</p>\n<ol>\n<li>Automatically read all the valid contents in the 6th column (parameter <code>--column</code>\naccepts column definition start with 0, like in Python list index).</li>\n<li>Read every row and check if a cell in that column is empty (<code>math.isnan()</code>).</li>\n<li>If so, check the column before it (6th column for our case) and see if it is\nsuffixed by any valid content found in step (1).</li>\n<li>Split the content correctly for the 6th and 7th column.</li>\n<li>Save the fix result back to the input file.</li>\n</ol>\n<h4><code>fix date-column-underflow</code></h4>\n<p>The command takes CSV filenames either from arguments or from <code>STDIN</code> (one filename)\nper line:</p>\n<pre><code>python3 -m scraparser fix date-column-underflow --column=1 --format=DD/MM/YYYY --in-place ./data/local_situation_covid19_tc.&lt;time-string&gt;.csv\n</code></pre>\n<p>This command will:</p>\n<ol>\n<li>Read every row and check if a cell in the 2nd column is empty (<code>math.isnan()</code>).</li>\n<li>If so, check the column before it (1st column for our case) and see if it is\nsuffixed by string that matches our specified date format.</li>\n<li>Split the content correctly for the 1st and 2nd column.</li>\n<li>Save the fix result back to the input file.</li>\n</ol>\n<h4><code>fix date-column-overflow</code></h4>\n<p>The command takes CSV filenames either from arguments or from <code>STDIN</code> (one filename)\nper line:</p>\n<pre><code>python3 -m scraparser fix date-column-overflow --column=1 --format=DD/MM/YYYY --in-place ./data/local_situation_covid19_tc.&lt;time-string&gt;.csv\n</code></pre>\n<p>This command will:</p>\n<ol>\n<li>Read every row and check if a cell in the column before target column\n(i.e. 1st column in our case) to see is empty (<code>math.isnan()</code>).</li>\n<li>If so, check the target column and see if it is some string\nsuffixed by date string of specified format.</li>\n<li>Split the content correctly for the 1st and 2nd column.</li>\n<li>Save the fix result back to the input file.</li>\n</ol>\n<h4><code>fix empty-rows</code></h4>\n<p>The command takes CSV filenames either from arguments or from <code>STDIN</code> (one filename)\nper line:</p>\n<pre><code>python3 -m scraparser fix empty-rows --in-place ./data/local_situation_covid19_tc.&lt;time-string&gt;.csv\n</code></pre>\n<p>This command will:</p>\n<ol>\n<li>Read every row and find all rows with all but 1 cell empty (<code>math.isnan()</code>).</li>\n<li>If so, append the content of that 1 cell to the cell directly above it.</li>\n<li>Drop all \"phantom rows\" found in step (1).</li>\n<li>Save the fix result back to the input file.</li>\n</ol>\n<h2>Advanced Piping usage</h2>\n<h3>Parse and Show Result Data</h3>\n<p>To correctly fix all the issue created from the parsed CSV file in local situation report:</p>\n<p><strong>Linux</strong></p>\n<pre><code>python3 -m scraparser scrap \"https://www.chp.gov.hk/files/pdf/local_situation_covid19_tc.pdf\" \\\n| python3 -m scraparser parse-pdf-to-csv --headers=\"\u500b\u6848\u7de8\u865f,\u5831\u544a\u65e5\u671f,\u767c\u75c5\u65e5\u671f,\u6027\u5225,\u5e74\u9f61,\u5165\u4f4f\u91ab\u9662\u540d\u7a31,\u4f4f\u9662/\u51fa\u9662/\u6b7b\u4ea1,\u9999\u6e2f/\u975e\u9999\u6e2f\u5c45\u6c11,\u500b\u6848\u5206\u985e,\u78ba\u8a3a/\u7591\u4f3c\u500b\u6848\" \\\n| python3 -m scraparser fix date-column-underflow --column=1 --in-place \\\n| python3 -m scraparser fix date-column-overerflow --column=1 --in-place \\\n| python3 -m scraparser fix column-underflow --column=6 --in-place \\\n| python3 -m scraparser fix column-underflow --column=5 --in-place \\\n| python3 -m scraparser fix empty-rows --in-place \\\n| python3 -m scraparser sort --in-place \\\n| xargs -i xdg-open \"{}\"\n</code></pre>\n<p><strong>macos</strong></p>\n<pre><code>python3 -m scraparser scrap \"https://www.chp.gov.hk/files/pdf/local_situation_covid19_tc.pdf\" \\\n| python3 -m scraparser parse-pdf-to-csv --headers=\"\u500b\u6848\u7de8\u865f,\u5831\u544a\u65e5\u671f,\u767c\u75c5\u65e5\u671f,\u6027\u5225,\u5e74\u9f61,\u5165\u4f4f\u91ab\u9662\u540d\u7a31,\u4f4f\u9662/\u51fa\u9662/\u6b7b\u4ea1,\u9999\u6e2f/\u975e\u9999\u6e2f\u5c45\u6c11,\u500b\u6848\u5206\u985e,\u78ba\u8a3a/\u7591\u4f3c\u500b\u6848\" \\\n| python3 -m scraparser fix date-column-underflow --column=1 --in-place \\\n| python3 -m scraparser fix date-column-overerflow --column=1 --in-place \\\n| python3 -m scraparser fix column-underflow --column=6 --in-place \\\n| python3 -m scraparser fix column-underflow --column=5 --in-place \\\n| python3 -m scraparser fix empty-rows --in-place \\\n| python3 -m scraparser sort --in-place\n| xargs -I{} open \"{}\"\n</code></pre>\n<h3>Parse Data then Update Google Sheet</h3>\n<p>This will overwrite the current data specified in the range. If there are not enough rows in\nthe Google Sheet, the file will be expanded automatically.</p>\n<p>Presume you have defined the string <code>$GOOGLE_SHEET_ID</code> and the target sheet\n'CHP/DH Local Situation Input' exists:</p>\n<pre><code>python3 -m scraparser scrap \"https://www.chp.gov.hk/files/pdf/local_situation_covid19_tc.pdf\" \\\n| python3 -m scraparser parse-pdf-to-csv --headers=\"\u500b\u6848\u7de8\u865f,\u5831\u544a\u65e5\u671f,\u767c\u75c5\u65e5\u671f,\u6027\u5225,\u5e74\u9f61,\u5165\u4f4f\u91ab\u9662\u540d\u7a31,\u4f4f\u9662/\u51fa\u9662/\u6b7b\u4ea1,\u9999\u6e2f/\u975e\u9999\u6e2f\u5c45\u6c11,\u500b\u6848\u5206\u985e,\u78ba\u8a3a/\u7591\u4f3c\u500b\u6848\" \\\n| python3 -m scraparser fix date-column-underflow --column=1 --in-place \\\n| python3 -m scraparser fix date-column-overerflow --column=1 --in-place \\\n| python3 -m scraparser fix column-underflow --column=6 --in-place \\\n| python3 -m scraparser fix column-underflow --column=5 --in-place \\\n| python3 -m scraparser fix empty-rows --in-place \\\n| python3 -m scraparser sort --in-place \\\n| python3 -m scraparser googlesheet \"$GOOGLE_SHEET_ID\" update --range=\"'CHP/DH Local Situation Input'!A2:Z\" \n</code></pre>\n<h2>Development</h2>\n<p>First clone this repository by:</p>\n<pre><code>git clone https://gitlab.com/yookoala/scraparser.git\ncd scraparser\n</code></pre>\n<p>You are recommended to use <a href=\"https://docs.python.org/3/library/venv.html\" rel=\"nofollow\">venv</a>\nfor the development environment.</p>\n<p>First you would need to initialize venv and install all the packages specified\nin the <a href=\"https://gitlab.com/yookoala/scraparser/-/blob/master/requirements.txt\" rel=\"nofollow\">requirements.txt</a>:</p>\n<pre><code>pip -m venv .venv\n. ./bin/activate.sh\npip install -r requirements.txt\n</code></pre>\n<p>Once this is done, you are ready to run the package in the repository folder\nas if the module was installed locally:</p>\n<pre><code>python3 -m scraparser &lt;command&gt;\n</code></pre>\n<p>You can change the <a href=\"https://gitlab.com/yookoala/scraparser/-/blob/master/scraparser\" rel=\"nofollow\">scraparser</a> folder in this repository and this\ncommand will run correctly.</p>\n<h3>Build and Submit</h3>\n<p>Should you want to fork and create your own <code>scraparser</code> package on Python Package index,\nyou may build and release your package (requires\n<a href=\"https://www.gnu.org/software/make/manual/make.html\" rel=\"nofollow\">make</a>) with commands.</p>\n<h4>Building</h4>\n<p>To build the package for upload, you need to rename the package to something other\nthan <code>scraparser</code>. Let's say you would suffix the package with <code>YOURNAME</code>:</p>\n<pre><code>PYPI_PKG_NAME=scraparser-YOURNAME make clean dist\n</code></pre>\n<p>The default version is specified by git commands. If it fails to work, you may\nforce a version string on it:</p>\n<pre><code>PYPI_PKG_VERSION=0.5.0 PYPI_PKG_NAME=scraparser-YOURNAME make clean dist\n</code></pre>\n<p>Please note that the version string <strong>MUST</strong> follow the\n<a href=\"https://www.python.org/dev/peps/pep-0440/#version-scheme\" rel=\"nofollow\">PEP 440</a> convension\nor it cannot be submitted.</p>\n<h4>Submitting to test.pypi.org</h4>\n<pre><code>PYPI_TEST_PASSWORD=&lt;your-pypi-test-token&gt; make upload-test\n</code></pre>\n<h4>Submitting to pypi.org</h4>\n<pre><code>PYPI_PASSWORD=&lt;your-pypi-test-token&gt; make upload\n</code></pre>\n<h2>License</h2>\n<p>License under <a href=\"https://gitlab.com/yookoala/scraparser/-/blob/master/LICENSE\" rel=\"nofollow\">the MIT License</a>. You may obtain the license in this repository.</p>\n\n          </div>"}, "last_serial": 7145348, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "d8941477b0c888563973c80d5d551eec", "sha256": "38376eead698ae32aa0237445f90d2c207a435f455d6e602ee5eaeee86a3aed2"}, "downloads": -1, "filename": "scraparser-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "d8941477b0c888563973c80d5d551eec", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 11267, "upload_time": "2020-04-23T09:50:11", "upload_time_iso_8601": "2020-04-23T09:50:11.281921Z", "url": "https://files.pythonhosted.org/packages/a1/2c/0054c30b881abb3f1c896563bdd4f05ff35ab1d5dd4da639ff7012ea18e5/scraparser-0.0.2-py3-none-any.whl", "yanked": true}, {"comment_text": "", "digests": {"md5": "dcdf60801263c7c202b590f51230887d", "sha256": "2ae9e2d7571f39890cc428f5b727ff11af067eeee9d26dbe4cd58147871b75b4"}, "downloads": -1, "filename": "scraparser-0.0.2.tar.gz", "has_sig": false, "md5_digest": "dcdf60801263c7c202b590f51230887d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11453, "upload_time": "2020-04-23T09:50:13", "upload_time_iso_8601": "2020-04-23T09:50:13.856389Z", "url": "https://files.pythonhosted.org/packages/a6/e5/8734427e7c7d06aa5b9f8cddd18f27177d742519abb639248348aee98c0f/scraparser-0.0.2.tar.gz", "yanked": true}], "0.2.1.post0.dev20200423171934": [{"comment_text": "", "digests": {"md5": "0ffffccebd79f37e536207869e4a1915", "sha256": "3f2d424e133e38f092218e521e9db6f8eedc3feefbc5fa1bbd4cd9c9d30817f7"}, "downloads": -1, "filename": "scraparser-0.2.1.post0.dev20200423171934-py3-none-any.whl", "has_sig": false, "md5_digest": "0ffffccebd79f37e536207869e4a1915", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 12518, "upload_time": "2020-04-23T17:19:46", "upload_time_iso_8601": "2020-04-23T17:19:46.042658Z", "url": "https://files.pythonhosted.org/packages/ce/03/8b630a4169533b6a7cf0b6a2ac35b38400979be70018bc8d9c81553df229/scraparser-0.2.1.post0.dev20200423171934-py3-none-any.whl", "yanked": true}, {"comment_text": "", "digests": {"md5": "d3aa01c590b4dacafc89df85d75bfd1a", "sha256": "7dbd61ab8b48ceb7c05c018c8fb863d48e4df49e21ccef3e001065c7972c7abd"}, "downloads": -1, "filename": "scraparser-0.2.1.post0.dev20200423171934.tar.gz", "has_sig": false, "md5_digest": "d3aa01c590b4dacafc89df85d75bfd1a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13425, "upload_time": "2020-04-23T17:19:46", "upload_time_iso_8601": "2020-04-23T17:19:46.865355Z", "url": "https://files.pythonhosted.org/packages/f1/07/7af1f6fadf1e3b95ae389c56aa4f39f1097b4d40cb171772aa706e4b53b7/scraparser-0.2.1.post0.dev20200423171934.tar.gz", "yanked": true}], "0.2.3": [{"comment_text": "", "digests": {"md5": "52422fb291be4a68a18d6c557e52365c", "sha256": "a07fd2682cd5ddb85dabafcd567255cab4360283e2f33a499c1fb6a12987b8dc"}, "downloads": -1, "filename": "scraparser-0.2.3-py3-none-any.whl", "has_sig": false, "md5_digest": "52422fb291be4a68a18d6c557e52365c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 12445, "upload_time": "2020-04-23T18:39:55", "upload_time_iso_8601": "2020-04-23T18:39:55.131474Z", "url": "https://files.pythonhosted.org/packages/07/3c/ac35a51ce3b9000a3f655bdde3d155e2616160a1530eb1552d51dcbf98bb/scraparser-0.2.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "37784b3013878ca6efe4a319904fdd22", "sha256": "db5a45117566678a2be0fcb7463f98d9fb82ac5c134f2afaa8ef84238d55e28f"}, "downloads": -1, "filename": "scraparser-0.2.3.tar.gz", "has_sig": false, "md5_digest": "37784b3013878ca6efe4a319904fdd22", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13889, "upload_time": "2020-04-23T18:39:56", "upload_time_iso_8601": "2020-04-23T18:39:56.160519Z", "url": "https://files.pythonhosted.org/packages/08/7a/b62a9d02e4a65bd2d46cb3fda2890c2f1938c0b0b1d33f04d3e1da856010/scraparser-0.2.3.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "2e6a6ffd49ff1908d782c6674637eef6", "sha256": "dd2a6b9c01a8324d9f00dba21b6763faf68884d045695dbff21fff06f26a1ea2"}, "downloads": -1, "filename": "scraparser-0.2.4-py3-none-any.whl", "has_sig": false, "md5_digest": "2e6a6ffd49ff1908d782c6674637eef6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 12743, "upload_time": "2020-04-24T16:05:02", "upload_time_iso_8601": "2020-04-24T16:05:02.373024Z", "url": "https://files.pythonhosted.org/packages/a3/92/0b43442abdc884e6315e1e6860465249b332375119383d66ab7922b1d4d0/scraparser-0.2.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a79ee638554b106c81f43e67c7d0af7a", "sha256": "57216671ce692565c438f0fb785dfbe86982da0cc4fd9bef575d9f3450b8b8a1"}, "downloads": -1, "filename": "scraparser-0.2.4.tar.gz", "has_sig": false, "md5_digest": "a79ee638554b106c81f43e67c7d0af7a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14222, "upload_time": "2020-04-24T16:05:03", "upload_time_iso_8601": "2020-04-24T16:05:03.313933Z", "url": "https://files.pythonhosted.org/packages/18/f6/e1ba577ff011da762698872d7f4ef0014d9d756e453871f359c48ae82e8c/scraparser-0.2.4.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "3e7cd5a021e23c3d318ef51ed2269061", "sha256": "892def6df2e1dbbf34023dc6b5569cbfba03a5635f8f115d5da2e6866f3178a3"}, "downloads": -1, "filename": "scraparser-0.2.5-py3-none-any.whl", "has_sig": false, "md5_digest": "3e7cd5a021e23c3d318ef51ed2269061", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 14942, "upload_time": "2020-04-27T15:03:49", "upload_time_iso_8601": "2020-04-27T15:03:49.881278Z", "url": "https://files.pythonhosted.org/packages/f4/50/342063bd96bcbc870561866a7133e016ce2069543e63c1c79b7c22221cbc/scraparser-0.2.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6f2f86ef7a7a81fea00b47042e479799", "sha256": "f1553c8cb282a1ac4ac6b2dcd86db6e505bf057e7e12d6063f20434fe9cdd382"}, "downloads": -1, "filename": "scraparser-0.2.5.tar.gz", "has_sig": false, "md5_digest": "6f2f86ef7a7a81fea00b47042e479799", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 15394, "upload_time": "2020-04-27T15:03:50", "upload_time_iso_8601": "2020-04-27T15:03:50.634457Z", "url": "https://files.pythonhosted.org/packages/9d/72/66860a6ea34e65bfeb7b96eda02c3de140e3735e7d36f037e12bda7864b3/scraparser-0.2.5.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "22dcfcc25df1706a89ac732df2d8cf75", "sha256": "61131808cf63df80d718af298c46ea6621ef0ff2a08ad739a242ab74616f50a7"}, "downloads": -1, "filename": "scraparser-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "22dcfcc25df1706a89ac732df2d8cf75", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 14941, "upload_time": "2020-04-27T15:18:58", "upload_time_iso_8601": "2020-04-27T15:18:58.906835Z", "url": "https://files.pythonhosted.org/packages/1a/bf/bd4702a571bd8b14c060a505f702fe7ba327a3aa32ea11e39d352ce2b656/scraparser-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2565211472606c423e0cb7f7397b5d7f", "sha256": "c4745bed54c13c8d2f06d488eacefa89c20fa260282b89fc1af1bc431d9a3dcc"}, "downloads": -1, "filename": "scraparser-0.3.0.tar.gz", "has_sig": false, "md5_digest": "2565211472606c423e0cb7f7397b5d7f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 15384, "upload_time": "2020-04-27T15:18:59", "upload_time_iso_8601": "2020-04-27T15:18:59.802391Z", "url": "https://files.pythonhosted.org/packages/b4/4c/6fae3b73f2946461744aaa6d6dad040076905c7623ddedbaeff0a544bad4/scraparser-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "fad1a89c6d3188a93f844a9cfc1f362b", "sha256": "ce7422c0febd8825aca4f7fa1bd4cf2bd9a146c0371482fa8be2df6c179a9f3d"}, "downloads": -1, "filename": "scraparser-0.3.1-py3-none-any.whl", "has_sig": false, "md5_digest": "fad1a89c6d3188a93f844a9cfc1f362b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 15137, "upload_time": "2020-05-01T14:30:27", "upload_time_iso_8601": "2020-05-01T14:30:27.332258Z", "url": "https://files.pythonhosted.org/packages/c1/c3/de9fd1aa0250172bc6545942c52195596d01f0d59695a6a972efeb42cc9d/scraparser-0.3.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d320e5aeb83a4f316f6289349dd4eee1", "sha256": "70de239adb8740316516799a26fffcae40396c261ff0a0311196c9f1da18b494"}, "downloads": -1, "filename": "scraparser-0.3.1.tar.gz", "has_sig": false, "md5_digest": "d320e5aeb83a4f316f6289349dd4eee1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 15777, "upload_time": "2020-05-01T14:30:28", "upload_time_iso_8601": "2020-05-01T14:30:28.469625Z", "url": "https://files.pythonhosted.org/packages/b7/e0/1a8961d17917c205d57b638c8b870eee77c0e3ec5cee97cea2c2b653d57b/scraparser-0.3.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fad1a89c6d3188a93f844a9cfc1f362b", "sha256": "ce7422c0febd8825aca4f7fa1bd4cf2bd9a146c0371482fa8be2df6c179a9f3d"}, "downloads": -1, "filename": "scraparser-0.3.1-py3-none-any.whl", "has_sig": false, "md5_digest": "fad1a89c6d3188a93f844a9cfc1f362b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 15137, "upload_time": "2020-05-01T14:30:27", "upload_time_iso_8601": "2020-05-01T14:30:27.332258Z", "url": "https://files.pythonhosted.org/packages/c1/c3/de9fd1aa0250172bc6545942c52195596d01f0d59695a6a972efeb42cc9d/scraparser-0.3.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d320e5aeb83a4f316f6289349dd4eee1", "sha256": "70de239adb8740316516799a26fffcae40396c261ff0a0311196c9f1da18b494"}, "downloads": -1, "filename": "scraparser-0.3.1.tar.gz", "has_sig": false, "md5_digest": "d320e5aeb83a4f316f6289349dd4eee1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 15777, "upload_time": "2020-05-01T14:30:28", "upload_time_iso_8601": "2020-05-01T14:30:28.469625Z", "url": "https://files.pythonhosted.org/packages/b7/e0/1a8961d17917c205d57b638c8b870eee77c0e3ec5cee97cea2c2b653d57b/scraparser-0.3.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:56 2020"}