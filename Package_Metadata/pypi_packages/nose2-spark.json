{"info": {"author": "Alex (Oleksii) Markov", "author_email": "alex@markovs.me", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.2", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Software Development :: Testing"], "description": "nose2-spark\n===========\n\n`nose2`_ plugin to run the tests with support of pyspark (`Apache Spark`_).\n\nFeatures:\n\n1. Make \"pyspark\" importable in you code executed by nose2.\n2. Add a list of `py-files`_ dependencies of your pyspark application (which\n   is usually supplied as an option ``spark-submit --py-files ...``).\n\n\nInstall\n-------\n\n.. code-block:: shell\n\n    $ pip install nose2-spark\n\nUsage\n-----\n\nLoad \"nose2-spark\" plugin into nose2 by creating ``nose2.cfg`` in your project\ndirectory::\n\n    [unittest]\n    plugins = nose2_spark\n\nRun tests with nose2-spark activated (pyspark and friends are added to\npythonpath)::\n\n    $ nose2 --pyspark\n\nnose2-spark will try to import pyspark by looking into:\n\n1. SPARK_HOME environment variable\n2. Some common Spark locations.\n\nYou can set it manually in case if all of mentioned methods are failing\nto find Spark. Add section \"nose2-spark\" to ``nose2.cfg``::\n\n    [nose2-spark]\n    spark_home = /opt/spark\n\nYou can add a list of required `py-files`_ to run your code::\n\n    [nose2-spark]\n    pyfiles = package1.zip\n              package2.zip\n\n\nExample\n-------\n\nExample of ``nose2.cfg`` with spark_home defined, one `py-files`_ dependency and\nauto activating nose2-spark plugin::\n\n    [unittest]\n    plugins = nose2_spark\n\n    [nose2-spark]\n    always-on = True\n    spark_home = /opt/spark\n    pyfiles = package1.zip\n\nThis will allow to run tests by single command::\n\n    $ nose2\n\n\n.. _nose2: http://nose2.readthedocs.io/\n.. _Apache Spark: https://spark.apache.org/\n.. _py-files: http://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/malexer/nose2-spark", "keywords": "nose2 spark pyspark unittest test", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "nose2-spark", "package_url": "https://pypi.org/project/nose2-spark/", "platform": "", "project_url": "https://pypi.org/project/nose2-spark/", "project_urls": {"Homepage": "https://github.com/malexer/nose2-spark"}, "release_url": "https://pypi.org/project/nose2-spark/0.3/", "requires_dist": ["findspark", "nose2"], "requires_python": "", "summary": "nose2 plugin to run the tests with support of pyspark.", "version": "0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"http://nose2.readthedocs.io/\" rel=\"nofollow\">nose2</a> plugin to run the tests with support of pyspark (<a href=\"https://spark.apache.org/\" rel=\"nofollow\">Apache Spark</a>).</p>\n<p>Features:</p>\n<ol>\n<li>Make \u201cpyspark\u201d importable in you code executed by nose2.</li>\n<li>Add a list of <a href=\"http://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies\" rel=\"nofollow\">py-files</a> dependencies of your pyspark application (which\nis usually supplied as an option <tt><span class=\"pre\">spark-submit</span> <span class=\"pre\">--py-files</span> ...</tt>).</li>\n</ol>\n<div id=\"install\">\n<h2>Install</h2>\n<pre>$ pip install nose2-spark\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>Load \u201cnose2-spark\u201d plugin into nose2 by creating <tt>nose2.cfg</tt> in your project\ndirectory:</p>\n<pre>[unittest]\nplugins = nose2_spark\n</pre>\n<p>Run tests with nose2-spark activated (pyspark and friends are added to\npythonpath):</p>\n<pre>$ nose2 --pyspark\n</pre>\n<p>nose2-spark will try to import pyspark by looking into:</p>\n<ol>\n<li>SPARK_HOME environment variable</li>\n<li>Some common Spark locations.</li>\n</ol>\n<p>You can set it manually in case if all of mentioned methods are failing\nto find Spark. Add section \u201cnose2-spark\u201d to <tt>nose2.cfg</tt>:</p>\n<pre>[nose2-spark]\nspark_home = /opt/spark\n</pre>\n<p>You can add a list of required <a href=\"http://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies\" rel=\"nofollow\">py-files</a> to run your code:</p>\n<pre>[nose2-spark]\npyfiles = package1.zip\n          package2.zip\n</pre>\n</div>\n<div id=\"example\">\n<h2>Example</h2>\n<p>Example of <tt>nose2.cfg</tt> with spark_home defined, one <a href=\"http://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies\" rel=\"nofollow\">py-files</a> dependency and\nauto activating nose2-spark plugin:</p>\n<pre>[unittest]\nplugins = nose2_spark\n\n[nose2-spark]\nalways-on = True\nspark_home = /opt/spark\npyfiles = package1.zip\n</pre>\n<p>This will allow to run tests by single command:</p>\n<pre>$ nose2\n</pre>\n</div>\n\n          </div>"}, "last_serial": 2462909, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "a22f73d5b9a9a6f7e295bd8314d5a94f", "sha256": "0569db97a38249343314680102603cc0ae88d37110dc9e82c2524201f368583a"}, "downloads": -1, "filename": "nose2_spark-0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "a22f73d5b9a9a6f7e295bd8314d5a94f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4174, "upload_time": "2016-11-14T18:37:23", "upload_time_iso_8601": "2016-11-14T18:37:23.875351Z", "url": "https://files.pythonhosted.org/packages/c3/16/d6b2edebaa563718ebf23760e2f32f78f1173d880a2dc674216725f2c27e/nose2_spark-0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "08c8d3fb1dd6545079ffe2d56f7a75a4", "sha256": "3cb5a7f508ecdf667479c20b765ee161fa375b02fc9621bd8eac5a219980af16"}, "downloads": -1, "filename": "nose2-spark-0.1.tar.gz", "has_sig": false, "md5_digest": "08c8d3fb1dd6545079ffe2d56f7a75a4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2509, "upload_time": "2016-11-14T18:37:26", "upload_time_iso_8601": "2016-11-14T18:37:26.241280Z", "url": "https://files.pythonhosted.org/packages/f6/6b/1e2d56ab72d25c08c74753f8032c9d24e2017047f2e1e8327a46013eca3a/nose2-spark-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "f75741d3788c3085d158dd3a7a204e58", "sha256": "9661848c9d4e3137bc527b07fc7ea1b5ca5765ddca78f18fc4a4b0c313a7c9ed"}, "downloads": -1, "filename": "nose2_spark-0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "f75741d3788c3085d158dd3a7a204e58", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4294, "upload_time": "2016-11-15T09:35:52", "upload_time_iso_8601": "2016-11-15T09:35:52.167491Z", "url": "https://files.pythonhosted.org/packages/10/c3/b2ff15307859002fb4a970b645fd7e68986b70727119e935c159543aaa4b/nose2_spark-0.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7226487ed70e31674568b9dad35cd0ff", "sha256": "53fb6a6aee5e09c7bd42a15f1742764642f3b4174033d84d0230f58aa5111844"}, "downloads": -1, "filename": "nose2-spark-0.2.tar.gz", "has_sig": false, "md5_digest": "7226487ed70e31674568b9dad35cd0ff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2578, "upload_time": "2016-11-15T09:35:54", "upload_time_iso_8601": "2016-11-15T09:35:54.337508Z", "url": "https://files.pythonhosted.org/packages/d1/64/2c9e0a6d86d2f3fcbf598e3915199279874fc975d9f57fc138d0643bfb59/nose2-spark-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "d68e0ae73d07f6318403b84d01381b0a", "sha256": "17172396291eb7fa27235cfc2c6d7b7c6dbe5a11bda3987faf446bf0854c1620"}, "downloads": -1, "filename": "nose2_spark-0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "d68e0ae73d07f6318403b84d01381b0a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4405, "upload_time": "2016-11-15T22:58:38", "upload_time_iso_8601": "2016-11-15T22:58:38.000926Z", "url": "https://files.pythonhosted.org/packages/77/c0/03ffaa0bd3cf7c1f93d5a952794a7723eb2068c3cea1b8d38d0be74e2113/nose2_spark-0.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "eb9a97addb704c6b56ffda75cbf635c6", "sha256": "443d341664c623de629e4f9eb444404740f71ab16a2f742b3b54429e048e6bb7"}, "downloads": -1, "filename": "nose2-spark-0.3.tar.gz", "has_sig": false, "md5_digest": "eb9a97addb704c6b56ffda75cbf635c6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2684, "upload_time": "2016-11-15T22:58:39", "upload_time_iso_8601": "2016-11-15T22:58:39.896260Z", "url": "https://files.pythonhosted.org/packages/b1/db/47d691433d4f0293904fdb1edb5fd759f89ef0018d0426f01cc7274e30cc/nose2-spark-0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d68e0ae73d07f6318403b84d01381b0a", "sha256": "17172396291eb7fa27235cfc2c6d7b7c6dbe5a11bda3987faf446bf0854c1620"}, "downloads": -1, "filename": "nose2_spark-0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "d68e0ae73d07f6318403b84d01381b0a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4405, "upload_time": "2016-11-15T22:58:38", "upload_time_iso_8601": "2016-11-15T22:58:38.000926Z", "url": "https://files.pythonhosted.org/packages/77/c0/03ffaa0bd3cf7c1f93d5a952794a7723eb2068c3cea1b8d38d0be74e2113/nose2_spark-0.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "eb9a97addb704c6b56ffda75cbf635c6", "sha256": "443d341664c623de629e4f9eb444404740f71ab16a2f742b3b54429e048e6bb7"}, "downloads": -1, "filename": "nose2-spark-0.3.tar.gz", "has_sig": false, "md5_digest": "eb9a97addb704c6b56ffda75cbf635c6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2684, "upload_time": "2016-11-15T22:58:39", "upload_time_iso_8601": "2016-11-15T22:58:39.896260Z", "url": "https://files.pythonhosted.org/packages/b1/db/47d691433d4f0293904fdb1edb5fd759f89ef0018d0426f01cc7274e30cc/nose2-spark-0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:44:48 2020"}