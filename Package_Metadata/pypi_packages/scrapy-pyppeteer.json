{"info": {"author": "Eugenio Lacuesta", "author_email": "eugenio.lacuesta@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Programming Language :: Python", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Internet :: WWW/HTTP", "Topic :: Software Development :: Libraries :: Application Frameworks", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Pyppeteer integration for Scrapy\n[![version](https://img.shields.io/pypi/v/scrapy-pyppeteer.svg)](https://pypi.python.org/pypi/scrapy-pyppeteer)\n[![pyversions](https://img.shields.io/pypi/pyversions/scrapy-pyppeteer.svg)](https://pypi.python.org/pypi/scrapy-pyppeteer)\n[![actions](https://github.com/elacuesta/scrapy-pyppeteer/workflows/Build/badge.svg)](https://github.com/elacuesta/scrapy-pyppeteer/actions)\n[![codecov](https://codecov.io/gh/elacuesta/scrapy-pyppeteer/branch/master/graph/badge.svg)](https://codecov.io/gh/elacuesta/scrapy-pyppeteer)\n\n\nThis project provides a Scrapy Download Handler which performs requests using\n[Pyppeteer](https://github.com/miyakogi/pyppeteer). It can be used to handle\npages that require JavaScript. This package does not interfere with regular\nScrapy workflows such as request scheduling or item processing.\n\n\n## Motivation\n\nAfter the release of [version 2.0](https://docs.scrapy.org/en/latest/news.html#scrapy-2-0-0-2020-03-03),\nwhich includes partial [coroutine syntax support](https://docs.scrapy.org/en/2.0/topics/coroutines.html)\nand experimental [asyncio support](https://docs.scrapy.org/en/2.0/topics/asyncio.html), Scrapy allows\nto integrate `asyncio`-based projects such as `Pyppeteer`.\n\n\n## Requirements\n\n* Python 3.6+\n* Scrapy 2.0+\n* Pyppeteer 0.0.23+\n\n\n## Installation\n\n```\n$ pip install scrapy-pyppeteer\n```\n\n## Configuration\n\nReplace the default `http` and `https` Download Handlers through\n[`DOWNLOAD_HANDLERS`](https://docs.scrapy.org/en/latest/topics/settings.html):\n\n```python\nDOWNLOAD_HANDLERS = {\n    \"http\": \"scrapy_pyppeteer.handler.ScrapyPyppeteerDownloadHandler\",\n    \"https\": \"scrapy_pyppeteer.handler.ScrapyPyppeteerDownloadHandler\",\n}\n```\n\nNote that the `ScrapyPyppeteerDownloadHandler` class inherits from the default\n`http/https` handler, and it will only use Pyppeteer for requests that are\nexplicitly marked (see the \"Basic usage\" section for details).\n\nAlso, be sure to [install the `asyncio`-based Twisted reactor](https://docs.scrapy.org/en/latest/topics/asyncio.html#installing-the-asyncio-reactor):\n\n```python\nTWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n```\n\n`scrapy-pyppeteer` accepts the following settings:\n\n* `PYPPETEER_LAUNCH_OPTIONS` (type `dict`, default `{}`)\n\n    A dictionary with options to be passed when launching the Browser.\n    See the docs for [pyppeteer.launcher.launch](https://miyakogi.github.io/pyppeteer/reference.html#pyppeteer.launcher.launch)\n\n* `PYPPETEER_NAVIGATION_TIMEOUT` (type `Optional[int]`, default `None`)\n\n    The timeout used when requesting pages by Pyppeteer. If `None` or unset,\n    the default value will be used (30000 ms at the time of writing this).\n    See the docs for [pyppeteer.page.Page.setDefaultNavigationTimeout](https://miyakogi.github.io/pyppeteer/reference.html#pyppeteer.page.Page.setDefaultNavigationTimeout)\n\n\n## Basic usage\n\nSet the `pyppeteer` [Request.meta](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta)\nkey to download a request using Pyppeteer:\n\n```python\nimport scrapy\n\nclass AwesomeSpider(scrapy.Spider):\n    name = \"awesome\"\n\n    def start_requests(self):\n        # GET request\n        yield scrapy.Request(\"https://httpbin.org/get\", meta={\"pyppeteer\": True})\n        # POST request\n        yield scrapy.FormRequest(\n            url=\"https://httpbin.org/post\",\n            formdata={\"foo\": \"bar\"},\n            meta={\"pyppeteer\": True},\n        )\n\n    def parse(self, response):\n        # 'response' contains the page as seen by the browser\n        yield {\"url\": response.url}\n```\n\n\n## Page coroutines\n\nA sorted iterable (`list`, `tuple` or `dict`, for instance) could be passed\nin the `pyppeteer_page_coroutines`\n[Request.meta](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta)\nkey to request coroutines to be awaited on the `Page` before returning the final\n`Response` to the callback.\n\nThis is useful when you need to perform certain actions on a page, like scrolling\ndown or clicking links, and you want everything to count as a single Scrapy\nResponse, containing the final result.\n\n### Supported actions\n\n* `scrapy_pyppeteer.page.PageCoroutine(method: str, *args, **kwargs)`:\n\n    _Represents a coroutine to be awaited on a `pyppeteer.page.Page` object,\n    such as \"click\", \"screenshot\", \"evaluate\", etc.\n    `method` should be the name of the coroutine, `*args` and `**kwargs`\n    are passed to the function call._\n\n    _The coroutine result will be stored in the `PageCoroutine.result` attribute_\n\n    For instance,\n    ```python\n    PageCoroutine(\"screenshot\", options={\"path\": \"quotes.png\", \"fullPage\": True})\n    ```\n\n    produces the same effect as:\n    ```python\n    # 'page' is a pyppeteer.page.Page object\n    await page.screenshot(options={\"path\": \"quotes.png\", \"fullPage\": True})\n    ```\n\n* `scrapy_pyppeteer.page.NavigationPageCoroutine(method: str, *args, **kwargs)`:\n\n    _Subclass of `PageCoroutine`. It waits for a navigation event: use this when you know\n    a coroutine will trigger a navigation event, for instance when clicking on a link.\n    This forces a `Page.waitForNavigation()` call wrapped in `asyncio.gather`, as recommended in\n    [the Pyppeteer docs](https://miyakogi.github.io/pyppeteer/reference.html#pyppeteer.page.Page.click)._\n\n    For instance,\n    ```python\n    NavigationPageCoroutine(\"click\", selector=\"a\")\n    ```\n\n    produces the same effect as:\n    ```python\n    # 'page' is a pyppeteer.page.Page object\n    await asyncio.gather(\n        page.waitForNavigation(),\n        page.click(selector=\"a\"),\n    )\n    ```\n\n\n### Receiving the Page object in the callback\n\nSpecifying `pyppeteer.page.Page` as the type for a callback argument will result\nin the corresponding `Page` object being injected in the callback. In order to\nable to `await` coroutines on the provided `Page` object, the callback needs to\nbe defined as a coroutine function (`async def`).\n\n```python\nimport scrapy\nimport pyppeteer\n\nclass AwesomeSpiderWithPage(scrapy.Spider):\n    name = \"page\"\n\n    def start_requests(self):\n        yield scrapy.Request(\"https://example.org\", meta={\"pyppeteer\": True})\n\n    async def parse(self, response, page: pyppeteer.page.Page):\n        title = await page.title()  # \"Example Domain\"\n        yield {\"title\": title}\n        await page.close()\n```\n\n**Notes:**\n\n* In order to avoid memory issues, it is recommended to manually close the page\n  by awaiting the `Page.close` coroutine.\n* Any network operations resulting from awaiting a coroutine on a `Page` object\n  (`goto`, `goBack`, etc) will be executed directly by Pyppeteer, bypassing the\n  Scrapy request workflow (Scheduler, Middlewares, etc).\n\n\n## Examples\n\n**Click on a link, save the resulting page as PDF**\n\n```python\nclass ClickAndSavePdfSpider(scrapy.Spider):\n    name = \"pdf\"\n\n    def start_requests(self):\n        yield scrapy.Request(\n            url=\"https://example.org\",\n            meta=dict(\n                pyppeteer=True,\n                pyppeteer_page_coroutines={\n                    \"click\": NavigationPageCoroutine(\"click\", selector=\"a\"),\n                    \"pdf\": PageCoroutine(\"pdf\", options={\"path\": \"/tmp/file.pdf\"}),\n                },\n            ),\n        )\n\n    def parse(self, response):\n        pdf_bytes = response.meta[\"pyppeteer_page_coroutines\"][\"pdf\"].result\n        with open(\"iana.pdf\", \"wb\") as fp:\n            fp.write(pdf_bytes)\n        yield {\"url\": response.url}  # response.url is \"https://www.iana.org/domains/reserved\"\n```\n\n**Scroll down on an infinite scroll page, take a screenshot of the full page**\n\n```python\nclass ScrollSpider(scrapy.Spider):\n    name = \"scroll\"\n\n    def start_requests(self):\n        yield scrapy.Request(\n            url=\"http://quotes.toscrape.com/scroll\",\n            meta=dict(\n                pyppeteer=True,\n                pyppeteer_page_coroutines=[\n                    PageCoroutine(\"waitForSelector\", \"div.quote\"),\n                    PageCoroutine(\"evaluate\", \"window.scrollBy(0, 2000)\"),\n                    PageCoroutine(\"waitForSelector\", \"div.quote:nth-child(11)\"),  # 10 per page\n                ],\n            ),\n        )\n\n    async def parse(self, response, page: pyppeteer.page.Page):\n        await page.screenshot(options={\"path\": \"quotes.png\", \"fullPage\": True})\n        yield {\"quote_count\": len(response.css(\"div.quote\"))}  # 100 quotes\n```\n\n\n## Acknowledgements\n\nThis project was inspired by:\n\n* https://github.com/scrapy/scrapy/pull/1455\n* https://github.com/michalmo/scrapy-browser\n* https://github.com/lopuhin/scrapy-pyppeteer\n* https://github.com/clemfromspace/scrapy-puppeteer\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/elacuesta/scrapy-pyppeteer", "keywords": "", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "scrapy-pyppeteer", "package_url": "https://pypi.org/project/scrapy-pyppeteer/", "platform": "", "project_url": "https://pypi.org/project/scrapy-pyppeteer/", "project_urls": {"Homepage": "https://github.com/elacuesta/scrapy-pyppeteer"}, "release_url": "https://pypi.org/project/scrapy-pyppeteer/0.0.9/", "requires_dist": ["scrapy (>=2.0)", "pyppeteer (>=0.0.23)"], "requires_python": "", "summary": "Pyppeteer integration for Scrapy", "version": "0.0.9", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Pyppeteer integration for Scrapy</h1>\n<p><a href=\"https://pypi.python.org/pypi/scrapy-pyppeteer\" rel=\"nofollow\"><img alt=\"version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b63ce7dd6589a150f32bf17d2961b9020cb0667c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363726170792d7079707065746565722e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/scrapy-pyppeteer\" rel=\"nofollow\"><img alt=\"pyversions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/34aa05417dc4c4399c58f3b306dd63613641f455/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363726170792d7079707065746565722e737667\"></a>\n<a href=\"https://github.com/elacuesta/scrapy-pyppeteer/actions\" rel=\"nofollow\"><img alt=\"actions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ebb672cd25718df912b450b172659c1336819acd/68747470733a2f2f6769746875622e636f6d2f656c616375657374612f7363726170792d7079707065746565722f776f726b666c6f77732f4275696c642f62616467652e737667\"></a>\n<a href=\"https://codecov.io/gh/elacuesta/scrapy-pyppeteer\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c4be1a263d536cd87308cae53667a44dc36bdad3/68747470733a2f2f636f6465636f762e696f2f67682f656c616375657374612f7363726170792d7079707065746565722f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a></p>\n<p>This project provides a Scrapy Download Handler which performs requests using\n<a href=\"https://github.com/miyakogi/pyppeteer\" rel=\"nofollow\">Pyppeteer</a>. It can be used to handle\npages that require JavaScript. This package does not interfere with regular\nScrapy workflows such as request scheduling or item processing.</p>\n<h2>Motivation</h2>\n<p>After the release of <a href=\"https://docs.scrapy.org/en/latest/news.html#scrapy-2-0-0-2020-03-03\" rel=\"nofollow\">version 2.0</a>,\nwhich includes partial <a href=\"https://docs.scrapy.org/en/2.0/topics/coroutines.html\" rel=\"nofollow\">coroutine syntax support</a>\nand experimental <a href=\"https://docs.scrapy.org/en/2.0/topics/asyncio.html\" rel=\"nofollow\">asyncio support</a>, Scrapy allows\nto integrate <code>asyncio</code>-based projects such as <code>Pyppeteer</code>.</p>\n<h2>Requirements</h2>\n<ul>\n<li>Python 3.6+</li>\n<li>Scrapy 2.0+</li>\n<li>Pyppeteer 0.0.23+</li>\n</ul>\n<h2>Installation</h2>\n<pre><code>$ pip install scrapy-pyppeteer\n</code></pre>\n<h2>Configuration</h2>\n<p>Replace the default <code>http</code> and <code>https</code> Download Handlers through\n<a href=\"https://docs.scrapy.org/en/latest/topics/settings.html\" rel=\"nofollow\"><code>DOWNLOAD_HANDLERS</code></a>:</p>\n<pre><span class=\"n\">DOWNLOAD_HANDLERS</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"http\"</span><span class=\"p\">:</span> <span class=\"s2\">\"scrapy_pyppeteer.handler.ScrapyPyppeteerDownloadHandler\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"https\"</span><span class=\"p\">:</span> <span class=\"s2\">\"scrapy_pyppeteer.handler.ScrapyPyppeteerDownloadHandler\"</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n</pre>\n<p>Note that the <code>ScrapyPyppeteerDownloadHandler</code> class inherits from the default\n<code>http/https</code> handler, and it will only use Pyppeteer for requests that are\nexplicitly marked (see the \"Basic usage\" section for details).</p>\n<p>Also, be sure to <a href=\"https://docs.scrapy.org/en/latest/topics/asyncio.html#installing-the-asyncio-reactor\" rel=\"nofollow\">install the <code>asyncio</code>-based Twisted reactor</a>:</p>\n<pre><span class=\"n\">TWISTED_REACTOR</span> <span class=\"o\">=</span> <span class=\"s2\">\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"</span>\n</pre>\n<p><code>scrapy-pyppeteer</code> accepts the following settings:</p>\n<ul>\n<li>\n<p><code>PYPPETEER_LAUNCH_OPTIONS</code> (type <code>dict</code>, default <code>{}</code>)</p>\n<p>A dictionary with options to be passed when launching the Browser.\nSee the docs for <a href=\"https://miyakogi.github.io/pyppeteer/reference.html#pyppeteer.launcher.launch\" rel=\"nofollow\">pyppeteer.launcher.launch</a></p>\n</li>\n<li>\n<p><code>PYPPETEER_NAVIGATION_TIMEOUT</code> (type <code>Optional[int]</code>, default <code>None</code>)</p>\n<p>The timeout used when requesting pages by Pyppeteer. If <code>None</code> or unset,\nthe default value will be used (30000 ms at the time of writing this).\nSee the docs for <a href=\"https://miyakogi.github.io/pyppeteer/reference.html#pyppeteer.page.Page.setDefaultNavigationTimeout\" rel=\"nofollow\">pyppeteer.page.Page.setDefaultNavigationTimeout</a></p>\n</li>\n</ul>\n<h2>Basic usage</h2>\n<p>Set the <code>pyppeteer</code> <a href=\"https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta\" rel=\"nofollow\">Request.meta</a>\nkey to download a request using Pyppeteer:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">scrapy</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">AwesomeSpider</span><span class=\"p\">(</span><span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Spider</span><span class=\"p\">):</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">\"awesome\"</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">start_requests</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"c1\"># GET request</span>\n        <span class=\"k\">yield</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Request</span><span class=\"p\">(</span><span class=\"s2\">\"https://httpbin.org/get\"</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"pyppeteer\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n        <span class=\"c1\"># POST request</span>\n        <span class=\"k\">yield</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">FormRequest</span><span class=\"p\">(</span>\n            <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"s2\">\"https://httpbin.org/post\"</span><span class=\"p\">,</span>\n            <span class=\"n\">formdata</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"foo\"</span><span class=\"p\">:</span> <span class=\"s2\">\"bar\"</span><span class=\"p\">},</span>\n            <span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"pyppeteer\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">},</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">):</span>\n        <span class=\"c1\"># 'response' contains the page as seen by the browser</span>\n        <span class=\"k\">yield</span> <span class=\"p\">{</span><span class=\"s2\">\"url\"</span><span class=\"p\">:</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">url</span><span class=\"p\">}</span>\n</pre>\n<h2>Page coroutines</h2>\n<p>A sorted iterable (<code>list</code>, <code>tuple</code> or <code>dict</code>, for instance) could be passed\nin the <code>pyppeteer_page_coroutines</code>\n<a href=\"https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta\" rel=\"nofollow\">Request.meta</a>\nkey to request coroutines to be awaited on the <code>Page</code> before returning the final\n<code>Response</code> to the callback.</p>\n<p>This is useful when you need to perform certain actions on a page, like scrolling\ndown or clicking links, and you want everything to count as a single Scrapy\nResponse, containing the final result.</p>\n<h3>Supported actions</h3>\n<ul>\n<li>\n<p><code>scrapy_pyppeteer.page.PageCoroutine(method: str, *args, **kwargs)</code>:</p>\n<p><em>Represents a coroutine to be awaited on a <code>pyppeteer.page.Page</code> object,\nsuch as \"click\", \"screenshot\", \"evaluate\", etc.\n<code>method</code> should be the name of the coroutine, <code>*args</code> and <code>**kwargs</code>\nare passed to the function call.</em></p>\n<p><em>The coroutine result will be stored in the <code>PageCoroutine.result</code> attribute</em></p>\n<p>For instance,</p>\n<pre><span class=\"n\">PageCoroutine</span><span class=\"p\">(</span><span class=\"s2\">\"screenshot\"</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"path\"</span><span class=\"p\">:</span> <span class=\"s2\">\"quotes.png\"</span><span class=\"p\">,</span> <span class=\"s2\">\"fullPage\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n</pre>\n<p>produces the same effect as:</p>\n<pre><span class=\"c1\"># 'page' is a pyppeteer.page.Page object</span>\n<span class=\"k\">await</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">screenshot</span><span class=\"p\">(</span><span class=\"n\">options</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"path\"</span><span class=\"p\">:</span> <span class=\"s2\">\"quotes.png\"</span><span class=\"p\">,</span> <span class=\"s2\">\"fullPage\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n</pre>\n</li>\n<li>\n<p><code>scrapy_pyppeteer.page.NavigationPageCoroutine(method: str, *args, **kwargs)</code>:</p>\n<p><em>Subclass of <code>PageCoroutine</code>. It waits for a navigation event: use this when you know\na coroutine will trigger a navigation event, for instance when clicking on a link.\nThis forces a <code>Page.waitForNavigation()</code> call wrapped in <code>asyncio.gather</code>, as recommended in\n<a href=\"https://miyakogi.github.io/pyppeteer/reference.html#pyppeteer.page.Page.click\" rel=\"nofollow\">the Pyppeteer docs</a>.</em></p>\n<p>For instance,</p>\n<pre><span class=\"n\">NavigationPageCoroutine</span><span class=\"p\">(</span><span class=\"s2\">\"click\"</span><span class=\"p\">,</span> <span class=\"n\">selector</span><span class=\"o\">=</span><span class=\"s2\">\"a\"</span><span class=\"p\">)</span>\n</pre>\n<p>produces the same effect as:</p>\n<pre><span class=\"c1\"># 'page' is a pyppeteer.page.Page object</span>\n<span class=\"k\">await</span> <span class=\"n\">asyncio</span><span class=\"o\">.</span><span class=\"n\">gather</span><span class=\"p\">(</span>\n    <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">waitForNavigation</span><span class=\"p\">(),</span>\n    <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">click</span><span class=\"p\">(</span><span class=\"n\">selector</span><span class=\"o\">=</span><span class=\"s2\">\"a\"</span><span class=\"p\">),</span>\n<span class=\"p\">)</span>\n</pre>\n</li>\n</ul>\n<h3>Receiving the Page object in the callback</h3>\n<p>Specifying <code>pyppeteer.page.Page</code> as the type for a callback argument will result\nin the corresponding <code>Page</code> object being injected in the callback. In order to\nable to <code>await</code> coroutines on the provided <code>Page</code> object, the callback needs to\nbe defined as a coroutine function (<code>async def</code>).</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">scrapy</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pyppeteer</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">AwesomeSpiderWithPage</span><span class=\"p\">(</span><span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Spider</span><span class=\"p\">):</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">\"page\"</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">start_requests</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Request</span><span class=\"p\">(</span><span class=\"s2\">\"https://example.org\"</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"pyppeteer\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n\n    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">,</span> <span class=\"n\">page</span><span class=\"p\">:</span> <span class=\"n\">pyppeteer</span><span class=\"o\">.</span><span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">Page</span><span class=\"p\">):</span>\n        <span class=\"n\">title</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">()</span>  <span class=\"c1\"># \"Example Domain\"</span>\n        <span class=\"k\">yield</span> <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"n\">title</span><span class=\"p\">}</span>\n        <span class=\"k\">await</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Notes:</strong></p>\n<ul>\n<li>In order to avoid memory issues, it is recommended to manually close the page\nby awaiting the <code>Page.close</code> coroutine.</li>\n<li>Any network operations resulting from awaiting a coroutine on a <code>Page</code> object\n(<code>goto</code>, <code>goBack</code>, etc) will be executed directly by Pyppeteer, bypassing the\nScrapy request workflow (Scheduler, Middlewares, etc).</li>\n</ul>\n<h2>Examples</h2>\n<p><strong>Click on a link, save the resulting page as PDF</strong></p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">ClickAndSavePdfSpider</span><span class=\"p\">(</span><span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Spider</span><span class=\"p\">):</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">\"pdf\"</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">start_requests</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Request</span><span class=\"p\">(</span>\n            <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"s2\">\"https://example.org\"</span><span class=\"p\">,</span>\n            <span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n                <span class=\"n\">pyppeteer</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"n\">pyppeteer_page_coroutines</span><span class=\"o\">=</span><span class=\"p\">{</span>\n                    <span class=\"s2\">\"click\"</span><span class=\"p\">:</span> <span class=\"n\">NavigationPageCoroutine</span><span class=\"p\">(</span><span class=\"s2\">\"click\"</span><span class=\"p\">,</span> <span class=\"n\">selector</span><span class=\"o\">=</span><span class=\"s2\">\"a\"</span><span class=\"p\">),</span>\n                    <span class=\"s2\">\"pdf\"</span><span class=\"p\">:</span> <span class=\"n\">PageCoroutine</span><span class=\"p\">(</span><span class=\"s2\">\"pdf\"</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"path\"</span><span class=\"p\">:</span> <span class=\"s2\">\"/tmp/file.pdf\"</span><span class=\"p\">}),</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">),</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">):</span>\n        <span class=\"n\">pdf_bytes</span> <span class=\"o\">=</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s2\">\"pyppeteer_page_coroutines\"</span><span class=\"p\">][</span><span class=\"s2\">\"pdf\"</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">result</span>\n        <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"iana.pdf\"</span><span class=\"p\">,</span> <span class=\"s2\">\"wb\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fp</span><span class=\"p\">:</span>\n            <span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">pdf_bytes</span><span class=\"p\">)</span>\n        <span class=\"k\">yield</span> <span class=\"p\">{</span><span class=\"s2\">\"url\"</span><span class=\"p\">:</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">url</span><span class=\"p\">}</span>  <span class=\"c1\"># response.url is \"https://www.iana.org/domains/reserved\"</span>\n</pre>\n<p><strong>Scroll down on an infinite scroll page, take a screenshot of the full page</strong></p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">ScrollSpider</span><span class=\"p\">(</span><span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Spider</span><span class=\"p\">):</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">\"scroll\"</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">start_requests</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Request</span><span class=\"p\">(</span>\n            <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"s2\">\"http://quotes.toscrape.com/scroll\"</span><span class=\"p\">,</span>\n            <span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n                <span class=\"n\">pyppeteer</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n                <span class=\"n\">pyppeteer_page_coroutines</span><span class=\"o\">=</span><span class=\"p\">[</span>\n                    <span class=\"n\">PageCoroutine</span><span class=\"p\">(</span><span class=\"s2\">\"waitForSelector\"</span><span class=\"p\">,</span> <span class=\"s2\">\"div.quote\"</span><span class=\"p\">),</span>\n                    <span class=\"n\">PageCoroutine</span><span class=\"p\">(</span><span class=\"s2\">\"evaluate\"</span><span class=\"p\">,</span> <span class=\"s2\">\"window.scrollBy(0, 2000)\"</span><span class=\"p\">),</span>\n                    <span class=\"n\">PageCoroutine</span><span class=\"p\">(</span><span class=\"s2\">\"waitForSelector\"</span><span class=\"p\">,</span> <span class=\"s2\">\"div.quote:nth-child(11)\"</span><span class=\"p\">),</span>  <span class=\"c1\"># 10 per page</span>\n                <span class=\"p\">],</span>\n            <span class=\"p\">),</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">,</span> <span class=\"n\">page</span><span class=\"p\">:</span> <span class=\"n\">pyppeteer</span><span class=\"o\">.</span><span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">Page</span><span class=\"p\">):</span>\n        <span class=\"k\">await</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">screenshot</span><span class=\"p\">(</span><span class=\"n\">options</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"path\"</span><span class=\"p\">:</span> <span class=\"s2\">\"quotes.png\"</span><span class=\"p\">,</span> <span class=\"s2\">\"fullPage\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n        <span class=\"k\">yield</span> <span class=\"p\">{</span><span class=\"s2\">\"quote_count\"</span><span class=\"p\">:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">css</span><span class=\"p\">(</span><span class=\"s2\">\"div.quote\"</span><span class=\"p\">))}</span>  <span class=\"c1\"># 100 quotes</span>\n</pre>\n<h2>Acknowledgements</h2>\n<p>This project was inspired by:</p>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/pull/1455\" rel=\"nofollow\">https://github.com/scrapy/scrapy/pull/1455</a></li>\n<li><a href=\"https://github.com/michalmo/scrapy-browser\" rel=\"nofollow\">https://github.com/michalmo/scrapy-browser</a></li>\n<li><a href=\"https://github.com/lopuhin/scrapy-pyppeteer\" rel=\"nofollow\">https://github.com/lopuhin/scrapy-pyppeteer</a></li>\n<li><a href=\"https://github.com/clemfromspace/scrapy-puppeteer\" rel=\"nofollow\">https://github.com/clemfromspace/scrapy-puppeteer</a></li>\n</ul>\n\n          </div>"}, "last_serial": 7148034, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "fd682f87a0cdc446d97122b990ce1e97", "sha256": "d23b1abdbbb2d9fb298d3cd948b523063a6f0956c1815b738407a6ae78b495d2"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "fd682f87a0cdc446d97122b990ce1e97", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 4787, "upload_time": "2020-04-20T19:15:56", "upload_time_iso_8601": "2020-04-20T19:15:56.243608Z", "url": "https://files.pythonhosted.org/packages/d6/93/a8c10f71b12e9e7f01e8c38684db49a2e4a4efed75618ed59e1ae9c52282/scrapy_pyppeteer-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "387209b7c6433e007a61f4895f7a2096", "sha256": "87e245d53f585c2a944a88af838324b42fd03b7b7d74802b643e11ccead1bb20"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.1.tar.gz", "has_sig": false, "md5_digest": "387209b7c6433e007a61f4895f7a2096", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5279, "upload_time": "2020-04-20T19:15:58", "upload_time_iso_8601": "2020-04-20T19:15:58.038747Z", "url": "https://files.pythonhosted.org/packages/b3/ed/726d7a3d7aa2416703887108e95fb0b581b53f354ce99b1aa34c146295b7/scrapy-pyppeteer-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "f4fe6b5147acc2a20b0fd8dde6fa960b", "sha256": "ce971cfdfc95c146082925ebeffc3cb46c750bf9dad9ace0cd5bd240964e9660"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "f4fe6b5147acc2a20b0fd8dde6fa960b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7442, "upload_time": "2020-04-20T19:37:17", "upload_time_iso_8601": "2020-04-20T19:37:17.050919Z", "url": "https://files.pythonhosted.org/packages/20/c2/425f33e74ab5ace6ecf9773a8a7d1b9473f8933cb878cc3cad7d4cd95550/scrapy_pyppeteer-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b359e5188d43d9813a78c920ea3830ff", "sha256": "91df7734dd4ae510faa28169e4a51fb26fd6fb3901d1f91e9b54c33c92342b01"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.2.tar.gz", "has_sig": false, "md5_digest": "b359e5188d43d9813a78c920ea3830ff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6182, "upload_time": "2020-04-20T19:37:18", "upload_time_iso_8601": "2020-04-20T19:37:18.606821Z", "url": "https://files.pythonhosted.org/packages/4b/20/0856833fb4b64bf5ee35ee420d269ef6db2c0eb3146c8a43c5706e140a1b/scrapy-pyppeteer-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "66c482dd20fc42e9032eb77363c7df8c", "sha256": "3aafc7a7faefe2db631826d83ee0a67a7d48f4b3e18d64a61fdf10d699c3be2f"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "66c482dd20fc42e9032eb77363c7df8c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7624, "upload_time": "2020-04-21T18:29:23", "upload_time_iso_8601": "2020-04-21T18:29:23.549012Z", "url": "https://files.pythonhosted.org/packages/ac/ab/23165725b29d40abac174121d3ba7a9feb506f58e0a5b2824e0ef951377a/scrapy_pyppeteer-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "22680836755b65668aca2e720e7f747b", "sha256": "72a8b39e43227f7dfbb7a8ff9516163c7e1f8da5ef908e5d3ccaeece6a1aa1a6"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.3.tar.gz", "has_sig": false, "md5_digest": "22680836755b65668aca2e720e7f747b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6429, "upload_time": "2020-04-21T18:29:24", "upload_time_iso_8601": "2020-04-21T18:29:24.986251Z", "url": "https://files.pythonhosted.org/packages/32/84/e11a1bcdca911af75b47dc95dc9d382bfb0e0318a99be6aa014afc3f29c0/scrapy-pyppeteer-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "07f676a4c633fe2a011c6cf29cc65252", "sha256": "030dbf20187b836e0468d072450d000a3ce1584b50edfba1b4e549b7840b114d"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "07f676a4c633fe2a011c6cf29cc65252", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7681, "upload_time": "2020-04-21T20:05:44", "upload_time_iso_8601": "2020-04-21T20:05:44.693903Z", "url": "https://files.pythonhosted.org/packages/e7/7a/235ec77450d1feb5efbeb5ebe9938db3eff33c3757f3b6ce2ce740a0a4c1/scrapy_pyppeteer-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c38aef8c2dfc2ef9f9d2469445859fca", "sha256": "13ea7be6f209ca4d099a86df7e7baa3bb3926409a75758a8a3327079d6786486"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.4.tar.gz", "has_sig": false, "md5_digest": "c38aef8c2dfc2ef9f9d2469445859fca", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7495, "upload_time": "2020-04-21T20:05:46", "upload_time_iso_8601": "2020-04-21T20:05:46.626249Z", "url": "https://files.pythonhosted.org/packages/c9/09/a30aa947131da57cde0b1bec58febf11e379092a893408d5af2fb0822d19/scrapy-pyppeteer-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "c808cee2d178b5585ff8e8cbcab0e020", "sha256": "5c6f51b76a27ba91f847efc8c5a3749d9229b7d4a816a64f33b6a3220badb6ee"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "c808cee2d178b5585ff8e8cbcab0e020", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7733, "upload_time": "2020-04-22T18:01:13", "upload_time_iso_8601": "2020-04-22T18:01:13.652605Z", "url": "https://files.pythonhosted.org/packages/f2/78/0069c4556a8dc363af83be3579f81646686926e69b12f469b40d8eff64f0/scrapy_pyppeteer-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "62b64bc5237a17f42db24a21e58baf6b", "sha256": "cff3aaf906ae352535c01ea1d4ea71a40c81bf5ba9fa78fd07f0ea1187ef1438"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.5.tar.gz", "has_sig": false, "md5_digest": "62b64bc5237a17f42db24a21e58baf6b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7545, "upload_time": "2020-04-22T18:01:14", "upload_time_iso_8601": "2020-04-22T18:01:14.999241Z", "url": "https://files.pythonhosted.org/packages/91/49/14a46e3a84a5cbe9a25c286eb6668664eedb6415edb66dd2c39e140ba4f6/scrapy-pyppeteer-0.0.5.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "8e5452da91eb82997f097a4eb1033c59", "sha256": "5fb64e0d8a492b403fdfd2a30090381b3859f28b925bc516c68e5b4728beb329"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "8e5452da91eb82997f097a4eb1033c59", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7803, "upload_time": "2020-04-22T18:10:09", "upload_time_iso_8601": "2020-04-22T18:10:09.719031Z", "url": "https://files.pythonhosted.org/packages/09/78/b1fe43b2599b304400a292e0213a37a83880e81602eaf3af3cfc1ebbd83f/scrapy_pyppeteer-0.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5a389f6b7c5f82a92d2bbbbddd4103ce", "sha256": "efb0af9ba458d20167c8fef4ea29307042981e19f0a0d10ac9a1a6eccadd1361"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.6.tar.gz", "has_sig": false, "md5_digest": "5a389f6b7c5f82a92d2bbbbddd4103ce", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7610, "upload_time": "2020-04-22T18:10:10", "upload_time_iso_8601": "2020-04-22T18:10:10.743175Z", "url": "https://files.pythonhosted.org/packages/47/33/bc8d53e5ef95cd3326f18422e7756289f74645d2a4c04aa55a44b0131fd6/scrapy-pyppeteer-0.0.6.tar.gz", "yanked": false}], "0.0.7": [{"comment_text": "", "digests": {"md5": "40504a4483af4a510d886be8285cfaf5", "sha256": "74dc0ad5f76dd8d3fbcd37d7b8ad9316915c302cc94ab25d313478464a45a7a1"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "40504a4483af4a510d886be8285cfaf5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7696, "upload_time": "2020-04-25T21:49:26", "upload_time_iso_8601": "2020-04-25T21:49:26.627574Z", "url": "https://files.pythonhosted.org/packages/34/6d/e039fbab5689fc1a767d2addb55f19229aa12a4eda2dd3e3420a9d92e74a/scrapy_pyppeteer-0.0.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dafaa61472bc2eeac276e00c8c1ff71c", "sha256": "2a32c6ba538d3aa179c5fc21435599e458d309da90fe1ad14da71a89c4e7c724"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.7.tar.gz", "has_sig": false, "md5_digest": "dafaa61472bc2eeac276e00c8c1ff71c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7592, "upload_time": "2020-04-25T21:49:27", "upload_time_iso_8601": "2020-04-25T21:49:27.399082Z", "url": "https://files.pythonhosted.org/packages/a6/f0/4cd8b1f7dffc4dee01a6aa9f0b60cc661836603ad5d32cbbb9dae80fa3b8/scrapy-pyppeteer-0.0.7.tar.gz", "yanked": false}], "0.0.8": [{"comment_text": "", "digests": {"md5": "a7f0486e4e54874aadb934b60db1404f", "sha256": "d997097ebe95be695452473ebcd9333dc7042acf8741f1b60dc6a2ae33c239d7"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.8-py3-none-any.whl", "has_sig": false, "md5_digest": "a7f0486e4e54874aadb934b60db1404f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8285, "upload_time": "2020-04-30T01:26:18", "upload_time_iso_8601": "2020-04-30T01:26:18.128273Z", "url": "https://files.pythonhosted.org/packages/e2/a9/84cd49dde1a8157aa4eacfe0523db333e9514f58cba7379d6f4ce3ee92e4/scrapy_pyppeteer-0.0.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7fff4228820cc40c22acb91bd7c7ff05", "sha256": "d4a9b8c8d0d518c0e30f2c0572929ac18cc79438b209ae15ffe1c3873e425b9e"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.8.tar.gz", "has_sig": false, "md5_digest": "7fff4228820cc40c22acb91bd7c7ff05", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7914, "upload_time": "2020-04-30T01:26:19", "upload_time_iso_8601": "2020-04-30T01:26:19.410651Z", "url": "https://files.pythonhosted.org/packages/e3/58/7ebd34996d2462b53f245b20e055f6ac3da9b635c7c9e3f8c61b45b8217b/scrapy-pyppeteer-0.0.8.tar.gz", "yanked": false}], "0.0.9": [{"comment_text": "", "digests": {"md5": "cf681d01e2c4fb53626c0e751a79e382", "sha256": "f8b75d1c74a97ba1ecaf645071e3e930bf43bf18c13f23bb60a426441ec4a34c"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.9-py3-none-any.whl", "has_sig": false, "md5_digest": "cf681d01e2c4fb53626c0e751a79e382", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8583, "upload_time": "2020-05-01T20:32:38", "upload_time_iso_8601": "2020-05-01T20:32:38.578308Z", "url": "https://files.pythonhosted.org/packages/6f/5c/3faed547f6ccb57c5e41bcb14c27d343d4a7e092583a5d78dd71ddaa2fd7/scrapy_pyppeteer-0.0.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "577e72f5dbaaa2f63232ae500c7f18f4", "sha256": "c1901151e44cb99d4e0606e6608f9e0ec8b2f14396b728a5e61b2c8edcfa2edd"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.9.tar.gz", "has_sig": false, "md5_digest": "577e72f5dbaaa2f63232ae500c7f18f4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8386, "upload_time": "2020-05-01T20:32:39", "upload_time_iso_8601": "2020-05-01T20:32:39.486210Z", "url": "https://files.pythonhosted.org/packages/7a/2a/903bde75df1430f45049d4ba46bc2d1e5a8eafb74085d67c66b661eded9f/scrapy-pyppeteer-0.0.9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cf681d01e2c4fb53626c0e751a79e382", "sha256": "f8b75d1c74a97ba1ecaf645071e3e930bf43bf18c13f23bb60a426441ec4a34c"}, "downloads": -1, "filename": "scrapy_pyppeteer-0.0.9-py3-none-any.whl", "has_sig": false, "md5_digest": "cf681d01e2c4fb53626c0e751a79e382", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8583, "upload_time": "2020-05-01T20:32:38", "upload_time_iso_8601": "2020-05-01T20:32:38.578308Z", "url": "https://files.pythonhosted.org/packages/6f/5c/3faed547f6ccb57c5e41bcb14c27d343d4a7e092583a5d78dd71ddaa2fd7/scrapy_pyppeteer-0.0.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "577e72f5dbaaa2f63232ae500c7f18f4", "sha256": "c1901151e44cb99d4e0606e6608f9e0ec8b2f14396b728a5e61b2c8edcfa2edd"}, "downloads": -1, "filename": "scrapy-pyppeteer-0.0.9.tar.gz", "has_sig": false, "md5_digest": "577e72f5dbaaa2f63232ae500c7f18f4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8386, "upload_time": "2020-05-01T20:32:39", "upload_time_iso_8601": "2020-05-01T20:32:39.486210Z", "url": "https://files.pythonhosted.org/packages/7a/2a/903bde75df1430f45049d4ba46bc2d1e5a8eafb74085d67c66b661eded9f/scrapy-pyppeteer-0.0.9.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:43 2020"}