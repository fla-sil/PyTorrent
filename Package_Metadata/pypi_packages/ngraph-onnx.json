{"info": {"author": "Intel", "author_email": "intelnervana@intel.com", "bugtrack_url": null, "classifiers": [], "description": "# ngraph-onnx [![Build Status](https://travis-ci.org/NervanaSystems/ngraph-onnx.svg?branch=master)](https://travis-ci.org/NervanaSystems/ngraph-onnx/branches)\n\nnGraph Backend for ONNX.\n\nThis repository contains tools to run [ONNX][onnx] models using the [Intel nGraph library][ngraph_github] as a backend.\n\n## Installation\n\nFollow our [build][building] instructions to install nGraph-ONNX from sources.\n\n<!-- @TODO: Restore pip installation section when new wheels are on PyPI\n\nnGraph and nGraph-ONNX are available as binary wheels you can install from PyPI.\n\nnGraph binary wheels are currently tested on Ubuntu 16.04, if you're using a different system, you may want to [build][building] nGraph-ONNX from sources.\n\n### Prerequisites\n\nPython 3.4 or higher is required. \n\n    # apt update\n    # apt install python3 python-virtualenv\n\n### Using a virtualenv (optional)\n\nYou may wish to use a virutualenv for your installation.\n\n    $ virtualenv -p $(which python3) venv\n    $ source venv/bin/activate\n    (venv) $\n\n### Installing\n\n    (venv) $ pip install ngraph-core\n    (venv) $ pip install ngraph-onnx\n-->\n\n## Usage example\n\n### Importing an ONNX model\n\nYou can download models from the [ONNX model zoo][onnx_model_zoo]. For example ResNet-50:\n\n```\n$ wget https://s3.amazonaws.com/download.onnx/models/opset_8/resnet50.tar.gz\n$ tar -xzvf resnet50.tar.gz\n```\n\nUse the following Python commands to convert the downloaded model to an nGraph model:\n\n```python\n# Import ONNX and load an ONNX file from disk\n>>> import onnx\n>>> onnx_protobuf = onnx.load('resnet50/model.onnx')\n\n# Convert ONNX model to an ngraph model\n>>> from ngraph_onnx.onnx_importer.importer import import_onnx_model\n>>> ng_function = import_onnx_model(onnx_protobuf)\n\n# The importer returns a list of ngraph models for every ONNX graph output:\n>>> print(ng_function)\n<Function: 'resnet50' ([1, 1000])>\n```\n\nThis creates an nGraph `Function` object, which can be used to execute a computation on a chosen backend.\n\n### Running a computation\n\nAfter importing an ONNX model, you will have an nGraph `Function` object. \nNow you can create an nGraph `Runtime` backend and use it to compile your `Function` to a backend-specific `Computation` object.\nFinally, you can execute your model by calling the created `Computation` object with input data.\n\n```python\n# Using an ngraph runtime (CPU backend) create a callable computation object\n>>> import ngraph as ng\n>>> runtime = ng.runtime(backend_name='CPU')\n>>> resnet_on_cpu = runtime.computation(ng_function)\n\n# Load an image (or create a mock as in this example)\n>>> import numpy as np\n>>> picture = np.ones([1, 3, 224, 224], dtype=np.float32)\n\n# Run computation on the picture:\n>>> resnet_on_cpu(picture)\n[array([[2.16105007e-04, 5.58412226e-04, 9.70510227e-05, 5.76671446e-05,\n         7.45318757e-05, 4.80892748e-04, 5.67404088e-04, 9.48728994e-05,\n         ...\n```\n\n[onnx]: http://onnx.ai/\n[onnx_model_zoo]: https://github.com/onnx/models\n[ngraph_github]: https://github.com/NervanaSystems/ngraph\n[building]: https://github.com/NervanaSystems/ngraph-onnx/blob/master/BUILDING.md\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/NervanaSystems/ngraph-onnx", "keywords": "", "license": "License :: OSI Approved :: Apache Software License", "maintainer": "", "maintainer_email": "", "name": "ngraph-onnx", "package_url": "https://pypi.org/project/ngraph-onnx/", "platform": "", "project_url": "https://pypi.org/project/ngraph-onnx/", "project_urls": {"Homepage": "https://github.com/NervanaSystems/ngraph-onnx"}, "release_url": "https://pypi.org/project/ngraph-onnx/0.24.0/", "requires_dist": ["cachetools", "ngraph-core", "numpy", "onnx", "setuptools"], "requires_python": "", "summary": "nGraph Backend for ONNX", "version": "0.24.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ngraph-onnx <a href=\"https://travis-ci.org/NervanaSystems/ngraph-onnx/branches\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0621aff369f2968359752ca4dffe4826ee3f39bf/68747470733a2f2f7472617669732d63692e6f72672f4e657276616e6153797374656d732f6e67726170682d6f6e6e782e7376673f6272616e63683d6d6173746572\"></a></h1>\n<p>nGraph Backend for ONNX.</p>\n<p>This repository contains tools to run <a href=\"http://onnx.ai/\" rel=\"nofollow\">ONNX</a> models using the <a href=\"https://github.com/NervanaSystems/ngraph\" rel=\"nofollow\">Intel nGraph library</a> as a backend.</p>\n<h2>Installation</h2>\n<p>Follow our <a href=\"https://github.com/NervanaSystems/ngraph-onnx/blob/master/BUILDING.md\" rel=\"nofollow\">build</a> instructions to install nGraph-ONNX from sources.</p>\n\n<h2>Usage example</h2>\n<h3>Importing an ONNX model</h3>\n<p>You can download models from the <a href=\"https://github.com/onnx/models\" rel=\"nofollow\">ONNX model zoo</a>. For example ResNet-50:</p>\n<pre><code>$ wget https://s3.amazonaws.com/download.onnx/models/opset_8/resnet50.tar.gz\n$ tar -xzvf resnet50.tar.gz\n</code></pre>\n<p>Use the following Python commands to convert the downloaded model to an nGraph model:</p>\n<pre><span class=\"c1\"># Import ONNX and load an ONNX file from disk</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">onnx_protobuf</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'resnet50/model.onnx'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Convert ONNX model to an ngraph model</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">ngraph_onnx.onnx_importer.importer</span> <span class=\"kn\">import</span> <span class=\"n\">import_onnx_model</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">ng_function</span> <span class=\"o\">=</span> <span class=\"n\">import_onnx_model</span><span class=\"p\">(</span><span class=\"n\">onnx_protobuf</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># The importer returns a list of ngraph models for every ONNX graph output:</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">ng_function</span><span class=\"p\">)</span>\n<span class=\"o\">&lt;</span><span class=\"n\">Function</span><span class=\"p\">:</span> <span class=\"s1\">'resnet50'</span> <span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">])</span><span class=\"o\">&gt;</span>\n</pre>\n<p>This creates an nGraph <code>Function</code> object, which can be used to execute a computation on a chosen backend.</p>\n<h3>Running a computation</h3>\n<p>After importing an ONNX model, you will have an nGraph <code>Function</code> object.\nNow you can create an nGraph <code>Runtime</code> backend and use it to compile your <code>Function</code> to a backend-specific <code>Computation</code> object.\nFinally, you can execute your model by calling the created <code>Computation</code> object with input data.</p>\n<pre><span class=\"c1\"># Using an ngraph runtime (CPU backend) create a callable computation object</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">ngraph</span> <span class=\"k\">as</span> <span class=\"nn\">ng</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">runtime</span> <span class=\"o\">=</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">runtime</span><span class=\"p\">(</span><span class=\"n\">backend_name</span><span class=\"o\">=</span><span class=\"s1\">'CPU'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">resnet_on_cpu</span> <span class=\"o\">=</span> <span class=\"n\">runtime</span><span class=\"o\">.</span><span class=\"n\">computation</span><span class=\"p\">(</span><span class=\"n\">ng_function</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Load an image (or create a mock as in this example)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">picture</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run computation on the picture:</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">resnet_on_cpu</span><span class=\"p\">(</span><span class=\"n\">picture</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">2.16105007e-04</span><span class=\"p\">,</span> <span class=\"mf\">5.58412226e-04</span><span class=\"p\">,</span> <span class=\"mf\">9.70510227e-05</span><span class=\"p\">,</span> <span class=\"mf\">5.76671446e-05</span><span class=\"p\">,</span>\n         <span class=\"mf\">7.45318757e-05</span><span class=\"p\">,</span> <span class=\"mf\">4.80892748e-04</span><span class=\"p\">,</span> <span class=\"mf\">5.67404088e-04</span><span class=\"p\">,</span> <span class=\"mf\">9.48728994e-05</span><span class=\"p\">,</span>\n         <span class=\"o\">...</span>\n</pre>\n\n          </div>"}, "last_serial": 5621083, "releases": {"0.10.0": [{"comment_text": "", "digests": {"md5": "133de2e09af2baddf49cb5feed176654", "sha256": "cb4fe0726f235329f2ccb2c913f24f0d2e1550a3ca7bbe3c3754d11c281cb5e8"}, "downloads": -1, "filename": "ngraph_onnx-0.10.0-py3-none-any.whl", "has_sig": false, "md5_digest": "133de2e09af2baddf49cb5feed176654", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 55378, "upload_time": "2018-12-01T00:23:29", "upload_time_iso_8601": "2018-12-01T00:23:29.748629Z", "url": "https://files.pythonhosted.org/packages/20/08/7fa2dd4533e303f1c896d47237701093b1eabce779bb69e0cf60a8804453/ngraph_onnx-0.10.0-py3-none-any.whl", "yanked": false}], "0.10.0rc5": [{"comment_text": "", "digests": {"md5": "87898b70fda24e87f1a3d96323f7b734", "sha256": "351f426e42b05cbc9274febb0f26a1e34b64116e35cafb85e1a4bd41c04ba8df"}, "downloads": -1, "filename": "ngraph_onnx-0.10.0rc5-py3-none-any.whl", "has_sig": false, "md5_digest": "87898b70fda24e87f1a3d96323f7b734", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 51419, "upload_time": "2018-11-30T05:57:23", "upload_time_iso_8601": "2018-11-30T05:57:23.081028Z", "url": "https://files.pythonhosted.org/packages/f0/6d/da0f658a366be0dd992b136dd8a7b67dd1781ef295a67308d9b0e89c8798/ngraph_onnx-0.10.0rc5-py3-none-any.whl", "yanked": false}], "0.24.0": [{"comment_text": "", "digests": {"md5": "100e29027837a495b2fd1f8fff844280", "sha256": "74610fe1883df36c3dd8e6071dde4b8833759db6aabcdcb3db8f3c1b1cacdd96"}, "downloads": -1, "filename": "ngraph_onnx-0.24.0-py3-none-any.whl", "has_sig": false, "md5_digest": "100e29027837a495b2fd1f8fff844280", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 20416, "upload_time": "2019-08-01T22:17:44", "upload_time_iso_8601": "2019-08-01T22:17:44.089351Z", "url": "https://files.pythonhosted.org/packages/53/82/8440d4bf3e0de3fbea9cdc4fe627dfb66757f2d63d8b342532f7067ce09e/ngraph_onnx-0.24.0-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "100e29027837a495b2fd1f8fff844280", "sha256": "74610fe1883df36c3dd8e6071dde4b8833759db6aabcdcb3db8f3c1b1cacdd96"}, "downloads": -1, "filename": "ngraph_onnx-0.24.0-py3-none-any.whl", "has_sig": false, "md5_digest": "100e29027837a495b2fd1f8fff844280", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 20416, "upload_time": "2019-08-01T22:17:44", "upload_time_iso_8601": "2019-08-01T22:17:44.089351Z", "url": "https://files.pythonhosted.org/packages/53/82/8440d4bf3e0de3fbea9cdc4fe627dfb66757f2d63d8b342532f7067ce09e/ngraph_onnx-0.24.0-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:45:30 2020"}