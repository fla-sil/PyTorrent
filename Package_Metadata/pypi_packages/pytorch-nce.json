{"info": {"author": "Kaiyu Shi", "author_email": "skyisno.1@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent"], "description": "An NCE implementation in pytorch\n===\n\nAbout NCE\n---\n\nNoise Contrastive Estimation (NCE) is an approximation method that is used to work around\nthe huge computational cost of large softmax layer. The basic idea is to convert the prediction\nproblem into classification problem at training stage. It has been proved that these two criterions\nconverges to the same minimal point as long as noise distribution is close enough to real one.\n\nNCE bridges the gap between generative models and discriminative models, rather than simply speedup\nthe softmax layer. With NCE, you can turn almost anything into posterior with less effort (I think).\n\nRefs:\n\nNCE:\n> http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf\n\nNCE on rnnlm:\n> https://pdfs.semanticscholar.org/144e/357b1339c27cce7a1e69f0899c21d8140c1f.pdf\n\n### Comparison with other methods\n\nA review of softmax speedup methods:\n> http://ruder.io/word-embeddings-softmax/\n\nNCE vs. IS (Importance Sampling): Nce is a binary classification while IS is sort of multi-class\nclassification problem.\n> http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf\n\nNCE vs. GAN (Generative Adversarial Network):\n> https://arxiv.org/abs/1412.6515\n\n### On improving NCE\n\n#### Sampling methods\n\nIn NCE, unigram distribution is usually used to approximate the noise distribution because it's fast to\nsample from. Sampling from a unigram is equal to multinomial sampling, which is of complexity $O(\\log(N))$\nvia binary search tree. The cost of sampling becomes significant when noise ratio increases.\n\nSince the unigram distribution can be obtained before training and remains unchanged across training,\nsome works are proposed to make use of this property to speedup the sampling procedure. Alias method is\none of them.\n\n<img src=\"https://github.com/Stonesjtu/Pytorch-NCE/blob/master/res/alias.gif?raw=true\" alt=\"diagram of constructing auxiliary data structure\" height=\"200\" />\n\nBy constructing data structures, alias method can reduce the sampling complexity from $O(log(N))$ to $O(1)$,\nand it's easy to parallelize.\n\nRefs:\n\nalias method:\n> https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n\n#### Generic NCE (full-NCE)\n\nConventional NCE only perform the contrasting on linear(softmax) layer, that is, given an input of a\nlinear layer, the model outputs are $p(noise|input)$ and $p(target|input)$. In fact NCE can be applied\nto more general situations where models are capable to output likelihood values for both real data and\nnoise data.\n\nIn this code base, I use a variant of generic NCE named full-NCE (f-NCE) to clarify. Unlike normal NCE,\nf-NCE samples the noises at input embedding.\n\nRefs:\n\nwhole sentence language model by IBM (ICASSP2018)\n\nBi-LSTM language model by speechlab,SJTU (ICSLP2016?)\n\n#### Batched NCE\n\nConventional NCE requires different noise samples per data token. Such computational pattern is not fully\nGPU-efficient because it needs batched matrix multiplication. A trick is to share the noise samples across\nthe whole mini-batch, thus sparse batched matrix multiplication is converted to more efficient\ndense matrix multiplication. The batched NCE is already supported by Tensorflow.\n\nA more aggressive approach is to called self contrasting (named by myself). Instead of sampling from noise\ndistribution, the noises are simply the other training tokens the within the same mini-batch.\n\nRef:\n\nbatched NCE\n> https://arxiv.org/pdf/1708.05997.pdf\n\nself contrasting:\n> https://www.isi.edu/natural-language/mt/simple-fast-noise.pdf\n\nRun the word language model example\n---\n\nThere's an example illustrating how to use the NCE module in `example` folder.\nThis example is forked from the pytorch/examples repo.\n\n### Requirements\n\nPlease run `pip install -r requirements` first to see if you have the required python lib.\n- `tqdm` is used for process bar during training\n- `dill` is a more flexible replacement for pickle\n\n### NCE related Arguments\n\n- `--nce`: whether to use NCE as approximation\n- `--noise-ratio <50>`: numbers of noise samples per batch, the noise is shared among the\ntokens in a single batch, for training speed.\n- `--norm-term <9>`: the constant normalization term `Ln(z)`\n- `--index-module <linear>`: index module to use for NCE module (currently\n<linear> and <gru> available, <gru> does not support PPL calculating )\n- `--train`: train or just evaluation existing model\n- `--vocab <None>`: use vocabulary file if specified, otherwise use the words in train.txt\n- `--loss [full, nce, sampled, mix]`: choose one of the loss type for training, the loss is\nconverted to `full` for PPL evaluation automatically.\n\n### Examples\n\nRun NCE criterion with linear module:\n```bash\npython main.py --cuda --noise-ratio 10 --norm-term 9 --nce --train\n```\n\nRun NCE criterion with gru module:\n```bash\npython main.py --cuda --noise-ratio 10 --norm-term 9 --nce --train --index-module gru\n```\n\nRun conventional CE criterion:\n```bash\npython main.py --cuda --train\n```\n\n### A small benchmark in swbd+fisher dataset\n\nIt's a performance showcase. The dataset is not bundled in this repo however.\nThe model is trained on concatenated sentences,but the hidden states are not\npassed across batches. An `<s>` is inserted between sentences. The model is\nevaluated on `<s>` padded sentences separately.\n\nGenerally a model trained on concatenated sentences performs slightly worse than\nthe one trained on separate sentences. But we saves 50% of training time by reducing\nthe sentence padding operation.\n\n#### dataset statistics\n- training samples: 2200000 sentences, 22403872 words\n- built vocabulary size: ~30K\n\n#### testbed\n- 1080 Ti\n- i7 7700K\n- pytorch-0.4.0\n- cuda-8.0\n- cudnn-6.0.1\n\n#### how to run:\n```bash\npython main.py --train --batch-size 96 --cuda --loss nce --noise-ratio 500 --nhid 300 \\\n  --emsize 300 --log-interval 1000 --nlayers 1 --dropout 0 --weight-decay 1e-8 \\\n  --data data/swb --min-freq 3 --lr 2 --save nce-500-swb --concat\n```\n\n#### Running time\n- crossentropy: 6.5 mins/epoch (56K tokens/sec)\n- nce: 2 mins/epoch (187K tokens/sec)\n\n#### performance\n\nThe rescore is performed on swbd 50-best, thanks to HexLee.\n\n| training loss type | evaluation type | PPL     | WER                          |\n| :---:              | :---:           | :--:    | :--:                         |\n| 3gram              | normed          | ??      | 19.4                         |\n| CE(no concat)      | normed(full)    | 53      | 13.1                         |\n| CE                 | normed(full)    | 55      | 13.3                         |\n| NCE                | unnormed(NCE)   | invalid | 13.4                         |\n| NCE                | normed(full)    | 55      | 13.4                         |\n| importance sample  | normed(full)    | 55      | 13.4                         |\n| importance sample  | sampled(500)    | invalid | 19.0(worse than w/o rescore) |\n\n\n### File structure\n\n- `example/log/`: some log files of this scripts\n- `nce/`: the NCE module wrapper\n    - `nce/nce_loss.py`: the NCE loss\n    - `nce/alias_multinomial.py`: alias method sampling\n    - `nce/index_linear.py`: an index module used by NCE, as a replacement for normal Linear module\n    - `nce/index_gru.py`: an index module used by NCE, as a replacement for the whole language model module\n- `sample.py`: a simple script for NCE linear.\n- `example`: a word langauge model sample to use NCE as loss.\n    - `example/vocab.py`: a wrapper for vocabulary object\n    - `example/model.py`: the wrapper of all `nn.Module`s.\n    - `example/generic_model.py`: the model wrapper for index_gru NCE module\n    - `example/main.py`: entry point\n    - `example/utils.py`: some util functions for better code structure\n\n-----------------\n### Modified README from Pytorch/examples\n\nThis example trains a multi-layer LSTM on a language modeling task.\nBy default, the training script uses the PTB dataset, provided.\n\n```bash\npython main.py --train --cuda --epochs 6        # Train a LSTM on PTB with CUDA\n```\n\nThe model will automatically use the cuDNN backend if run on CUDA with\ncuDNN installed.\n\nDuring training, if a keyboard interrupt (Ctrl-C) is received,\ntraining is stopped and the current model is evaluated against the test dataset.\n\nThe `main.py` script accepts the following arguments:\n\n```bash\noptional arguments:\n  -h, --help         show this help message and exit\n  --data DATA        location of the data corpus\n  --emsize EMSIZE    size of word embeddings\n  --nhid NHID        humber of hidden units per layer\n  --nlayers NLAYERS  number of layers\n  --lr LR            initial learning rate\n  --lr-decay         learning rate decay when no progress is observed on validation set\n  --weight-decay     weight decay(L2 normalization)\n  --clip CLIP        gradient clipping\n  --epochs EPOCHS    upper epoch limit\n  --batch-size N     batch size\n  --dropout DROPOUT  dropout applied to layers (0 = no dropout)\n  --seed SEED        random seed\n  --cuda             use CUDA\n  --log-interval N   report interval\n  --save SAVE        path to save the final model\n  --bptt             max length of truncated bptt\n  --concat           use concatenated sentence instead of individual sentence\n```\n\n\n# CHANGELOG\n\n- 2019.09.09: Improve numeric stability by directly calculation on logits", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Stonesjtu/Pytorch-NCE", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "pytorch-nce", "package_url": "https://pypi.org/project/pytorch-nce/", "platform": "", "project_url": "https://pypi.org/project/pytorch-nce/", "project_urls": {"Homepage": "https://github.com/Stonesjtu/Pytorch-NCE"}, "release_url": "https://pypi.org/project/pytorch-nce/0.0.1/", "requires_dist": null, "requires_python": "", "summary": "An NCE implementation in pytorch", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>An NCE implementation in pytorch</h1>\n<h2>About NCE</h2>\n<p>Noise Contrastive Estimation (NCE) is an approximation method that is used to work around\nthe huge computational cost of large softmax layer. The basic idea is to convert the prediction\nproblem into classification problem at training stage. It has been proved that these two criterions\nconverges to the same minimal point as long as noise distribution is close enough to real one.</p>\n<p>NCE bridges the gap between generative models and discriminative models, rather than simply speedup\nthe softmax layer. With NCE, you can turn almost anything into posterior with less effort (I think).</p>\n<p>Refs:</p>\n<p>NCE:</p>\n<blockquote>\n<p><a href=\"http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf\" rel=\"nofollow\">http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf</a></p>\n</blockquote>\n<p>NCE on rnnlm:</p>\n<blockquote>\n<p><a href=\"https://pdfs.semanticscholar.org/144e/357b1339c27cce7a1e69f0899c21d8140c1f.pdf\" rel=\"nofollow\">https://pdfs.semanticscholar.org/144e/357b1339c27cce7a1e69f0899c21d8140c1f.pdf</a></p>\n</blockquote>\n<h3>Comparison with other methods</h3>\n<p>A review of softmax speedup methods:</p>\n<blockquote>\n<p><a href=\"http://ruder.io/word-embeddings-softmax/\" rel=\"nofollow\">http://ruder.io/word-embeddings-softmax/</a></p>\n</blockquote>\n<p>NCE vs. IS (Importance Sampling): Nce is a binary classification while IS is sort of multi-class\nclassification problem.</p>\n<blockquote>\n<p><a href=\"http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf\" rel=\"nofollow\">http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf</a></p>\n</blockquote>\n<p>NCE vs. GAN (Generative Adversarial Network):</p>\n<blockquote>\n<p><a href=\"https://arxiv.org/abs/1412.6515\" rel=\"nofollow\">https://arxiv.org/abs/1412.6515</a></p>\n</blockquote>\n<h3>On improving NCE</h3>\n<h4>Sampling methods</h4>\n<p>In NCE, unigram distribution is usually used to approximate the noise distribution because it's fast to\nsample from. Sampling from a unigram is equal to multinomial sampling, which is of complexity $O(\\log(N))$\nvia binary search tree. The cost of sampling becomes significant when noise ratio increases.</p>\n<p>Since the unigram distribution can be obtained before training and remains unchanged across training,\nsome works are proposed to make use of this property to speedup the sampling procedure. Alias method is\none of them.</p>\n<img alt=\"diagram of constructing auxiliary data structure\" height=\"200\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1d73806dbec85315adde05bf9343a848a832b383/68747470733a2f2f6769746875622e636f6d2f53746f6e65736a74752f5079746f7263682d4e43452f626c6f622f6d61737465722f7265732f616c6961732e6769663f7261773d74727565\">\n<p>By constructing data structures, alias method can reduce the sampling complexity from $O(log(N))$ to $O(1)$,\nand it's easy to parallelize.</p>\n<p>Refs:</p>\n<p>alias method:</p>\n<blockquote>\n<p><a href=\"https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\" rel=\"nofollow\">https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/</a></p>\n</blockquote>\n<h4>Generic NCE (full-NCE)</h4>\n<p>Conventional NCE only perform the contrasting on linear(softmax) layer, that is, given an input of a\nlinear layer, the model outputs are $p(noise|input)$ and $p(target|input)$. In fact NCE can be applied\nto more general situations where models are capable to output likelihood values for both real data and\nnoise data.</p>\n<p>In this code base, I use a variant of generic NCE named full-NCE (f-NCE) to clarify. Unlike normal NCE,\nf-NCE samples the noises at input embedding.</p>\n<p>Refs:</p>\n<p>whole sentence language model by IBM (ICASSP2018)</p>\n<p>Bi-LSTM language model by speechlab,SJTU (ICSLP2016?)</p>\n<h4>Batched NCE</h4>\n<p>Conventional NCE requires different noise samples per data token. Such computational pattern is not fully\nGPU-efficient because it needs batched matrix multiplication. A trick is to share the noise samples across\nthe whole mini-batch, thus sparse batched matrix multiplication is converted to more efficient\ndense matrix multiplication. The batched NCE is already supported by Tensorflow.</p>\n<p>A more aggressive approach is to called self contrasting (named by myself). Instead of sampling from noise\ndistribution, the noises are simply the other training tokens the within the same mini-batch.</p>\n<p>Ref:</p>\n<p>batched NCE</p>\n<blockquote>\n<p><a href=\"https://arxiv.org/pdf/1708.05997.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1708.05997.pdf</a></p>\n</blockquote>\n<p>self contrasting:</p>\n<blockquote>\n<p><a href=\"https://www.isi.edu/natural-language/mt/simple-fast-noise.pdf\" rel=\"nofollow\">https://www.isi.edu/natural-language/mt/simple-fast-noise.pdf</a></p>\n</blockquote>\n<h2>Run the word language model example</h2>\n<p>There's an example illustrating how to use the NCE module in <code>example</code> folder.\nThis example is forked from the pytorch/examples repo.</p>\n<h3>Requirements</h3>\n<p>Please run <code>pip install -r requirements</code> first to see if you have the required python lib.</p>\n<ul>\n<li><code>tqdm</code> is used for process bar during training</li>\n<li><code>dill</code> is a more flexible replacement for pickle</li>\n</ul>\n<h3>NCE related Arguments</h3>\n<ul>\n<li><code>--nce</code>: whether to use NCE as approximation</li>\n<li><code>--noise-ratio &lt;50&gt;</code>: numbers of noise samples per batch, the noise is shared among the\ntokens in a single batch, for training speed.</li>\n<li><code>--norm-term &lt;9&gt;</code>: the constant normalization term <code>Ln(z)</code></li>\n<li><code>--index-module &lt;linear&gt;</code>: index module to use for NCE module (currently\n&lt;linear&gt; and &lt;gru&gt; available, &lt;gru&gt; does not support PPL calculating )</li>\n<li><code>--train</code>: train or just evaluation existing model</li>\n<li><code>--vocab &lt;None&gt;</code>: use vocabulary file if specified, otherwise use the words in train.txt</li>\n<li><code>--loss [full, nce, sampled, mix]</code>: choose one of the loss type for training, the loss is\nconverted to <code>full</code> for PPL evaluation automatically.</li>\n</ul>\n<h3>Examples</h3>\n<p>Run NCE criterion with linear module:</p>\n<pre>python main.py --cuda --noise-ratio <span class=\"m\">10</span> --norm-term <span class=\"m\">9</span> --nce --train\n</pre>\n<p>Run NCE criterion with gru module:</p>\n<pre>python main.py --cuda --noise-ratio <span class=\"m\">10</span> --norm-term <span class=\"m\">9</span> --nce --train --index-module gru\n</pre>\n<p>Run conventional CE criterion:</p>\n<pre>python main.py --cuda --train\n</pre>\n<h3>A small benchmark in swbd+fisher dataset</h3>\n<p>It's a performance showcase. The dataset is not bundled in this repo however.\nThe model is trained on concatenated sentences,but the hidden states are not\npassed across batches. An <code>&lt;s&gt;</code> is inserted between sentences. The model is\nevaluated on <code>&lt;s&gt;</code> padded sentences separately.</p>\n<p>Generally a model trained on concatenated sentences performs slightly worse than\nthe one trained on separate sentences. But we saves 50% of training time by reducing\nthe sentence padding operation.</p>\n<h4>dataset statistics</h4>\n<ul>\n<li>training samples: 2200000 sentences, 22403872 words</li>\n<li>built vocabulary size: ~30K</li>\n</ul>\n<h4>testbed</h4>\n<ul>\n<li>1080 Ti</li>\n<li>i7 7700K</li>\n<li>pytorch-0.4.0</li>\n<li>cuda-8.0</li>\n<li>cudnn-6.0.1</li>\n</ul>\n<h4>how to run:</h4>\n<pre>python main.py --train --batch-size <span class=\"m\">96</span> --cuda --loss nce --noise-ratio <span class=\"m\">500</span> --nhid <span class=\"m\">300</span> <span class=\"se\">\\</span>\n  --emsize <span class=\"m\">300</span> --log-interval <span class=\"m\">1000</span> --nlayers <span class=\"m\">1</span> --dropout <span class=\"m\">0</span> --weight-decay 1e-8 <span class=\"se\">\\</span>\n  --data data/swb --min-freq <span class=\"m\">3</span> --lr <span class=\"m\">2</span> --save nce-500-swb --concat\n</pre>\n<h4>Running time</h4>\n<ul>\n<li>crossentropy: 6.5 mins/epoch (56K tokens/sec)</li>\n<li>nce: 2 mins/epoch (187K tokens/sec)</li>\n</ul>\n<h4>performance</h4>\n<p>The rescore is performed on swbd 50-best, thanks to HexLee.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">training loss type</th>\n<th align=\"center\">evaluation type</th>\n<th align=\"center\">PPL</th>\n<th align=\"center\">WER</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">3gram</td>\n<td align=\"center\">normed</td>\n<td align=\"center\">??</td>\n<td align=\"center\">19.4</td>\n</tr>\n<tr>\n<td align=\"center\">CE(no concat)</td>\n<td align=\"center\">normed(full)</td>\n<td align=\"center\">53</td>\n<td align=\"center\">13.1</td>\n</tr>\n<tr>\n<td align=\"center\">CE</td>\n<td align=\"center\">normed(full)</td>\n<td align=\"center\">55</td>\n<td align=\"center\">13.3</td>\n</tr>\n<tr>\n<td align=\"center\">NCE</td>\n<td align=\"center\">unnormed(NCE)</td>\n<td align=\"center\">invalid</td>\n<td align=\"center\">13.4</td>\n</tr>\n<tr>\n<td align=\"center\">NCE</td>\n<td align=\"center\">normed(full)</td>\n<td align=\"center\">55</td>\n<td align=\"center\">13.4</td>\n</tr>\n<tr>\n<td align=\"center\">importance sample</td>\n<td align=\"center\">normed(full)</td>\n<td align=\"center\">55</td>\n<td align=\"center\">13.4</td>\n</tr>\n<tr>\n<td align=\"center\">importance sample</td>\n<td align=\"center\">sampled(500)</td>\n<td align=\"center\">invalid</td>\n<td align=\"center\">19.0(worse than w/o rescore)</td>\n</tr></tbody></table>\n<h3>File structure</h3>\n<ul>\n<li><code>example/log/</code>: some log files of this scripts</li>\n<li><code>nce/</code>: the NCE module wrapper\n<ul>\n<li><code>nce/nce_loss.py</code>: the NCE loss</li>\n<li><code>nce/alias_multinomial.py</code>: alias method sampling</li>\n<li><code>nce/index_linear.py</code>: an index module used by NCE, as a replacement for normal Linear module</li>\n<li><code>nce/index_gru.py</code>: an index module used by NCE, as a replacement for the whole language model module</li>\n</ul>\n</li>\n<li><code>sample.py</code>: a simple script for NCE linear.</li>\n<li><code>example</code>: a word langauge model sample to use NCE as loss.\n<ul>\n<li><code>example/vocab.py</code>: a wrapper for vocabulary object</li>\n<li><code>example/model.py</code>: the wrapper of all <code>nn.Module</code>s.</li>\n<li><code>example/generic_model.py</code>: the model wrapper for index_gru NCE module</li>\n<li><code>example/main.py</code>: entry point</li>\n<li><code>example/utils.py</code>: some util functions for better code structure</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3>Modified README from Pytorch/examples</h3>\n<p>This example trains a multi-layer LSTM on a language modeling task.\nBy default, the training script uses the PTB dataset, provided.</p>\n<pre>python main.py --train --cuda --epochs <span class=\"m\">6</span>        <span class=\"c1\"># Train a LSTM on PTB with CUDA</span>\n</pre>\n<p>The model will automatically use the cuDNN backend if run on CUDA with\ncuDNN installed.</p>\n<p>During training, if a keyboard interrupt (Ctrl-C) is received,\ntraining is stopped and the current model is evaluated against the test dataset.</p>\n<p>The <code>main.py</code> script accepts the following arguments:</p>\n<pre>optional arguments:\n  -h, --help         show this <span class=\"nb\">help</span> message and <span class=\"nb\">exit</span>\n  --data DATA        location of the data corpus\n  --emsize EMSIZE    size of word embeddings\n  --nhid NHID        humber of hidden units per layer\n  --nlayers NLAYERS  number of layers\n  --lr LR            initial learning rate\n  --lr-decay         learning rate decay when no progress is observed on validation <span class=\"nb\">set</span>\n  --weight-decay     weight decay<span class=\"o\">(</span>L2 normalization<span class=\"o\">)</span>\n  --clip CLIP        gradient clipping\n  --epochs EPOCHS    upper epoch limit\n  --batch-size N     batch size\n  --dropout DROPOUT  dropout applied to layers <span class=\"o\">(</span><span class=\"nv\">0</span> <span class=\"o\">=</span> no dropout<span class=\"o\">)</span>\n  --seed SEED        random seed\n  --cuda             use CUDA\n  --log-interval N   report interval\n  --save SAVE        path to save the final model\n  --bptt             max length of truncated bptt\n  --concat           use concatenated sentence instead of individual sentence\n</pre>\n<h1>CHANGELOG</h1>\n<ul>\n<li>2019.09.09: Improve numeric stability by directly calculation on logits</li>\n</ul>\n\n          </div>"}, "last_serial": 6087703, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "3ae9eae7ef0b9c5b3a996b5c94fb0200", "sha256": "dea6d2653a2080ca1a05cc7ac8f8b297e875273720022bfa5843ea4e4cc9c1ec"}, "downloads": -1, "filename": "pytorch-nce-0.0.1.tar.gz", "has_sig": false, "md5_digest": "3ae9eae7ef0b9c5b3a996b5c94fb0200", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16002, "upload_time": "2019-11-06T14:36:12", "upload_time_iso_8601": "2019-11-06T14:36:12.161174Z", "url": "https://files.pythonhosted.org/packages/d3/66/f50065edee0e95072f9501d64caab6c628a973a5397bcc2d7dcb008e4c02/pytorch-nce-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3ae9eae7ef0b9c5b3a996b5c94fb0200", "sha256": "dea6d2653a2080ca1a05cc7ac8f8b297e875273720022bfa5843ea4e4cc9c1ec"}, "downloads": -1, "filename": "pytorch-nce-0.0.1.tar.gz", "has_sig": false, "md5_digest": "3ae9eae7ef0b9c5b3a996b5c94fb0200", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16002, "upload_time": "2019-11-06T14:36:12", "upload_time_iso_8601": "2019-11-06T14:36:12.161174Z", "url": "https://files.pythonhosted.org/packages/d3/66/f50065edee0e95072f9501d64caab6c628a973a5397bcc2d7dcb008e4c02/pytorch-nce-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:13:48 2020"}