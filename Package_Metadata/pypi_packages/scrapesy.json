{"info": {"author": "ScoopGracie", "author_email": "scoopgracie@scoopgracie.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9"], "description": "# scrapesy\n[![Build Status](https://api.travis-ci.com/scoopgracie/scrapesy.svg?branch=master)](https://travis-ci.com/scoopgracie/scrapesy)\n\nEasy and Pythonic way to get and parse a Web page\n\n## Usage\n\nTo get a `Page` object, use `scrapesy.get(url)`. The `Page` object has two\nproperties, `page` and `request`. `page` is a `BeautifulSoup` object.\n`request` is a Requests `Response` object.\n\n### Caching\n\nBy default, Scrapesy implements a cache, allowing for near-instantaneous\nresults on pages that have been requested previously. This cache operates\nautomatically, and it operates transparently to any code that does not\nspecifically interact with it. It is possible to use Scrapesy without any\nunderstanding of the cache.\n\nHowever, it is possible to disable the cache. Simply run `scrapesy.caching =\nFalse`. To re-enable it, use `scrapesy.caching = True`. If you simply need to\nignore the cache for a single call, simply add `use_cache=False` to your\n`scrapesy.get()` call.\n\nTo empty the cache, call `scrapesy.empty_cache()`.\n\nTo remove a single page from the cache, call `scrapesy.uncache(url)`.\n\nTo enable selective caching, set `scrapesy.cache_check` to a function that\ntakes `url` as an input and returns `True` if the page should be cached and\n`False` otherwise.\n\nRun `demo.py` for a demonstration of the impact of the cache.\n\n## Requirements\n\n* Beautiful Soup 4\n* Requests\n* Python 3 (it may work on 2.7, but is not tested)\n\n## Note\n\nThis project was originally called PyScrape. If you find that name used\nanywhere in this repo, please report it as an issue!", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/scoopgracie/scrapesy", "keywords": "scrape,web,scraper,scraping", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "scrapesy", "package_url": "https://pypi.org/project/scrapesy/", "platform": "", "project_url": "https://pypi.org/project/scrapesy/", "project_urls": {"Homepage": "https://github.com/scoopgracie/scrapesy"}, "release_url": "https://pypi.org/project/scrapesy/1.0.4/", "requires_dist": null, "requires_python": "", "summary": "Simple Python web scraper/page fetcher with cache", "version": "1.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>scrapesy</h1>\n<p><a href=\"https://travis-ci.com/scoopgracie/scrapesy\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9244856d0af31caf04f708a161a9458a8b16dd39/68747470733a2f2f6170692e7472617669732d63692e636f6d2f73636f6f706772616369652f73637261706573792e7376673f6272616e63683d6d6173746572\"></a></p>\n<p>Easy and Pythonic way to get and parse a Web page</p>\n<h2>Usage</h2>\n<p>To get a <code>Page</code> object, use <code>scrapesy.get(url)</code>. The <code>Page</code> object has two\nproperties, <code>page</code> and <code>request</code>. <code>page</code> is a <code>BeautifulSoup</code> object.\n<code>request</code> is a Requests <code>Response</code> object.</p>\n<h3>Caching</h3>\n<p>By default, Scrapesy implements a cache, allowing for near-instantaneous\nresults on pages that have been requested previously. This cache operates\nautomatically, and it operates transparently to any code that does not\nspecifically interact with it. It is possible to use Scrapesy without any\nunderstanding of the cache.</p>\n<p>However, it is possible to disable the cache. Simply run <code>scrapesy.caching = False</code>. To re-enable it, use <code>scrapesy.caching = True</code>. If you simply need to\nignore the cache for a single call, simply add <code>use_cache=False</code> to your\n<code>scrapesy.get()</code> call.</p>\n<p>To empty the cache, call <code>scrapesy.empty_cache()</code>.</p>\n<p>To remove a single page from the cache, call <code>scrapesy.uncache(url)</code>.</p>\n<p>To enable selective caching, set <code>scrapesy.cache_check</code> to a function that\ntakes <code>url</code> as an input and returns <code>True</code> if the page should be cached and\n<code>False</code> otherwise.</p>\n<p>Run <code>demo.py</code> for a demonstration of the impact of the cache.</p>\n<h2>Requirements</h2>\n<ul>\n<li>Beautiful Soup 4</li>\n<li>Requests</li>\n<li>Python 3 (it may work on 2.7, but is not tested)</li>\n</ul>\n<h2>Note</h2>\n<p>This project was originally called PyScrape. If you find that name used\nanywhere in this repo, please report it as an issue!</p>\n\n          </div>"}, "last_serial": 6603446, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "b98262b82d752ec875715e3247f02c20", "sha256": "e9263ecb539ca4d9e1adee042997efcf46f4dec8290e44be1b2ebfb9f35c76ad"}, "downloads": -1, "filename": "scrapesy-0.1.tar.gz", "has_sig": false, "md5_digest": "b98262b82d752ec875715e3247f02c20", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1747, "upload_time": "2020-02-05T00:51:47", "upload_time_iso_8601": "2020-02-05T00:51:47.464536Z", "url": "https://files.pythonhosted.org/packages/16/23/bde597561ddcd7d9e9c03a5cae520665bc41d337f3335c0bc765672db244/scrapesy-0.1.tar.gz", "yanked": false}], "1.0": [{"comment_text": "", "digests": {"md5": "be016cbea676bd62baa64a0f2579ad0c", "sha256": "5450dbf2e4512bc1accb405919c9a951a3a8ed8e85d81cfb7d5dc92af58f0be3"}, "downloads": -1, "filename": "scrapesy-1.0.tar.gz", "has_sig": false, "md5_digest": "be016cbea676bd62baa64a0f2579ad0c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1709, "upload_time": "2020-02-05T00:57:46", "upload_time_iso_8601": "2020-02-05T00:57:46.502137Z", "url": "https://files.pythonhosted.org/packages/87/8a/0c049bf0645ce8164ff33506b6db606c40c94e8febf1b025857ab214a7a2/scrapesy-1.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "9fe50c72cf652f8fc1c3e2521d5ae71b", "sha256": "4e9f7dddb4e8db4a3d05d6c84d91167106b4714b90175c7f7355895526ddaa97"}, "downloads": -1, "filename": "scrapesy-1.0.1.tar.gz", "has_sig": false, "md5_digest": "9fe50c72cf652f8fc1c3e2521d5ae71b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2459, "upload_time": "2020-02-10T15:46:40", "upload_time_iso_8601": "2020-02-10T15:46:40.681205Z", "url": "https://files.pythonhosted.org/packages/bc/c1/7ebe03430bdd9a3dd8cb462df8f36009ec7cb54512849d756be3bbedf078/scrapesy-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "ef5efa185dc88f6808efdcfcd5427e5b", "sha256": "40a47ad5ac3ad25c602211e78f417f3c65d7bc89dd6189518fc639ec6de4a5b1"}, "downloads": -1, "filename": "scrapesy-1.0.2.tar.gz", "has_sig": false, "md5_digest": "ef5efa185dc88f6808efdcfcd5427e5b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1715, "upload_time": "2020-02-10T15:51:27", "upload_time_iso_8601": "2020-02-10T15:51:27.003523Z", "url": "https://files.pythonhosted.org/packages/cf/97/d336604066b56ae2f9906427aea49966227629d4b78b1f109390fa7e0c1b/scrapesy-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "9e66ef4a5827c7906111bef81adf1fe4", "sha256": "c16ad5225c8a0258615b8c887f3ab0d8638ce7c7d9c12eec9aaa8b8bd4609550"}, "downloads": -1, "filename": "scrapesy-1.0.3.tar.gz", "has_sig": false, "md5_digest": "9e66ef4a5827c7906111bef81adf1fe4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2612, "upload_time": "2020-02-10T16:05:01", "upload_time_iso_8601": "2020-02-10T16:05:01.875656Z", "url": "https://files.pythonhosted.org/packages/51/7f/31cb110182f64599514bdf6060b2aa5dc8ce99452e093a4898705d052ce7/scrapesy-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "d7ee3a7740455d39b795df4390f0ee63", "sha256": "89ac230177bb3b660398abce4f98dced8fb456d184f6072c1c96f35019675366"}, "downloads": -1, "filename": "scrapesy-1.0.4.tar.gz", "has_sig": false, "md5_digest": "d7ee3a7740455d39b795df4390f0ee63", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3013, "upload_time": "2020-02-10T16:08:41", "upload_time_iso_8601": "2020-02-10T16:08:41.434813Z", "url": "https://files.pythonhosted.org/packages/98/81/8f68f3f765ef28e1af7cab1e74954435dbe4f376c5e7e1462120137a5acf/scrapesy-1.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d7ee3a7740455d39b795df4390f0ee63", "sha256": "89ac230177bb3b660398abce4f98dced8fb456d184f6072c1c96f35019675366"}, "downloads": -1, "filename": "scrapesy-1.0.4.tar.gz", "has_sig": false, "md5_digest": "d7ee3a7740455d39b795df4390f0ee63", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3013, "upload_time": "2020-02-10T16:08:41", "upload_time_iso_8601": "2020-02-10T16:08:41.434813Z", "url": "https://files.pythonhosted.org/packages/98/81/8f68f3f765ef28e1af7cab1e74954435dbe4f376c5e7e1462120137a5acf/scrapesy-1.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:53 2020"}