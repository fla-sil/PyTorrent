{"info": {"author": "Dekki", "author_email": "dekkiaero@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "DDPG\n============\n\n- Implimenting DDPG Algorithm in Tensorflow-2.0\n- Tested on Open-AI Pendulum-v0 and Continous mountain car gym environments.\n- DDPG - algorthim : https://arxiv.org/abs/1509.02971\n\nInstall :\n------------------\n- pip install DDPG-TF\n\n\npython code:\n------------------\n```python\nimport gym\n\nfrom ddpg import DDPG\n\nenv = gym.make('Pendulum-v0')\n\nddpg = DDPG(\n                 env , # Gym environment with continous action space\n                 actor(None), # Tensorflow/keras model\n                 critic (None), # Tensorflow/keras model\n                 buffer (None), # pre-recorded buffer\n                 action_bound_range=1,\n                 max_buffer_size =10000, # maximum transitions to be stored in buffer\n                 batch_size =64, # batch size for training actor and critic networks\n                 max_time_steps = 1000 ,# no of time steps per epoch\n                 tow = 0.001, # for soft target update\n                 discount_factor  = 0.99,\n                 explore_time = 1000, # time steps for random actions for exploration\n                 actor_learning_rate = 0.0001,\n                 critic_learning_rate = 0.001\n                 dtype = 'float32',\n                 n_episodes = 1000 ,# no of episodes to run\n                 reward_plot = True ,# (bool)  to plot reward progress per episode\n                 model_save = 1) # epochs to save models and buffer\n\nddpg.train()\n```\n\n------------\n\n\n## Results :\n\n- On pendulum problem explored for 5 episodes\n\n\n[![Reward plot of Pendulum problem](https://github.com/Dekki-Aero/DDPG/blob/master/DDPG-Pendulum_Performance.png \"Reward plot of Pendulum problem\")](http://https://github.com/Dekki-Aero/DDPG/blob/master/DDPG-Pendulum_Performance.png \"Reward plot of Pendulum problem\")\n\n- On Continous mountain car problem explored for 100 episodes\n\n\n[![Reward plot of CountinousMountain car](https://github.com/Dekki-Aero/DDPG/blob/master/mountain_car_continous.png \"Reward plot of CountinousMountain car\")](http://https://github.com/Dekki-Aero/DDPG/blob/master/mountain_car_continous.png \"Reward plot of CountinousMountain car\")\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Dekki-Aero/DDPG", "keywords": "Deep Determnistic policy gradient,Actor Critic,Reinforcement Learning,DDPG", "license": "", "maintainer": "", "maintainer_email": "", "name": "DDPG-TF", "package_url": "https://pypi.org/project/DDPG-TF/", "platform": "", "project_url": "https://pypi.org/project/DDPG-TF/", "project_urls": {"Homepage": "https://github.com/Dekki-Aero/DDPG"}, "release_url": "https://pypi.org/project/DDPG-TF/2.0.3/", "requires_dist": ["tensorflow (==2.0)", "gym", "numpy", "matplotlib"], "requires_python": "", "summary": "DDPG implimentaion in Tensorflow-2.0", "version": "2.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>DDPG</h1>\n<ul>\n<li>Implimenting DDPG Algorithm in Tensorflow-2.0</li>\n<li>Tested on Open-AI Pendulum-v0 and Continous mountain car gym environments.</li>\n<li>DDPG - algorthim : <a href=\"https://arxiv.org/abs/1509.02971\" rel=\"nofollow\">https://arxiv.org/abs/1509.02971</a></li>\n</ul>\n<h2>Install :</h2>\n<ul>\n<li>pip install DDPG-TF</li>\n</ul>\n<h2>python code:</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">ddpg</span> <span class=\"kn\">import</span> <span class=\"n\">DDPG</span>\n\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">'Pendulum-v0'</span><span class=\"p\">)</span>\n\n<span class=\"n\">ddpg</span> <span class=\"o\">=</span> <span class=\"n\">DDPG</span><span class=\"p\">(</span>\n                 <span class=\"n\">env</span> <span class=\"p\">,</span> <span class=\"c1\"># Gym environment with continous action space</span>\n                 <span class=\"n\">actor</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">),</span> <span class=\"c1\"># Tensorflow/keras model</span>\n                 <span class=\"n\">critic</span> <span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">),</span> <span class=\"c1\"># Tensorflow/keras model</span>\n                 <span class=\"n\">buffer</span> <span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">),</span> <span class=\"c1\"># pre-recorded buffer</span>\n                 <span class=\"n\">action_bound_range</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n                 <span class=\"n\">max_buffer_size</span> <span class=\"o\">=</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"c1\"># maximum transitions to be stored in buffer</span>\n                 <span class=\"n\">batch_size</span> <span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"c1\"># batch size for training actor and critic networks</span>\n                 <span class=\"n\">max_time_steps</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span> <span class=\"p\">,</span><span class=\"c1\"># no of time steps per epoch</span>\n                 <span class=\"n\">tow</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span><span class=\"p\">,</span> <span class=\"c1\"># for soft target update</span>\n                 <span class=\"n\">discount_factor</span>  <span class=\"o\">=</span> <span class=\"mf\">0.99</span><span class=\"p\">,</span>\n                 <span class=\"n\">explore_time</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"c1\"># time steps for random actions for exploration</span>\n                 <span class=\"n\">actor_learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.0001</span><span class=\"p\">,</span>\n                 <span class=\"n\">critic_learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span>\n                 <span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"s1\">'float32'</span><span class=\"p\">,</span>\n                 <span class=\"n\">n_episodes</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span> <span class=\"p\">,</span><span class=\"c1\"># no of episodes to run</span>\n                 <span class=\"n\">reward_plot</span> <span class=\"o\">=</span> <span class=\"kc\">True</span> <span class=\"p\">,</span><span class=\"c1\"># (bool)  to plot reward progress per episode</span>\n                 <span class=\"n\">model_save</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\"># epochs to save models and buffer</span>\n\n<span class=\"n\">ddpg</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n</pre>\n<hr>\n<h2>Results :</h2>\n<ul>\n<li>On pendulum problem explored for 5 episodes</li>\n</ul>\n<p><a href=\"http://https://github.com/Dekki-Aero/DDPG/blob/master/DDPG-Pendulum_Performance.png\" rel=\"nofollow\" title=\"Reward plot of Pendulum problem\"><img alt=\"Reward plot of Pendulum problem\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8aef0c377688cfdb843a9c671538a28ac3074b3b/68747470733a2f2f6769746875622e636f6d2f44656b6b692d4165726f2f444450472f626c6f622f6d61737465722f444450472d50656e64756c756d5f506572666f726d616e63652e706e67\"></a></p>\n<ul>\n<li>On Continous mountain car problem explored for 100 episodes</li>\n</ul>\n<p><a href=\"http://https://github.com/Dekki-Aero/DDPG/blob/master/mountain_car_continous.png\" rel=\"nofollow\" title=\"Reward plot of CountinousMountain car\"><img alt=\"Reward plot of CountinousMountain car\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/35d9b7159002937257db9947820055c2d1624a1d/68747470733a2f2f6769746875622e636f6d2f44656b6b692d4165726f2f444450472f626c6f622f6d61737465722f6d6f756e7461696e5f6361725f636f6e74696e6f75732e706e67\"></a></p>\n\n          </div>"}, "last_serial": 6253701, "releases": {"2.0.3": [{"comment_text": "", "digests": {"md5": "d1f258f5cdeb987690850fa9df9ad6ee", "sha256": "dd45af53271b1cf9401c4322f72253012636efad885bda6665481c923a3087fe"}, "downloads": -1, "filename": "DDPG_TF-2.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "d1f258f5cdeb987690850fa9df9ad6ee", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5899, "upload_time": "2019-12-06T11:17:04", "upload_time_iso_8601": "2019-12-06T11:17:04.374978Z", "url": "https://files.pythonhosted.org/packages/ce/0f/6ef8856e8e97fa5e7d972a773569e70f26cb6868912cb0a12372f40afed6/DDPG_TF-2.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5fbd3d3483c436782a5380f71fd8ad64", "sha256": "fc55e3b96bc6619ab3cd80a2f4c8ae61349bf1e8bd32f85d08e75ebfc1791c41"}, "downloads": -1, "filename": "DDPG-TF-2.0.3.tar.gz", "has_sig": false, "md5_digest": "5fbd3d3483c436782a5380f71fd8ad64", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4758, "upload_time": "2019-12-06T11:17:05", "upload_time_iso_8601": "2019-12-06T11:17:05.872749Z", "url": "https://files.pythonhosted.org/packages/68/5a/00c19b0d674729fb605c87b98c66b92c85686614c3484010ba0c4e32a882/DDPG-TF-2.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d1f258f5cdeb987690850fa9df9ad6ee", "sha256": "dd45af53271b1cf9401c4322f72253012636efad885bda6665481c923a3087fe"}, "downloads": -1, "filename": "DDPG_TF-2.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "d1f258f5cdeb987690850fa9df9ad6ee", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5899, "upload_time": "2019-12-06T11:17:04", "upload_time_iso_8601": "2019-12-06T11:17:04.374978Z", "url": "https://files.pythonhosted.org/packages/ce/0f/6ef8856e8e97fa5e7d972a773569e70f26cb6868912cb0a12372f40afed6/DDPG_TF-2.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5fbd3d3483c436782a5380f71fd8ad64", "sha256": "fc55e3b96bc6619ab3cd80a2f4c8ae61349bf1e8bd32f85d08e75ebfc1791c41"}, "downloads": -1, "filename": "DDPG-TF-2.0.3.tar.gz", "has_sig": false, "md5_digest": "5fbd3d3483c436782a5380f71fd8ad64", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4758, "upload_time": "2019-12-06T11:17:05", "upload_time_iso_8601": "2019-12-06T11:17:05.872749Z", "url": "https://files.pythonhosted.org/packages/68/5a/00c19b0d674729fb605c87b98c66b92c85686614c3484010ba0c4e32a882/DDPG-TF-2.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:39:42 2020"}