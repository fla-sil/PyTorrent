{"info": {"author": "Michael Mauderer, Frederic Kerber", "author_email": "fkerber@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# perception-model-tool\n\nWe present a model allowing inferences of perceivable screen content in relation to position and orientation of mobile or wearable devices with respect to their user. The model is based on findings from vision science and allows prediction of a value of effective resolution that can be perceived by a user. It considers distance and angle between the device and the eyes of the observer as well as the resulting retinal eccentricity when the device is not directly focused but observed in the periphery. To validate our model, we conducted a study with 12 participants. Based on our results, we outline implications for the design of mobile applications that are able to adapt themselves to facilitate information throughput and usability.\n\nTo visualize the predictions of the model, we provide a tool that \u2013 given a display position and orientation in relation to the user\u2019s eyes \u2013 renders a picture representing the effective display resolution, e.g. to assess text readability for different sizes or fonts. We distinguish whether a person is (a) directly looking at the display or (b) looking straight ahead and observing the display in the periphery. The tool takes a picture, e.g. a screenshot of a smartwatch application, converts it to the CIE L*a*b* space, and only the luminance information is further considered. A second-order Butterworth filter is used to remove frequencies that would not be visible according to our model.\n\nPlease see here for the full paper: https://doi.org/10.1145/3173574.3174184\n\n## Tool usage\n\nThe tool requires Python 3 in combination with numpy, scipy, click and colour-science. For the GUI version, Tkinter is used.\n\n### GUI version\n\nFirst, an input file (jpg, png) and an output file (jpg, png) to save the adjusted version to have to be specified. The parameters allow to specify characteristics of the device under investigation (display size and resolution) as well as the distance and orientation in respect to the observer. A selector gives the option to decide whether the observer is directly looking at the device or whether s/he is looking straight (peripheral observation).\nBy pressing the \"Process\" button, the image is being processed and the adjusted version is saved to the specified output file.\n\n### Command-line interface (CLI)\n\n```\npython filter_screenshot.py ./image.png ./out.png -d 0.4 -s 0.02 0.02 -r 200 200 -ha 10 -va 20\n```\n\nFor a full list of commands see the help page\n```\npython filter_screenshot.py --help\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/fkerber/perception-model-tool", "keywords": "", "license": "GNU Lesser General Public License v3.0", "maintainer": "", "maintainer_email": "", "name": "perception-model", "package_url": "https://pypi.org/project/perception-model/", "platform": "", "project_url": "https://pypi.org/project/perception-model/", "project_urls": {"Homepage": "https://github.com/fkerber/perception-model-tool"}, "release_url": "https://pypi.org/project/perception-model/1.0/", "requires_dist": ["numpy", "scipy", "colour-science", "click"], "requires_python": ">=3", "summary": "Perception Model", "version": "1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>perception-model-tool</h1>\n<p>We present a model allowing inferences of perceivable screen content in relation to position and orientation of mobile or wearable devices with respect to their user. The model is based on findings from vision science and allows prediction of a value of effective resolution that can be perceived by a user. It considers distance and angle between the device and the eyes of the observer as well as the resulting retinal eccentricity when the device is not directly focused but observed in the periphery. To validate our model, we conducted a study with 12 participants. Based on our results, we outline implications for the design of mobile applications that are able to adapt themselves to facilitate information throughput and usability.</p>\n<p>To visualize the predictions of the model, we provide a tool that \u2013 given a display position and orientation in relation to the user\u2019s eyes \u2013 renders a picture representing the effective display resolution, e.g. to assess text readability for different sizes or fonts. We distinguish whether a person is (a) directly looking at the display or (b) looking straight ahead and observing the display in the periphery. The tool takes a picture, e.g. a screenshot of a smartwatch application, converts it to the CIE L<em>a</em>b* space, and only the luminance information is further considered. A second-order Butterworth filter is used to remove frequencies that would not be visible according to our model.</p>\n<p>Please see here for the full paper: <a href=\"https://doi.org/10.1145/3173574.3174184\" rel=\"nofollow\">https://doi.org/10.1145/3173574.3174184</a></p>\n<h2>Tool usage</h2>\n<p>The tool requires Python 3 in combination with numpy, scipy, click and colour-science. For the GUI version, Tkinter is used.</p>\n<h3>GUI version</h3>\n<p>First, an input file (jpg, png) and an output file (jpg, png) to save the adjusted version to have to be specified. The parameters allow to specify characteristics of the device under investigation (display size and resolution) as well as the distance and orientation in respect to the observer. A selector gives the option to decide whether the observer is directly looking at the device or whether s/he is looking straight (peripheral observation).\nBy pressing the \"Process\" button, the image is being processed and the adjusted version is saved to the specified output file.</p>\n<h3>Command-line interface (CLI)</h3>\n<pre><code>python filter_screenshot.py ./image.png ./out.png -d 0.4 -s 0.02 0.02 -r 200 200 -ha 10 -va 20\n</code></pre>\n<p>For a full list of commands see the help page</p>\n<pre><code>python filter_screenshot.py --help\n</code></pre>\n\n          </div>"}, "last_serial": 3781139, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "f82acea8126f7eb6fc2076d5d002f165", "sha256": "884a98bdce68aed8d7601e777466b2a59c1a9e2e43c7642b8ef75137a575d5fa"}, "downloads": -1, "filename": "perception_model-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f82acea8126f7eb6fc2076d5d002f165", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 11980, "upload_time": "2018-04-19T15:35:19", "upload_time_iso_8601": "2018-04-19T15:35:19.921591Z", "url": "https://files.pythonhosted.org/packages/25/02/66270fbe12a87959b973f310a96b5b91101ab4751bbe2e92cb2d874c5ef9/perception_model-1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "eafefe48e83f86f9d2242621d73d374b", "sha256": "79e5288c54298bda75ae23b2a722ba9b8f5faaa1b9e6c2e62ab664a7cdd4c7fb"}, "downloads": -1, "filename": "perception-model-1.0.tar.gz", "has_sig": false, "md5_digest": "eafefe48e83f86f9d2242621d73d374b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 11603, "upload_time": "2018-04-19T15:35:20", "upload_time_iso_8601": "2018-04-19T15:35:20.924176Z", "url": "https://files.pythonhosted.org/packages/77/ff/b217d6ae2106eb0a0c1991666913d3a9aa6b635e3ef2a14f8998be517800/perception-model-1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f82acea8126f7eb6fc2076d5d002f165", "sha256": "884a98bdce68aed8d7601e777466b2a59c1a9e2e43c7642b8ef75137a575d5fa"}, "downloads": -1, "filename": "perception_model-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f82acea8126f7eb6fc2076d5d002f165", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 11980, "upload_time": "2018-04-19T15:35:19", "upload_time_iso_8601": "2018-04-19T15:35:19.921591Z", "url": "https://files.pythonhosted.org/packages/25/02/66270fbe12a87959b973f310a96b5b91101ab4751bbe2e92cb2d874c5ef9/perception_model-1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "eafefe48e83f86f9d2242621d73d374b", "sha256": "79e5288c54298bda75ae23b2a722ba9b8f5faaa1b9e6c2e62ab664a7cdd4c7fb"}, "downloads": -1, "filename": "perception-model-1.0.tar.gz", "has_sig": false, "md5_digest": "eafefe48e83f86f9d2242621d73d374b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 11603, "upload_time": "2018-04-19T15:35:20", "upload_time_iso_8601": "2018-04-19T15:35:20.924176Z", "url": "https://files.pythonhosted.org/packages/77/ff/b217d6ae2106eb0a0c1991666913d3a9aa6b635e3ef2a14f8998be517800/perception-model-1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:22 2020"}