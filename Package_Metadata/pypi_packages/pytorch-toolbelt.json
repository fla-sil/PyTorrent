{"info": {"author": "Eugene Khvedchenya", "author_email": "ekhvedchenya@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Image Recognition", "Topic :: Scientific/Engineering :: Mathematics", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Application Frameworks", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Pytorch-toolbelt\n\n[![Build Status](https://travis-ci.org/BloodAxe/pytorch-toolbelt.svg?branch=develop)](https://travis-ci.org/BloodAxe/pytorch-toolbelt)\n[![Documentation Status](https://readthedocs.org/projects/pytorch-toolbelt/badge/?version=latest)](https://pytorch-toolbelt.readthedocs.io/en/latest/?badge=latest)\n[![DeepSource](https://static.deepsource.io/deepsource-badge-light-mini.svg)](https://deepsource.io/gh/BloodAxe/pytorch-toolbelt/?ref=repository-badge)\n\nA `pytorch-toolbelt` is a Python library with a set of bells and whistles for PyTorch for fast R&D prototyping and Kaggle farming:\n\n## What's inside\n\n* Easy model building using flexible encoder-decoder architecture.\n* Modules: CoordConv, SCSE, Hypercolumn, Depthwise separable convolution and more.\n* GPU-friendly test-time augmentation TTA for segmentation and classification\n* GPU-friendly inference on huge (5000x5000) images\n* Every-day common routines (fix/restore random seed, filesystem utils, metrics)\n* Losses: BinaryFocalLoss, Focal, ReducedFocal, Lovasz, Jaccard and Dice losses, Wing Loss and more.\n* Extras for [Catalyst](https://github.com/catalyst-team/catalyst) library (Visualization of batch predictions, additional metrics) \n\nShowcase: [Catalyst, Albumentations, Pytorch Toolbelt example: Semantic Segmentation @ CamVid](https://colab.research.google.com/drive/1OUPJYU7TzH5Vz1si6FBkooackuIlzaGr#scrollTo=GUWuiO5K3aUm)\n\n# Why\n\nHonest answer is \"I needed a convenient way to re-use code for my Kaggle career\". \nDuring 2018 I achieved a [Kaggle Master](https://www.kaggle.com/bloodaxe) badge and this been a long path. \nVery often I found myself re-using most of the old pipelines over and over again. \nAt some point it crystallized into this repository. \n\nThis lib is not meant to replace catalyst / ignite / fast.ai high-level frameworks. Instead it's designed to complement them.\n\n# Installation\n\n`pip install pytorch_toolbelt`\n\n# How do I ... \n\n## Model creation\n\n### Create Encoder-Decoder U-Net model\n\nBelow a code snippet that creates vanilla U-Net model for binary segmentation. \nBy design, both encoder and decoder produces a list of tensors, from fine (high-resolution, indexed `0`) to coarse (low-resolution) feature maps. \nAccess to all intermediate feature maps is beneficial if you want to apply deep supervision losses on them or encoder-decoder of object detection task, \nwhere access to intermediate feature maps is necessary.\n \n```python\nfrom torch import nn\nfrom pytorch_toolbelt.modules import encoders as E\nfrom pytorch_toolbelt.modules import decoders as D\n\nclass UNet(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super().__init__()\n        self.encoder = E.UnetEncoder(in_channels=input_channels, out_channels=32, growth_factor=2)\n        self.decoder = D.UNetDecoder(self.encoder.channels, decoder_features=32)\n        self.logits = nn.Conv2d(self.decoder.channels[0], num_classes, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.logits(x[0])\n```\n\n### Create Encoder-Decoder FPN model with pretrained encoder\n\nSimilarly to previous example, you can change decoder to FPN with contatenation. \n\n ```python\nfrom torch import nn\nfrom pytorch_toolbelt.modules import encoders as E\nfrom pytorch_toolbelt.modules import decoders as D\n\nclass SEResNeXt50FPN(nn.Module):\n    def __init__(self, num_classes, fpn_channels):\n        super().__init__()\n        self.encoder = E.SEResNeXt50Encoder()\n        self.decoder = D.FPNCatDecoder(self.encoder.channels, fpn_channels)\n        self.logits = nn.Conv2d(self.decoder.channels[0], num_classes, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.logits(x[0])\n```\n\n### Change number of input channels for the Encoder\n\nAll encoders from `pytorch_toolbelt` supports changing number of input channels. Simply call `encoder.change_input_channels(num_channels)` and first convolution layer will be changed.\nWhenever possible, existing weights of convolutional layer will be re-used (in case new number of channels is greater than default, new weight tensor will be padded with randomly-initialized weigths).\nClass method returns `self`, so this call can be chained.\n\n\n```python\nfrom pytorch_toolbelt.modules import encoders as E\n\nencoder = E.SEResnet101Encoder()\nencoder = encoder.change_input_channels(6)\n```\n\n\n## Misc\n\n\n## Count number of parameters in encoder/decoder and other modules\n\nWhen designing a model and optimizing number of features in neural network, I found it's quite useful to print number of parameters in high-level blocks (like `encoder` and `decoder`).\nHere is how to do it with `pytorch_toolbelt`:\n\n\n```python\nfrom torch import nn\nfrom pytorch_toolbelt.modules import encoders as E\nfrom pytorch_toolbelt.modules import decoders as D\nfrom pytorch_toolbelt.utils import count_parameters\n\nclass SEResNeXt50FPN(nn.Module):\n    def __init__(self, num_classes, fpn_channels):\n        super().__init__()\n        self.encoder = E.SEResNeXt50Encoder()\n        self.decoder = D.FPNCatDecoder(self.encoder.channels, fpn_channels)\n        self.logits = nn.Conv2d(self.decoder.channels[0], num_classes, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.logits(x[0])\n\nnet = SEResNeXt50FPN(1, 128)\nprint(count_parameters(net))\n# Prints {'total': 34232561, 'trainable': 34232561, 'encoder': 25510896, 'decoder': 8721536, 'logits': 129}\n\n```\n\n### Compose multiple losses\n\nThere are multiple ways to combine multiple losses, and high-level DL frameworks like Catalyst offers way more flexible way to achieve this, but here's 100%-pure PyTorch implementation of mine:\n\n```python\nfrom pytorch_toolbelt import losses as L\n\n# Creates a loss function that is a weighted sum of focal loss \n# and lovasz loss with weigths 1.0 and 0.5 accordingly.\nloss = L.JointLoss(L.FocalLoss(), 1.0, L.LovaszLoss(), 0.5)\n```\n\n\n## TTA / Inferencing\n\n### Apply Test-time augmentation (TTA) for the model\n\nTest-time augmetnation (TTA) can be used in both training and testing phases. \n\n```python\nfrom pytorch_toolbelt.inference import tta\n\nmodel = UNet()\n\n# Truly functional TTA for image classification using horizontal flips:\nlogits = tta.fliplr_image2label(model, input)\n\n# Truly functional TTA for image segmentation using D4 augmentation:\nlogits = tta.d4_image2mask(model, input)\n\n# TTA using wrapper module:\ntta_model = tta.TTAWrapper(model, tta.fivecrop_image2label, crop_size=512)\nlogits = tta_model(input)\n```\n\n### Inference on huge images:\n\nQuite often, there is a need to perform image segmentation for enormously big image (5000px and more). There are a few problems with such a big pixel arrays:\n 1. There are size limitations on maximum size of CUDA tensors (Concrete numbers depends on driver and GPU version)\n 2. Heavy CNNs architectures may eat up all available GPU memory with ease when inferencing relatively small 1024x1024 images, leaving no room to bigger image resolution.\n  \nOne of the solutions is to slice input image into tiles (optionally overlapping) and feed each through model and concatenate the results back. \nIn this way you can guarantee upper limit of GPU ram usage, while keeping ability to process arbitrary-sized images on GPU.\n  \n\n```python\nimport numpy as np\nfrom torch.utils.data import DataLoader\nimport cv2\n\nfrom pytorch_toolbelt.inference.tiles import ImageSlicer, CudaTileMerger\nfrom pytorch_toolbelt.utils.torch_utils import tensor_from_rgb_image, to_numpy\n\n\nimage = cv2.imread('really_huge_image.jpg')\nmodel = get_model(...)\n\n# Cut large image into overlapping tiles\ntiler = ImageSlicer(image.shape, tile_size=(512, 512), tile_step=(256, 256))\n\n# HCW -> CHW. Optionally, do normalization here\ntiles = [tensor_from_rgb_image(tile) for tile in tiler.split(image)]\n\n# Allocate a CUDA buffer for holding entire mask\nmerger = CudaTileMerger(tiler.target_shape, 1, tiler.weight)\n\n# Run predictions for tiles and accumulate them\nfor tiles_batch, coords_batch in DataLoader(list(zip(tiles, tiler.crops)), batch_size=8, pin_memory=True):\n    tiles_batch = tiles_batch.float().cuda()\n    pred_batch = model(tiles_batch)\n\n    merger.integrate_batch(pred_batch, coords_batch)\n\n# Normalize accumulated mask and convert back to numpy\nmerged_mask = np.moveaxis(to_numpy(merger.merge()), 0, -1).astype(np.uint8)\nmerged_mask = tiler.crop_to_orignal_size(merged_mask)\n```\n\n## Advanced examples\n\n1. [Inria Sattelite Segmentation](https://github.com/BloodAxe/Catalyst-Inria-Segmentation-Example)\n1. [CamVid Semantic Segmentation](https://github.com/BloodAxe/Catalyst-CamVid-Segmentation-Example)\n\n\n## Citation\n\n```\n@misc{Khvedchenya_Eugene_2019_PyTorch_Toolbelt,\n  author = {Khvedchenya, Eugene},\n  title = {PyTorch Toolbelt},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/BloodAxe/pytorch-toolbelt}},\n  commit = {cc5e9973cdb0dcbf1c6b6e1401bf44b9c69e13f3}\n}\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/BloodAxe/pytorch-toolbelt", "keywords": "PyTorch,Kaggle,Deep Learning,Machine Learning,ResNet,VGG,ResNext,Unet,Focal,FPN", "license": "License :: OSI Approved :: MIT License", "maintainer": "", "maintainer_email": "", "name": "pytorch-toolbelt", "package_url": "https://pypi.org/project/pytorch-toolbelt/", "platform": "", "project_url": "https://pypi.org/project/pytorch-toolbelt/", "project_urls": {"Homepage": "https://github.com/BloodAxe/pytorch-toolbelt"}, "release_url": "https://pypi.org/project/pytorch-toolbelt/0.3.2/", "requires_dist": null, "requires_python": ">=3.6.0", "summary": "PyTorch extensions for fast R&D prototyping and Kaggle farming", "version": "0.3.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Pytorch-toolbelt</h1>\n<p><a href=\"https://travis-ci.org/BloodAxe/pytorch-toolbelt\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/10a5ec48c10ad3a40e12e7f04fc830a0e1a1a7a1/68747470733a2f2f7472617669732d63692e6f72672f426c6f6f644178652f7079746f7263682d746f6f6c62656c742e7376673f6272616e63683d646576656c6f70\"></a>\n<a href=\"https://pytorch-toolbelt.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6bbd85a9aa0a7f5d0dc0d3b792993fdb84b87514/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7079746f7263682d746f6f6c62656c742f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<a href=\"https://deepsource.io/gh/BloodAxe/pytorch-toolbelt/?ref=repository-badge\" rel=\"nofollow\"><img alt=\"DeepSource\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6837f002aa8f2f99f307c9febf81cee78d869751/68747470733a2f2f7374617469632e64656570736f757263652e696f2f64656570736f757263652d62616467652d6c696768742d6d696e692e737667\"></a></p>\n<p>A <code>pytorch-toolbelt</code> is a Python library with a set of bells and whistles for PyTorch for fast R&amp;D prototyping and Kaggle farming:</p>\n<h2>What's inside</h2>\n<ul>\n<li>Easy model building using flexible encoder-decoder architecture.</li>\n<li>Modules: CoordConv, SCSE, Hypercolumn, Depthwise separable convolution and more.</li>\n<li>GPU-friendly test-time augmentation TTA for segmentation and classification</li>\n<li>GPU-friendly inference on huge (5000x5000) images</li>\n<li>Every-day common routines (fix/restore random seed, filesystem utils, metrics)</li>\n<li>Losses: BinaryFocalLoss, Focal, ReducedFocal, Lovasz, Jaccard and Dice losses, Wing Loss and more.</li>\n<li>Extras for <a href=\"https://github.com/catalyst-team/catalyst\" rel=\"nofollow\">Catalyst</a> library (Visualization of batch predictions, additional metrics)</li>\n</ul>\n<p>Showcase: <a href=\"https://colab.research.google.com/drive/1OUPJYU7TzH5Vz1si6FBkooackuIlzaGr#scrollTo=GUWuiO5K3aUm\" rel=\"nofollow\">Catalyst, Albumentations, Pytorch Toolbelt example: Semantic Segmentation @ CamVid</a></p>\n<h1>Why</h1>\n<p>Honest answer is \"I needed a convenient way to re-use code for my Kaggle career\".\nDuring 2018 I achieved a <a href=\"https://www.kaggle.com/bloodaxe\" rel=\"nofollow\">Kaggle Master</a> badge and this been a long path.\nVery often I found myself re-using most of the old pipelines over and over again.\nAt some point it crystallized into this repository.</p>\n<p>This lib is not meant to replace catalyst / ignite / fast.ai high-level frameworks. Instead it's designed to complement them.</p>\n<h1>Installation</h1>\n<p><code>pip install pytorch_toolbelt</code></p>\n<h1>How do I ...</h1>\n<h2>Model creation</h2>\n<h3>Create Encoder-Decoder U-Net model</h3>\n<p>Below a code snippet that creates vanilla U-Net model for binary segmentation.\nBy design, both encoder and decoder produces a list of tensors, from fine (high-resolution, indexed <code>0</code>) to coarse (low-resolution) feature maps.\nAccess to all intermediate feature maps is beneficial if you want to apply deep supervision losses on them or encoder-decoder of object detection task,\nwhere access to intermediate feature maps is necessary.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"kn\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.modules</span> <span class=\"kn\">import</span> <span class=\"n\">encoders</span> <span class=\"k\">as</span> <span class=\"n\">E</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.modules</span> <span class=\"kn\">import</span> <span class=\"n\">decoders</span> <span class=\"k\">as</span> <span class=\"n\">D</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">UNet</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">input_channels</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">E</span><span class=\"o\">.</span><span class=\"n\">UnetEncoder</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"n\">input_channels</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"n\">growth_factor</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span> <span class=\"o\">=</span> <span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">UNetDecoder</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"n\">decoder_features</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span><span class=\"o\">.</span><span class=\"n\">channels</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">logits</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n</pre>\n<h3>Create Encoder-Decoder FPN model with pretrained encoder</h3>\n<p>Similarly to previous example, you can change decoder to FPN with contatenation.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"kn\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.modules</span> <span class=\"kn\">import</span> <span class=\"n\">encoders</span> <span class=\"k\">as</span> <span class=\"n\">E</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.modules</span> <span class=\"kn\">import</span> <span class=\"n\">decoders</span> <span class=\"k\">as</span> <span class=\"n\">D</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">SEResNeXt50FPN</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n   <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">fpn_channels</span><span class=\"p\">):</span>\n       <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n       <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">E</span><span class=\"o\">.</span><span class=\"n\">SEResNeXt50Encoder</span><span class=\"p\">()</span>\n       <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span> <span class=\"o\">=</span> <span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">FPNCatDecoder</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"n\">fpn_channels</span><span class=\"p\">)</span>\n       <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span><span class=\"o\">.</span><span class=\"n\">channels</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n   <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n       <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n       <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n       <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">logits</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n</pre>\n<h3>Change number of input channels for the Encoder</h3>\n<p>All encoders from <code>pytorch_toolbelt</code> supports changing number of input channels. Simply call <code>encoder.change_input_channels(num_channels)</code> and first convolution layer will be changed.\nWhenever possible, existing weights of convolutional layer will be re-used (in case new number of channels is greater than default, new weight tensor will be padded with randomly-initialized weigths).\nClass method returns <code>self</code>, so this call can be chained.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.modules</span> <span class=\"kn\">import</span> <span class=\"n\">encoders</span> <span class=\"k\">as</span> <span class=\"n\">E</span>\n\n<span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">E</span><span class=\"o\">.</span><span class=\"n\">SEResnet101Encoder</span><span class=\"p\">()</span>\n<span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">change_input_channels</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n</pre>\n<h2>Misc</h2>\n<h2>Count number of parameters in encoder/decoder and other modules</h2>\n<p>When designing a model and optimizing number of features in neural network, I found it's quite useful to print number of parameters in high-level blocks (like <code>encoder</code> and <code>decoder</code>).\nHere is how to do it with <code>pytorch_toolbelt</code>:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"kn\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.modules</span> <span class=\"kn\">import</span> <span class=\"n\">encoders</span> <span class=\"k\">as</span> <span class=\"n\">E</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.modules</span> <span class=\"kn\">import</span> <span class=\"n\">decoders</span> <span class=\"k\">as</span> <span class=\"n\">D</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.utils</span> <span class=\"kn\">import</span> <span class=\"n\">count_parameters</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">SEResNeXt50FPN</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">fpn_channels</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">E</span><span class=\"o\">.</span><span class=\"n\">SEResNeXt50Encoder</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span> <span class=\"o\">=</span> <span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">FPNCatDecoder</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"n\">fpn_channels</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span><span class=\"o\">.</span><span class=\"n\">channels</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">logits</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">SEResNeXt50FPN</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">count_parameters</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"p\">))</span>\n<span class=\"c1\"># Prints {'total': 34232561, 'trainable': 34232561, 'encoder': 25510896, 'decoder': 8721536, 'logits': 129}</span>\n</pre>\n<h3>Compose multiple losses</h3>\n<p>There are multiple ways to combine multiple losses, and high-level DL frameworks like Catalyst offers way more flexible way to achieve this, but here's 100%-pure PyTorch implementation of mine:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt</span> <span class=\"kn\">import</span> <span class=\"n\">losses</span> <span class=\"k\">as</span> <span class=\"n\">L</span>\n\n<span class=\"c1\"># Creates a loss function that is a weighted sum of focal loss </span>\n<span class=\"c1\"># and lovasz loss with weigths 1.0 and 0.5 accordingly.</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">JointLoss</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">FocalLoss</span><span class=\"p\">(),</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"o\">.</span><span class=\"n\">LovaszLoss</span><span class=\"p\">(),</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n</pre>\n<h2>TTA / Inferencing</h2>\n<h3>Apply Test-time augmentation (TTA) for the model</h3>\n<p>Test-time augmetnation (TTA) can be used in both training and testing phases.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.inference</span> <span class=\"kn\">import</span> <span class=\"n\">tta</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">UNet</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Truly functional TTA for image classification using horizontal flips:</span>\n<span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">tta</span><span class=\"o\">.</span><span class=\"n\">fliplr_image2label</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"nb\">input</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Truly functional TTA for image segmentation using D4 augmentation:</span>\n<span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">tta</span><span class=\"o\">.</span><span class=\"n\">d4_image2mask</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"nb\">input</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># TTA using wrapper module:</span>\n<span class=\"n\">tta_model</span> <span class=\"o\">=</span> <span class=\"n\">tta</span><span class=\"o\">.</span><span class=\"n\">TTAWrapper</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">tta</span><span class=\"o\">.</span><span class=\"n\">fivecrop_image2label</span><span class=\"p\">,</span> <span class=\"n\">crop_size</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">)</span>\n<span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">tta_model</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n</pre>\n<h3>Inference on huge images:</h3>\n<p>Quite often, there is a need to perform image segmentation for enormously big image (5000px and more). There are a few problems with such a big pixel arrays:</p>\n<ol>\n<li>There are size limitations on maximum size of CUDA tensors (Concrete numbers depends on driver and GPU version)</li>\n<li>Heavy CNNs architectures may eat up all available GPU memory with ease when inferencing relatively small 1024x1024 images, leaving no room to bigger image resolution.</li>\n</ol>\n<p>One of the solutions is to slice input image into tiles (optionally overlapping) and feed each through model and concatenate the results back.\nIn this way you can guarantee upper limit of GPU ram usage, while keeping ability to process arbitrary-sized images on GPU.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n<span class=\"kn\">import</span> <span class=\"nn\">cv2</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.inference.tiles</span> <span class=\"kn\">import</span> <span class=\"n\">ImageSlicer</span><span class=\"p\">,</span> <span class=\"n\">CudaTileMerger</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_toolbelt.utils.torch_utils</span> <span class=\"kn\">import</span> <span class=\"n\">tensor_from_rgb_image</span><span class=\"p\">,</span> <span class=\"n\">to_numpy</span>\n\n\n<span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">cv2</span><span class=\"o\">.</span><span class=\"n\">imread</span><span class=\"p\">(</span><span class=\"s1\">'really_huge_image.jpg'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">get_model</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Cut large image into overlapping tiles</span>\n<span class=\"n\">tiler</span> <span class=\"o\">=</span> <span class=\"n\">ImageSlicer</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">tile_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">tile_step</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># HCW -&gt; CHW. Optionally, do normalization here</span>\n<span class=\"n\">tiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">tensor_from_rgb_image</span><span class=\"p\">(</span><span class=\"n\">tile</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">tile</span> <span class=\"ow\">in</span> <span class=\"n\">tiler</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)]</span>\n\n<span class=\"c1\"># Allocate a CUDA buffer for holding entire mask</span>\n<span class=\"n\">merger</span> <span class=\"o\">=</span> <span class=\"n\">CudaTileMerger</span><span class=\"p\">(</span><span class=\"n\">tiler</span><span class=\"o\">.</span><span class=\"n\">target_shape</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">tiler</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run predictions for tiles and accumulate them</span>\n<span class=\"k\">for</span> <span class=\"n\">tiles_batch</span><span class=\"p\">,</span> <span class=\"n\">coords_batch</span> <span class=\"ow\">in</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">tiles</span><span class=\"p\">,</span> <span class=\"n\">tiler</span><span class=\"o\">.</span><span class=\"n\">crops</span><span class=\"p\">)),</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">pin_memory</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">):</span>\n    <span class=\"n\">tiles_batch</span> <span class=\"o\">=</span> <span class=\"n\">tiles_batch</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n    <span class=\"n\">pred_batch</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">tiles_batch</span><span class=\"p\">)</span>\n\n    <span class=\"n\">merger</span><span class=\"o\">.</span><span class=\"n\">integrate_batch</span><span class=\"p\">(</span><span class=\"n\">pred_batch</span><span class=\"p\">,</span> <span class=\"n\">coords_batch</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Normalize accumulated mask and convert back to numpy</span>\n<span class=\"n\">merged_mask</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">moveaxis</span><span class=\"p\">(</span><span class=\"n\">to_numpy</span><span class=\"p\">(</span><span class=\"n\">merger</span><span class=\"o\">.</span><span class=\"n\">merge</span><span class=\"p\">()),</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\n<span class=\"n\">merged_mask</span> <span class=\"o\">=</span> <span class=\"n\">tiler</span><span class=\"o\">.</span><span class=\"n\">crop_to_orignal_size</span><span class=\"p\">(</span><span class=\"n\">merged_mask</span><span class=\"p\">)</span>\n</pre>\n<h2>Advanced examples</h2>\n<ol>\n<li><a href=\"https://github.com/BloodAxe/Catalyst-Inria-Segmentation-Example\" rel=\"nofollow\">Inria Sattelite Segmentation</a></li>\n<li><a href=\"https://github.com/BloodAxe/Catalyst-CamVid-Segmentation-Example\" rel=\"nofollow\">CamVid Semantic Segmentation</a></li>\n</ol>\n<h2>Citation</h2>\n<pre><code>@misc{Khvedchenya_Eugene_2019_PyTorch_Toolbelt,\n  author = {Khvedchenya, Eugene},\n  title = {PyTorch Toolbelt},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/BloodAxe/pytorch-toolbelt}},\n  commit = {cc5e9973cdb0dcbf1c6b6e1401bf44b9c69e13f3}\n}\n</code></pre>\n\n          </div>"}, "last_serial": 7122561, "releases": {"0.0.3": [{"comment_text": "", "digests": {"md5": "2b5d300c85449002ad331afad0e5dbb4", "sha256": "03756f036d3822088327e466a66621ae3680f2f0a4b41ee3ad8f7efe19c94828"}, "downloads": -1, "filename": "pytorch_toolbelt-0.0.3.tar.gz", "has_sig": false, "md5_digest": "2b5d300c85449002ad331afad0e5dbb4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 40655, "upload_time": "2019-04-22T12:56:39", "upload_time_iso_8601": "2019-04-22T12:56:39.017488Z", "url": "https://files.pythonhosted.org/packages/e8/35/b624a1e2eb907d964c3f5c4ec62bcf80de642ad784b3d79419383e836258/pytorch_toolbelt-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "a64b6c3183f91beda58e0f939af96b34", "sha256": "b09f83d6d0902c75a7d57080b109fe68bf953fc9a255ffa9e08bf52eefbb1cb4"}, "downloads": -1, "filename": "pytorch_toolbelt-0.0.4.tar.gz", "has_sig": false, "md5_digest": "a64b6c3183f91beda58e0f939af96b34", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 43252, "upload_time": "2019-04-25T19:08:31", "upload_time_iso_8601": "2019-04-25T19:08:31.322779Z", "url": "https://files.pythonhosted.org/packages/24/eb/0a7f0291e4ec5c8dee87ca1e673ef046884a18ddd246cb92343a309c43ab/pytorch_toolbelt-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "194fbd5fd949e549d3fb8d49328b8c1d", "sha256": "43f3619b2c926bc52befe33d98964e8ca4638c866793c6fb5a095126ce12d1a3"}, "downloads": -1, "filename": "pytorch_toolbelt-0.0.5.tar.gz", "has_sig": false, "md5_digest": "194fbd5fd949e549d3fb8d49328b8c1d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 43598, "upload_time": "2019-04-26T15:56:48", "upload_time_iso_8601": "2019-04-26T15:56:48.014782Z", "url": "https://files.pythonhosted.org/packages/f2/a3/9451cc5702777a9447826aaa70ed9b0df954f3afab4419433a563fc04755/pytorch_toolbelt-0.0.5.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "34b92eb98d7572679bc3144b4ffa9da7", "sha256": "e9d47ec70ce7b7a5742b41c6c19100612eeb32acba7451855bad0628eea6f207"}, "downloads": -1, "filename": "pytorch_toolbelt-0.0.6.tar.gz", "has_sig": false, "md5_digest": "34b92eb98d7572679bc3144b4ffa9da7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 46733, "upload_time": "2019-05-06T21:36:36", "upload_time_iso_8601": "2019-05-06T21:36:36.390644Z", "url": "https://files.pythonhosted.org/packages/39/75/95a9c6b237e9490f2a2ed63a19045221f46aa4d6f61a5c1c4513d757bfb1/pytorch_toolbelt-0.0.6.tar.gz", "yanked": false}], "0.0.7": [{"comment_text": "", "digests": {"md5": "d6bf3ffd9a5e53efbd0c67c06da5c95b", "sha256": "f07421181c58a2f4ef4ee284091e644746a7912c1c810303ad1c568f1667d27c"}, "downloads": -1, "filename": "pytorch_toolbelt-0.0.7.tar.gz", "has_sig": false, "md5_digest": "d6bf3ffd9a5e53efbd0c67c06da5c95b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 50123, "upload_time": "2019-05-08T20:52:09", "upload_time_iso_8601": "2019-05-08T20:52:09.571848Z", "url": "https://files.pythonhosted.org/packages/97/9e/c1b22d6c216a6401021c63c9dfeeb46b305f64ab8b48396452a3e3c308b7/pytorch_toolbelt-0.0.7.tar.gz", "yanked": false}], "0.0.8": [{"comment_text": "", "digests": {"md5": "17fc31cdaea17b6575575804c0fd10ee", "sha256": "32ebafbc96e023e4ff6d96b1c2307563e6af65bca9fd9d4754db7811d6cea86d"}, "downloads": -1, "filename": "pytorch_toolbelt-0.0.8.tar.gz", "has_sig": false, "md5_digest": "17fc31cdaea17b6575575804c0fd10ee", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 50806, "upload_time": "2019-05-19T08:21:17", "upload_time_iso_8601": "2019-05-19T08:21:17.213141Z", "url": "https://files.pythonhosted.org/packages/aa/38/726e9391750a98ed847209a606a96124f13da830b54fd7a0b273b9fe30d7/pytorch_toolbelt-0.0.8.tar.gz", "yanked": false}], "0.0.9": [{"comment_text": "", "digests": {"md5": "f83d6153f4749ef75e5996f1b5fabc56", "sha256": "87e4305b70f8867c8cdf4e38b1abe2ed020c2217cedf3571d752e60a7d577a5b"}, "downloads": -1, "filename": "pytorch_toolbelt-0.0.9.tar.gz", "has_sig": false, "md5_digest": "f83d6153f4749ef75e5996f1b5fabc56", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 51547, "upload_time": "2019-06-03T19:32:53", "upload_time_iso_8601": "2019-06-03T19:32:53.397902Z", "url": "https://files.pythonhosted.org/packages/75/48/0d0e5d993c4b2df528cd78c1f6bf85e11a6461167ebb234f2025723ff1cc/pytorch_toolbelt-0.0.9.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "fd8fdd40fcdc0d1b0a07b7c31eecc0e2", "sha256": "04ed8e056ea430c40d46a3f5ea16101e872940381e529fb8951d4e6b6e1cad75"}, "downloads": -1, "filename": "pytorch_toolbelt-0.1.0.tar.gz", "has_sig": false, "md5_digest": "fd8fdd40fcdc0d1b0a07b7c31eecc0e2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 56306, "upload_time": "2019-06-12T20:09:21", "upload_time_iso_8601": "2019-06-12T20:09:21.389392Z", "url": "https://files.pythonhosted.org/packages/ac/c8/0c19c9d96a9f660cf9f286817cac5cd5c93e7623093542933eec5e6f5605/pytorch_toolbelt-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "2fcc31ae2f86dbfd8679295e7c1227d5", "sha256": "2d0cb639fbf00e3fcb1ccd0cd37937e3207d39790f26e5487ad4060799e1a8b8"}, "downloads": -1, "filename": "pytorch_toolbelt-0.1.1.tar.gz", "has_sig": false, "md5_digest": "2fcc31ae2f86dbfd8679295e7c1227d5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 59289, "upload_time": "2019-06-29T14:14:08", "upload_time_iso_8601": "2019-06-29T14:14:08.894200Z", "url": "https://files.pythonhosted.org/packages/15/04/78f2237dae2f0b55bb0416b68e5f3707b5d228090f0c1eaad2c754835364/pytorch_toolbelt-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "f8f666c49161c4c8d8f714e5de4755b2", "sha256": "4e292a9867c25b5844cf12be318eab89a05bac72fa115c19b433e51fe35a7abd"}, "downloads": -1, "filename": "pytorch_toolbelt-0.1.2.tar.gz", "has_sig": false, "md5_digest": "f8f666c49161c4c8d8f714e5de4755b2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 59397, "upload_time": "2019-07-01T00:21:59", "upload_time_iso_8601": "2019-07-01T00:21:59.934309Z", "url": "https://files.pythonhosted.org/packages/73/37/c5cde0e02fec372ed77b3547c385a32d900cc6f02fbd050e8b6668240145/pytorch_toolbelt-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "d7a5569d85849786c8236fd1bd9e8f79", "sha256": "559d3fad6d87ca846533b491e4222a690d7867ec80728776c7dc0e5d17361e70"}, "downloads": -1, "filename": "pytorch_toolbelt-0.1.3.tar.gz", "has_sig": false, "md5_digest": "d7a5569d85849786c8236fd1bd9e8f79", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 59531, "upload_time": "2019-07-24T10:49:26", "upload_time_iso_8601": "2019-07-24T10:49:26.301973Z", "url": "https://files.pythonhosted.org/packages/63/00/2459d5f8abbd48b0f1a9af1d34f92ba84d7b3d28341e17e8ebe414aaa367/pytorch_toolbelt-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "88b324f104a7d37606171af2f21e6e45", "sha256": "25e8122967f1e3cb5419c3bd35aec3931dbdbfb02ea71ff8879e18c2a5cb31f0"}, "downloads": -1, "filename": "pytorch_toolbelt-0.1.4.tar.gz", "has_sig": false, "md5_digest": "88b324f104a7d37606171af2f21e6e45", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 59580, "upload_time": "2019-09-12T09:43:11", "upload_time_iso_8601": "2019-09-12T09:43:11.654788Z", "url": "https://files.pythonhosted.org/packages/c7/5c/3109f40bba8c056e4ad413f2dcad86a80013e56f3bdc2fc41b2a9589ae3a/pytorch_toolbelt-0.1.4.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "64ea7d586aa7d8fe22ba1b10613bbfde", "sha256": "8a7f91468fde49ce329bd8bd44e3d53d6a5690218f4774a136b2399604c0b8e6"}, "downloads": -1, "filename": "pytorch_toolbelt-0.2.0.tar.gz", "has_sig": false, "md5_digest": "64ea7d586aa7d8fe22ba1b10613bbfde", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 62750, "upload_time": "2019-10-04T20:54:07", "upload_time_iso_8601": "2019-10-04T20:54:07.594698Z", "url": "https://files.pythonhosted.org/packages/f8/cc/1cfbe81580deb7f5197fb26e8a310f922083896a2c8c796a4d4fa15553f4/pytorch_toolbelt-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "9c9a0fdf2f40750772cbc2f12873334e", "sha256": "9bae901f8d8a5f077a3a708e54fafb52b7b588096295d31c93da577740e3e79a"}, "downloads": -1, "filename": "pytorch_toolbelt-0.2.1.tar.gz", "has_sig": false, "md5_digest": "9c9a0fdf2f40750772cbc2f12873334e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 62592, "upload_time": "2019-10-07T19:50:23", "upload_time_iso_8601": "2019-10-07T19:50:23.342417Z", "url": "https://files.pythonhosted.org/packages/30/00/eb830ae0eb3f46751c1927d69e8e9eebd36bf63c2c9d259fcec4eecd5f4e/pytorch_toolbelt-0.2.1.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "09f34760bb2566e205ddd10b09872326", "sha256": "a47d09c8db498606678f1ed32d5b9305ef0a021388d1382531767ff9add3e7c1"}, "downloads": -1, "filename": "pytorch_toolbelt-0.3.0.tar.gz", "has_sig": false, "md5_digest": "09f34760bb2566e205ddd10b09872326", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 78299, "upload_time": "2020-01-17T20:45:20", "upload_time_iso_8601": "2020-01-17T20:45:20.765332Z", "url": "https://files.pythonhosted.org/packages/00/58/f26a87bad48da144cc7efc48a655819893ffcd30fea49299a4733f89d694/pytorch_toolbelt-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "44521eb21d75c7805124e9d7a117c50a", "sha256": "185ff285b62ee62591457a3f870d08c7d7c138bbf3129f47d6e369d58bb81e8d"}, "downloads": -1, "filename": "pytorch_toolbelt-0.3.1.tar.gz", "has_sig": false, "md5_digest": "44521eb21d75c7805124e9d7a117c50a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 79690, "upload_time": "2020-02-25T14:32:01", "upload_time_iso_8601": "2020-02-25T14:32:01.960558Z", "url": "https://files.pythonhosted.org/packages/f7/ac/367338b9814f334abf86a84935e05c565ff7f7ae2a6a6090b5a98bc5ed72/pytorch_toolbelt-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "21e7a61f920f09345bb7b5cd9e32ddb2", "sha256": "7f74a23c4a2697578c0b36f9e98d936fef82d4123711a575741b15d74a0c11bc"}, "downloads": -1, "filename": "pytorch_toolbelt-0.3.2.tar.gz", "has_sig": false, "md5_digest": "21e7a61f920f09345bb7b5cd9e32ddb2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 91636, "upload_time": "2020-04-28T19:34:09", "upload_time_iso_8601": "2020-04-28T19:34:09.092713Z", "url": "https://files.pythonhosted.org/packages/be/0d/90964571b4ddf71e455280395ed871cfeb29441d167c37a1ff21de5cf96c/pytorch_toolbelt-0.3.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "21e7a61f920f09345bb7b5cd9e32ddb2", "sha256": "7f74a23c4a2697578c0b36f9e98d936fef82d4123711a575741b15d74a0c11bc"}, "downloads": -1, "filename": "pytorch_toolbelt-0.3.2.tar.gz", "has_sig": false, "md5_digest": "21e7a61f920f09345bb7b5cd9e32ddb2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 91636, "upload_time": "2020-04-28T19:34:09", "upload_time_iso_8601": "2020-04-28T19:34:09.092713Z", "url": "https://files.pythonhosted.org/packages/be/0d/90964571b4ddf71e455280395ed871cfeb29441d167c37a1ff21de5cf96c/pytorch_toolbelt-0.3.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:13:42 2020"}