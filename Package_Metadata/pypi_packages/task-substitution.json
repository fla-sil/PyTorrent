{"info": {"author": "Abhishek Sharma", "author_email": "numb3r303@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "# task_substitution\n> Solve an auxiliary task using ML.\n\n\n**This library is created by using [nbdev](https://github.com/fastai/nbdev), please check it out.**\n\n**Task Substitution** is a method of solving an auxiliary problem ( with different features and different target ) in order to better understand the initial problem and solving it efficiently. \n\nLet's take a look at standard machine learning task, in the figure below you see a regression task with features `f1`, `f2`, `f3` and target variable `y`.\n\n<img src=\"images/training_set.png\">\n\nWe want to build on a model on the above dataset to predict for unknown `y` values in the test dataset shown below.\n\n<img src=\"images/test.png\">\n\n**Exploratory Data Analysis**\n\nFirst step we take to solve the problem is to look at the data, there can be many features with *missing values* or *outliers* which needs to be understood. It is possible that there is a relationship between a missing value and values of other features.\n\n## Recover Missing Values\n\nIt is possible for a feature to have a missing value, it could be a data recording issue or bug etc. Often times for numerical features we replace missing value with `mean` or `median` value as a approximation. Sometimes we replace missing value with values like `-9999` so that model treats them differently or sometimes we leave them as is as libraries like `xgboost` and `lightgbm` can handle `nulls`. Let's look at following dataset\n\n<img src=\"images/missing_full.png\">\n\nHere we have a feature `f3` with missing values, this is a numerical feature, what we can do is that we can consider `f3` as target feature and reframe this as regresion problem where we try to predict for missing values.\n\n<img src=\"images/missing_train.png\">\n\n<img src=\"images/missing_test.png\">\n\nThe above setup is identical to the original regression task, here we would build a model to use `f1` and `f2` to predict for `f3`. So instead of using `mean`, `median` etc. we can build a model to restore missing values which can help us solve the original problem efficiently.\n\nWe have to be careful to not overfit when building such models.\n\n## Check Train Test Distributions\n\nWhenever we train a model we want to use it on a new sample, but what if the new sample comes from a different distribution compared to the data on which the model was trained on. When we deploy our solutions on production we want to be very careful of this change as our models would fail if there is a mismatch in train and test sets. We can pose this problem as an auxiliary task and create a new binary target `y`, where `1` represents whether row comes from `test` set and `0` represents whether it comes from `train` set and then we train our model to predict whether a row comes from `train` or `test` set if the performance ( e.g. `AUC` score ) is high we can conclude that the train and test set come from different distributions. Ofcourse, we need to remove the `original target` from the analysis.\n\n<img src=\"images/ttm_train.png\">\n\n<img src=\"images/ttm_test.png\">\n\n**In the above images you can see two different datasets, we want to verify whether these two come from same distributions or not.** \n\nConsider the first example set as training set and second one as test set for this example.\n\n<img src=\"images/ttm_train_with_target.png\">\n\n<img src=\"images/ttm_test_with_target.png\">\n\nWe create a new target called `is_test` which denotes whether a row belongs to test set or not.\n\n<img src=\"images/ttm_train_test_combined.png\">\n\n**Then we combine training and test set and train a model to predict whether a row comes from train or test set, if our model performs well then we know that these two datasets are from different distributions.**\n\nWe would still have to dig deep into looking at whether that's the case but the above method can help identifying which features are have drifted apart in train and test datasets. If you look at feature importance of the model that was used to separated train and test apart you can identify such features.\n\n\n## Feature Selection using Null Importance\n\nOften we create many features to enhance the power of our ML model, but not all features are beneficial to model performance. There needs to be a way to identify\nfeatures which can introduce `noise` to the model and hence must be removed. We can make use of `null importance` to identify and remove such features.\n\n- Use LightGBM model to train on all features and obtain feature importance ( gain ), call it gain_1\n- For the same data shuffle the target variable and train model and obtain feature importance ( gain ), call it gain_2\n- Calculate ratio = gain_2 / gain_1 and remove features with ratio > 3.\n- Use the remaining features to train the model again.\n\n**Note: The ratio threshold of `3` seems arbitrary here, but we can treat this as `hyper-parameter` and choose the value that gives us the best result on the CV.**\n\nDataframe representing feature importance of useful, useless model and their ratio.\n<img src=\"images/feature_imp.png\">\n\n## Feature Importance\n\nGradient Boosting Trees are very powerful methods when it comes to solving problems in industry and often are the first choice of every practitioner. We often to look at feature importance which different implementations of GBDT provide in help us understanding what features are useful. But during the inference when we have to predict for an unseen example, e.g. in a classification task if our model spits out `0` or `1` it would be useful to know which features were instrumental in identifying it as positive or negative class. This formulation can be extended to multi-classification and regression problems.\n\n- This module implements this [paper](http://www.cs.sjtu.edu.cn/~kzhu/papers/kzhu-infocode.pdf)\n- It currently supports XGBoost model and will soon support other implementations.\n- We just pass our trained model and it converts the tree representation into a dataframe.\n<img src=\"images/trees_to_df.png\">\n- Then we calculate score for each node of the tree where we start off from the terminal node and then propagate it up to the root node.\n<img src=\"images/feature_contribution.png\">\n\n\n## Install\n\ntask_substitution in on pypi:\n\n```\npip install task_substitution\n```\n\nFor an editable install, use the following:\n\n```\ngit clone https://github.com/numb3r33/task_substitution.git\npip install -e task_substitution\n```\n\n## How to use\n\n**Recover Missing Values**\n\n>Currently we only support missing value recovery for numerical features, we plan to extend support for other feature types as well. Also the model currently uses LightGBM model to recover missing values.\n\n```\nfrom task_substitution.recover_missing import *\nfrom sklearn.metrics import mean_squared_error\n\ntrain = train.drop('original_target', axis=1)\n\nmodel_args = {\n          'objective': 'regression',\n          'learning_rate': 0.1,\n          'num_leaves': 31,\n          'min_data_in_leaf': 100,\n          'num_boost_round': 100,\n          'verbosity': -1,\n          'seed': 41\n             }\n\nsplit_args = {\n    'test_size': .2,\n    'random_state': 41\n}\n\n# target_fld: feature with missing values.\n# cat_flds: categorical features in the original dataset.\n# ignore_flds: features you want to ignore. ( these won't be used by LightGBM model to recover missing values)\n\nrec = RecoverMissing(target_fld='f3',\n                     cat_flds=[],\n                     ignore_flds=['f2'],\n                     perf_fn=lambda tr,pe: np.sqrt(mean_squared_error(tr, pe)),\n                     split_args=split_args,\n                     model_args=model_args\n                    )\n\ntrain_recovered = rec.run(train)\n```\n\n**Check train test distributions**\n\n>We use LightGBM model to predict whether a row comes from test or train distribution.\n\n```\nimport lightgbm as lgb\nfrom task_substitution.train_test_similarity import *\nfrom sklearn.metrics import roc_auc_score\n\ntrain = train.drop('original_target', axis=1)\n\nsplit_args = {'test_size': 0.2, 'random_state': 41}\n\nmodel_args = {\n    'num_boost_round': 100,\n    'objective': 'binary',\n    'learning_rate': 0.1,\n    'num_leaves': 31,\n    'nthread': -1,\n    'verbosity': -1,\n    'seed': 41\n}\n\n# cat_flds: categorical features in the original dataset.\n# ignore_flds: features you want to ignore. ( these won't be used by LightGBM model )\n\ntts = TrainTestSimilarity(cat_flds=[], \n                          ignore_flds=None,\n                          perf_fn=roc_auc_score,\n                          split_args=split_args, \n                          model_args=model_args)\ntts.run(train, test)\n\n# to get feature importance\nfig, ax = plt.subplots(1, figsize=(16, 10)\nlgb.plot_importance(tts.trained_model, ax=ax, max_num_features=5, importance_type='gain')\n```\n\n**Feature Selection using Null Importance**\n\n>We use LightGBM model to remove features based on null importance.\n\n```\nfrom sklearn.datasets import load_boston\ndata = load_boston()\n\nX = pd.DataFrame(data['data'], columns=data['feature_names'])\ny = pd.Series(data['target'])\n\nmodel_args = {\n    'num_boost_round': 300,\n    'objective': 'regression',\n    'learning_rate': 0.1,\n    'num_leaves': 31,\n    'nthread': -1,\n    'verbosity': -1,\n    'seed': 41\n}\n\nprint(f'Feature list: {X.columns.tolist()}')\nfs = FeatureSelection(model_args)\nselected_features = fs.select_features(X, y)\nprint(f'Selected features: {selected_features}')\n\n# ratio of feature importance of useless / useful model\nprint(fs.ratio_df)\n```\n\n**Feature Importance for GBDT**\n\n>Make your GBDT more explainable by calculating feature contribution that reveals the relationship between specific instance and related output. Now in addition to global feature importance we can breakdown our predictions and find out which features played positive or negative role during inference.\n\n```\nx,y = make_regression(n_samples=1000,n_features=6,n_informative=3)\nxtr, xval, ytr, yval = train_test_split(x, y, test_size=0.5, random_state=41)\n\nmodel = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10, max_depth=4)\nmodel.fit(xtr,ytr)\n\nlfi     = LocalFeatureImportance(model)\nscores  = lfi.propagate_scores()\n\nsi = pd.DataFrame(xval[5:6, :], columns=['f0', 'f1', 'f2', 'f3', 'f4', 'f5'])\nfc = lfi.get_fi(scores, si)\n```\n\n\n## Contributing\n\nIf you want to contribute to `task_substitution` please refer to [contributions guidelines](./CONTRIBUTING.md)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/abhishek/task_substitution", "keywords": "ML", "license": "Apache Software License 2.0", "maintainer": "", "maintainer_email": "", "name": "task-substitution", "package_url": "https://pypi.org/project/task-substitution/", "platform": "", "project_url": "https://pypi.org/project/task-substitution/", "project_urls": {"Homepage": "https://github.com/abhishek/task_substitution"}, "release_url": "https://pypi.org/project/task-substitution/0.0.3/", "requires_dist": ["cycler (==0.10.0)", "joblib (==0.14.1)", "kiwisolver (==1.2.0)", "lightgbm (==2.3.1)", "matplotlib (==3.2.1)", "numpy (==1.18.2)", "pandas (==1.0.3)", "pyparsing (==2.4.7)", "python-dateutil (==2.8.1)", "scikit-learn (==0.22.2.post1)", "scipy (==1.4.1)", "six (==1.14.0)", "xgboost (==1.0.2)"], "requires_python": ">=3.6", "summary": "Solve a different task which could help solve the main task", "version": "0.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>task_substitution</h1>\n<blockquote>\n<p>Solve an auxiliary task using ML.</p>\n</blockquote>\n<p><strong>This library is created by using <a href=\"https://github.com/fastai/nbdev\" rel=\"nofollow\">nbdev</a>, please check it out.</strong></p>\n<p><strong>Task Substitution</strong> is a method of solving an auxiliary problem ( with different features and different target ) in order to better understand the initial problem and solving it efficiently.</p>\n<p>Let's take a look at standard machine learning task, in the figure below you see a regression task with features <code>f1</code>, <code>f2</code>, <code>f3</code> and target variable <code>y</code>.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0380093e307e13039cac00c1ce7dbc0bb134bef1/696d616765732f747261696e696e675f7365742e706e67\">\n<p>We want to build on a model on the above dataset to predict for unknown <code>y</code> values in the test dataset shown below.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6bc490c61d2936d3b71dc66e24bcb5d5d8f47822/696d616765732f746573742e706e67\">\n<p><strong>Exploratory Data Analysis</strong></p>\n<p>First step we take to solve the problem is to look at the data, there can be many features with <em>missing values</em> or <em>outliers</em> which needs to be understood. It is possible that there is a relationship between a missing value and values of other features.</p>\n<h2>Recover Missing Values</h2>\n<p>It is possible for a feature to have a missing value, it could be a data recording issue or bug etc. Often times for numerical features we replace missing value with <code>mean</code> or <code>median</code> value as a approximation. Sometimes we replace missing value with values like <code>-9999</code> so that model treats them differently or sometimes we leave them as is as libraries like <code>xgboost</code> and <code>lightgbm</code> can handle <code>nulls</code>. Let's look at following dataset</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bebae91e040c32b26b533b670aa45ae313ca1984/696d616765732f6d697373696e675f66756c6c2e706e67\">\n<p>Here we have a feature <code>f3</code> with missing values, this is a numerical feature, what we can do is that we can consider <code>f3</code> as target feature and reframe this as regresion problem where we try to predict for missing values.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d6a8881bfcb07ff2fe8592ea04b0bed8f4e4b258/696d616765732f6d697373696e675f747261696e2e706e67\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/26475a349d13535d2fa262b9baa108c29bebdee3/696d616765732f6d697373696e675f746573742e706e67\">\n<p>The above setup is identical to the original regression task, here we would build a model to use <code>f1</code> and <code>f2</code> to predict for <code>f3</code>. So instead of using <code>mean</code>, <code>median</code> etc. we can build a model to restore missing values which can help us solve the original problem efficiently.</p>\n<p>We have to be careful to not overfit when building such models.</p>\n<h2>Check Train Test Distributions</h2>\n<p>Whenever we train a model we want to use it on a new sample, but what if the new sample comes from a different distribution compared to the data on which the model was trained on. When we deploy our solutions on production we want to be very careful of this change as our models would fail if there is a mismatch in train and test sets. We can pose this problem as an auxiliary task and create a new binary target <code>y</code>, where <code>1</code> represents whether row comes from <code>test</code> set and <code>0</code> represents whether it comes from <code>train</code> set and then we train our model to predict whether a row comes from <code>train</code> or <code>test</code> set if the performance ( e.g. <code>AUC</code> score ) is high we can conclude that the train and test set come from different distributions. Ofcourse, we need to remove the <code>original target</code> from the analysis.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/602194bdf9ab36606675e3153ccddc3376848549/696d616765732f74746d5f747261696e2e706e67\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ed688c13d3248fbac230bbaa8e5eba66fddba9ca/696d616765732f74746d5f746573742e706e67\">\n<p><strong>In the above images you can see two different datasets, we want to verify whether these two come from same distributions or not.</strong></p>\n<p>Consider the first example set as training set and second one as test set for this example.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ac1148f04173bfe8bdcdea98b4b20e72e03fa32a/696d616765732f74746d5f747261696e5f776974685f7461726765742e706e67\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/58be81c74b28cad0e8b114fcbc1ca4babc3aa39c/696d616765732f74746d5f746573745f776974685f7461726765742e706e67\">\n<p>We create a new target called <code>is_test</code> which denotes whether a row belongs to test set or not.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8f33ab4ec88c8053d89d579f662ff2469ecebab1/696d616765732f74746d5f747261696e5f746573745f636f6d62696e65642e706e67\">\n<p><strong>Then we combine training and test set and train a model to predict whether a row comes from train or test set, if our model performs well then we know that these two datasets are from different distributions.</strong></p>\n<p>We would still have to dig deep into looking at whether that's the case but the above method can help identifying which features are have drifted apart in train and test datasets. If you look at feature importance of the model that was used to separated train and test apart you can identify such features.</p>\n<h2>Feature Selection using Null Importance</h2>\n<p>Often we create many features to enhance the power of our ML model, but not all features are beneficial to model performance. There needs to be a way to identify\nfeatures which can introduce <code>noise</code> to the model and hence must be removed. We can make use of <code>null importance</code> to identify and remove such features.</p>\n<ul>\n<li>Use LightGBM model to train on all features and obtain feature importance ( gain ), call it gain_1</li>\n<li>For the same data shuffle the target variable and train model and obtain feature importance ( gain ), call it gain_2</li>\n<li>Calculate ratio = gain_2 / gain_1 and remove features with ratio &gt; 3.</li>\n<li>Use the remaining features to train the model again.</li>\n</ul>\n<p><strong>Note: The ratio threshold of <code>3</code> seems arbitrary here, but we can treat this as <code>hyper-parameter</code> and choose the value that gives us the best result on the CV.</strong></p>\n<p>Dataframe representing feature importance of useful, useless model and their ratio.\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7ca7e44c097028a60b8639c943490fb0aaafb518/696d616765732f666561747572655f696d702e706e67\"></p>\n<h2>Feature Importance</h2>\n<p>Gradient Boosting Trees are very powerful methods when it comes to solving problems in industry and often are the first choice of every practitioner. We often to look at feature importance which different implementations of GBDT provide in help us understanding what features are useful. But during the inference when we have to predict for an unseen example, e.g. in a classification task if our model spits out <code>0</code> or <code>1</code> it would be useful to know which features were instrumental in identifying it as positive or negative class. This formulation can be extended to multi-classification and regression problems.</p>\n<ul>\n<li>This module implements this <a href=\"http://www.cs.sjtu.edu.cn/%7Ekzhu/papers/kzhu-infocode.pdf\" rel=\"nofollow\">paper</a></li>\n<li>It currently supports XGBoost model and will soon support other implementations.</li>\n<li>We just pass our trained model and it converts the tree representation into a dataframe.</li>\n</ul>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bd5e11bc5b51a6ec74c9de978d17c9f547a28192/696d616765732f74726565735f746f5f64662e706e67\">\n- Then we calculate score for each node of the tree where we start off from the terminal node and then propagate it up to the root node.\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/085071394170e3204d4030bc21e615f49d5dbec7/696d616765732f666561747572655f636f6e747269627574696f6e2e706e67\">\n<h2>Install</h2>\n<p>task_substitution in on pypi:</p>\n<pre><code>pip install task_substitution\n</code></pre>\n<p>For an editable install, use the following:</p>\n<pre><code>git clone https://github.com/numb3r33/task_substitution.git\npip install -e task_substitution\n</code></pre>\n<h2>How to use</h2>\n<p><strong>Recover Missing Values</strong></p>\n<blockquote>\n<p>Currently we only support missing value recovery for numerical features, we plan to extend support for other feature types as well. Also the model currently uses LightGBM model to recover missing values.</p>\n</blockquote>\n<pre><code>from task_substitution.recover_missing import *\nfrom sklearn.metrics import mean_squared_error\n\ntrain = train.drop('original_target', axis=1)\n\nmodel_args = {\n          'objective': 'regression',\n          'learning_rate': 0.1,\n          'num_leaves': 31,\n          'min_data_in_leaf': 100,\n          'num_boost_round': 100,\n          'verbosity': -1,\n          'seed': 41\n             }\n\nsplit_args = {\n    'test_size': .2,\n    'random_state': 41\n}\n\n# target_fld: feature with missing values.\n# cat_flds: categorical features in the original dataset.\n# ignore_flds: features you want to ignore. ( these won't be used by LightGBM model to recover missing values)\n\nrec = RecoverMissing(target_fld='f3',\n                     cat_flds=[],\n                     ignore_flds=['f2'],\n                     perf_fn=lambda tr,pe: np.sqrt(mean_squared_error(tr, pe)),\n                     split_args=split_args,\n                     model_args=model_args\n                    )\n\ntrain_recovered = rec.run(train)\n</code></pre>\n<p><strong>Check train test distributions</strong></p>\n<blockquote>\n<p>We use LightGBM model to predict whether a row comes from test or train distribution.</p>\n</blockquote>\n<pre><code>import lightgbm as lgb\nfrom task_substitution.train_test_similarity import *\nfrom sklearn.metrics import roc_auc_score\n\ntrain = train.drop('original_target', axis=1)\n\nsplit_args = {'test_size': 0.2, 'random_state': 41}\n\nmodel_args = {\n    'num_boost_round': 100,\n    'objective': 'binary',\n    'learning_rate': 0.1,\n    'num_leaves': 31,\n    'nthread': -1,\n    'verbosity': -1,\n    'seed': 41\n}\n\n# cat_flds: categorical features in the original dataset.\n# ignore_flds: features you want to ignore. ( these won't be used by LightGBM model )\n\ntts = TrainTestSimilarity(cat_flds=[], \n                          ignore_flds=None,\n                          perf_fn=roc_auc_score,\n                          split_args=split_args, \n                          model_args=model_args)\ntts.run(train, test)\n\n# to get feature importance\nfig, ax = plt.subplots(1, figsize=(16, 10)\nlgb.plot_importance(tts.trained_model, ax=ax, max_num_features=5, importance_type='gain')\n</code></pre>\n<p><strong>Feature Selection using Null Importance</strong></p>\n<blockquote>\n<p>We use LightGBM model to remove features based on null importance.</p>\n</blockquote>\n<pre><code>from sklearn.datasets import load_boston\ndata = load_boston()\n\nX = pd.DataFrame(data['data'], columns=data['feature_names'])\ny = pd.Series(data['target'])\n\nmodel_args = {\n    'num_boost_round': 300,\n    'objective': 'regression',\n    'learning_rate': 0.1,\n    'num_leaves': 31,\n    'nthread': -1,\n    'verbosity': -1,\n    'seed': 41\n}\n\nprint(f'Feature list: {X.columns.tolist()}')\nfs = FeatureSelection(model_args)\nselected_features = fs.select_features(X, y)\nprint(f'Selected features: {selected_features}')\n\n# ratio of feature importance of useless / useful model\nprint(fs.ratio_df)\n</code></pre>\n<p><strong>Feature Importance for GBDT</strong></p>\n<blockquote>\n<p>Make your GBDT more explainable by calculating feature contribution that reveals the relationship between specific instance and related output. Now in addition to global feature importance we can breakdown our predictions and find out which features played positive or negative role during inference.</p>\n</blockquote>\n<pre><code>x,y = make_regression(n_samples=1000,n_features=6,n_informative=3)\nxtr, xval, ytr, yval = train_test_split(x, y, test_size=0.5, random_state=41)\n\nmodel = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=10, max_depth=4)\nmodel.fit(xtr,ytr)\n\nlfi     = LocalFeatureImportance(model)\nscores  = lfi.propagate_scores()\n\nsi = pd.DataFrame(xval[5:6, :], columns=['f0', 'f1', 'f2', 'f3', 'f4', 'f5'])\nfc = lfi.get_fi(scores, si)\n</code></pre>\n<h2>Contributing</h2>\n<p>If you want to contribute to <code>task_substitution</code> please refer to <a href=\"./CONTRIBUTING.md\" rel=\"nofollow\">contributions guidelines</a></p>\n\n          </div>"}, "last_serial": 6970964, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "e77c32a2b24e09ee35573911e2e0c71d", "sha256": "df93579d19d070cff9233de1672880efa8ed410774cc0245bb92845051cf7c86"}, "downloads": -1, "filename": "task_substitution-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "e77c32a2b24e09ee35573911e2e0c71d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 14733, "upload_time": "2020-02-09T11:39:36", "upload_time_iso_8601": "2020-02-09T11:39:36.843344Z", "url": "https://files.pythonhosted.org/packages/77/5b/553a7750f09c2b75322c9a898ba5498a679935565911fe7ffa79ef273cca/task_substitution-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "10f5eb8dc261dd7001708f87cd4b40f8", "sha256": "6ab7eef01d6e095cce6431f58c8170d4f80544386b5ca5e569573ac9556fa8b7"}, "downloads": -1, "filename": "task_substitution-0.0.1.tar.gz", "has_sig": false, "md5_digest": "10f5eb8dc261dd7001708f87cd4b40f8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16273, "upload_time": "2020-02-09T11:39:39", "upload_time_iso_8601": "2020-02-09T11:39:39.643607Z", "url": "https://files.pythonhosted.org/packages/bb/60/e09abf6f5af34789ea85a71993ca92d32c707f5ffb0d26490507f96b55f6/task_substitution-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "e0b699d736d35bcbb5ffdad47ad8ea6a", "sha256": "e7aa5951c6e78616f7437d7f080ebe32eb1e22d83030a01f30c81fabc7e7ca1d"}, "downloads": -1, "filename": "task_substitution-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "e0b699d736d35bcbb5ffdad47ad8ea6a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 15764, "upload_time": "2020-03-23T17:40:54", "upload_time_iso_8601": "2020-03-23T17:40:54.580708Z", "url": "https://files.pythonhosted.org/packages/3f/de/ebfc39ab25a2f65d44f2c51b6b898e6ae6aa795fbfd1a30003a13ce89dc0/task_substitution-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6c00caa22d3108b3a32780e5dc9ced70", "sha256": "991b4533f2779d843a641d705625bf04b92185ea7e58cbcc0313d9de616eddbd"}, "downloads": -1, "filename": "task_substitution-0.0.2.tar.gz", "has_sig": false, "md5_digest": "6c00caa22d3108b3a32780e5dc9ced70", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17034, "upload_time": "2020-03-23T17:40:56", "upload_time_iso_8601": "2020-03-23T17:40:56.465914Z", "url": "https://files.pythonhosted.org/packages/ea/73/bc323eed53e3bb13550d763e1c3bd794821e2961a50f5397291b78e43d58/task_substitution-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "a30e0776d0dc0130237b0d18f3302ba8", "sha256": "2159ef76c06ca861cddb308c1ffbe5bf30376d1472ab29741ba3e3002089285c"}, "downloads": -1, "filename": "task_substitution-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "a30e0776d0dc0130237b0d18f3302ba8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19781, "upload_time": "2020-04-07T15:49:01", "upload_time_iso_8601": "2020-04-07T15:49:01.288276Z", "url": "https://files.pythonhosted.org/packages/52/a1/e688f3ee5b140d0f021fc8cbcd7d4a34ca675107e1929df54b1531c241a1/task_substitution-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "250a4a09ed1163c6b8318643756508fb", "sha256": "fcbe92c47d7759cd85e6ba2ffb7b0573742433e6b08c86c6bd6e6246a1baa905"}, "downloads": -1, "filename": "task_substitution-0.0.3.tar.gz", "has_sig": false, "md5_digest": "250a4a09ed1163c6b8318643756508fb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 22012, "upload_time": "2020-04-07T15:49:03", "upload_time_iso_8601": "2020-04-07T15:49:03.908816Z", "url": "https://files.pythonhosted.org/packages/c3/85/a322574f90a286b9d395cabc64a9b01d0ca0888699a5df870e2056bd6bf1/task_substitution-0.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a30e0776d0dc0130237b0d18f3302ba8", "sha256": "2159ef76c06ca861cddb308c1ffbe5bf30376d1472ab29741ba3e3002089285c"}, "downloads": -1, "filename": "task_substitution-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "a30e0776d0dc0130237b0d18f3302ba8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19781, "upload_time": "2020-04-07T15:49:01", "upload_time_iso_8601": "2020-04-07T15:49:01.288276Z", "url": "https://files.pythonhosted.org/packages/52/a1/e688f3ee5b140d0f021fc8cbcd7d4a34ca675107e1929df54b1531c241a1/task_substitution-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "250a4a09ed1163c6b8318643756508fb", "sha256": "fcbe92c47d7759cd85e6ba2ffb7b0573742433e6b08c86c6bd6e6246a1baa905"}, "downloads": -1, "filename": "task_substitution-0.0.3.tar.gz", "has_sig": false, "md5_digest": "250a4a09ed1163c6b8318643756508fb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 22012, "upload_time": "2020-04-07T15:49:03", "upload_time_iso_8601": "2020-04-07T15:49:03.908816Z", "url": "https://files.pythonhosted.org/packages/c3/85/a322574f90a286b9d395cabc64a9b01d0ca0888699a5df870e2056bd6bf1/task_substitution-0.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:57:43 2020"}