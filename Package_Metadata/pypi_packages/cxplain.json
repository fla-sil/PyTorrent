{"info": {"author": "Patrick Schwab", "author_email": "patrick.schwab@hest.ethz.ch", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent"], "description": "![CXPlain](http://schwabpatrick.com/img/cxplain_logo.png)\n\n![Code Coverage](https://img.shields.io/badge/Python-2.7,%203.7-blue)![Code Coverage](https://img.shields.io/badge/Coverage-88%25-green)\n\nCausal Explanations (CXPlain) is a method for explaining the decisions of any machine-learning model. CXPlain uses explanation models trained with a causal objective to learn to explain machine-learning models, and to quantify the uncertainty of its explanations. This repository contains a reference implementation for neural explanation models, and several practical examples for different data modalities. Please see the manuscript at https://arxiv.org/abs/1910.12336 (NeurIPS 2019) for a description and experimental evaluation of CXPlain.\n\n## Install\n\nTo install the latest release:\n\n```\n$ pip install cxplain\n```\n\n## Use\n\nA CXPlain model consists of four main components:\n- The model to be explained which can be any type of machine-learning model, including black-box models, such as neural networks and ensemble models.\n- The model builder that defines the structure of the explanation model to be used to explain the explained model.\n- The masking operation that defines how CXPlain will internally simulate the removal of input features from the set of available features.\n- The loss function that defines how the change in prediction accuracy incurred by removing an input feature will be measured by CXPlain.\n\nAfter configuring these four components, you can fit a CXPlain instance to the same training data that was used to train your original model. The CXPlain instance can then explain any prediction of your explained model - even when no labels are available for that sample.\n\n```python\nfrom tensorflow.python.keras.losses import categorical_crossentropy\nfrom cxplain import MLPModelBuilder, ZeroMasking, CXPlain\n\nx_train, y_train, x_test = ....  # Your dataset\nexplained_model = ...    # The model you wish to explain.\n\n# Define the model you want to use to explain your __explained_model__.\n# Here, we use a neural explanation model with a\n# multilayer perceptron (MLP) architecture.\nmodel_builder = MLPModelBuilder(num_layers=2, num_units=64, batch_size=256, learning_rate=0.001)\n\n# Define your masking operation - the method of simulating the\n# removal of input features used internally by CXPlain - ZeroMasking is typically a sensible default choice for tabular and image data.\nmasking_operation = ZeroMasking()\n\n# Define the loss with which each input features' associated reduction in prediction error is calculated.\nloss = categorical_crossentropy\n\n# Build and fit a CXPlain instance.\nexplainer = CXPlain(explained_model, model_builder, masking_operation, loss)\nexplainer.fit(x_train, y_train)\n\n# Use the __explainer__ to obtain explanations for the predictions of your __explained_model__.\nattributions = explainer.explain(x_test)\n```\n\n## Examples\n\nMore practical examples for various input data modalities, including images, textual data and tabular data, and both regression and classification tasks are provided in form of Jupyter notebooks in the [examples/](examples) directory:\n- [Regression task on tabular data (Boston Housing)](examples/boston_housing.ipynb)\n- [Classification task on image data (CIFAR10)](examples/cifar10.ipynb)\n- [Classification task on image data (MNIST)](examples/mnist.ipynb)\n- [Classification task on textual data (IMDB)](examples/nlp.ipynb)\n- [Saving and loading CXPlain instances](examples/save_and_load.ipynb)\n\n![MNIST](http://schwabpatrick.com/img/mnist_samples.png)\n![ImageNet](http://schwabpatrick.com/img/imagenet_samples.png)\n<img src=\"http://schwabpatrick.com/img/twitter_samples.png\" width=\"310\">\n## Cite\n\nPlease consider citing, if you reference or use our methodology, code or results in your work:\n\n    @inproceedings{schwab2019cxplain,\n      title={{CXPlain: Causal Explanations for Model Interpretation under Uncertainty}},\n      author={Schwab, Patrick and Karlen, Walter},\n      booktitle={{Advances in Neural Information Processing Systems (NeurIPS)}},\n      year={2019}\n    }\n\n## License\n\n[MIT License](LICENSE.txt)\n\n## Acknowledgements\n\nThis work was partially funded by the Swiss National Science Foundation (SNSF) project No. 167302 within the National Research Program (NRP) 75 \"Big Data\". We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPUs used for this research. Patrick Schwab is an affiliated PhD fellow at the Max Planck ETH Center for Learning Systems.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://schwabpatrick.com", "keywords": "", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "cxplain", "package_url": "https://pypi.org/project/cxplain/", "platform": "", "project_url": "https://pypi.org/project/cxplain/", "project_urls": {"Homepage": "http://schwabpatrick.com"}, "release_url": "https://pypi.org/project/cxplain/1.0.3/", "requires_dist": null, "requires_python": "", "summary": "", "version": "1.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"CXPlain\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/68e4d93016872ffa7ad113bc1c27b3a6a95d26c2/687474703a2f2f7363687761627061747269636b2e636f6d2f696d672f6378706c61696e5f6c6f676f2e706e67\"></p>\n<p><img alt=\"Code Coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7586daab3f31090a12a7a5e4325aeb12d6a5c5c2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d322e372c253230332e372d626c7565\"><img alt=\"Code Coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1651cd0a4fe7307c6c9d78257774cec8f68a0cb1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f7665726167652d38382532352d677265656e\"></p>\n<p>Causal Explanations (CXPlain) is a method for explaining the decisions of any machine-learning model. CXPlain uses explanation models trained with a causal objective to learn to explain machine-learning models, and to quantify the uncertainty of its explanations. This repository contains a reference implementation for neural explanation models, and several practical examples for different data modalities. Please see the manuscript at <a href=\"https://arxiv.org/abs/1910.12336\" rel=\"nofollow\">https://arxiv.org/abs/1910.12336</a> (NeurIPS 2019) for a description and experimental evaluation of CXPlain.</p>\n<h2>Install</h2>\n<p>To install the latest release:</p>\n<pre><code>$ pip install cxplain\n</code></pre>\n<h2>Use</h2>\n<p>A CXPlain model consists of four main components:</p>\n<ul>\n<li>The model to be explained which can be any type of machine-learning model, including black-box models, such as neural networks and ensemble models.</li>\n<li>The model builder that defines the structure of the explanation model to be used to explain the explained model.</li>\n<li>The masking operation that defines how CXPlain will internally simulate the removal of input features from the set of available features.</li>\n<li>The loss function that defines how the change in prediction accuracy incurred by removing an input feature will be measured by CXPlain.</li>\n</ul>\n<p>After configuring these four components, you can fit a CXPlain instance to the same training data that was used to train your original model. The CXPlain instance can then explain any prediction of your explained model - even when no labels are available for that sample.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tensorflow.python.keras.losses</span> <span class=\"kn\">import</span> <span class=\"n\">categorical_crossentropy</span>\n<span class=\"kn\">from</span> <span class=\"nn\">cxplain</span> <span class=\"kn\">import</span> <span class=\"n\">MLPModelBuilder</span><span class=\"p\">,</span> <span class=\"n\">ZeroMasking</span><span class=\"p\">,</span> <span class=\"n\">CXPlain</span>\n\n<span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"o\">....</span>  <span class=\"c1\"># Your dataset</span>\n<span class=\"n\">explained_model</span> <span class=\"o\">=</span> <span class=\"o\">...</span>    <span class=\"c1\"># The model you wish to explain.</span>\n\n<span class=\"c1\"># Define the model you want to use to explain your __explained_model__.</span>\n<span class=\"c1\"># Here, we use a neural explanation model with a</span>\n<span class=\"c1\"># multilayer perceptron (MLP) architecture.</span>\n<span class=\"n\">model_builder</span> <span class=\"o\">=</span> <span class=\"n\">MLPModelBuilder</span><span class=\"p\">(</span><span class=\"n\">num_layers</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">num_units</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Define your masking operation - the method of simulating the</span>\n<span class=\"c1\"># removal of input features used internally by CXPlain - ZeroMasking is typically a sensible default choice for tabular and image data.</span>\n<span class=\"n\">masking_operation</span> <span class=\"o\">=</span> <span class=\"n\">ZeroMasking</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Define the loss with which each input features' associated reduction in prediction error is calculated.</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">categorical_crossentropy</span>\n\n<span class=\"c1\"># Build and fit a CXPlain instance.</span>\n<span class=\"n\">explainer</span> <span class=\"o\">=</span> <span class=\"n\">CXPlain</span><span class=\"p\">(</span><span class=\"n\">explained_model</span><span class=\"p\">,</span> <span class=\"n\">model_builder</span><span class=\"p\">,</span> <span class=\"n\">masking_operation</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"p\">)</span>\n<span class=\"n\">explainer</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Use the __explainer__ to obtain explanations for the predictions of your __explained_model__.</span>\n<span class=\"n\">attributions</span> <span class=\"o\">=</span> <span class=\"n\">explainer</span><span class=\"o\">.</span><span class=\"n\">explain</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">)</span>\n</pre>\n<h2>Examples</h2>\n<p>More practical examples for various input data modalities, including images, textual data and tabular data, and both regression and classification tasks are provided in form of Jupyter notebooks in the <a href=\"examples\" rel=\"nofollow\">examples/</a> directory:</p>\n<ul>\n<li><a href=\"examples/boston_housing.ipynb\" rel=\"nofollow\">Regression task on tabular data (Boston Housing)</a></li>\n<li><a href=\"examples/cifar10.ipynb\" rel=\"nofollow\">Classification task on image data (CIFAR10)</a></li>\n<li><a href=\"examples/mnist.ipynb\" rel=\"nofollow\">Classification task on image data (MNIST)</a></li>\n<li><a href=\"examples/nlp.ipynb\" rel=\"nofollow\">Classification task on textual data (IMDB)</a></li>\n<li><a href=\"examples/save_and_load.ipynb\" rel=\"nofollow\">Saving and loading CXPlain instances</a></li>\n</ul>\n<p><img alt=\"MNIST\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f4dc898638937dcce4052989b353425716ff3a10/687474703a2f2f7363687761627061747269636b2e636f6d2f696d672f6d6e6973745f73616d706c65732e706e67\">\n<img alt=\"ImageNet\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/614a6cc2a5247af27f9d15a6bc965f4a858c2caa/687474703a2f2f7363687761627061747269636b2e636f6d2f696d672f696d6167656e65745f73616d706c65732e706e67\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b0666544ebf0fc1af05e9692919fbc802367f409/687474703a2f2f7363687761627061747269636b2e636f6d2f696d672f747769747465725f73616d706c65732e706e67\" width=\"310\"></p>\n<h2>Cite</h2>\n<p>Please consider citing, if you reference or use our methodology, code or results in your work:</p>\n<pre><code>@inproceedings{schwab2019cxplain,\n  title={{CXPlain: Causal Explanations for Model Interpretation under Uncertainty}},\n  author={Schwab, Patrick and Karlen, Walter},\n  booktitle={{Advances in Neural Information Processing Systems (NeurIPS)}},\n  year={2019}\n}\n</code></pre>\n<h2>License</h2>\n<p><a href=\"LICENSE.txt\" rel=\"nofollow\">MIT License</a></p>\n<h2>Acknowledgements</h2>\n<p>This work was partially funded by the Swiss National Science Foundation (SNSF) project No. 167302 within the National Research Program (NRP) 75 \"Big Data\". We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPUs used for this research. Patrick Schwab is an affiliated PhD fellow at the Max Planck ETH Center for Learning Systems.</p>\n\n          </div>"}, "last_serial": 6527871, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "73300cd70f15c6b47955995ac718098e", "sha256": "656e121dc5f3ef1a5326d1c17458a3f97fb354311690c7f7a22fa38289bae2f0"}, "downloads": -1, "filename": "cxplain-1.0.1.tar.gz", "has_sig": false, "md5_digest": "73300cd70f15c6b47955995ac718098e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32542, "upload_time": "2019-12-17T00:51:59", "upload_time_iso_8601": "2019-12-17T00:51:59.458482Z", "url": "https://files.pythonhosted.org/packages/3e/e1/7e0a87b212e82f0f9e17200bd8ae0d2b817a7233b9b61f04a3947b7e3e45/cxplain-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "45166557b67720bd7201502479952ad4", "sha256": "79f8e6552da2f7fc7df9dfa5ceea3231cd714782bc9109f371366a0b4876657f"}, "downloads": -1, "filename": "cxplain-1.0.2.tar.gz", "has_sig": false, "md5_digest": "45166557b67720bd7201502479952ad4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32434, "upload_time": "2020-01-08T16:14:22", "upload_time_iso_8601": "2020-01-08T16:14:22.031849Z", "url": "https://files.pythonhosted.org/packages/69/8a/11a101c3d383294716e317fc70bf8fd10af448df91408f0b5bc042d7488c/cxplain-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "befc36873566d14516c7df6a7ad8c055", "sha256": "af0b6f780f3a54a55c2da21257ad136ee972dcaec9a623a435f31e877bb8564d"}, "downloads": -1, "filename": "cxplain-1.0.3.tar.gz", "has_sig": false, "md5_digest": "befc36873566d14516c7df6a7ad8c055", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32566, "upload_time": "2020-01-27T18:28:44", "upload_time_iso_8601": "2020-01-27T18:28:44.591927Z", "url": "https://files.pythonhosted.org/packages/d6/e8/0ccf470ac093e25e5c0bcf913ea7461980e0ea4ec052692713f12b37e8bc/cxplain-1.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "befc36873566d14516c7df6a7ad8c055", "sha256": "af0b6f780f3a54a55c2da21257ad136ee972dcaec9a623a435f31e877bb8564d"}, "downloads": -1, "filename": "cxplain-1.0.3.tar.gz", "has_sig": false, "md5_digest": "befc36873566d14516c7df6a7ad8c055", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32566, "upload_time": "2020-01-27T18:28:44", "upload_time_iso_8601": "2020-01-27T18:28:44.591927Z", "url": "https://files.pythonhosted.org/packages/d6/e8/0ccf470ac093e25e5c0bcf913ea7461980e0ea4ec052692713f12b37e8bc/cxplain-1.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:41:09 2020"}