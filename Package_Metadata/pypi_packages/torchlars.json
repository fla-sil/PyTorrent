{"info": {"author": "Kakao Brain", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3 :: Only", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# torchlars\n\n[![PyPI](https://img.shields.io/pypi/v/torchlars.svg)](https://pypi.org/project/torchlars)\n[![Build Status](https://travis-ci.org/kakaobrain/torchlars.svg?branch=master)](https://travis-ci.org/kakaobrain/torchlars)\n\nA [LARS](https://arxiv.org/abs/1708.03888) implementation in PyTorch.\n\n```python\nfrom torchlars import LARS\noptimizer = LARS(optim.SGD(model.parameters(), lr=0.1))\n```\n\n## What is LARS?\n\nLARS (Layer-wise Adaptive Rate Scaling) is an optimization algorithm designed\nfor large-batch training published by You, Gitman, and Ginsburg, which\ncalculates the local learning rate per layer at each optimization step.\nAccording to the paper, when training ResNet-50 on ImageNet ILSVRC (2016)\nclassification task with LARS, the learning curve and the best top-1 accuracy\nstay similar to that of the baseline (training with batch size 256 without\nLARS) even if the batch size is scaled up to 32K.\n\n[Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)\n\nOriginally, LARS is formulated in terms of SGD optimizer and extension to other\noptimizers was not mentioned in the paper. In contrast, `torchlars` implements\nLARS as a wrapper which can take any optimizers including SGD as the base.\n\nAdditionally, LARS of torchlars is designed to more consider operation in the\nCUDA environment compared to existing implementations. Thanks to this, you can\nsee only the little speed loss appears compared to just using SGD, in an\nenvironment where CPU to GPU synchronization does not occur.\n\n## Usage\n\nCurrently, torchlars requires the following environments:\n\n- Linux\n- Python 3.6+\n- PyTorch 1.1+\n- CUDA 10+\n\nTo use torchlars, install it via PyPI:\n\n```bash\n$ pip install torchlars\n```\n\nTo use LARS, simply wrap your base optimizer with `torchlars.LARS`. LARS\ninherits `torch.optim.Optimizer`, so you can simply use LARS as optimizer on\nyour code. After then, when you call step method of LARS, LARS automatically\ncalculates local learning rate before running base optimizer such as SGD or\nAdam\n\nThe example code below shows how to use LARS using SGD as base optimizer.\n\n```python\nfrom torchlars import LARS\n\nbase_optimizer = optim.SGD(model.parameters(), lr=0.1)\noptimizer = LARS(optimizer=base_optimizer, eps=1e-8, trust_coef=0.001)\n\noutput = model(input)\nloss = loss_fn(output, target)\nloss.backward()\n\noptimizer.step()\n```\n\n## Benchmarking\n\n### ResNet-50 on ImageNet classification\n\n| Batch Size | LR policy    | lr   | warm-up | epoch | Best Top-1 accuracy, % |\n| :--------- | :----------- | :--- | :------ | :---- | :--------------------- |\n| 256        | poly(2)      | 0.2  | N/A     | 90    | 73.79                  |\n| 8k         | LARS+poly(2) | 12.8 | 5       | 90    | 73.78                  |\n| 16K        | LARS+poly(2) | 25.0 | 5       | 90    | 73.36                  |\n| 32K        | LARS+poly(2) | 29.0 | 5       | 90    | 72.26                  |\n\n![](img/resnet50_test_learning_curves.jpg)\n\nAbove image and table show the reproduced performance benchmark on ResNet-50,\nas reported in Table 4 and Figure 5 of the paper.\n\nThe cyan line represents the baseline result, which is training result with\nbatch size 256, and others represent training result of 8K, 16K, 32K\nrespectively. As you see, every result shows a similar learning curve and best\ntop-1 accuracy.\n\nMost experimental conditions are similar to used in the paper, but we slightly\nchange some conditions like learning rate to observe comparable results as\nproposed by the LARS paper.\n\n_Note: We refer\n[log file](https://people.eecs.berkeley.edu/~youyang/publications/batch/)\nprovided by paper to obtain above hyper-parameters._\n\n## Authors and Licensing\n\ntorchlars project is developed by [Chunmyong Park] at [Kakao Brain], with\n[Heungsub Lee], [Myungryong Jeong], [Woonhyuk Baek], and [Chiheon Kim]'s help.\nIt is distributed under [Apache License 2.0](LICENSE).\n\n[chiheon kim]: https://github.com/chiheonk\n[chunmyong park]: https://github.com/cmpark0126\n[heungsub lee]: https://subl.ee/\n[kakao brain]: https://kakaobrain.com/\n[woonhyuk baek]: https://github.com/wbaek\n[myungryong jeong]: https://github.com/mrJeong\n\n## Citation\n\nIf you apply this library to any project and research, please cite our code:\n\n```\n@misc{torchlars,\n  author       = {Park, Chunmyong and Lee, Heungsub and Jeong, Myungryong and\n                  Baek, Woonhyuk and Kim, Chiheon},\n  title        = {torchlars, {A} {LARS} implementation in {PyTorch}},\n  howpublished = {\\url{https://github.com/kakaobrain/torchlars}},\n  year         = {2019}\n}\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "", "maintainer": "Chunmyong Park", "maintainer_email": "", "name": "torchlars", "package_url": "https://pypi.org/project/torchlars/", "platform": "", "project_url": "https://pypi.org/project/torchlars/", "project_urls": null, "release_url": "https://pypi.org/project/torchlars/0.1.2/", "requires_dist": null, "requires_python": "", "summary": "A LARS implementation in PyTorch", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>torchlars</h1>\n<p><a href=\"https://pypi.org/project/torchlars\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7832596429fb47c40d1bd92e3e89f8e927f59b28/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f746f7263686c6172732e737667\"></a>\n<a href=\"https://travis-ci.org/kakaobrain/torchlars\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a3e4b79157753634ede8798998831c4b0baff930/68747470733a2f2f7472617669732d63692e6f72672f6b616b616f627261696e2f746f7263686c6172732e7376673f6272616e63683d6d6173746572\"></a></p>\n<p>A <a href=\"https://arxiv.org/abs/1708.03888\" rel=\"nofollow\">LARS</a> implementation in PyTorch.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torchlars</span> <span class=\"kn\">import</span> <span class=\"n\">LARS</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">LARS</span><span class=\"p\">(</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">SGD</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">))</span>\n</pre>\n<h2>What is LARS?</h2>\n<p>LARS (Layer-wise Adaptive Rate Scaling) is an optimization algorithm designed\nfor large-batch training published by You, Gitman, and Ginsburg, which\ncalculates the local learning rate per layer at each optimization step.\nAccording to the paper, when training ResNet-50 on ImageNet ILSVRC (2016)\nclassification task with LARS, the learning curve and the best top-1 accuracy\nstay similar to that of the baseline (training with batch size 256 without\nLARS) even if the batch size is scaled up to 32K.</p>\n<p><a href=\"https://arxiv.org/abs/1708.03888\" rel=\"nofollow\">Large Batch Training of Convolutional Networks</a></p>\n<p>Originally, LARS is formulated in terms of SGD optimizer and extension to other\noptimizers was not mentioned in the paper. In contrast, <code>torchlars</code> implements\nLARS as a wrapper which can take any optimizers including SGD as the base.</p>\n<p>Additionally, LARS of torchlars is designed to more consider operation in the\nCUDA environment compared to existing implementations. Thanks to this, you can\nsee only the little speed loss appears compared to just using SGD, in an\nenvironment where CPU to GPU synchronization does not occur.</p>\n<h2>Usage</h2>\n<p>Currently, torchlars requires the following environments:</p>\n<ul>\n<li>Linux</li>\n<li>Python 3.6+</li>\n<li>PyTorch 1.1+</li>\n<li>CUDA 10+</li>\n</ul>\n<p>To use torchlars, install it via PyPI:</p>\n<pre>$ pip install torchlars\n</pre>\n<p>To use LARS, simply wrap your base optimizer with <code>torchlars.LARS</code>. LARS\ninherits <code>torch.optim.Optimizer</code>, so you can simply use LARS as optimizer on\nyour code. After then, when you call step method of LARS, LARS automatically\ncalculates local learning rate before running base optimizer such as SGD or\nAdam</p>\n<p>The example code below shows how to use LARS using SGD as base optimizer.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torchlars</span> <span class=\"kn\">import</span> <span class=\"n\">LARS</span>\n\n<span class=\"n\">base_optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">SGD</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">LARS</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">base_optimizer</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">,</span> <span class=\"n\">trust_coef</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">)</span>\n\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<h2>Benchmarking</h2>\n<h3>ResNet-50 on ImageNet classification</h3>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Batch Size</th>\n<th align=\"left\">LR policy</th>\n<th align=\"left\">lr</th>\n<th align=\"left\">warm-up</th>\n<th align=\"left\">epoch</th>\n<th align=\"left\">Best Top-1 accuracy, %</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">256</td>\n<td align=\"left\">poly(2)</td>\n<td align=\"left\">0.2</td>\n<td align=\"left\">N/A</td>\n<td align=\"left\">90</td>\n<td align=\"left\">73.79</td>\n</tr>\n<tr>\n<td align=\"left\">8k</td>\n<td align=\"left\">LARS+poly(2)</td>\n<td align=\"left\">12.8</td>\n<td align=\"left\">5</td>\n<td align=\"left\">90</td>\n<td align=\"left\">73.78</td>\n</tr>\n<tr>\n<td align=\"left\">16K</td>\n<td align=\"left\">LARS+poly(2)</td>\n<td align=\"left\">25.0</td>\n<td align=\"left\">5</td>\n<td align=\"left\">90</td>\n<td align=\"left\">73.36</td>\n</tr>\n<tr>\n<td align=\"left\">32K</td>\n<td align=\"left\">LARS+poly(2)</td>\n<td align=\"left\">29.0</td>\n<td align=\"left\">5</td>\n<td align=\"left\">90</td>\n<td align=\"left\">72.26</td>\n</tr></tbody></table>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2eb13027778affa82af449051c69a4b35fcbf510/696d672f7265736e657435305f746573745f6c6561726e696e675f6375727665732e6a7067\"></p>\n<p>Above image and table show the reproduced performance benchmark on ResNet-50,\nas reported in Table 4 and Figure 5 of the paper.</p>\n<p>The cyan line represents the baseline result, which is training result with\nbatch size 256, and others represent training result of 8K, 16K, 32K\nrespectively. As you see, every result shows a similar learning curve and best\ntop-1 accuracy.</p>\n<p>Most experimental conditions are similar to used in the paper, but we slightly\nchange some conditions like learning rate to observe comparable results as\nproposed by the LARS paper.</p>\n<p><em>Note: We refer\n<a href=\"https://people.eecs.berkeley.edu/%7Eyouyang/publications/batch/\" rel=\"nofollow\">log file</a>\nprovided by paper to obtain above hyper-parameters.</em></p>\n<h2>Authors and Licensing</h2>\n<p>torchlars project is developed by <a href=\"https://github.com/cmpark0126\" rel=\"nofollow\">Chunmyong Park</a> at <a href=\"https://kakaobrain.com/\" rel=\"nofollow\">Kakao Brain</a>, with\n<a href=\"https://subl.ee/\" rel=\"nofollow\">Heungsub Lee</a>, <a href=\"https://github.com/mrJeong\" rel=\"nofollow\">Myungryong Jeong</a>, <a href=\"https://github.com/wbaek\" rel=\"nofollow\">Woonhyuk Baek</a>, and <a href=\"https://github.com/chiheonk\" rel=\"nofollow\">Chiheon Kim</a>'s help.\nIt is distributed under <a href=\"LICENSE\" rel=\"nofollow\">Apache License 2.0</a>.</p>\n<h2>Citation</h2>\n<p>If you apply this library to any project and research, please cite our code:</p>\n<pre><code>@misc{torchlars,\n  author       = {Park, Chunmyong and Lee, Heungsub and Jeong, Myungryong and\n                  Baek, Woonhyuk and Kim, Chiheon},\n  title        = {torchlars, {A} {LARS} implementation in {PyTorch}},\n  howpublished = {\\url{https://github.com/kakaobrain/torchlars}},\n  year         = {2019}\n}\n</code></pre>\n\n          </div>"}, "last_serial": 6676357, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "5c575625d470b9616685514804f7559b", "sha256": "99b7b46ac4957e7268e4eda35b52867a0abc4d8ecb15a6445c9c0932d6daed92"}, "downloads": -1, "filename": "torchlars-0.1.0.tar.gz", "has_sig": false, "md5_digest": "5c575625d470b9616685514804f7559b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6219, "upload_time": "2019-10-07T10:50:01", "upload_time_iso_8601": "2019-10-07T10:50:01.105338Z", "url": "https://files.pythonhosted.org/packages/88/34/65e3fbf961665b4a3407c1bd5446b6aa53c53161f19a2dc2d4e84b6b8a17/torchlars-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "78621ffde6e5bfb8f17bb285a5271d6a", "sha256": "ed4777f164e78ae83c4b0d2a5ece9d6384e20226ea659e3314c4490703d21d18"}, "downloads": -1, "filename": "torchlars-0.1.1.tar.gz", "has_sig": false, "md5_digest": "78621ffde6e5bfb8f17bb285a5271d6a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6492, "upload_time": "2020-01-31T15:42:47", "upload_time_iso_8601": "2020-01-31T15:42:47.283446Z", "url": "https://files.pythonhosted.org/packages/3d/36/b5a0e48535e9cf372cc58eb8abbefdc85787f7f8ee8c7b7fb3c247ec63f6/torchlars-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "c43fa383faf23082fc69ff4ac8a4e201", "sha256": "efef03da466de95b34c736e6d19469478b98f74105572f5a816949dc104fe299"}, "downloads": -1, "filename": "torchlars-0.1.2.tar.gz", "has_sig": false, "md5_digest": "c43fa383faf23082fc69ff4ac8a4e201", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6492, "upload_time": "2020-02-21T16:25:19", "upload_time_iso_8601": "2020-02-21T16:25:19.256434Z", "url": "https://files.pythonhosted.org/packages/f5/12/633c1822dc87d72ad2a80ba40706c7a77056c68d6211351313ff0e96bda0/torchlars-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c43fa383faf23082fc69ff4ac8a4e201", "sha256": "efef03da466de95b34c736e6d19469478b98f74105572f5a816949dc104fe299"}, "downloads": -1, "filename": "torchlars-0.1.2.tar.gz", "has_sig": false, "md5_digest": "c43fa383faf23082fc69ff4ac8a4e201", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6492, "upload_time": "2020-02-21T16:25:19", "upload_time_iso_8601": "2020-02-21T16:25:19.256434Z", "url": "https://files.pythonhosted.org/packages/f5/12/633c1822dc87d72ad2a80ba40706c7a77056c68d6211351313ff0e96bda0/torchlars-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:19 2020"}