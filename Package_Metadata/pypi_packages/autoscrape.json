{"info": {"author": "Brandon Roberts", "author_email": "brandon@bxroberts.org", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: End Users/Desktop", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU Affero General Public License v3", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Utilities"], "description": "AutoScrape\n==========\n\n.. image:: https://pypip.in/v/autoscrape/badge.svg\n        :target: https://pypi.python.org/pypi/autoscrape/\n\n.. image:: https://pypip.in/license/autoscrape/badge.svg\n        :target: https://pypi.python.org/pypi/autoscrape/\n\n\n.. figure:: https://github.com/brandonrobertz/autoscrape-py/blob/master/images/ai.png\n   :alt: Artificial Informer Labs\n\nA project of `Artificial Informer Labs <https://artificialinformer.com>`__.\n\nAutoScrape is an automated scraper of structured data from interactive\nweb pages. You point this scraper at a site, give it a little information\nand structured data can then be extracted. No brittle, site-specific\nprogramming necessary.\n\nThis is an implementation of the web scraping framework described in the\npaper, `Robust Web Scraping in the Public Interest with AutoScrape <https://bxroberts.org/files/autoscrape.pdf>`__ and presented at\n`Computation + Journalism Symposium 2019 <http://cplusj.org/>`__.\n\nCurrently there are two methods of running AutoScrape:\n\n- as a local CLI python script\n- a full Web interface for scraping (see bottom of page)\n\nInstallation and running instructions are provided for both below.\n\nQuickstart\n----------\n\nTwo ways, easiest first.\n\n::\n\n    pip install autoscrape[all]\n    autoscrape --backend requests --output outdir --maxdepth 2 https://bxroberts.org\n\nThis will install all dependencies for all backends and various options.\n\nOr:\n\n::\n\n    git clone https://github.com/brandonrobertz/autoscrape-py\n    cd autoscrape-py/\n    pip install .[all]\n    autoscrape --backend requests --output outdir --maxdepth 2 https://bxroberts.org\n\nEither way, you can now use ``autoscrape`` from the command line.\n\nUsage Examples\n--------------\n\nHere are some straightforward use cases for AutoScrape and how you'd use\nthe CLI tool to execute them. These, of course, assume you have the\ndependencies installed.\n\nCrawler Backends\n~~~~~~~~~~~~~~~~\n\nThere are two backends available for driving AutoScrape: ``requests``,\n``selenium`` and ``warc``. The ``requests`` backend (the default) is based on the\nPython requests library and is only capable of crawling sites and submitting\nsimple HTTP forms. For any interaction with forms or JavaScript powered\nbuttons, you'll need to use the ``selenium`` backend.\n\nYou can control the backened with the ``--backend`` option:\n\n::\n\n    autoscrape \\\n      --backend requests \\\n      --output requests_crawled_site \\\n      'https://some.page/to-crawl'\n\nIn order to use backends other than requests, you need to install\nthe proper dependencies. `pip install autoscrape[all]` will\ninstall everything required for all backends/functionality, but\nyou can also install dependencies in isolation:\n\n::\n    Selenium backend:\n    pip install autoscrape[selenium-backend]\n\n    Crawl graph builder (for use in --save-graph)\n    pip install autoscrape[graph]\n\n    WARC backend:\n    pip install autoscrape[warc-backend]\n\nNote that for the Selenium backend, you need to install geckodriver or\nchromedriver, depending if you're using Firefox or Chrome, respectively.\nMore information is below in the External Dependencies section.\n\nCrawl\n~~~~~\n\nCrawl an entire website, saving all HTML and stylesheets (no\nscreenshots):\n\n::\n\n    autoscrape \\\n      --backend requests \\\n      --maxdepth -1 \\\n      --output crawled_site \\\n      'https://some.page/to-crawl'\n\nArchive Page (Screenshot & Code)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nArchive a single webpage, both code and full-content screenshot (PNG),\nfor future reference:\n\n::\n\n    autoscrape \\\n      --backend selenium \\\n      --full-page-screenshots \\\n      --load-images --maxdepth 0 \\\n      --save-screenshots --driver Firefox \\\n      --output archived_webpage \\\n      'https://some.page/to-archive'\n\nSearch Forms and Crawl Result Pages\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nQuery a web form, identified by containing the text \"I'm a search form\",\nentering \"NAME\" into the first (0th) text input field and select January\n20th, 1992 in the second (1st) date field. Then click all buttons with\nthe text \"Next ->\" to get all results pages:\n\n::\n\n    autoscrape \\\n      --backend selenium \\\n      --output search_query_data \\\n      --form-match \"I'm a search form\" \\\n      --input \"i:0:NAME,d:1:1992-01-20\" \\\n      --next-match \"Next ->\" \\\n      'https://some.page/search?s=newquery'\n\nSetup for Standalone Local CLI\n------------------------------\n\nExternal Dependencies\n~~~~~~~~~~~~~~~~~~~~~\n\nIf you want to use the ``selenium`` backend for interactive crawling,\nyou need to have geckodriver installed. You can do that here:\n\n::\n\n    https://github.com/mozilla/geckodriver/releases\n\nOr through your package manager:\n\n::\n    apt install firefox-geckodriver\n\nYour ``geckodriver`` needs to be compatible with your current version of\nFirefox or you will get errors. If you install FF and the driver\nthrough your package manager, you *should* be okay, but it's\nnot guaranteed. We have specific versions of both pinned in the\n``Dockerfile``.\n\nIf you prefer to use Chrome, you will need the ChromeDriver (we've\ntested using v2.41). It can be found in your distribution's package\nmanager or here:\n\n::\n\n    https://sites.google.com/a/chromium.org/chromedriver/downloads\n\nInstalling the remaining Python dependencies can be done using pip.\n\nPip Install Method\n~~~~~~~~~~~~~~~~~~\n\nNext you need to set up your python virtual environment (Python 3.6\nrequired) and install the Python dependencies:\n\n::\n\n    pip install -r requirements.txt\n\nRunning Standalone Scraper\n--------------------------\n\nEnvironment Test Crawler\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can run a test to ensure your webdriver is set up correctly by\nrunning the ``test`` crawler:\n\n::\n\n    ./autoscrape --backend selenium --show-browser [SITE_URL]\n\nThe ``test`` crawler will just do a depth-first click-only crawl of an\nentire website. It will not interact with forms or POST data. Data will\nbe saved to ``./autoscrape-data/`` (the default output directory).\n\nManual Config-Based Scraper\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAutoscrape has a manually controlled mode, similar to wget, except this\nuses interactive capabilities and can input data to search forms, follow\n\"next page\"-type buttons, etc. This functionality can be used either as\na standalone crawler/scraper or as a method to build a training set for\nthe automated scrapers.\n\nAutoscrape manual-mode full options:\n\n::\n\n    AUTOSCRAPE - Interactively crawl, find searchable forms,\n    input data to them and scrape data on the results, from an\n    initial BASEURL.\n\n    Usage:\n        autoscrape [options] BASEURL\n\n    General Options:\n        --backend BACKEND\n            The backend to use. Currently one of \"selenium\", \"requests\" or\n            \"warc\".  The requests browser is only capable of crawling, but\n            is approximately 2-3.5x faster. WARC is for emulating browsing\n            through Common Crawl archival data.\n            [default: selenium]\n\n        --loglevel LEVEL\n            Loglevel, note that DEBUG is extremely verbose.\n            [default: INFO]\n\n        --quiet\n            This will silence all logging to console.\n\n    Crawl-Specific Options:\n        --maxdepth DEPTH\n            Maximum depth to crawl a site (in search of form\n            if the option --form-match STRING is specified,\n            see below). Setting to 0 means don't crawl at all,\n            all operations are limited to the BASEURL page.\n            Setting to -1 means unlimited maximum crawl depth.\n            [default: 10]\n\n        --max-pages NUM\n            Maximum number of unique pages, in total, to fetch.\n            AutoScrape will stop crawling once this is hit.\n\n        --leave-host\n            By default, autoscrape will not leave the host given\n            in the BASEURL. This option lets the scraper leave\n            the host.\n\n        --only-links MATCH_STREING\n            A whitelist of links to follow. All others will\n            be ignored. Can be a string or a regex with\n            multiple strings to match separated by a pipe\n            (|) character.\n\n        --ignore-links MATCH_STRING\n            This option can be used to remove any links matching\n            MATCH_STRING (can be a regex or just a string match)\n            from consideration for clicking. Accepts the same\n            argument format as --only-links.\n\n        --link-priority SORT_STRING\n            A string to sort the links by. In this case, any link\n            containing \"SORT_STRING\" will be clicked before any other\n            links. In most cases you probably want to use the\n            whitelist, --only-links, option.\n\n        --ignore-extensions IGNORE_EXTENSIONS\n            Don't click on or download URLs pointing to files with\n            these extensions.\n\n        --result-page-links MATCH_STRINGS_LIST\n            If specified, AutoScrape will click on any links matching\n            this string when it arrives on a search result page.\n\n    Interactive Form Search Options:\n        --form-match SEARCH_STRING\n            The crawler will identify a form to search/scrape if it\n            contains the specified string. If matched, it will be\n            interactively scraped using the below instructions.\n\n        --input INPUT_DESCRIPTION\n            Interactive search descriptor. This describes how to\n            interact with a matched form. The inputs are\n            described in the following format:\n\n            \"c:0:True,i:0:atext,s:1:France:d:0:1991-01-20\"\n\n            A single-input type can be one of three types:\n            checkbox (\"c\"), input box (\"i\"), option select\n            (\"s\"), and date inputs (\"d\", with inputs in the\n            \"YYYY-MM-DD\" format). The type is separated by a\n            colon, and the input index position is next. (Each\n            input type has its own list, so a form with one\n            input, one checkbox, and one option select, will all\n            be at index 0.) The final command, sepearated by\n            another colon, describes what to do with the input.\n\n            Multiple inputs are separated by a comma, so you can\n            interact with multiple inputs before submitting the\n            form.\n\n            To illustrate this, the above command does the following:\n                - first input checkbox is checked (uncheck is False)\n                - first input box gets filled with the string \"first\"\n                - second select input gets the \"France\" option chosen\n                - first date input gets set to Jan 20, 1991\n\n        --next-match NEXT_BTN_STRING\n            A string to match a \"next\" button with, after\n            searching a form.  The scraper will continue to\n            click \"next\" buttons after a search until no matches\n            are found, unless limited by the --formdepth option\n            (see below). [default: next page]\n\n        --formdepth DEPTH\n            How deep the scraper will iterate, by clicking\n            \"next\" buttons. Zero means infinite depth.\n            [default: 0]\n\n        --form-submit-natural-click\n            Some webpages make clicking a link element difficult\n            due to JavaScript onClick events. In cases where a\n            click does nothing, you can use this option to get\n            the scraper to emulate a mouse click over the link's\n            poition on the page, activating any higher level JS\n            interactions.\n\n        --form-submit-wait SECONDS\n            How many seconds to force wait after a submit to a form.\n            This should be used in cases where the builtin\n            wait-for-page-load isn't working properly (JS-heavy\n            pages, etc). [default: 5]\n\n    Webdriver-Specific and General Options:\n        --load-images\n            By default, images on a page will not be fetched.\n            This speeds up scrapes on sites and lowers bandwidth\n            needs. This option fetches all images on a page.\n\n        --show-browser\n            By default, we hide the browser during operation.\n            This option displays a browser window, mostly\n            for debugging purposes.\n\n        --driver DRIVER\n            Which browser to use. Current support for \"Firefox\",\n            \"Chrome\", and \"remote\". [default: Firefox]\n\n        --browser-binary PATH_TO_BROWSER\n            Path to a specific browser binary. If left blank\n            selenium will pull the browser found on your path.\n\n        --remote-hub URI\n            If using \"remote\" driver, specify the hub URI to\n            connect to. Needs the proto, address, port, and path.\n            [default: http://localhost:4444/wd/hub]\n\n    WARC Options:\n        --warc-directory PATH_TO_WARCS\n            Path to the folder containing GZipped WARC files. These can be\n            downloaded from Common Crawl. Required when using the \"warc\"\n            backend.\n\n        --warc-index-file PATH_TO_LEVELDB\n            Path to the level DB database holding the URL-to-file\n            index: URL => (filename, record_number)\n            This will be generated from the WARCS in the --warc-directory\n            speficied if it's not already. Required when using the \"warc\"\n            backend.\n\n    Data Saving Options:\n        --output DIRECTORY_OR_URL\n            If specified, this indicates where to save pages during a\n            crawl. This directory will be created if it does not\n            currently exist.  This directory will have several\n            sub-directories that contain the different types of pages\n            found (i.e., search_pages, data_pages, screenshots).\n            This can also accept a URL (i.e., http://localhost:5000/files)\n            and AutoScrape will POST to that endpoint with each\n            file scraped.\n            [default: autoscrape-data]\n\n        --keep-filename\n            By default, we hash the files in a scrape in order to\n            account for dynamic content under a single-page app\n            (SPA) website implmentation. This option will force\n            the scraper to retain the original filename, from the\n            URL when saving scrape data.\n\n        --save-screenshots\n            This option makes the scraper save screenshots of each\n            page, interaction, and search. Screenshots will be\n            saved to the screenshots folder of the output dir.\n\n        --full-page-screenshots\n            By default, we only save the first displayed part of the\n            webpage. The remaining portion that you can only see\n            by scrolling down isn't captured. Setting this option\n            forces AutoScrape to scroll down and capture the entire\n            web content. This can fail in certain circumstances, like\n            in API output mode and should be used with care.\n\n        --save-graph\n            This option allows the scraper to build a directed graph\n            of the entire scrape and will save it to the \"graph\"\n            subdirectory under the output dir. The output file\n            is a timestamped networkx pickled graph.\n\n        --disable-style-saving\n            By default, AutoScrape saves the stylesheets associated\n            with a scraped page. To save storage, you can disable this\n            functionality by using this option.\n\nAutoScrape Web UI (Docker)\n--------------------------\n\nAutoScrape can be ran as a containerized cluster environment, where\nscrapes can be triggered and stopped via API calls and data can be\nstreamed to this server.\n\nThis requires the `autoscrape-www <https://github.com/brandonrobertz/autoscrape-www>`__ submodule to be pulled:\n\n::\n\n    git submodule init\n    git submodule update\n\nThis will pull the browser-based UI into the `www/` folder.\n\nYou need\n`docker-ce <https://docs.docker.com/install/#server>`__ and\n`docker-compose <https://docs.docker.com/compose/install/>`__. Once you\nhave these dependencies installed, simply run:\n\n::\n\n    docker-compose build --pull\n    docker-compose up\n\nThis will build the containers and launch a API server running on local\nport 5000. More information about the API calls can be found in\n``autoscrape-server.py``.\n\nIf you have make installed, you can simply run ``make start``.\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/brandonrobertz/autoscrape-py", "keywords": "", "license": "AGPLv3", "maintainer": "", "maintainer_email": "", "name": "autoscrape", "package_url": "https://pypi.org/project/autoscrape/", "platform": "", "project_url": "https://pypi.org/project/autoscrape/", "project_urls": {"Homepage": "https://github.com/brandonrobertz/autoscrape-py"}, "release_url": "https://pypi.org/project/autoscrape/1.6.8/", "requires_dist": null, "requires_python": "", "summary": "An automated, programming-free web scraper for interactive sites", "version": "1.6.8", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://pypi.python.org/pypi/autoscrape/\" rel=\"nofollow\"><img alt=\"https://pypip.in/v/autoscrape/badge.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/37e57975c34acd1f5597381c4c2bb7872e31f4eb/68747470733a2f2f70797069702e696e2f762f6175746f7363726170652f62616467652e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/autoscrape/\" rel=\"nofollow\"><img alt=\"https://pypip.in/license/autoscrape/badge.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b1d12a57f03c0296e33d00756ab1c349a7e0d2c4/68747470733a2f2f70797069702e696e2f6c6963656e73652f6175746f7363726170652f62616467652e737667\"></a>\n<div>\n<img alt=\"Artificial Informer Labs\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/718ffb4cbeb71d9b4e13847203fefdc0ed4e2dad/68747470733a2f2f6769746875622e636f6d2f6272616e646f6e726f626572747a2f6175746f7363726170652d70792f626c6f622f6d61737465722f696d616765732f61692e706e67\">\n</div>\n<p>A project of <a href=\"https://artificialinformer.com\" rel=\"nofollow\">Artificial Informer Labs</a>.</p>\n<p>AutoScrape is an automated scraper of structured data from interactive\nweb pages. You point this scraper at a site, give it a little information\nand structured data can then be extracted. No brittle, site-specific\nprogramming necessary.</p>\n<p>This is an implementation of the web scraping framework described in the\npaper, <a href=\"https://bxroberts.org/files/autoscrape.pdf\" rel=\"nofollow\">Robust Web Scraping in the Public Interest with AutoScrape</a> and presented at\n<a href=\"http://cplusj.org/\" rel=\"nofollow\">Computation + Journalism Symposium 2019</a>.</p>\n<p>Currently there are two methods of running AutoScrape:</p>\n<ul>\n<li>as a local CLI python script</li>\n<li>a full Web interface for scraping (see bottom of page)</li>\n</ul>\n<p>Installation and running instructions are provided for both below.</p>\n<div id=\"quickstart\">\n<h2>Quickstart</h2>\n<p>Two ways, easiest first.</p>\n<pre>pip install autoscrape[all]\nautoscrape --backend requests --output outdir --maxdepth 2 https://bxroberts.org\n</pre>\n<p>This will install all dependencies for all backends and various options.</p>\n<p>Or:</p>\n<pre>git clone https://github.com/brandonrobertz/autoscrape-py\ncd autoscrape-py/\npip install .[all]\nautoscrape --backend requests --output outdir --maxdepth 2 https://bxroberts.org\n</pre>\n<p>Either way, you can now use <tt>autoscrape</tt> from the command line.</p>\n</div>\n<div id=\"usage-examples\">\n<h2>Usage Examples</h2>\n<p>Here are some straightforward use cases for AutoScrape and how you\u2019d use\nthe CLI tool to execute them. These, of course, assume you have the\ndependencies installed.</p>\n<div id=\"crawler-backends\">\n<h3>Crawler Backends</h3>\n<p>There are two backends available for driving AutoScrape: <tt>requests</tt>,\n<tt>selenium</tt> and <tt>warc</tt>. The <tt>requests</tt> backend (the default) is based on the\nPython requests library and is only capable of crawling sites and submitting\nsimple HTTP forms. For any interaction with forms or JavaScript powered\nbuttons, you\u2019ll need to use the <tt>selenium</tt> backend.</p>\n<p>You can control the backened with the <tt><span class=\"pre\">--backend</span></tt> option:</p>\n<pre>autoscrape \\\n  --backend requests \\\n  --output requests_crawled_site \\\n  'https://some.page/to-crawl'\n</pre>\n<p>In order to use backends other than requests, you need to install\nthe proper dependencies. <cite>pip install autoscrape[all]</cite> will\ninstall everything required for all backends/functionality, but\nyou can also install dependencies in isolation:</p>\n<dl>\n<dt>::</dt>\n<dd><p>Selenium backend:\npip install autoscrape[selenium-backend]</p>\n<p>Crawl graph builder (for use in \u2013save-graph)\npip install autoscrape[graph]</p>\n<p>WARC backend:\npip install autoscrape[warc-backend]</p>\n</dd>\n</dl>\n<p>Note that for the Selenium backend, you need to install geckodriver or\nchromedriver, depending if you\u2019re using Firefox or Chrome, respectively.\nMore information is below in the External Dependencies section.</p>\n</div>\n<div id=\"crawl\">\n<h3>Crawl</h3>\n<p>Crawl an entire website, saving all HTML and stylesheets (no\nscreenshots):</p>\n<pre>autoscrape \\\n  --backend requests \\\n  --maxdepth -1 \\\n  --output crawled_site \\\n  'https://some.page/to-crawl'\n</pre>\n</div>\n<div id=\"archive-page-screenshot-code\">\n<h3>Archive Page (Screenshot &amp; Code)</h3>\n<p>Archive a single webpage, both code and full-content screenshot (PNG),\nfor future reference:</p>\n<pre>autoscrape \\\n  --backend selenium \\\n  --full-page-screenshots \\\n  --load-images --maxdepth 0 \\\n  --save-screenshots --driver Firefox \\\n  --output archived_webpage \\\n  'https://some.page/to-archive'\n</pre>\n</div>\n<div id=\"search-forms-and-crawl-result-pages\">\n<h3>Search Forms and Crawl Result Pages</h3>\n<p>Query a web form, identified by containing the text \u201cI\u2019m a search form\u201d,\nentering \u201cNAME\u201d into the first (0th) text input field and select January\n20th, 1992 in the second (1st) date field. Then click all buttons with\nthe text \u201cNext -&gt;\u201d to get all results pages:</p>\n<pre>autoscrape \\\n  --backend selenium \\\n  --output search_query_data \\\n  --form-match \"I'm a search form\" \\\n  --input \"i:0:NAME,d:1:1992-01-20\" \\\n  --next-match \"Next -&gt;\" \\\n  'https://some.page/search?s=newquery'\n</pre>\n</div>\n</div>\n<div id=\"setup-for-standalone-local-cli\">\n<h2>Setup for Standalone Local CLI</h2>\n<div id=\"external-dependencies\">\n<h3>External Dependencies</h3>\n<p>If you want to use the <tt>selenium</tt> backend for interactive crawling,\nyou need to have geckodriver installed. You can do that here:</p>\n<pre>https://github.com/mozilla/geckodriver/releases\n</pre>\n<p>Or through your package manager:</p>\n<dl>\n<dt>::</dt>\n<dd>apt install firefox-geckodriver</dd>\n</dl>\n<p>Your <tt>geckodriver</tt> needs to be compatible with your current version of\nFirefox or you will get errors. If you install FF and the driver\nthrough your package manager, you <em>should</em> be okay, but it\u2019s\nnot guaranteed. We have specific versions of both pinned in the\n<tt>Dockerfile</tt>.</p>\n<p>If you prefer to use Chrome, you will need the ChromeDriver (we\u2019ve\ntested using v2.41). It can be found in your distribution\u2019s package\nmanager or here:</p>\n<pre>https://sites.google.com/a/chromium.org/chromedriver/downloads\n</pre>\n<p>Installing the remaining Python dependencies can be done using pip.</p>\n</div>\n<div id=\"pip-install-method\">\n<h3>Pip Install Method</h3>\n<p>Next you need to set up your python virtual environment (Python 3.6\nrequired) and install the Python dependencies:</p>\n<pre>pip install -r requirements.txt\n</pre>\n</div>\n</div>\n<div id=\"running-standalone-scraper\">\n<h2>Running Standalone Scraper</h2>\n<div id=\"environment-test-crawler\">\n<h3>Environment Test Crawler</h3>\n<p>You can run a test to ensure your webdriver is set up correctly by\nrunning the <tt>test</tt> crawler:</p>\n<pre>./autoscrape --backend selenium --show-browser [SITE_URL]\n</pre>\n<p>The <tt>test</tt> crawler will just do a depth-first click-only crawl of an\nentire website. It will not interact with forms or POST data. Data will\nbe saved to <tt><span class=\"pre\">./autoscrape-data/</span></tt> (the default output directory).</p>\n</div>\n<div id=\"manual-config-based-scraper\">\n<h3>Manual Config-Based Scraper</h3>\n<p>Autoscrape has a manually controlled mode, similar to wget, except this\nuses interactive capabilities and can input data to search forms, follow\n\u201cnext page\u201d-type buttons, etc. This functionality can be used either as\na standalone crawler/scraper or as a method to build a training set for\nthe automated scrapers.</p>\n<p>Autoscrape manual-mode full options:</p>\n<pre>AUTOSCRAPE - Interactively crawl, find searchable forms,\ninput data to them and scrape data on the results, from an\ninitial BASEURL.\n\nUsage:\n    autoscrape [options] BASEURL\n\nGeneral Options:\n    --backend BACKEND\n        The backend to use. Currently one of \"selenium\", \"requests\" or\n        \"warc\".  The requests browser is only capable of crawling, but\n        is approximately 2-3.5x faster. WARC is for emulating browsing\n        through Common Crawl archival data.\n        [default: selenium]\n\n    --loglevel LEVEL\n        Loglevel, note that DEBUG is extremely verbose.\n        [default: INFO]\n\n    --quiet\n        This will silence all logging to console.\n\nCrawl-Specific Options:\n    --maxdepth DEPTH\n        Maximum depth to crawl a site (in search of form\n        if the option --form-match STRING is specified,\n        see below). Setting to 0 means don't crawl at all,\n        all operations are limited to the BASEURL page.\n        Setting to -1 means unlimited maximum crawl depth.\n        [default: 10]\n\n    --max-pages NUM\n        Maximum number of unique pages, in total, to fetch.\n        AutoScrape will stop crawling once this is hit.\n\n    --leave-host\n        By default, autoscrape will not leave the host given\n        in the BASEURL. This option lets the scraper leave\n        the host.\n\n    --only-links MATCH_STREING\n        A whitelist of links to follow. All others will\n        be ignored. Can be a string or a regex with\n        multiple strings to match separated by a pipe\n        (|) character.\n\n    --ignore-links MATCH_STRING\n        This option can be used to remove any links matching\n        MATCH_STRING (can be a regex or just a string match)\n        from consideration for clicking. Accepts the same\n        argument format as --only-links.\n\n    --link-priority SORT_STRING\n        A string to sort the links by. In this case, any link\n        containing \"SORT_STRING\" will be clicked before any other\n        links. In most cases you probably want to use the\n        whitelist, --only-links, option.\n\n    --ignore-extensions IGNORE_EXTENSIONS\n        Don't click on or download URLs pointing to files with\n        these extensions.\n\n    --result-page-links MATCH_STRINGS_LIST\n        If specified, AutoScrape will click on any links matching\n        this string when it arrives on a search result page.\n\nInteractive Form Search Options:\n    --form-match SEARCH_STRING\n        The crawler will identify a form to search/scrape if it\n        contains the specified string. If matched, it will be\n        interactively scraped using the below instructions.\n\n    --input INPUT_DESCRIPTION\n        Interactive search descriptor. This describes how to\n        interact with a matched form. The inputs are\n        described in the following format:\n\n        \"c:0:True,i:0:atext,s:1:France:d:0:1991-01-20\"\n\n        A single-input type can be one of three types:\n        checkbox (\"c\"), input box (\"i\"), option select\n        (\"s\"), and date inputs (\"d\", with inputs in the\n        \"YYYY-MM-DD\" format). The type is separated by a\n        colon, and the input index position is next. (Each\n        input type has its own list, so a form with one\n        input, one checkbox, and one option select, will all\n        be at index 0.) The final command, sepearated by\n        another colon, describes what to do with the input.\n\n        Multiple inputs are separated by a comma, so you can\n        interact with multiple inputs before submitting the\n        form.\n\n        To illustrate this, the above command does the following:\n            - first input checkbox is checked (uncheck is False)\n            - first input box gets filled with the string \"first\"\n            - second select input gets the \"France\" option chosen\n            - first date input gets set to Jan 20, 1991\n\n    --next-match NEXT_BTN_STRING\n        A string to match a \"next\" button with, after\n        searching a form.  The scraper will continue to\n        click \"next\" buttons after a search until no matches\n        are found, unless limited by the --formdepth option\n        (see below). [default: next page]\n\n    --formdepth DEPTH\n        How deep the scraper will iterate, by clicking\n        \"next\" buttons. Zero means infinite depth.\n        [default: 0]\n\n    --form-submit-natural-click\n        Some webpages make clicking a link element difficult\n        due to JavaScript onClick events. In cases where a\n        click does nothing, you can use this option to get\n        the scraper to emulate a mouse click over the link's\n        poition on the page, activating any higher level JS\n        interactions.\n\n    --form-submit-wait SECONDS\n        How many seconds to force wait after a submit to a form.\n        This should be used in cases where the builtin\n        wait-for-page-load isn't working properly (JS-heavy\n        pages, etc). [default: 5]\n\nWebdriver-Specific and General Options:\n    --load-images\n        By default, images on a page will not be fetched.\n        This speeds up scrapes on sites and lowers bandwidth\n        needs. This option fetches all images on a page.\n\n    --show-browser\n        By default, we hide the browser during operation.\n        This option displays a browser window, mostly\n        for debugging purposes.\n\n    --driver DRIVER\n        Which browser to use. Current support for \"Firefox\",\n        \"Chrome\", and \"remote\". [default: Firefox]\n\n    --browser-binary PATH_TO_BROWSER\n        Path to a specific browser binary. If left blank\n        selenium will pull the browser found on your path.\n\n    --remote-hub URI\n        If using \"remote\" driver, specify the hub URI to\n        connect to. Needs the proto, address, port, and path.\n        [default: http://localhost:4444/wd/hub]\n\nWARC Options:\n    --warc-directory PATH_TO_WARCS\n        Path to the folder containing GZipped WARC files. These can be\n        downloaded from Common Crawl. Required when using the \"warc\"\n        backend.\n\n    --warc-index-file PATH_TO_LEVELDB\n        Path to the level DB database holding the URL-to-file\n        index: URL =&gt; (filename, record_number)\n        This will be generated from the WARCS in the --warc-directory\n        speficied if it's not already. Required when using the \"warc\"\n        backend.\n\nData Saving Options:\n    --output DIRECTORY_OR_URL\n        If specified, this indicates where to save pages during a\n        crawl. This directory will be created if it does not\n        currently exist.  This directory will have several\n        sub-directories that contain the different types of pages\n        found (i.e., search_pages, data_pages, screenshots).\n        This can also accept a URL (i.e., http://localhost:5000/files)\n        and AutoScrape will POST to that endpoint with each\n        file scraped.\n        [default: autoscrape-data]\n\n    --keep-filename\n        By default, we hash the files in a scrape in order to\n        account for dynamic content under a single-page app\n        (SPA) website implmentation. This option will force\n        the scraper to retain the original filename, from the\n        URL when saving scrape data.\n\n    --save-screenshots\n        This option makes the scraper save screenshots of each\n        page, interaction, and search. Screenshots will be\n        saved to the screenshots folder of the output dir.\n\n    --full-page-screenshots\n        By default, we only save the first displayed part of the\n        webpage. The remaining portion that you can only see\n        by scrolling down isn't captured. Setting this option\n        forces AutoScrape to scroll down and capture the entire\n        web content. This can fail in certain circumstances, like\n        in API output mode and should be used with care.\n\n    --save-graph\n        This option allows the scraper to build a directed graph\n        of the entire scrape and will save it to the \"graph\"\n        subdirectory under the output dir. The output file\n        is a timestamped networkx pickled graph.\n\n    --disable-style-saving\n        By default, AutoScrape saves the stylesheets associated\n        with a scraped page. To save storage, you can disable this\n        functionality by using this option.\n</pre>\n</div>\n</div>\n<div id=\"autoscrape-web-ui-docker\">\n<h2>AutoScrape Web UI (Docker)</h2>\n<p>AutoScrape can be ran as a containerized cluster environment, where\nscrapes can be triggered and stopped via API calls and data can be\nstreamed to this server.</p>\n<p>This requires the <a href=\"https://github.com/brandonrobertz/autoscrape-www\" rel=\"nofollow\">autoscrape-www</a> submodule to be pulled:</p>\n<pre>git submodule init\ngit submodule update\n</pre>\n<p>This will pull the browser-based UI into the <cite>www/</cite> folder.</p>\n<p>You need\n<a href=\"https://docs.docker.com/install/#server\" rel=\"nofollow\">docker-ce</a> and\n<a href=\"https://docs.docker.com/compose/install/\" rel=\"nofollow\">docker-compose</a>. Once you\nhave these dependencies installed, simply run:</p>\n<pre>docker-compose build --pull\ndocker-compose up\n</pre>\n<p>This will build the containers and launch a API server running on local\nport 5000. More information about the API calls can be found in\n<tt><span class=\"pre\">autoscrape-server.py</span></tt>.</p>\n<p>If you have make installed, you can simply run <tt>make start</tt>.</p>\n</div>\n\n          </div>"}, "last_serial": 7080470, "releases": {"1.0.0rc5": [{"comment_text": "", "digests": {"md5": "9e308ece7cb02fe3fccbafc91f4b7e53", "sha256": "dde0351f8be13a8aeebacc147753de6e8912436c79efb5227b516ac2b982c722"}, "downloads": -1, "filename": "autoscrape-1.0.0rc5.tar.gz", "has_sig": false, "md5_digest": "9e308ece7cb02fe3fccbafc91f4b7e53", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60922, "upload_time": "2020-01-25T07:45:52", "upload_time_iso_8601": "2020-01-25T07:45:52.611328Z", "url": "https://files.pythonhosted.org/packages/6a/26/93384a4ae4beb4ebf8d48a67621fa9b08370dcdaae2726cb4ece366df4dd/autoscrape-1.0.0rc5.tar.gz", "yanked": false}], "1.0.0rc6": [{"comment_text": "", "digests": {"md5": "66d08e9fa0a52459235c309910498fc3", "sha256": "bbb228c049334b70bb56b73e2775023a35ad03408c52ca938cac8e988a3c1bd9"}, "downloads": -1, "filename": "autoscrape-1.0.0rc6.tar.gz", "has_sig": false, "md5_digest": "66d08e9fa0a52459235c309910498fc3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59956, "upload_time": "2020-01-25T07:50:04", "upload_time_iso_8601": "2020-01-25T07:50:04.087437Z", "url": "https://files.pythonhosted.org/packages/ff/d4/e9917e424e472a69fbad5e8169dc88cd23e634744a6bd07e5588ab9c3b4a/autoscrape-1.0.0rc6.tar.gz", "yanked": false}], "1.0.0rc7": [{"comment_text": "", "digests": {"md5": "7af8ea180b01a6c15b42a5444b333ee7", "sha256": "e3a719fc0b4cd9e21e92982b490bdfaf7c1640523b65ec223c4e4385ae86579b"}, "downloads": -1, "filename": "autoscrape-1.0.0rc7.tar.gz", "has_sig": false, "md5_digest": "7af8ea180b01a6c15b42a5444b333ee7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59676, "upload_time": "2020-01-25T09:45:57", "upload_time_iso_8601": "2020-01-25T09:45:57.215914Z", "url": "https://files.pythonhosted.org/packages/53/de/e119b6ae3fc570929052eb5bda5b4c1ad75d5073f6b768e3313998329a64/autoscrape-1.0.0rc7.tar.gz", "yanked": false}], "1.0.0rc8": [{"comment_text": "", "digests": {"md5": "7f936e5b11148f9307e6739328fe0bfb", "sha256": "cea0e093d319f5121afcc13451f52fd9fae1f63489ab07b9d891c159d64f0602"}, "downloads": -1, "filename": "autoscrape-1.0.0rc8.tar.gz", "has_sig": false, "md5_digest": "7f936e5b11148f9307e6739328fe0bfb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60347, "upload_time": "2020-01-27T02:10:39", "upload_time_iso_8601": "2020-01-27T02:10:39.796211Z", "url": "https://files.pythonhosted.org/packages/b0/92/67ed6c1adaa43182ef7c41f640ba9ed7b544bdec00c0f32be16013a86ffc/autoscrape-1.0.0rc8.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "89025df0f40f9c52ad117502dae3e7ba", "sha256": "7521e46683f0a6768039b7d313ac0e1f3070d1bcb1264dd3a4eaf505aacaa0f4"}, "downloads": -1, "filename": "autoscrape-1.1.0.tar.gz", "has_sig": false, "md5_digest": "89025df0f40f9c52ad117502dae3e7ba", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 56938, "upload_time": "2020-02-05T03:43:55", "upload_time_iso_8601": "2020-02-05T03:43:55.226561Z", "url": "https://files.pythonhosted.org/packages/6a/eb/230d206c0e002ba9a3050d5e34fc138b8fab2798f9d14e075d151d5dce94/autoscrape-1.1.0.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "7848e81c12872d8498c85600cf6b9bc0", "sha256": "d3951536451b26464bfa2c8c84811729e15003b55f40e004c9a9545be3cd124d"}, "downloads": -1, "filename": "autoscrape-1.1.1.tar.gz", "has_sig": false, "md5_digest": "7848e81c12872d8498c85600cf6b9bc0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 56933, "upload_time": "2020-02-05T04:44:29", "upload_time_iso_8601": "2020-02-05T04:44:29.934265Z", "url": "https://files.pythonhosted.org/packages/7e/78/f9292349068727742487ac020b7cc415e0e0f627eac3a24c33648d47cc11/autoscrape-1.1.1.tar.gz", "yanked": false}], "1.1.2": [{"comment_text": "", "digests": {"md5": "a76f14bf7dbeae5ac33786385c7d43bb", "sha256": "bb7741c0de7673e33c8aa18070e69937b2ea626f4e532fe642879954675158bb"}, "downloads": -1, "filename": "autoscrape-1.1.2.tar.gz", "has_sig": false, "md5_digest": "a76f14bf7dbeae5ac33786385c7d43bb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 58659, "upload_time": "2020-02-05T05:00:17", "upload_time_iso_8601": "2020-02-05T05:00:17.366595Z", "url": "https://files.pythonhosted.org/packages/d5/8e/8dcfb5db508a434bd506565069dccd547595cb5a749840a9f407019b0bf4/autoscrape-1.1.2.tar.gz", "yanked": false}], "1.1.3": [{"comment_text": "", "digests": {"md5": "5fbd1f64ae1737356bfa0d48fe18ab22", "sha256": "65911adfccd51c5c4e86a5898bc9665bf00f182c06d2f3235ab57f834a07bd2d"}, "downloads": -1, "filename": "autoscrape-1.1.3.tar.gz", "has_sig": false, "md5_digest": "5fbd1f64ae1737356bfa0d48fe18ab22", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62769, "upload_time": "2020-02-05T05:09:18", "upload_time_iso_8601": "2020-02-05T05:09:18.609474Z", "url": "https://files.pythonhosted.org/packages/09/32/fa4caa3a77d141d827f7c92dc975df71d1b876e3abffcef5feddceb2ec5d/autoscrape-1.1.3.tar.gz", "yanked": false}], "1.1.4": [{"comment_text": "", "digests": {"md5": "59d482b0c2eddcef2c58865b5b06bc46", "sha256": "d9feab056be52574341782569f365712ee69ce813208e1b498cb93e6c1ad84fe"}, "downloads": -1, "filename": "autoscrape-1.1.4.tar.gz", "has_sig": false, "md5_digest": "59d482b0c2eddcef2c58865b5b06bc46", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62765, "upload_time": "2020-02-05T05:11:21", "upload_time_iso_8601": "2020-02-05T05:11:21.461686Z", "url": "https://files.pythonhosted.org/packages/f7/0a/7a0a3762b91f4c69616f2aff83c55f39405eaf7de89ed225a4cff8b8d1a1/autoscrape-1.1.4.tar.gz", "yanked": false}], "1.1.5": [{"comment_text": "", "digests": {"md5": "615ed6420121ac63a4149536679a0abf", "sha256": "b30880cd14eea0ad483d600c95b27ad01858ad244cb27a2c7a33031644b560d9"}, "downloads": -1, "filename": "autoscrape-1.1.5.tar.gz", "has_sig": false, "md5_digest": "615ed6420121ac63a4149536679a0abf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62725, "upload_time": "2020-02-05T05:13:19", "upload_time_iso_8601": "2020-02-05T05:13:19.084338Z", "url": "https://files.pythonhosted.org/packages/d8/de/8ca31861b3130074f12fa066a5f8862856a571037a70166ce61c89a6fde4/autoscrape-1.1.5.tar.gz", "yanked": false}], "1.1.6": [{"comment_text": "", "digests": {"md5": "c39cb3f55a6360af0ef644e0478aec10", "sha256": "31fc1f41e19186978c6f021afeb10f5586bc17613f452dc9faadbad45e727847"}, "downloads": -1, "filename": "autoscrape-1.1.6.tar.gz", "has_sig": false, "md5_digest": "c39cb3f55a6360af0ef644e0478aec10", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 58599, "upload_time": "2020-02-05T22:37:53", "upload_time_iso_8601": "2020-02-05T22:37:53.849178Z", "url": "https://files.pythonhosted.org/packages/65/60/07dd7d2ad7851a73f4275af422b42edcb71894b054263d686827884e37b3/autoscrape-1.1.6.tar.gz", "yanked": false}], "1.1.8": [{"comment_text": "", "digests": {"md5": "4114a4a61d4c40dbbab9319f8a99af0e", "sha256": "09b8b56615c58fb045687a255cc9ddf3bbad12a52b741232fec61c58a8c7a743"}, "downloads": -1, "filename": "autoscrape-1.1.8.tar.gz", "has_sig": false, "md5_digest": "4114a4a61d4c40dbbab9319f8a99af0e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59970, "upload_time": "2020-02-11T20:49:52", "upload_time_iso_8601": "2020-02-11T20:49:52.316245Z", "url": "https://files.pythonhosted.org/packages/8c/05/caea111f7f1dcaabaf545d6789d168888d6c04c81ee43fd47bfd0d38d37c/autoscrape-1.1.8.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "008a5dd74f48ff78b74e57471200e1c4", "sha256": "43cd65958bd211e3c1ecfc1d0e41bfc8fa055734510d51dd29e8efec64657bf0"}, "downloads": -1, "filename": "autoscrape-1.2.0.tar.gz", "has_sig": false, "md5_digest": "008a5dd74f48ff78b74e57471200e1c4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 61884, "upload_time": "2020-02-28T20:01:41", "upload_time_iso_8601": "2020-02-28T20:01:41.771803Z", "url": "https://files.pythonhosted.org/packages/aa/03/ffd838f27a08afa9167eb1cb25aa47f7c50c3997b1a0ea55681e32c6cd3b/autoscrape-1.2.0.tar.gz", "yanked": false}], "1.2.1": [{"comment_text": "", "digests": {"md5": "2b1a4caaee07da0f0fbb2b7031af7b07", "sha256": "6633e74fda0694b5a7a0c884f7bda5a6f663bac246b3c6754e9c35e81379859c"}, "downloads": -1, "filename": "autoscrape-1.2.1.tar.gz", "has_sig": false, "md5_digest": "2b1a4caaee07da0f0fbb2b7031af7b07", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62039, "upload_time": "2020-02-29T01:15:27", "upload_time_iso_8601": "2020-02-29T01:15:27.362027Z", "url": "https://files.pythonhosted.org/packages/f1/6e/a0eac3427b6923cbd9d86606bd4f8216eb67e3c5b7925a6e782a7f79023a/autoscrape-1.2.1.tar.gz", "yanked": false}], "1.2.2": [{"comment_text": "", "digests": {"md5": "a8f5db37a6930df782fd187639c09f41", "sha256": "3b1e62cdcf04297618634ebf5233203dbd05f9c53cf7b2c40e4fd20dbb1b2e79"}, "downloads": -1, "filename": "autoscrape-1.2.2.tar.gz", "has_sig": false, "md5_digest": "a8f5db37a6930df782fd187639c09f41", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62034, "upload_time": "2020-02-29T01:48:50", "upload_time_iso_8601": "2020-02-29T01:48:50.738901Z", "url": "https://files.pythonhosted.org/packages/a1/80/98e88a1d75364522554ae34ad818412ab1b1b144b7473ee0c26d37fdd214/autoscrape-1.2.2.tar.gz", "yanked": false}], "1.2.3": [{"comment_text": "", "digests": {"md5": "6544ffad94dea67fb9e4ba419fa358f0", "sha256": "169e7d23a3f76458bc1bd5554ac2ac7a62e338483ef442123fac6fe12d02eab8"}, "downloads": -1, "filename": "autoscrape-1.2.3.tar.gz", "has_sig": false, "md5_digest": "6544ffad94dea67fb9e4ba419fa358f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62041, "upload_time": "2020-02-29T02:26:50", "upload_time_iso_8601": "2020-02-29T02:26:50.636327Z", "url": "https://files.pythonhosted.org/packages/39/64/393a2ad5764e244d294d1f5b38d1648e87633263cf2bb024390567a2e6fb/autoscrape-1.2.3.tar.gz", "yanked": false}], "1.2.4": [{"comment_text": "", "digests": {"md5": "afda9f0b480b08c27d366a799b252c25", "sha256": "e0723dcb4774dc7215b97b021c87e2fbea238071bbabcc917e6e327fbe35f671"}, "downloads": -1, "filename": "autoscrape-1.2.4.tar.gz", "has_sig": false, "md5_digest": "afda9f0b480b08c27d366a799b252c25", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62025, "upload_time": "2020-02-29T03:19:14", "upload_time_iso_8601": "2020-02-29T03:19:14.888260Z", "url": "https://files.pythonhosted.org/packages/de/83/e18b7c88dab37258feaa3a90b0668499daec2949cda363cc403932f5ee34/autoscrape-1.2.4.tar.gz", "yanked": false}], "1.2.5": [{"comment_text": "", "digests": {"md5": "2a570df3121a982d2d43254dcca72a53", "sha256": "7c015c885be60ea700f7dc9daa9ba21fab4525975635ead89fdc2980e5208003"}, "downloads": -1, "filename": "autoscrape-1.2.5.tar.gz", "has_sig": false, "md5_digest": "2a570df3121a982d2d43254dcca72a53", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62056, "upload_time": "2020-02-29T04:22:55", "upload_time_iso_8601": "2020-02-29T04:22:55.841705Z", "url": "https://files.pythonhosted.org/packages/32/49/6b32ea9cc20271094978b49c2cec2278714aeaf01ba61d4bd4a13e2feec0/autoscrape-1.2.5.tar.gz", "yanked": false}], "1.2.6": [{"comment_text": "", "digests": {"md5": "fbe90c398203b62899949a50c690bd69", "sha256": "ac87d4028110d1e69f6d218b640994c69992734ac118656fb17756c5e13fea55"}, "downloads": -1, "filename": "autoscrape-1.2.6.tar.gz", "has_sig": false, "md5_digest": "fbe90c398203b62899949a50c690bd69", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62208, "upload_time": "2020-03-03T02:11:26", "upload_time_iso_8601": "2020-03-03T02:11:26.861296Z", "url": "https://files.pythonhosted.org/packages/47/e9/ac7c8a1e8dfee0827da0ea7de7b493ef46569e03dcc18d06a860d0d6b2da/autoscrape-1.2.6.tar.gz", "yanked": false}], "1.2.7": [{"comment_text": "", "digests": {"md5": "7cbd61c339ce31abeeb93ee50906ece3", "sha256": "c7bc801fd06f17feaaaec48e03e089cd6ef3df95a5e70f379254bd7e5bbd9419"}, "downloads": -1, "filename": "autoscrape-1.2.7.tar.gz", "has_sig": false, "md5_digest": "7cbd61c339ce31abeeb93ee50906ece3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 63342, "upload_time": "2020-03-04T13:21:08", "upload_time_iso_8601": "2020-03-04T13:21:08.093462Z", "url": "https://files.pythonhosted.org/packages/87/f0/73dd28269c9991532e2a8c55d0c1265e7b217ba1b3c472219ea490e1a334/autoscrape-1.2.7.tar.gz", "yanked": false}], "1.6.4": [{"comment_text": "", "digests": {"md5": "f229708fb9cb257e73d62bf316846383", "sha256": "dd42e7c47f0e0ccf334b17c0e4dfa753576eadb4852cb94a890431f205d7a9ae"}, "downloads": -1, "filename": "autoscrape-1.6.4.tar.gz", "has_sig": false, "md5_digest": "f229708fb9cb257e73d62bf316846383", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66521, "upload_time": "2020-04-13T02:00:57", "upload_time_iso_8601": "2020-04-13T02:00:57.029445Z", "url": "https://files.pythonhosted.org/packages/6b/87/cafe31bc238a99129177d666168e9ebac7e4305f7f80aa2ea77fcd733ae6/autoscrape-1.6.4.tar.gz", "yanked": false}], "1.6.5": [{"comment_text": "", "digests": {"md5": "8412c198d3137e7388a6509731c16d34", "sha256": "18c4f946ec4204189913969f85d4b2a73a436482820064eacf81836c3044e4c7"}, "downloads": -1, "filename": "autoscrape-1.6.5.tar.gz", "has_sig": false, "md5_digest": "8412c198d3137e7388a6509731c16d34", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66512, "upload_time": "2020-04-13T02:07:31", "upload_time_iso_8601": "2020-04-13T02:07:31.769958Z", "url": "https://files.pythonhosted.org/packages/89/df/d714c93e4c15332b263524ea5d39f7db3cc693c3b380db92bfc7ecddaca0/autoscrape-1.6.5.tar.gz", "yanked": false}], "1.6.8": [{"comment_text": "", "digests": {"md5": "83b64c3454b1c268677fa4a050f947bb", "sha256": "65950e1a5f6e7168b6a03fc17574b9610ec4ed911bc9ba8933c05d1d9b11d873"}, "downloads": -1, "filename": "autoscrape-1.6.8.tar.gz", "has_sig": false, "md5_digest": "83b64c3454b1c268677fa4a050f947bb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 67348, "upload_time": "2020-04-22T22:22:11", "upload_time_iso_8601": "2020-04-22T22:22:11.997141Z", "url": "https://files.pythonhosted.org/packages/59/16/958445b837c8205812519866cd00f9937f520b7ffd1ceb42399463263295/autoscrape-1.6.8.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "83b64c3454b1c268677fa4a050f947bb", "sha256": "65950e1a5f6e7168b6a03fc17574b9610ec4ed911bc9ba8933c05d1d9b11d873"}, "downloads": -1, "filename": "autoscrape-1.6.8.tar.gz", "has_sig": false, "md5_digest": "83b64c3454b1c268677fa4a050f947bb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 67348, "upload_time": "2020-04-22T22:22:11", "upload_time_iso_8601": "2020-04-22T22:22:11.997141Z", "url": "https://files.pythonhosted.org/packages/59/16/958445b837c8205812519866cd00f9937f520b7ffd1ceb42399463263295/autoscrape-1.6.8.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:16:09 2020"}