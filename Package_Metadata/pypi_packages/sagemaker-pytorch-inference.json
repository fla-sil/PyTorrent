{"info": {"author": "Amazon Web Services", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.6"], "description": "===================================\nSageMaker PyTorch Serving Container\n===================================\n\nSageMaker PyTorch Serving Container is an open source library for making the\nPyTorch framework run on Amazon SageMaker.\n\nThis repository also contains Dockerfiles which install this library, PyTorch, and dependencies\nfor building SageMaker PyTorch images.\n\nThe SageMaker team uses this repository to build its official PyTorch image. To use this image on SageMaker,\nsee `Python SDK <https://github.com/aws/sagemaker-python-sdk>`__.\nFor end users, this repository is typically of interest if you need implementation details for\nthe official image, or if you want to use it to build your own customized PyTorch image.\n\nFor information on running PyTorch jobs on SageMaker: `SageMaker PyTorch Estimators and Models\n<https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/pytorch>`__.\n\nFor notebook examples: `SageMaker Notebook\nExamples <https://github.com/awslabs/amazon-sagemaker-examples>`__.\n\nTable of Contents\n-----------------\n\n#. `Getting Started <#getting-started>`__\n#. `Building your Image <#building-your-image>`__\n#. `Amazon Elastic Inference with PyTorch in SageMaker <#amazon-elastic-inference-with-pytorch-in-sagemaker>`__\n#. `Running the tests <#running-the-tests>`__\n\nGetting Started\n---------------\n\nPrerequisites\n~~~~~~~~~~~~~\n\nMake sure you have installed all of the following prerequisites on your\ndevelopment machine:\n\n- `Docker <https://www.docker.com/>`__\n\nFor Testing on GPU\n^^^^^^^^^^^^^^^^^^\n\n-  `Nvidia-Docker <https://github.com/NVIDIA/nvidia-docker>`__\n\nRecommended\n^^^^^^^^^^^\n\n-  A Python environment management tool (e.g.\n   `PyEnv <https://github.com/pyenv/pyenv>`__,\n   `VirtualEnv <https://virtualenv.pypa.io/en/stable/>`__)\n\nBuilding your image\n-------------------\n\n`Amazon SageMaker <https://aws.amazon.com/documentation/sagemaker/>`__\nutilizes Docker containers to run all training jobs & inference endpoints.\n\nThe Docker images are built from the Dockerfiles specified in\n`Docker/ <https://github.com/aws/sagemaker-pytorch-container/tree/master/docker>`__.\n\nThe Docker files are grouped based on PyTorch version and separated\nbased on Python version and processor type.\n\nThe Docker images, used to run training & inference jobs, are built from\nboth corresponding \"base\" and \"final\" Dockerfiles.\n\nBase Images\n~~~~~~~~~~~\n\nThe \"base\" Dockerfile encompass the installation of the framework and all of the dependencies\nneeded.\n\nTagging scheme is based on <PyTorch_version>-<processor>-py<python_version>. (e.g.1.0.0-cpu-py3)\n\nAll \"final\" Dockerfiles build images using base images that use the tagging scheme\nabove.\n\nIf you want to build your base docker image, then use:\n\n::\n\n    # All build instructions assume you're building from the root directory of the sagemaker-pytorch-container.\n\n    # CPU\n    docker build -t pytorch-base:<PyTorch_version>-cpu-py<python_version> -f docker/<PyTorch_version>/base/Dockerfile.cpu --build-arg py_version=<python_version> .\n\n    # GPU\n    docker build -t pytorch-base:<PyTorch_version>-gpu-py<python_version> -f docker/<PyTorch_version>/base/Dockerfile.gpu --build-arg py_version=<python_version> .\n\n::\n\n    # Example\n\n    # CPU\n    docker build -t pytorch-base:1.0.0-cpu-py3 -f docker/1.0.0/base/Dockerfile.cpu --build-arg py_version=3 .\n\n    # GPU\n    docker build -t pytorch-base:1.0.0-gpu-py3 -f docker/1.0.0/base/Dockerfile.gpu --build-arg py_version=3 .\n\nFinal Images\n~~~~~~~~~~~~\n\nThe \"final\" Dockerfiles encompass the installation of the SageMaker specific support code.\n\nAll \"final\" Dockerfiles use `base images for building <https://github.com/aws/sagemaker-pytorch-container/blob/master/docker/1.0.0/final/Dockerfile.cpu#L2>`__.\n\nThese \"base\" images are specified with the naming convention of\npytorch-base:<PyTorch_version>-<processor>-py<python_version>.\n\nBefore building \"final\" images:\n\nBuild your \"base\" image. Make sure it is named and tagged in accordance with your \"final\"\nDockerfile.\n\n\n::\n\n    # Create the SageMaker PyTorch Serving Container Python package.\n    cd sagemaker-pytorch-container\n    python setup.py bdist_wheel\n\nIf you want to build \"final\" Docker images, then use:\n\n::\n\n    # All build instructions assume you're building from the root directory of the sagemaker-pytorch-container.\n\n    # CPU\n    docker build -t <image_name>:<tag> -f docker/<PyTorch_version>/final/Dockerfile.cpu --build-arg py_version=<python_version> .\n\n    # GPU\n    docker build -t <image_name>:<tag> -f docker/<PyTorch_version>/final/Dockerfile.gpu --build-arg py_version=<python_version> .\n\n::\n\n    # Example\n\n    # CPU\n    docker build -t preprod-pytorch:1.0.0-cpu-py3 -f docker/1.0.0/final/Dockerfile.cpu --build-arg py_version=3 .\n\n    # GPU\n    docker build -t preprod-pytorch:1.0.0-gpu-py3 -f docker/1.0.0/final/Dockerfile.gpu --build-arg py_version=3 .\n\nAmazon Elastic Inference with PyTorch in SageMaker\n--------------------------------------------------\n`Amazon Elastic Inference <https://aws.amazon.com/machine-learning/elastic-inference/>`__ allows you to to attach\nlow-cost GPU-powered acceleration to Amazon EC2 and Amazon SageMaker instances to reduce the cost of running deep\nlearning inference by up to 75%. Currently, Amazon Elastic Inference supports TensorFlow, Apache MXNet, PyTorch,\nand ONNX models.\n\nSupport for using PyTorch with Amazon Elastic Inference in SageMaker is supported in the public SageMaker PyTorch serving containers.\n\n* For information on how to use the Python SDK to create an endpoint with Amazon Elastic Inference and PyTorch in SageMaker, see `Deploying PyTorch Models <https://sagemaker.readthedocs.io/en/stable/using_pytorch.html#deploy-pytorch-models>`__.\n* For information on how Amazon Elastic Inference works, see `How EI Works <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html#ei-how-it-works>`__.\n* For more information in regards to using Amazon Elastic Inference in SageMaker, see `Amazon SageMaker Elastic Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html>`__.\n\nBuilding the SageMaker Elastic Inference PyTorch Serving container\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nAmazon Elastic Inference is designed to be used with AWS enhanced versions of TensorFlow serving, Apache MXNet or PyTorch serving.\nThe SageMaker PyTorch containers with Amazon Elastic Inference support were built utilizing the\nsame instructions listed `above <#building-your-image>`__ with the\nEIA Dockerfiles, which are all named ``Dockerfile.eia``, and can be found in the same ``docker/`` directory.\n\nExample:\n\n::\n\n    # PyTorch 1.3.1, Python 3, EI\n    $ docker build -t preprod-pytorch-serving-eia:1.3.1-cpu-py3 -f docker/1.3.1/py3/Dockerfile.eia .\n\n\n* Currently, only PyTorch serving 1.3.1 is supported for Elastic Inference.\n\nRunning the tests\n-----------------\n\nRunning the tests requires installation of the SageMaker PyTorch Serving Container code and its test\ndependencies.\n\n::\n\n    git clone https://github.com/aws/sagemaker-pytorch-container.git\n    cd sagemaker-pytorch-container\n    pip install -e .[test]\n\nTests are defined in\n`test/ <https://github.com/aws/sagemaker-pytorch-container/tree/master/test>`__\nand include unit, local integration, and SageMaker integration tests.\n\nUnit Tests\n~~~~~~~~~~\n\nIf you want to run unit tests, then use:\n\n::\n\n    # All test instructions should be run from the top level directory\n\n    pytest test/unit\n\n    # or you can use tox to run unit tests as well as flake8 and code coverage\n\n    tox\n\n\nLocal Integration Tests\n~~~~~~~~~~~~~~~~~~~~~~~\n\nRunning local integration tests require `Docker <https://www.docker.com/>`__ and `AWS\ncredentials <https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html>`__,\nas the local integration tests make calls to a couple AWS services. The local integration tests and\nSageMaker integration tests require configurations specified within their respective\n`conftest.py <https://github.com/aws/sagemaker-pytorch-container/blob/master/test/conftest.py>`__.\n\nLocal integration tests on GPU require `Nvidia-Docker <https://github.com/NVIDIA/nvidia-docker>`__.\n\nBefore running local integration tests:\n\n#. Build your Docker image.\n#. Pass in the correct pytest arguments to run tests against your Docker image.\n\nIf you want to run local integration tests, then use:\n\n::\n\n    # Required arguments for integration tests are found in test/conftest.py\n\n    pytest test/integration/local --docker-base-name <your_docker_image> \\\n                      --tag <your_docker_image_tag> \\\n                      --py-version <2_or_3> \\\n                      --framework-version <PyTorch_version> \\\n                      --processor <cpu_or_gpu>\n\n::\n\n    # Example\n    pytest test/integration/local --docker-base-name preprod-pytorch \\\n                      --tag 1.0 \\\n                      --py-version 3 \\\n                      --framework-version 1.0.0 \\\n                      --processor cpu\n\nSageMaker Integration Tests\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSageMaker integration tests require your Docker image to be within an `Amazon ECR repository <https://docs\n.aws.amazon.com/AmazonECS/latest/developerguide/ECS_Console_Repositories.html>`__.\n\nThe Docker base name is your `ECR repository namespace <https://docs.aws.amazon\n.com/AmazonECR/latest/userguide/Repositories.html>`__.\n\nThe instance type is your specified `Amazon SageMaker Instance Type\n<https://aws.amazon.com/sagemaker/pricing/instance-types/>`__ that the SageMaker integration test will run on.\n\nBefore running SageMaker integration tests:\n\n#. Build your Docker image.\n#. Push the image to your ECR repository.\n#. Pass in the correct pytest arguments to run tests on SageMaker against the image within your ECR repository.\n\nIf you want to run a SageMaker integration end to end test on `Amazon\nSageMaker <https://aws.amazon.com/sagemaker/>`__, then use:\n\n::\n\n    # Required arguments for integration tests are found in test/conftest.py\n\n    pytest test/integration/sagemaker --aws-id <your_aws_id> \\\n                           --docker-base-name <your_docker_image> \\\n                           --instance-type <amazon_sagemaker_instance_type> \\\n                           --tag <your_docker_image_tag> \\\n\n::\n\n    # Example\n    pytest test/integration/sagemaker --aws-id 12345678910 \\\n                           --docker-base-name preprod-pytorch \\\n                           --instance-type ml.m4.xlarge \\\n                           --tag 1.0\n\nContributing\n------------\n\nPlease read\n`CONTRIBUTING.md <https://github.com/aws/sagemaker-pytorch-container/blob/master/CONTRIBUTING.md>`__\nfor details on our code of conduct, and the process for submitting pull\nrequests to us.\n\nLicense\n-------\n\nSageMaker PyTorch Serving Container is licensed under the Apache 2.0 License. It is copyright 2018 Amazon\n.com, Inc. or its affiliates. All Rights Reserved. The license is available at:\nhttp://aws.amazon.com/apache2.0/", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "Apache License 2.0", "maintainer": "", "maintainer_email": "", "name": "sagemaker-pytorch-inference", "package_url": "https://pypi.org/project/sagemaker-pytorch-inference/", "platform": "", "project_url": "https://pypi.org/project/sagemaker-pytorch-inference/", "project_urls": null, "release_url": "https://pypi.org/project/sagemaker-pytorch-inference/1.4.4/", "requires_dist": null, "requires_python": "", "summary": "Open source library for creating PyTorch containers to run on Amazon SageMaker.", "version": "1.4.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>SageMaker PyTorch Serving Container is an open source library for making the\nPyTorch framework run on Amazon SageMaker.</p>\n<p>This repository also contains Dockerfiles which install this library, PyTorch, and dependencies\nfor building SageMaker PyTorch images.</p>\n<p>The SageMaker team uses this repository to build its official PyTorch image. To use this image on SageMaker,\nsee <a href=\"https://github.com/aws/sagemaker-python-sdk\" rel=\"nofollow\">Python SDK</a>.\nFor end users, this repository is typically of interest if you need implementation details for\nthe official image, or if you want to use it to build your own customized PyTorch image.</p>\n<p>For information on running PyTorch jobs on SageMaker: <a href=\"https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/pytorch\" rel=\"nofollow\">SageMaker PyTorch Estimators and Models</a>.</p>\n<p>For notebook examples: <a href=\"https://github.com/awslabs/amazon-sagemaker-examples\" rel=\"nofollow\">SageMaker Notebook\nExamples</a>.</p>\n<div id=\"table-of-contents\">\n<h2>Table of Contents</h2>\n<ol>\n<li><a href=\"#getting-started\" rel=\"nofollow\">Getting Started</a></li>\n<li><a href=\"#building-your-image\" rel=\"nofollow\">Building your Image</a></li>\n<li><a href=\"#amazon-elastic-inference-with-pytorch-in-sagemaker\" rel=\"nofollow\">Amazon Elastic Inference with PyTorch in SageMaker</a></li>\n<li><a href=\"#running-the-tests\" rel=\"nofollow\">Running the tests</a></li>\n</ol>\n</div>\n<div id=\"getting-started\">\n<h2>Getting Started</h2>\n<h2 id=\"prerequisites\"><span class=\"section-subtitle\">Prerequisites</span></h2>\n<p>Make sure you have installed all of the following prerequisites on your\ndevelopment machine:</p>\n<ul>\n<li><a href=\"https://www.docker.com/\" rel=\"nofollow\">Docker</a></li>\n</ul>\n<div id=\"for-testing-on-gpu\">\n<h3>For Testing on GPU</h3>\n<ul>\n<li><a href=\"https://github.com/NVIDIA/nvidia-docker\" rel=\"nofollow\">Nvidia-Docker</a></li>\n</ul>\n</div>\n<div id=\"recommended\">\n<h3>Recommended</h3>\n<ul>\n<li>A Python environment management tool (e.g.\n<a href=\"https://github.com/pyenv/pyenv\" rel=\"nofollow\">PyEnv</a>,\n<a href=\"https://virtualenv.pypa.io/en/stable/\" rel=\"nofollow\">VirtualEnv</a>)</li>\n</ul>\n</div>\n</div>\n<div id=\"building-your-image\">\n<h2>Building your image</h2>\n<p><a href=\"https://aws.amazon.com/documentation/sagemaker/\" rel=\"nofollow\">Amazon SageMaker</a>\nutilizes Docker containers to run all training jobs &amp; inference endpoints.</p>\n<p>The Docker images are built from the Dockerfiles specified in\n<a href=\"https://github.com/aws/sagemaker-pytorch-container/tree/master/docker\" rel=\"nofollow\">Docker/</a>.</p>\n<p>The Docker files are grouped based on PyTorch version and separated\nbased on Python version and processor type.</p>\n<p>The Docker images, used to run training &amp; inference jobs, are built from\nboth corresponding \u201cbase\u201d and \u201cfinal\u201d Dockerfiles.</p>\n<div id=\"base-images\">\n<h3>Base Images</h3>\n<p>The \u201cbase\u201d Dockerfile encompass the installation of the framework and all of the dependencies\nneeded.</p>\n<p>Tagging scheme is based on &lt;PyTorch_version&gt;-&lt;processor&gt;-py&lt;python_version&gt;. (e.g.1.0.0-cpu-py3)</p>\n<p>All \u201cfinal\u201d Dockerfiles build images using base images that use the tagging scheme\nabove.</p>\n<p>If you want to build your base docker image, then use:</p>\n<pre># All build instructions assume you're building from the root directory of the sagemaker-pytorch-container.\n\n# CPU\ndocker build -t pytorch-base:&lt;PyTorch_version&gt;-cpu-py&lt;python_version&gt; -f docker/&lt;PyTorch_version&gt;/base/Dockerfile.cpu --build-arg py_version=&lt;python_version&gt; .\n\n# GPU\ndocker build -t pytorch-base:&lt;PyTorch_version&gt;-gpu-py&lt;python_version&gt; -f docker/&lt;PyTorch_version&gt;/base/Dockerfile.gpu --build-arg py_version=&lt;python_version&gt; .\n</pre>\n<pre># Example\n\n# CPU\ndocker build -t pytorch-base:1.0.0-cpu-py3 -f docker/1.0.0/base/Dockerfile.cpu --build-arg py_version=3 .\n\n# GPU\ndocker build -t pytorch-base:1.0.0-gpu-py3 -f docker/1.0.0/base/Dockerfile.gpu --build-arg py_version=3 .\n</pre>\n</div>\n<div id=\"final-images\">\n<h3>Final Images</h3>\n<p>The \u201cfinal\u201d Dockerfiles encompass the installation of the SageMaker specific support code.</p>\n<p>All \u201cfinal\u201d Dockerfiles use <a href=\"https://github.com/aws/sagemaker-pytorch-container/blob/master/docker/1.0.0/final/Dockerfile.cpu#L2\" rel=\"nofollow\">base images for building</a>.</p>\n<p>These \u201cbase\u201d images are specified with the naming convention of\npytorch-base:&lt;PyTorch_version&gt;-&lt;processor&gt;-py&lt;python_version&gt;.</p>\n<p>Before building \u201cfinal\u201d images:</p>\n<p>Build your \u201cbase\u201d image. Make sure it is named and tagged in accordance with your \u201cfinal\u201d\nDockerfile.</p>\n<pre># Create the SageMaker PyTorch Serving Container Python package.\ncd sagemaker-pytorch-container\npython setup.py bdist_wheel\n</pre>\n<p>If you want to build \u201cfinal\u201d Docker images, then use:</p>\n<pre># All build instructions assume you're building from the root directory of the sagemaker-pytorch-container.\n\n# CPU\ndocker build -t &lt;image_name&gt;:&lt;tag&gt; -f docker/&lt;PyTorch_version&gt;/final/Dockerfile.cpu --build-arg py_version=&lt;python_version&gt; .\n\n# GPU\ndocker build -t &lt;image_name&gt;:&lt;tag&gt; -f docker/&lt;PyTorch_version&gt;/final/Dockerfile.gpu --build-arg py_version=&lt;python_version&gt; .\n</pre>\n<pre># Example\n\n# CPU\ndocker build -t preprod-pytorch:1.0.0-cpu-py3 -f docker/1.0.0/final/Dockerfile.cpu --build-arg py_version=3 .\n\n# GPU\ndocker build -t preprod-pytorch:1.0.0-gpu-py3 -f docker/1.0.0/final/Dockerfile.gpu --build-arg py_version=3 .\n</pre>\n</div>\n</div>\n<div id=\"amazon-elastic-inference-with-pytorch-in-sagemaker\">\n<h2>Amazon Elastic Inference with PyTorch in SageMaker</h2>\n<p><a href=\"https://aws.amazon.com/machine-learning/elastic-inference/\" rel=\"nofollow\">Amazon Elastic Inference</a> allows you to to attach\nlow-cost GPU-powered acceleration to Amazon EC2 and Amazon SageMaker instances to reduce the cost of running deep\nlearning inference by up to 75%. Currently, Amazon Elastic Inference supports TensorFlow, Apache MXNet, PyTorch,\nand ONNX models.</p>\n<p>Support for using PyTorch with Amazon Elastic Inference in SageMaker is supported in the public SageMaker PyTorch serving containers.</p>\n<ul>\n<li>For information on how to use the Python SDK to create an endpoint with Amazon Elastic Inference and PyTorch in SageMaker, see <a href=\"https://sagemaker.readthedocs.io/en/stable/using_pytorch.html#deploy-pytorch-models\" rel=\"nofollow\">Deploying PyTorch Models</a>.</li>\n<li>For information on how Amazon Elastic Inference works, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html#ei-how-it-works\" rel=\"nofollow\">How EI Works</a>.</li>\n<li>For more information in regards to using Amazon Elastic Inference in SageMaker, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\" rel=\"nofollow\">Amazon SageMaker Elastic Inference</a>.</li>\n</ul>\n<div id=\"building-the-sagemaker-elastic-inference-pytorch-serving-container\">\n<h3>Building the SageMaker Elastic Inference PyTorch Serving container</h3>\n<p>Amazon Elastic Inference is designed to be used with AWS enhanced versions of TensorFlow serving, Apache MXNet or PyTorch serving.\nThe SageMaker PyTorch containers with Amazon Elastic Inference support were built utilizing the\nsame instructions listed <a href=\"#building-your-image\" rel=\"nofollow\">above</a> with the\nEIA Dockerfiles, which are all named <tt>Dockerfile.eia</tt>, and can be found in the same <tt>docker/</tt> directory.</p>\n<p>Example:</p>\n<pre># PyTorch 1.3.1, Python 3, EI\n$ docker build -t preprod-pytorch-serving-eia:1.3.1-cpu-py3 -f docker/1.3.1/py3/Dockerfile.eia .\n</pre>\n<ul>\n<li>Currently, only PyTorch serving 1.3.1 is supported for Elastic Inference.</li>\n</ul>\n</div>\n</div>\n<div id=\"running-the-tests\">\n<h2>Running the tests</h2>\n<p>Running the tests requires installation of the SageMaker PyTorch Serving Container code and its test\ndependencies.</p>\n<pre>git clone https://github.com/aws/sagemaker-pytorch-container.git\ncd sagemaker-pytorch-container\npip install -e .[test]\n</pre>\n<p>Tests are defined in\n<a href=\"https://github.com/aws/sagemaker-pytorch-container/tree/master/test\" rel=\"nofollow\">test/</a>\nand include unit, local integration, and SageMaker integration tests.</p>\n<div id=\"unit-tests\">\n<h3>Unit Tests</h3>\n<p>If you want to run unit tests, then use:</p>\n<pre># All test instructions should be run from the top level directory\n\npytest test/unit\n\n# or you can use tox to run unit tests as well as flake8 and code coverage\n\ntox\n</pre>\n</div>\n<div id=\"local-integration-tests\">\n<h3>Local Integration Tests</h3>\n<p>Running local integration tests require <a href=\"https://www.docker.com/\" rel=\"nofollow\">Docker</a> and <a href=\"https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html\" rel=\"nofollow\">AWS\ncredentials</a>,\nas the local integration tests make calls to a couple AWS services. The local integration tests and\nSageMaker integration tests require configurations specified within their respective\n<a href=\"https://github.com/aws/sagemaker-pytorch-container/blob/master/test/conftest.py\" rel=\"nofollow\">conftest.py</a>.</p>\n<p>Local integration tests on GPU require <a href=\"https://github.com/NVIDIA/nvidia-docker\" rel=\"nofollow\">Nvidia-Docker</a>.</p>\n<p>Before running local integration tests:</p>\n<ol>\n<li>Build your Docker image.</li>\n<li>Pass in the correct pytest arguments to run tests against your Docker image.</li>\n</ol>\n<p>If you want to run local integration tests, then use:</p>\n<pre># Required arguments for integration tests are found in test/conftest.py\n\npytest test/integration/local --docker-base-name &lt;your_docker_image&gt; \\\n                  --tag &lt;your_docker_image_tag&gt; \\\n                  --py-version &lt;2_or_3&gt; \\\n                  --framework-version &lt;PyTorch_version&gt; \\\n                  --processor &lt;cpu_or_gpu&gt;\n</pre>\n<pre># Example\npytest test/integration/local --docker-base-name preprod-pytorch \\\n                  --tag 1.0 \\\n                  --py-version 3 \\\n                  --framework-version 1.0.0 \\\n                  --processor cpu\n</pre>\n</div>\n<div id=\"sagemaker-integration-tests\">\n<h3>SageMaker Integration Tests</h3>\n<p>SageMaker integration tests require your Docker image to be within an <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_Console_Repositories.html\" rel=\"nofollow\">Amazon ECR repository</a>.</p>\n<p>The Docker base name is your <a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/Repositories.html\" rel=\"nofollow\">ECR repository namespace</a>.</p>\n<p>The instance type is your specified <a href=\"https://aws.amazon.com/sagemaker/pricing/instance-types/\" rel=\"nofollow\">Amazon SageMaker Instance Type</a> that the SageMaker integration test will run on.</p>\n<p>Before running SageMaker integration tests:</p>\n<ol>\n<li>Build your Docker image.</li>\n<li>Push the image to your ECR repository.</li>\n<li>Pass in the correct pytest arguments to run tests on SageMaker against the image within your ECR repository.</li>\n</ol>\n<p>If you want to run a SageMaker integration end to end test on <a href=\"https://aws.amazon.com/sagemaker/\" rel=\"nofollow\">Amazon\nSageMaker</a>, then use:</p>\n<pre># Required arguments for integration tests are found in test/conftest.py\n\npytest test/integration/sagemaker --aws-id &lt;your_aws_id&gt; \\\n                       --docker-base-name &lt;your_docker_image&gt; \\\n                       --instance-type &lt;amazon_sagemaker_instance_type&gt; \\\n                       --tag &lt;your_docker_image_tag&gt; \\\n</pre>\n<pre># Example\npytest test/integration/sagemaker --aws-id 12345678910 \\\n                       --docker-base-name preprod-pytorch \\\n                       --instance-type ml.m4.xlarge \\\n                       --tag 1.0\n</pre>\n</div>\n</div>\n<div id=\"contributing\">\n<h2>Contributing</h2>\n<p>Please read\n<a href=\"https://github.com/aws/sagemaker-pytorch-container/blob/master/CONTRIBUTING.md\" rel=\"nofollow\">CONTRIBUTING.md</a>\nfor details on our code of conduct, and the process for submitting pull\nrequests to us.</p>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>SageMaker PyTorch Serving Container is licensed under the Apache 2.0 License. It is copyright 2018 Amazon\n.com, Inc. or its affiliates. All Rights Reserved. The license is available at:\n<a href=\"http://aws.amazon.com/apache2.0/\" rel=\"nofollow\">http://aws.amazon.com/apache2.0/</a></p>\n</div>\n\n          </div>"}, "last_serial": 7164732, "releases": {"1.1.0": [{"comment_text": "", "digests": {"md5": "17ae65c08095018f04fcc5b6e8921947", "sha256": "16709fb4c548baab22b839f5622104736d0f785c70973c3cf1663f38668129c1"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.1.0.tar.gz", "has_sig": true, "md5_digest": "17ae65c08095018f04fcc5b6e8921947", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10561, "upload_time": "2020-02-13T21:55:43", "upload_time_iso_8601": "2020-02-13T21:55:43.943306Z", "url": "https://files.pythonhosted.org/packages/34/31/239668e74420540b9347821484c893ffa7293dac0847fbea99e8656a5ea4/sagemaker_pytorch_inference-1.1.0.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "aa255d5aa347c0cdddbef4d7ca027438", "sha256": "1b1d8dc4674efd138fd9ae588437f912680a03c3101ea7e3c94bb2d92c519f88"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.1.1.tar.gz", "has_sig": true, "md5_digest": "aa255d5aa347c0cdddbef4d7ca027438", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10559, "upload_time": "2020-02-17T14:22:02", "upload_time_iso_8601": "2020-02-17T14:22:02.854784Z", "url": "https://files.pythonhosted.org/packages/34/46/3dda7226744bb59d198abdfceb2e56be9c18767c47c25258d3d6e7fe7080/sagemaker_pytorch_inference-1.1.1.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "d42a50d5cd473de288705b5b4ccaccf2", "sha256": "7e34e8198b77e09c8ebc4e5781683b8021c33ca2bcc3cfea53fcd57a4a89aace"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.2.0.tar.gz", "has_sig": true, "md5_digest": "d42a50d5cd473de288705b5b4ccaccf2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10550, "upload_time": "2020-02-20T14:21:23", "upload_time_iso_8601": "2020-02-20T14:21:23.216848Z", "url": "https://files.pythonhosted.org/packages/60/86/f99470819ff1596df1349d6996930f9c95ffe283f423368abf5f9f55871f/sagemaker_pytorch_inference-1.2.0.tar.gz", "yanked": false}], "1.2.1": [{"comment_text": "", "digests": {"md5": "2174f6be5dca2cbef7ed5f0c368150c7", "sha256": "03aa255cb60a615e83bd98687b6204e6a72b750395e1a8aa09bcbdc920b08172"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.2.1.tar.gz", "has_sig": true, "md5_digest": "2174f6be5dca2cbef7ed5f0c368150c7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10550, "upload_time": "2020-02-24T14:22:00", "upload_time_iso_8601": "2020-02-24T14:22:00.878465Z", "url": "https://files.pythonhosted.org/packages/4f/09/c6a790d7e72792d7b122a82348d6991f15e24a09e5f0ef0f89c55befd1ad/sagemaker_pytorch_inference-1.2.1.tar.gz", "yanked": false}], "1.3.0": [{"comment_text": "", "digests": {"md5": "6f984d44aefa30e43fba950ff8e8d808", "sha256": "ad8ea3e4966f9c84c15559638b44932ab6f6c7bfb8714e98eb61504dd9b1eca6"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.3.0.tar.gz", "has_sig": true, "md5_digest": "6f984d44aefa30e43fba950ff8e8d808", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11609, "upload_time": "2020-03-18T14:20:37", "upload_time_iso_8601": "2020-03-18T14:20:37.063195Z", "url": "https://files.pythonhosted.org/packages/ac/d8/9f5f2692cbd4fade475012838bc6e5e70e2c80a021750aff5e18d4d91c71/sagemaker_pytorch_inference-1.3.0.tar.gz", "yanked": false}], "1.3.1": [{"comment_text": "", "digests": {"md5": "c4d257da261815c67b968b5d39c74bcf", "sha256": "1d824f23368d4fe8d1c7dcdd82b1eb6d95e434fdf5bf598a6b8a4569e98e1cdc"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.3.1.tar.gz", "has_sig": true, "md5_digest": "c4d257da261815c67b968b5d39c74bcf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11611, "upload_time": "2020-04-01T22:04:03", "upload_time_iso_8601": "2020-04-01T22:04:03.292625Z", "url": "https://files.pythonhosted.org/packages/fb/b1/115a9989f3471e86e1ca51ff008a068b252390b8a152b2ebb7240ff1cfa1/sagemaker_pytorch_inference-1.3.1.tar.gz", "yanked": false}], "1.3.2": [{"comment_text": "", "digests": {"md5": "102f5f2a927aea125a2817fc5556070a", "sha256": "86859bc3470570f7423490353629ea95d02e1e90086f85ac991cd4aaf430211e"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.3.2.tar.gz", "has_sig": true, "md5_digest": "102f5f2a927aea125a2817fc5556070a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11617, "upload_time": "2020-04-02T14:21:52", "upload_time_iso_8601": "2020-04-02T14:21:52.453351Z", "url": "https://files.pythonhosted.org/packages/92/f6/c7450ea3449809962f769b3c169c0b36916e7bde441c0f4339fb2728525e/sagemaker_pytorch_inference-1.3.2.tar.gz", "yanked": false}], "1.4.0": [{"comment_text": "", "digests": {"md5": "5908537cd19e5c945d6322fbc065e259", "sha256": "0b3fdbda6891723e1979f513c1be85375f6881062476712bc1641c45d0877ba1"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.4.0.tar.gz", "has_sig": true, "md5_digest": "5908537cd19e5c945d6322fbc065e259", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11648, "upload_time": "2020-04-06T14:21:48", "upload_time_iso_8601": "2020-04-06T14:21:48.303128Z", "url": "https://files.pythonhosted.org/packages/c3/95/ced87fde9d660bee3fe62d8e6ff5292ef83ed7f8a0aa9f01cbe379931bf1/sagemaker_pytorch_inference-1.4.0.tar.gz", "yanked": false}], "1.4.1": [{"comment_text": "", "digests": {"md5": "92522382db00b58b5984c98cca27e63e", "sha256": "aec8bd4d634bf519d0bd811ceafb95fcbab633483c8861370e40aeda4f91755a"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.4.1.tar.gz", "has_sig": true, "md5_digest": "92522382db00b58b5984c98cca27e63e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11701, "upload_time": "2020-04-16T14:20:09", "upload_time_iso_8601": "2020-04-16T14:20:09.195292Z", "url": "https://files.pythonhosted.org/packages/a6/4c/e7231cd94b433917aa458e2b5599d57e5809152279e2f9aa3cf73b65a113/sagemaker_pytorch_inference-1.4.1.tar.gz", "yanked": false}], "1.4.2": [{"comment_text": "", "digests": {"md5": "8cd8d752f1348579bc5f74d942d6c0f0", "sha256": "49725557ff163fb0a426a091289c53fa6e611ebffc5b0cdfc8a3e51bdb3f1ef1"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.4.2.tar.gz", "has_sig": true, "md5_digest": "8cd8d752f1348579bc5f74d942d6c0f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11700, "upload_time": "2020-04-20T14:20:08", "upload_time_iso_8601": "2020-04-20T14:20:08.486122Z", "url": "https://files.pythonhosted.org/packages/73/75/44048e8a9ce738bbb48bd8c5a0f73835dc8066a7645e722f6a5702c4b132/sagemaker_pytorch_inference-1.4.2.tar.gz", "yanked": false}], "1.4.3": [{"comment_text": "", "digests": {"md5": "0ae02d5bf514695357e6359c679c529a", "sha256": "2c4cfe7b15022edb338646ca9eabb84cfcc5cae7c3cd94bdf4f551d94cd6f9a5"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.4.3.tar.gz", "has_sig": true, "md5_digest": "0ae02d5bf514695357e6359c679c529a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11694, "upload_time": "2020-04-28T14:20:10", "upload_time_iso_8601": "2020-04-28T14:20:10.275109Z", "url": "https://files.pythonhosted.org/packages/1f/2b/fdfa0912c31e87b96d7bc5c8244cff780b5fa49b559f668f818bee9de4c5/sagemaker_pytorch_inference-1.4.3.tar.gz", "yanked": false}], "1.4.3.post0": [{"comment_text": "", "digests": {"md5": "92e1a5dc522e2f55cc284a19379e87ea", "sha256": "19de1f3a330590b4619ba9e274b3d64f6867c162bb4534db570000b20563ce71"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.4.3.post0.tar.gz", "has_sig": true, "md5_digest": "92e1a5dc522e2f55cc284a19379e87ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11738, "upload_time": "2020-04-30T14:20:54", "upload_time_iso_8601": "2020-04-30T14:20:54.129011Z", "url": "https://files.pythonhosted.org/packages/94/19/09d6ed36804951c4544d2d1bfeb717edf4b993e41c45572a853cfe1eca07/sagemaker_pytorch_inference-1.4.3.post0.tar.gz", "yanked": false}], "1.4.4": [{"comment_text": "", "digests": {"md5": "5ca669e60c5820010d06ae5ce5056220", "sha256": "bc59f94ff71d38fcee2b81376436d370900af2978e76627daf43ede8c5e18da4"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.4.4.tar.gz", "has_sig": true, "md5_digest": "5ca669e60c5820010d06ae5ce5056220", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11898, "upload_time": "2020-05-04T14:20:48", "upload_time_iso_8601": "2020-05-04T14:20:48.508044Z", "url": "https://files.pythonhosted.org/packages/7a/05/c74be912b826dc1e6eaa2ad56e9e2fef8c64d8159b8a4c180db3357eaa5e/sagemaker_pytorch_inference-1.4.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5ca669e60c5820010d06ae5ce5056220", "sha256": "bc59f94ff71d38fcee2b81376436d370900af2978e76627daf43ede8c5e18da4"}, "downloads": -1, "filename": "sagemaker_pytorch_inference-1.4.4.tar.gz", "has_sig": true, "md5_digest": "5ca669e60c5820010d06ae5ce5056220", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11898, "upload_time": "2020-05-04T14:20:48", "upload_time_iso_8601": "2020-05-04T14:20:48.508044Z", "url": "https://files.pythonhosted.org/packages/7a/05/c74be912b826dc1e6eaa2ad56e9e2fef8c64d8159b8a4c180db3357eaa5e/sagemaker_pytorch_inference-1.4.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:59:17 2020"}