{"info": {"author": "Kaan Gen\u00e7", "author_email": "pypi@kaangenc.me", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)", "Natural Language :: English", "Programming Language :: Python :: 3 :: Only", "Topic :: Software Development :: Libraries", "Topic :: Utilities"], "description": "redditcurl\n==========\n    \n.. image:: https://travis-ci.org/SeriousBug/redditcurl.svg?branch=master\n   :target: https://travis-ci.org/SeriousBug/redditcurl\n\n.. image:: https://coveralls.io/repos/SeriousBug/redditcurl/badge.svg?branch=master&service=github\n   :target: https://coveralls.io/github/SeriousBug/redditcurl?branch=master\n\n\nredditcurl is a small tool to download the pages you have saved on Reddit.\nredditcurl can handle:\n\n* Direct image links\n* imgur links and albums\n* Gfycat links\n* Deviantart submissions\n* RedditBooru galleries\n* Twitter posts\n\nInstallation\n------------\n\nredditcurl is available on the `Python Package Index <https://pypi.python.org/pypi/redditcurl/>`_\n. You can install it via ``pip3 install redditcurl``. \nIf you are installing from sources instead, you can install it by running ``python3 setup.py build install``.\n\nRequirements\n------------\n\nOutside standard library, redditcurl requires:\n\n* ``requests``\n* ``praw``\n* ``beautifulsoup4``\n\nSetuptools should automatically install these while installing redditcurl.\n\nUsage\n-----\n\nTo start using redditcurl, simply call it with a save directory::\n\n    % redditcurl -d /home/karmanaut/images\n    Please visit https://www.reddit.com/api/v1/authorize/?state=... to authorize access to your account history.\n\nRedditcurl will print an authorization URL for you. Visit that and allow redditcurl to access your history.\nOnce you accept the authorization, you will be redirected to a page. Copy the authorization code you are given, and paste it into redditcurl, and you are done.\n\nBy default, redditcurl will use 20 processes to process the links and download the images.\nIf you want to disable multiprocessing, or use more processes, you can pick the number of processes with `-c` or `--processes`.\n\nAlso by default, redditcurl will give names to the downloaded images, based on the titles of the reddit submissions. If you want to simply keep the names of the downloaded files, you can use `-n` or `--notitles`.\n\nSo for example, if you want redditcurl to download your saved images, and store them at `/home/karmanaut/images` without using their titles, and use 10 processes while downloading, you can do::\n\n    % redditcurl -d /home/karmanaut/images -c 10 -n\n\nIf you want redditcurl to remove the images from your saved images as it downloads them, you can use `-r` or `--remove`. Note that redditcurl keeps track of the images that have been downloaded, and will not re-download them the next time it is run, so you should not need to use this flag.\n\nFinally, redditcurl will print out some messages while it runs, including some warnings about failed downloads, and a total count of downloaded and failed images. If you don't want any output, you can use `-s` or `--silent`.\n\nFor example, if you want redditcurl to download the images, without using multiprocessing, print nothing while running, and remove the saved images if the downloads succeed, you can do::\n\n    % redditcurl -d /home/karmanaut/images -c 1 -s -r\n\nConfiguring\n-----------\n\nYou can set up a configuration file instead of setting everything up with command-line flags. This configuration file should be located at `$XDG_CONFIG_HOME/redditcurl`, or `.config/redditcurl` if that variable is not set. A sample configuration file is below::\n\n    [redditcurl]\n    savedir = /home/karmanaut/Pictures\n    processes = 10\n    subfolders = true\n\nYou can set up all command line options in the configuration file as well, simply use their long names without the dashes. Note that no variable expansion or globbing is done on savedir, so it must be set to an absolute path.\n\nEven if you don't create a configuration file yourself, redditcurl will write one the first time it is run.\n    \nImporting\n---------\n\nredditcurl can be easily imported into your project.\n\n    >>> from redditcurl import manager\n    >>> my_image = \"http://i.imgur.com/gUQnp73.gif\"\n    >>> save_path = \"images/\"\n    >>> file_name = \"funny_picture\"\n    >>> manager.manage_download(my_image, save_path, file_name)\n    ('http://i.imgur.com/gUQnp73.gif', True)\n\n`redditcurl.manage_download` will pick the correct function and download the image for you.\nWhen download finishes, it will return a tuple, contaning the url and `True` if the download succeeded, `False` otherwise.\n\nExtending\n---------\n\nAdding support for new websites to redditcurl is easy. You will need to write a python package that provides two functions:\n\n``match(url)``\n  Accepts a string 'url', and returns True if the package can download this image, False otherwise.\n\n``download(url, path, filename=\"\")``\n  Accepts 3 strings; 'url', the image that will be downloaded, 'path', the directory where the downloaded image will be saved, and 'filename', the name that should be given to this file. Note that filename can be an empty string, in which case the downloader should keep the name of the file as it is on the website. Also keep in mind that filename will not contain the extension of the file, the downloader should add the extension.\n\nPlace this package or file into ``redditcurl/websites``, and edit ``redditcurl/websites/__init__.py`` to import this new package and add it into ``downloaders`` list.\n\nLicensing\n---------\n\nredditcurl is licensed with GPLv3. Please see the file `COPYING` for details.\n", "description_content_type": null, "docs_url": null, "download_url": "https://github.com/SeriousBug/redditcurl/releases", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/SeriousBug/redditcurl", "keywords": "reddit,images,download", "license": "GPLv3", "maintainer": null, "maintainer_email": null, "name": "redditcurl", "package_url": "https://pypi.org/project/redditcurl/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/redditcurl/", "project_urls": {"Download": "https://github.com/SeriousBug/redditcurl/releases", "Homepage": "https://github.com/SeriousBug/redditcurl"}, "release_url": "https://pypi.org/project/redditcurl/2.1.1/", "requires_dist": null, "requires_python": null, "summary": "Download the images you saved on Reddit.", "version": "2.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://travis-ci.org/SeriousBug/redditcurl\" rel=\"nofollow\"><img alt=\"https://travis-ci.org/SeriousBug/redditcurl.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4b0ff7266ba18ce50cb32e89d5f09c4414140bb8/68747470733a2f2f7472617669732d63692e6f72672f536572696f75734275672f7265646469746375726c2e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/SeriousBug/redditcurl?branch=master\" rel=\"nofollow\"><img alt=\"https://coveralls.io/repos/SeriousBug/redditcurl/badge.svg?branch=master&amp;service=github\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/23a1a55cd91a276ec8414ed52678c3ae47b27f49/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f536572696f75734275672f7265646469746375726c2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562\"></a>\n<p>redditcurl is a small tool to download the pages you have saved on Reddit.\nredditcurl can handle:</p>\n<ul>\n<li>Direct image links</li>\n<li>imgur links and albums</li>\n<li>Gfycat links</li>\n<li>Deviantart submissions</li>\n<li>RedditBooru galleries</li>\n<li>Twitter posts</li>\n</ul>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>redditcurl is available on the <a href=\"https://pypi.python.org/pypi/redditcurl/\" rel=\"nofollow\">Python Package Index</a>\n. You can install it via <tt>pip3 install redditcurl</tt>.\nIf you are installing from sources instead, you can install it by running <tt>python3 setup.py build install</tt>.</p>\n</div>\n<div id=\"requirements\">\n<h2>Requirements</h2>\n<p>Outside standard library, redditcurl requires:</p>\n<ul>\n<li><tt>requests</tt></li>\n<li><tt>praw</tt></li>\n<li><tt>beautifulsoup4</tt></li>\n</ul>\n<p>Setuptools should automatically install these while installing redditcurl.</p>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>To start using redditcurl, simply call it with a save directory:</p>\n<pre>% redditcurl -d /home/karmanaut/images\nPlease visit https://www.reddit.com/api/v1/authorize/?state=... to authorize access to your account history.\n</pre>\n<p>Redditcurl will print an authorization URL for you. Visit that and allow redditcurl to access your history.\nOnce you accept the authorization, you will be redirected to a page. Copy the authorization code you are given, and paste it into redditcurl, and you are done.</p>\n<p>By default, redditcurl will use 20 processes to process the links and download the images.\nIf you want to disable multiprocessing, or use more processes, you can pick the number of processes with <cite>-c</cite> or <cite>\u2013processes</cite>.</p>\n<p>Also by default, redditcurl will give names to the downloaded images, based on the titles of the reddit submissions. If you want to simply keep the names of the downloaded files, you can use <cite>-n</cite> or <cite>\u2013notitles</cite>.</p>\n<p>So for example, if you want redditcurl to download your saved images, and store them at <cite>/home/karmanaut/images</cite> without using their titles, and use 10 processes while downloading, you can do:</p>\n<pre>% redditcurl -d /home/karmanaut/images -c 10 -n\n</pre>\n<p>If you want redditcurl to remove the images from your saved images as it downloads them, you can use <cite>-r</cite> or <cite>\u2013remove</cite>. Note that redditcurl keeps track of the images that have been downloaded, and will not re-download them the next time it is run, so you should not need to use this flag.</p>\n<p>Finally, redditcurl will print out some messages while it runs, including some warnings about failed downloads, and a total count of downloaded and failed images. If you don\u2019t want any output, you can use <cite>-s</cite> or <cite>\u2013silent</cite>.</p>\n<p>For example, if you want redditcurl to download the images, without using multiprocessing, print nothing while running, and remove the saved images if the downloads succeed, you can do:</p>\n<pre>% redditcurl -d /home/karmanaut/images -c 1 -s -r\n</pre>\n</div>\n<div id=\"configuring\">\n<h2>Configuring</h2>\n<p>You can set up a configuration file instead of setting everything up with command-line flags. This configuration file should be located at <cite>$XDG_CONFIG_HOME/redditcurl</cite>, or <cite>.config/redditcurl</cite> if that variable is not set. A sample configuration file is below:</p>\n<pre>[redditcurl]\nsavedir = /home/karmanaut/Pictures\nprocesses = 10\nsubfolders = true\n</pre>\n<p>You can set up all command line options in the configuration file as well, simply use their long names without the dashes. Note that no variable expansion or globbing is done on savedir, so it must be set to an absolute path.</p>\n<p>Even if you don\u2019t create a configuration file yourself, redditcurl will write one the first time it is run.</p>\n</div>\n<div id=\"importing\">\n<h2>Importing</h2>\n<p>redditcurl can be easily imported into your project.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; from redditcurl import manager\n&gt;&gt;&gt; my_image = \"http://i.imgur.com/gUQnp73.gif\"\n&gt;&gt;&gt; save_path = \"images/\"\n&gt;&gt;&gt; file_name = \"funny_picture\"\n&gt;&gt;&gt; manager.manage_download(my_image, save_path, file_name)\n('http://i.imgur.com/gUQnp73.gif', True)\n</pre>\n</blockquote>\n<p><cite>redditcurl.manage_download</cite> will pick the correct function and download the image for you.\nWhen download finishes, it will return a tuple, contaning the url and <cite>True</cite> if the download succeeded, <cite>False</cite> otherwise.</p>\n</div>\n<div id=\"extending\">\n<h2>Extending</h2>\n<p>Adding support for new websites to redditcurl is easy. You will need to write a python package that provides two functions:</p>\n<dl>\n<dt><tt>match(url)</tt></dt>\n<dd>Accepts a string \u2018url\u2019, and returns True if the package can download this image, False otherwise.</dd>\n<dt><tt>download(url, path, <span class=\"pre\">filename=\"\")</span></tt></dt>\n<dd>Accepts 3 strings; \u2018url\u2019, the image that will be downloaded, \u2018path\u2019, the directory where the downloaded image will be saved, and \u2018filename\u2019, the name that should be given to this file. Note that filename can be an empty string, in which case the downloader should keep the name of the file as it is on the website. Also keep in mind that filename will not contain the extension of the file, the downloader should add the extension.</dd>\n</dl>\n<p>Place this package or file into <tt>redditcurl/websites</tt>, and edit <tt>redditcurl/websites/__init__.py</tt> to import this new package and add it into <tt>downloaders</tt> list.</p>\n</div>\n<div id=\"licensing\">\n<h2>Licensing</h2>\n<p>redditcurl is licensed with GPLv3. Please see the file <cite>COPYING</cite> for details.</p>\n</div>\n\n          </div>"}, "last_serial": 2081013, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "17e4c665c3bf23e5c4c2dbde8975c747", "sha256": "bfb75e2d2834eeb89a628e5517a66977797008ddd0f0f1480470cc37c5c3c090"}, "downloads": -1, "filename": "redditcurl-0.1.1.tar.gz", "has_sig": false, "md5_digest": "17e4c665c3bf23e5c4c2dbde8975c747", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18856, "upload_time": "2015-03-03T08:55:35", "upload_time_iso_8601": "2015-03-03T08:55:35.278508Z", "url": "https://files.pythonhosted.org/packages/ce/78/19fd04b2ba33098d00f9dfa2ba4b38c09398a18fce1125a6990728b727bf/redditcurl-0.1.1.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "907e187f28f93bd1632a9f0b2244c691", "sha256": "c3abd801cf76e4c62329827f671931bd5ac981ff30645e9fc82136df37f7113b"}, "downloads": -1, "filename": "redditcurl-0.2.0.tar.gz", "has_sig": false, "md5_digest": "907e187f28f93bd1632a9f0b2244c691", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19111, "upload_time": "2015-03-08T10:03:07", "upload_time_iso_8601": "2015-03-08T10:03:07.458191Z", "url": "https://files.pythonhosted.org/packages/fc/7a/50f6d8862643fd0c3fae8729237d83d409547cc8adcca97d56209bbc0216/redditcurl-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "9c89e66b9987d13e061fc5effea1f80d", "sha256": "29a32161cb1b7a2e6fb58d02381e7b18663da68dce017cd35a2047411e83787c"}, "downloads": -1, "filename": "redditcurl-0.3.0.tar.gz", "has_sig": false, "md5_digest": "9c89e66b9987d13e061fc5effea1f80d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19362, "upload_time": "2015-03-11T17:52:17", "upload_time_iso_8601": "2015-03-11T17:52:17.904872Z", "url": "https://files.pythonhosted.org/packages/c5/01/3387f253ea36f1360a613731868c00a0a881a5a981462c3200b340b500ac/redditcurl-0.3.0.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "d3af8a9a5c48e1e726643f106d888ff2", "sha256": "541c3c9d32e9bd7428b4e5045a4c96303f32d2d547a99f260213a0204e30c581"}, "downloads": -1, "filename": "redditcurl-0.4.0.tar.gz", "has_sig": false, "md5_digest": "d3af8a9a5c48e1e726643f106d888ff2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21075, "upload_time": "2015-09-12T12:37:14", "upload_time_iso_8601": "2015-09-12T12:37:14.727202Z", "url": "https://files.pythonhosted.org/packages/3e/2b/46d91a8291ae5c725a3d0a2c53cfee3c2a1e6fa8cf27ab3473c03b1ea099/redditcurl-0.4.0.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "3776cdd0bef3e547cfdbdfffc376011e", "sha256": "a20d994334a720f7f3cf4a9cd762ec27cd9f9b04f406378123deccb4cfd52aad"}, "downloads": -1, "filename": "redditcurl-0.4.1.tar.gz", "has_sig": false, "md5_digest": "3776cdd0bef3e547cfdbdfffc376011e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21566, "upload_time": "2015-09-13T16:59:55", "upload_time_iso_8601": "2015-09-13T16:59:55.234713Z", "url": "https://files.pythonhosted.org/packages/95/9b/3eebabc10b3252529ea92a09cbec867cbfb8a598a0c3004fd4b879197763/redditcurl-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "2a5aea5431a49875e79efebc3c41602c", "sha256": "5641eff72ef788e0b19b0ad89770e14d30a728a6f1b7c31ce75d859331fd2176"}, "downloads": -1, "filename": "redditcurl-0.4.2.tar.gz", "has_sig": false, "md5_digest": "2a5aea5431a49875e79efebc3c41602c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21820, "upload_time": "2015-09-17T16:37:48", "upload_time_iso_8601": "2015-09-17T16:37:48.053427Z", "url": "https://files.pythonhosted.org/packages/b9/2b/59312b142007106257698f82b7e1ddd0061962ce138cd27ab7bde153e8c0/redditcurl-0.4.2.tar.gz", "yanked": false}], "0.4.3": [{"comment_text": "", "digests": {"md5": "56b7acdd4e4e675dcbf76d9b3af1a27b", "sha256": "197b53b4e796e42adb0ffb69d5b20101dcbff5b80d5123a9899931d389962002"}, "downloads": -1, "filename": "redditcurl-0.4.3.tar.gz", "has_sig": false, "md5_digest": "56b7acdd4e4e675dcbf76d9b3af1a27b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22025, "upload_time": "2015-09-19T13:55:55", "upload_time_iso_8601": "2015-09-19T13:55:55.749662Z", "url": "https://files.pythonhosted.org/packages/e5/de/e923ed4faf0f64770f868aa841a3ce903eadec41beb723d80a86642c0568/redditcurl-0.4.3.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "89d145d47797ce3b91aeefe1133b0a62", "sha256": "a9769d1a80136a4150f53443b8c11cab4e20a767efbb693b3c05430baf8b1d6f"}, "downloads": -1, "filename": "redditcurl-1.0.0.tar.gz", "has_sig": false, "md5_digest": "89d145d47797ce3b91aeefe1133b0a62", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23082, "upload_time": "2015-09-24T22:19:57", "upload_time_iso_8601": "2015-09-24T22:19:57.126180Z", "url": "https://files.pythonhosted.org/packages/bd/6d/eaabb7aadf07f5c0d60975385c7ed877758c40691f00169df96d06a8ce34/redditcurl-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "935fe94f6358d47a239b483d0d578f46", "sha256": "bc7c707ea515d12f6e7c456fe257b1c8af3d283fd94bc0f39cbe3c4c1479d687"}, "downloads": -1, "filename": "redditcurl-1.0.1.tar.gz", "has_sig": false, "md5_digest": "935fe94f6358d47a239b483d0d578f46", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23315, "upload_time": "2015-10-05T22:11:27", "upload_time_iso_8601": "2015-10-05T22:11:27.727752Z", "url": "https://files.pythonhosted.org/packages/47/86/6c9c2b9a9ff9449aa87a040c45ffbb2e137c53a268f2dae4a1d2376ca015/redditcurl-1.0.1.tar.gz", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "fa9889839934692b6ebe9de7dd2d0821", "sha256": "8f5545207df0668fa51ddaa240b590a6c5d9c930e7a4fcc567f4ed35d8ea90ae"}, "downloads": -1, "filename": "redditcurl-2.0.0.tar.gz", "has_sig": false, "md5_digest": "fa9889839934692b6ebe9de7dd2d0821", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23805, "upload_time": "2015-11-28T19:50:12", "upload_time_iso_8601": "2015-11-28T19:50:12.974223Z", "url": "https://files.pythonhosted.org/packages/c8/ef/546b9c736d6b6c97883031450dc18f5b5aecda41fce91a752132a1d4e253/redditcurl-2.0.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "c489af3341fdb07d0010aef56325f458", "sha256": "b215cc7f2083cf1148b4191df7445a454bcfa629316f00d26c8b607e836dde52"}, "downloads": -1, "filename": "redditcurl-2.0.1.tar.gz", "has_sig": false, "md5_digest": "c489af3341fdb07d0010aef56325f458", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23858, "upload_time": "2015-12-21T14:45:32", "upload_time_iso_8601": "2015-12-21T14:45:32.579818Z", "url": "https://files.pythonhosted.org/packages/dd/29/e3df5eb25babb419a82e2b2f23e4d4c47f6d8f6758d1350b9fb9fdf46bab/redditcurl-2.0.1.tar.gz", "yanked": false}], "2.1.0": [{"comment_text": "", "digests": {"md5": "318488c736561ca3995eb707489861c6", "sha256": "9d0cfd98de5a07413ba97763811254af1037124dc25a2db28b5059745fd94e22"}, "downloads": -1, "filename": "redditcurl-2.1.0.tar.gz", "has_sig": false, "md5_digest": "318488c736561ca3995eb707489861c6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24136, "upload_time": "2016-01-18T13:58:44", "upload_time_iso_8601": "2016-01-18T13:58:44.836768Z", "url": "https://files.pythonhosted.org/packages/65/bb/a4401ab409904749bb4d8ba28d5ac909ccea62cf3aa3a4103647ad963fea/redditcurl-2.1.0.tar.gz", "yanked": false}], "2.1.1": [{"comment_text": "", "digests": {"md5": "bdb2b5dff200512e6b5a4f635720abc0", "sha256": "c8c14bd945fe93db0a6368be07486b6b863cfd7e2aac3b0ccdf8d59f67c8bfb7"}, "downloads": -1, "filename": "redditcurl-2.1.1.tar.gz", "has_sig": false, "md5_digest": "bdb2b5dff200512e6b5a4f635720abc0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23983, "upload_time": "2016-04-24T13:27:04", "upload_time_iso_8601": "2016-04-24T13:27:04.492948Z", "url": "https://files.pythonhosted.org/packages/36/8e/db4fa073e3dd90e71f348bc72afb3c17e23374a050272f0aa71c3db9d6a2/redditcurl-2.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "bdb2b5dff200512e6b5a4f635720abc0", "sha256": "c8c14bd945fe93db0a6368be07486b6b863cfd7e2aac3b0ccdf8d59f67c8bfb7"}, "downloads": -1, "filename": "redditcurl-2.1.1.tar.gz", "has_sig": false, "md5_digest": "bdb2b5dff200512e6b5a4f635720abc0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23983, "upload_time": "2016-04-24T13:27:04", "upload_time_iso_8601": "2016-04-24T13:27:04.492948Z", "url": "https://files.pythonhosted.org/packages/36/8e/db4fa073e3dd90e71f348bc72afb3c17e23374a050272f0aa71c3db9d6a2/redditcurl-2.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:05:58 2020"}