{"info": {"author": "Adobe", "author_email": "noreply@adobe.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Web Environment", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy", "Topic :: Internet :: WWW/HTTP :: Dynamic Content", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Text Processing :: Markup :: HTML"], "description": "# Ops CLI\n[![Build Status](https://www.travis-ci.com/adobe/ops-cli.svg?token=8uHqfhgsxdvJ93qWAxhn&branch=master)](https://www.travis-ci.com/adobe/ops-cli) [![Docker pull](https://img.shields.io/docker/pulls/adobe/ops-cli)](https://hub.docker.com/r/adobe/ops-cli) [![](https://images.microbadger.com/badges/version/adobe/ops-cli.svg)](https://microbadger.com/images/adobe/ops-cli \"Get your own version badge on microbadger.com\") [![License](https://img.shields.io/github/license/adobe/ops-cli)](https://github.com/adobe/ops-cli/blob/master/LICENSE)\n\n**From version 2.0 onward, `ops-cli` requires Python3.  \nIf you're still using Python2, use `ops-cli` version <2.0**\n\n**ops-cli** is a python wrapper for [Terraform](https://www.terraform.io/), [Ansible](https://www.ansible.com/) and SSH for cloud automation. \n\nWe use multiple tools to manage our infrastructure at Adobe. The purpose of `ops-cli` is to gather the common cluster configurations in a single place and, based on these, interact with the above mentioned tools. In this way, we can avoid duplication and can quickly spin up new clusters (either production or development ones). All we need to do is customize the cluster configuration file ([example here](https://github.com/adobe/ops-cli/blob/master/examples/aws-kubernetes/clusters/my-kubernetes-cluster.yaml)).\n\n`ops-cli` integrates with the Azure and AWS cli, in order to provide inventory, ssh, sync, tunnel and the possibility to run ansible playbooks on a fleet of EC2 instances.\nIt can be used to add a layer of templating (using jinja2) on top of Terraform files. This is useful for removing duplicated code when it comes to spinning up infrastructure across multiple environments (stage/sandbox/prod) and across teams. Useful for both AWS and [Kubernetes deployments](https://github.com/adobe/ops-cli/tree/master/examples/aws-kubernetes).\n\n# Table of Contents\n\n<!--ts-->\n   * [How it works?](#how-it-works)\n   * [Use cases](#use-cases)\n      * [Manage AWS EC2 instances](#manage-aws-ec2-instances)\n      * [Terraform](#terraform)\n      * [Run terraform by using hierarchical configs](#run-terraform-by-using-hierarchical-configs)\n      * [Create Kubernetes cluster (using AWS EKS)](#create-kubernetes-cluster-using-aws-eks)\n   * [Installing](#installing)\n      * [Local](#local)\n         * [Virtualenv](#virtualenv)\n         * [Ops tool installation](#ops-tool-installation)\n            * [Python 3](#python-3)\n         * [Terraform](#terraform-1)\n      * [Using docker image](#using-docker-image)\n      * [Configuring](#configuring)\n         * [AWS](#aws)\n         * [Azure](#azure)\n      * [Examples](#examples)\n      * [Usage help](#usage-help)\n      * [More help](#more-help)\n      * [Tool configuration: .opsconfig.yaml](#tool-configuration-opsconfigyaml)\n         * [Inventory](#inventory)\n            * [AWS example](#aws-example)\n            * [Azure example](#azure-example)\n            * [Inventory usage](#inventory-usage)\n         * [Terraform](#terraform-2)\n            * [Terraform landscape](#terraform-landscape)\n         * [SSH](#ssh)\n            * [SSHPass](#sshpass)\n         * [Play](#play)\n         * [Run command](#run-command)\n         * [Sync files](#sync-files)\n         * [Noop](#noop)\n         * [Packer](#packer)\n      * [Secrets Management](#secrets-management)\n         * [Vault](#vault)\n         * [Amazon Secrets Manager (SSM)](#amazon-secrets-manager-ssm)\n      * [Using jinja2 filters in playbooks and terraform templates](#using-jinja2-filters-in-playbooks-and-terraform-templates)\n      * [SKMS](#skms)\n   * [Development](#development)\n      * [Install ops in development mode](#install-ops-in-development-mode)\n      * [Running tests](#running-tests)\n   * [Troubleshooting](#troubleshooting)\n   * [License](#license)\n\n<!-- Added by: amuraru, at: Tue Nov 12 10:23:17 EET 2019 -->\n\n<!--te-->\n\n# How it works?\n\nYou define a cluster configuration, using a yaml file. The yaml file contains different kind of sections, one for each plugin. For instance, you could have a section for Terraform files, a section for AWS instructions, Kubernetes Helm charts and so forth.\n\n# Use cases\n\n## Manage AWS EC2 instances\n\nOnce you define your cluster configuration, you can run `ops` commands such as seeing the instance inventory.\n```sh\n# fetch instances from AWS and prints them\nops clusters/mycluster.yaml inventory --limit webapp \n```\n\nThis would output something like:\n![ops](https://user-images.githubusercontent.com/952836/52021401-9f553c80-24fd-11e9-802c-155f5a0e7f63.png)\n\nThen you can run `ssh`, `play`, `run`, `sync` etc.\n\n```sh\n# SSH to one of the nodes (can handle bastion as well)\nops clusters/mycluster.yaml ssh webapp-01\n\n# run a deployment playbook via ansible\nops clusters/mycluster.yaml play ansible/playbooks/task/webapp/deployment.yaml -- -e version=5.36.2 -u ec2-user --limit webapp\n\n# run command on all selected nodes\nops clusters/mycluster.yaml run \"sudo yum upgrade myawesomeapp; sudo service myawesomeapp restart\" -- -u ec2-user --limit '\"aam_app_group=canary;az=us-east-1a\"'\n\n# copy file to all servers\nops clusters/mycluster.yaml sync /tmp/myfile webapp: -l ec2-user\n\n# create a tunnel\nops clusters/stage.yaml ssh --tunnel --local 8080 --remote 8080 stage-thanos-1 -l ec2-user\n```\n\nSee [examples/features/inventory](https://github.com/adobe/ops-cli/tree/master/examples/features/inventory)\n\n## Terraform\n\n```sh\n# Performs jinja templating (if any) and runs terraform plan\nops clusters/mycluster.yaml terraform --path-name aws-eks plan\n\n# Run terraform apply, with the possibility to sync the tf state files remotely (currently, AWS S3 bucket is supported + DynamoDB for locking). \nops clusters/mycluster.yaml terraform --path-name aws-eks apply\n```\n\n![ops-terraform](https://user-images.githubusercontent.com/952836/52021396-9bc1b580-24fd-11e9-9da8-00fb68bd5c72.png)\n\n## Run terraform by using hierarchical configs\n\nSee [examples/features/terraform-hierarchical](https://github.com/adobe/ops-cli/tree/master/examples/features/terraform-hierarchical)\n\n## Create Kubernetes cluster (using AWS EKS)\n\nSee [examples/aws-kubernetes](https://github.com/adobe/ops-cli/tree/master/examples/aws-kubernetes)\n\n# Installing\n\n## Local\n\n### Virtualenv\nHere is a link about how to install and use virtualenv: \nhttps://virtualenv.pypa.io/en/stable/\n\n### Ops tool installation\n\n#### Python 3\n```sh\n# Make sure pip is up to date\ncurl https://bootstrap.pypa.io/get-pip.py | python3\n\n# Install virtualenv\npip install --upgrade virtualenv\npip install --upgrade virtualenvwrapper\n\necho 'export WORKON_HOME=$HOME/.virtualenvs' >> ~/.bash_profile\necho 'source /usr/local/bin/virtualenvwrapper.sh' >> ~/.bash_profile\nsource ~/.bash_profile\n\n# create virtualenv\nmkvirtualenv ops\nworkon ops\n\n# uninstall previous `ops` version (if you have it)\npip uninstall ops --yes\n\n# install ops-cli v2.0.4 stable release\npip install --upgrade ops-cli\n```\n\n\n### Terraform\nOptionally, install terraform to be able to access terraform plugin. See https://www.terraform.io/intro/getting-started/install.html\nAlso for pretty formatting of terraform plan output you can install https://github.com/coinbase/terraform-landscape (use gem install for MacOS)\n\n\n## Using docker image\n\nYou can try out `ops-cli`, by using docker. The docker image has all required prerequisites (python, terraform, helm, git, ops-cli etc).\n\nTo start out a container, running the latest `ops-cli` docker image run:\n```sh\ndocker run -it adobe/ops-cli:2.0.4 bash\n```\n\nAfter the container has started, you can start using `ops-cli`:\n```sh\nops help\n# usage: ops [-h] [--root-dir ROOT_DIR] [--verbose] [-e EXTRA_VARS]\n#           cluster_config_path\n#           {inventory,terraform,packer,ssh,play,run,sync,noop} ...\n\ngit clone https://github.com/adobe/ops-cli.git\ncd ops-cli\nls examples\n# aws-kubernetes\n# cassandra-stress\n# features\n\ncd examples/aws-kubernetes\nops clusters/my-kubernetes-cluster.yaml terraform --path-name aws-eks plan\n# in order to setup aws-kubernetes follow the steps from https://github.com/adobe/ops-cli/blob/master/examples/aws-kubernetes/README.md\n```\n\n\n## Configuring\n\n### AWS\nIf you plan to use ops with AWS, you must configure credentials for each account\n```shell\n$ aws configure --profile aws_account_name\n```\n\n### Azure\nTBD\n\n## Examples\n\nSee [examples/](https://github.com/adobe/ops-cli/tree/master/examples) folder:\n- cassandra-stress - n-node cassandra cluster used for stress-testing; a basic stress profile is included\n- spin up a Kubernetes cluster\n- distinct `ops` features\n\n## Usage help\nTo see all commands and a short description run `ops --help`\n```\nusage: ops [-h] [--root-dir ROOT_DIR] [--verbose] [-e EXTRA_VARS]\n           cluster_config_path\n           {inventory,terraform,packer,ssh,play,run,sync,noop} ...\n\nRun commands against a cluster definition\n\npositional arguments:\n  cluster_config_path   The cluster config path cluster.yaml\n  {inventory,terraform,packer,ssh,play,run,sync,noop}\n    inventory           Show current inventory data\n    terraform           Wrap common terraform tasks with full templated\n                        configuration support\n    packer              Wrap common packer tasks and inject variables from a\n                        cluster file\n    ssh                 SSH or create an SSH tunnel to a server in the cluster\n    play                Run an Ansible playbook\n    run                 Runs a command against hosts in the cluster\n    sync                Sync files from/to a cluster\n    noop                used to initialize the full container for api usage\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --root-dir ROOT_DIR   The root of the resource tree - it can be an absolute\n                        path or relative to the current dir\n  --verbose, -v         Get more verbose output from commands\n  -e EXTRA_VARS, --extra-vars EXTRA_VARS\n                        Extra variables to use. Eg: -e ssh_user=ssh_user\n```\n\n## More help\n\nEach sub-command includes additional help information that you can get by running:\n`ops examples/inventory/aam.yaml sync --help`\n\n## Tool configuration: .opsconfig.yaml\n\nSome tool settings are available via a [.opsconfig.yaml](https://github.com/adobe/ops/blob/master/src/ops/opsconfig.py) configuration file.\nThe file is looked-up in `/etc/opswrapper/.opsconfig.yaml`, then in `~/.opsconfig.yaml` and then in the project folder starting from the current dir and up to the root dir.\nAll the files found this way are merged together so that you can set some global defaults, then project defaults in the root dir of the project and\noverwrite them for individual envs. Eg: `~/.opsconfig.yaml`, `/project/.opsconfig.yaml`, `/project/clusters/dev/.opsconfig.yaml`\n\n### Inventory\n\nThe `inventory` command will list all the servers in a given cluster and cache the results for further operations on them (for instance, SSHing to a given node or running an ansible playbook).\n\nYou can always filter which nodes you want to display or use to run an ansible playbook on, by using the `--limit` argument (eg. `--limit webapp`). The extra filter is applied on the instance tags, which includes the instance name.\n\nThe way `inventory` works is by doing a describe command in AWS/Azure. The describe command matches all the nodes that have the tag \"cluster\" equal to the cluster name you have defined.\n\nIn order to configure it, you need to add the `inventory` section in your cluster configuration file ([example here](https://github.com/adobe/ops-cli/blob/master/examples/features/inventory/my-aws-cluster.yaml)).\n\n#### AWS example\n```\n---\ninventory:\n  - plugin: cns\n    args:\n      clusters:\n        - region: us-east-1\n          boto_profile: aam-npe # make sure you have this profile in your ~/.aws/credentials file\n          names: [mycluster1] # this assumes the EC2 nodes have the Tag Name \"cluster\" with Value \"mycluster1\"\n```\n\n#### Azure example\n```\n---\ninventory:\n  - plugin: azr\n    args:\n      tags: environment=prod\n      locations: westeurope,northeurope\n```\n\n#### Inventory usage\n```\nusage: ops cluster_config_path inventory [-h] [-e EXTRA_VARS]\n                                         [--refresh-cache] [--limit LIMIT]\n                                         [--facts]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTRA_VARS, --extra-vars EXTRA_VARS\n                        Extra variables to use. Eg: -e ssh_user=ssh_user\n  --refresh-cache       Refresh the cache for the inventory\n  --limit LIMIT         Limit run to a specific server subgroup. Eg: --limit\n                        newton-dcs\n  --facts               Show inventory facts for the given hosts\n```\n\n### Terraform\n```\nusage: ops cluster_config_path terraform [-h] [--var VAR] [--module MODULE]\n                                         [--resource RESOURCE] [--name NAME]\n                                         [--plan]\n                                         subcommand\n\npositional arguments:\n  subcommand           apply | console | destroy | import | output | plan |\n                       refresh | show | taint | template | untaint\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --var VAR             the output var to show\n  --module MODULE       for use with \"taint\", \"untaint\" and \"import\". The\n                        module to use. e.g.: vpc\n  --resource RESOURCE   for use with \"taint\", \"untaint\" and \"import\". The\n                        resource to target. e.g.: aws_instance.nat\n  --name NAME           for use with \"import\". The name or ID of the imported\n                        resource. e.g.: i-abcd1234\n  --plan                for use with \"show\", show the plan instead of the\n                        statefile\n  --skip-refresh        for use with \"plan\". Skip refresh of statefile\n  --raw-output          for use with \"plan\". Show raw plan output without piping through terraform landscape (if terraform landscape is not enabled in opsconfig.yaml this will have no impact)\n  --path-name PATH_NAME in case multiple terraform paths are defined, this\n                        allows to specify which one to use when running\n                        terraform\n\n    Examples:\n        # Create a new cluster with Terraform\n        ops clusters/qe1.yaml terraform plan\n        ops clusters/qe1.yaml terraform apply\n\n        # Update an existing cluster\n        ops clusters/qe1.yaml terraform plan\n        ops clusters/qe1.yaml terraform apply\n\n        # Get rid of a cluster and all of its components\n        ops clusters/qe1.yaml terraform destroy\n\n        # Retrieve all output from a previously created Terraform cluster\n        ops clusters/qe1.yaml terraform output\n\n        # Retrieve a specific output from a previously created Terraform cluster\n        ops clusters/qe1.yaml terraform output --var nat_public_ip\n\n        # Refresh a statefile (no longer part of plan)\n        ops clusters/qe1.yaml terraform refresh\n\n        # Taint a resource- forces a destroy, then recreate on next plan/apply\n        ops clusters/qe1.yaml terraform taint --module vpc --resource aws_instance.nat\n\n        # Untaint a resource\n        ops clusters/qe1.yaml terraform untaint --module vpc --resource aws_instance.nat\n\n        # Show the statefile in human-readable form\n        ops clusters/qe1.yaml terraform show\n\n        # Show the plan in human-readable form\n        ops clusters/qe1.yaml terraform show --plan\n\n        # View parsed jinja on the terminal\n        ops clusters/qe1.yaml terraform template\n\n        # Import an unmanaged existing resource to a statefile\n        ops clusters/qe1.yaml terraform import --module vpc --resource aws_instance.nat --name i-abcd1234\n\n        # Use the Terraform Console on a cluster\n        ops clusters/qe1.yaml terraform console\n\n        # Validate the syntax of Terraform files\n        ops clusters/qe1.yaml terraform validate\n\n        # Specify which terraform path to use\n        ops clusters/qe1.yaml terraform plan --path-name terraformFolder1\n```\n#### Terraform landscape\nFor pretty formatting of terraform plan output you can install https://github.com/coinbase/terraform-landscape (use gem install for MacOS). \nTo make `ops` use it you need to add `terraform.landscape: True` in opsconfig.yaml file.\n\n### SSH\n```\nusage: ops cluster_config_path ssh [-h] [-e EXTRA_VARS] [-l USER]\n                                   [--ssh-config SSH_CONFIG] [--index INDEX]\n                                   [--tunnel] [--ipaddress] [--local LOCAL]\n                                   [--remote REMOTE] [--proxy] [--nossh]\n                                   role [ssh_opts [ssh_opts ...]]\n\npositional arguments:\n  role                  Server role to ssh to. Eg: dcs\n  ssh_opts              Manual ssh options\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTRA_VARS, --extra-vars EXTRA_VARS\n                        Extra variables to use. Eg: -e ssh_user=ssh_user\n  -l USER, --user USER  SSH User\n  --ssh-config SSH_CONFIG\n                        Ssh config file name in the ./ansible dir\n  --index INDEX         Index of the server from the group\n  --tunnel              Use SSH tunnel, must pass --local and --remote\n  --ipaddress\n  --local LOCAL         local port for ssh proxy or ssh tunnel\n  --remote REMOTE       remote port for ssh tunnel\n  --proxy               Use SSH proxy, must pass --local\n  --nossh               Port tunnel a machine that does not have SSH. Implies\n                        --ipaddress, and --tunnel; requires --local and\n                        --remote\n\n    Examples:\n        # SSH using current username as remote username\n        ops clusters/qe1.yaml ssh nagios\n\n        # SSH using a different username\n        ops clusters/qe1.yaml ssh nagios -l ec2-user\n\n        # SSH to the second nagios instance\n        ops clusters/qe1.yaml ssh nagios --index 2\n\n        # SSH to a specific hostname, instead of the tagged role\n        ops clusters/qe1.yaml ssh full-hostname-here-1\n\n        # Create an SSH tunnel to Nagios forwarding the remote port 80 to local port 8080\n        ops clusters/qe1.yaml ssh --tunnel --remote 80 --local 8080 nagios\n\n        # Create an SSH tunnel to a host where the service is NOT listening on `localhost`\n        ops clusters/qe1.yaml ssh --tunnel --remote 80 --local 8080 nagios --ipaddress\n\n        # Create an SSH tunnel to a host with an open port which does NOT have SSH itself (Windows)\n        # Note that the connection will be made from the Bastion host\n        ops clusters/qe1.yaml ssh --tunnel --local 3389 --remote 3389 --nossh windowshost\n\n        # Create a proxy to a remote server that listens on a local port\n        ops clusters/qe1.yaml ssh --proxy --local 8080 bastion\n```\n\n#### SSHPass\n\nIn case you want to use the OSX Keychain to store your password and reuse across multiple nodes (e.g. running a playbook on 300 nodes and not having to enter the password for every node) follow the tutorial below:\n\n1. Open `Keychain Access` app on OSX\n  1. Create a new keychain (`File -> New Keychain`), let's say `aam`\n  2. Select the `aam` keychain and add a new password entry in this (`File -> New Password Item`):\n    - Name: `idm`\n    - Kind: `application password`\n    - Account: `your_ldap_account` (e.g. `johnsmith`)\n    - Where: `idm`\n\n2. Create `$HOME/bin` dir - this is where the scripts below are saved\n\n3. Create `~/bin/askpass` script and update the ldap account there:\n\n  ```bash\n  cat > ~/bin/askpass  <<\"EOF\"\n  #!/usr/bin/env bash\n  /usr/bin/security find-generic-password -a <your_ldap_account> -s idm -w $HOME/Library/Keychains/aam.keychain\n  EOF\n  chmod +x ~/bin/askpass\n  ```\n\n1. Checkout [notty github repo](https://github.com/pharaujo/notty), build and move the binary to `$HOME/bin/`\n\n1. Create `~/bin/sshpass` script:\n\n  ```bash\n  cat > $HOME/bin/sshpass <<\"EOF\"\n  #!/usr/bin/env bash\n  export DISPLAY=:99\n  export SSH_ASKPASS=\"$HOME/bin/askpass\"\n  [[ $1 == -d* ]] && shift\n  $HOME/bin/notty $@\n  EOF\n\n  chmod +x $HOME/bin/sshpass\n  ```\n\n1. Verify the setup works:\n\n  ```bash\n  # Connect to bastion\n  ~/bin/sshpass ssh -o StrictHostKeyChecking=no -l <your_ldap_account> <52.5.5.5>\n  ```\n\n1. Run `ops` tool\n\n\n### Play\n\nRun an ansible playbook.\n\n```\nusage: ops cluster_config_path play [-h] [-e EXTRA_VARS] [--ask-sudo-pass]\n                                    [--limit LIMIT]\n                                    playbook_path\n                                    [ansible_args [ansible_args ...]]\n\npositional arguments:\n  playbook_path         The playbook path\n  ansible_args          Extra ansible args\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTRA_VARS, --extra-vars EXTRA_VARS\n                        Extra variables to use. Eg: -e ssh_user=ssh_user\n  --ask-sudo-pass       Ask sudo pass for commands that need sudo\n  --limit LIMIT         Limit run to a specific server subgroup. Eg: --limit\n                        newton-dcs\n\n    Examples:\n        # Run an ansible playbook\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml\n\n        # Limit the run of a playbook to a subgroup\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml -- --limit dcs\n\n        # Overwrite or set a variable\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml -- -e city=paris\n\n        # Filter with tags\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml -- -t common\n\n        # Run a playbook and overwrite the default user\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml -- -u ec2-user\n```\n\n### Run command\n\nRun a bash command on the selected nodes.\n\n```\nusage: ops cluster_config_path run [-h] [--ask-sudo-pass] [--limit LIMIT]\n                                   host_pattern shell_command\n                                   [extra_args [extra_args ...]]\n\npositional arguments:\n  host_pattern     Limit the run to the following hosts\n  shell_command    Shell command you want to run\n  extra_args       Extra ansible arguments\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --ask-sudo-pass  Ask sudo pass for commands that need sudo\n  --limit LIMIT    Limit run to a specific server subgroup. Eg: --limit\n                   newton-dcs\n\n    Examples:\n        # Last 5 installed packages on each host\n        ops qe1.yaml run all 'sudo grep Installed /var/log/yum.log | tail -5'\n\n        # See nodetool status on each cassandra node\n        ops qe1.yaml run qe1-cassandra 'nodetool status'\n\n        # Complex limits\n        ops qe1.yaml run 'qe1-cassandra,!qe1-cassandra-0' 'nodetool status'\n\n        # Show how to pass other args\n```\n\n### Sync files\n\nPerforms `rsync` to/from a given set of nodes.\n\n```\nusage: ops cluster_config_path sync [-h] [-l USER] src dest [opts [opts ...]]\n\npositional arguments:\n  src                   Source dir\n  dest                  Dest dir\n  opts                  Rsync opts\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -l USER, --user USER  Value for remote user that will be used for ssh\n\n        rsync wrapper for ops inventory conventions\n\n        Example:\n\n        # rsync from remote dcs role\n        ops cluster.yml sync 'dcs[0]:/usr/local/demdex/conf' /tmp/configurator-data --user remote_user\n\n        # extra rsync options\n        ops cluster.yml sync 'dcs[0]:/usr/local/demdex/conf' /tmp/configurator-data -l remote_user -- --progress\n```\n\n### Noop\n```\nusage: ops cluster_config_path noop [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n```\n\n### Packer\n\nRuns [packer](https://www.packer.io/intro/), for creating images.\n\n```\nusage: ops cluster_config_path packer [-h] subcommand\n\npositional arguments:\n  subcommand  build | validate\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n    Examples:\n        # Validate a packer file\n        ops clusters/centos7.yaml packer validate\n\n        # Build a packer file\n        ops clusters/centos7.yaml packer build\n```\n\n## Secrets Management\n\nThere are cases where you need to reference sensitive data in your `cluster.yaml` file (credentials, passwords, tokens etc). Given that the cluster configuration file can be stored in a version control system (such as Git), the best practice is to not put sensitive data in the file itself. Instead, we can use `ops-cli` to fetch the desired credentials from a secrets manager such as Vault or Amazon SSM, at runtime.\n\n### Vault\n\nOps can manage the automatic generation of secrets and their push in Vault, without actually persisting the secrets in the cluster file.\nA cluster file will only need to use a construct like the following:\n```\ndb_password: \"{{'secret/campaign/generated_password'|managed_vault_secret(policy=128)}}\"\n```\nWhich will translate behind the scenes in :\n- look up in vault the secrets at secret/campaign/generated_password in the default key 'value' (Adobe convention that can be overridden with the key parameter)\n- if the value there is missing, generate a new secret using the engine passgen with a policy of length 128 characters\n- return the generated value\n- if the value at that path already exist, just return that value.\nThis allows us to just refer in cluster files a secret that actually exists in vault and make sure we only generate it once - if it was already created by os or any other system, we will just use what is already there.\nThe reference is by means of fixed form jinja call  added to the cluster file, which ends up interpreted later during the templating phase.\n\n### Amazon Secrets Manager (SSM)\n\nAmazon offers the possibility to use their [Secrets Manager](https://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html) in order to manage configuration data such as credentials, passwords and license keys.\n\nWe can use `ops-cli` to fetch the sensitive data from SSM, at runtime. Just define this in your cluster configuration file (eg. `mycluster.yaml`).\n\n```\ndb_password: \"{{ '/my/ssm/path' | read_ssm(aws_profile='myprofile') }}\"\n```\n\n`ops-cli` will read the SSM value by running a command similar to: `AWS_PROFILE=aam-npe aws ssm get-parameter --name \"/my/ssm/path\"  --region us-east-1 --with-decryption`.\nNote that you can specify the AWS region via `read_ssm(aws_profile='myprofile', region_name='us-west-2')`.\n\n\n## Using jinja2 filters in playbooks and terraform templates\n\nYou can register your own jinja2 filters that you can  use in the cluster config file, terraform templates and ansible playbooks\n\nAll ops commands look for filters in the following locations:\n- the python path\n- the .opsconfig.yaml [ansible.filter_plugins](https://github.com/adobe/ops/blob/master/src/ops/opsconfig.py#L58) setting (defaults to plugins/filter_plugins)\n\nExample simple filter:\n\n```\n# plugins/filter_plugin/myfilters.py\n\ndef my_filter(string):\n    return 'filtered: ' + string\n\n\nclass FilterModule(object):\n    def filters(self):\n        return {\n            'my_filter': my_filter\n        }\n\n# usage in playbook, templates, cluster config\n# test_custom_filters: \"{{ 'value' | my_filter }}\"\n```\n\n## SKMS\nCreate a file in `~/.skms/credentials.yaml` which looks like the following:\n```yaml\nendpoint: \"api.skms.mycompany.com\"\nusername: <username>\npassword: <password>\n```\n\n# Development\n\n## Install `ops` in development mode\n\n```\ngit clone https://github.com/adobe/ops-cli.git\ncd ops\n# Install openssl\nbrew install openssl libyaml\nenv LDFLAGS=\"-L$(brew --prefix openssl)/lib\" CFLAGS=\"-I$(brew --prefix openssl)/include\" python setup.py develop\n```\n\n## Running tests\n\n- on your machine: `py.test tests`\n\n# Troubleshooting\n\n- Permission issues when installing: you should install the tool in a python virtualenv\n\n- Exception when running: `ops`\n    `pkg_resources._vendor.packaging.requirements.InvalidRequirement: Invalid requirement, parse error at \"'!= 2.4'\"`\n\n    Caused by a broken paramiko version, reinstall paramiko: `pip2 uninstall paramiko; pip2 install paramiko`\n\n- Exception when installing ops because the cryptography package fails to install:\n\nEither install the tool in a virtualenv or:\n\n```\n    brew install libffi\n    brew link libffi --force\n    brew install openssl  \n    brew link openssl --force\n```\n\n# License\n[Apache License 2.0](/LICENSE)\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/adobe/ops-cli", "keywords": "", "license": "Apache2", "maintainer": "", "maintainer_email": "", "name": "ops-cli", "package_url": "https://pypi.org/project/ops-cli/", "platform": "", "project_url": "https://pypi.org/project/ops-cli/", "project_urls": {"Homepage": "https://github.com/adobe/ops-cli"}, "release_url": "https://pypi.org/project/ops-cli/2.0.4/", "requires_dist": ["adal (==1.2.2)", "ansible (==2.7.13)", "awscli (==1.16.206)", "azure-applicationinsights (==0.1.0)", "azure-batch (==4.1.3)", "azure-common (==1.1.20)", "azure-cosmosdb-nspkg (==2.0.2)", "azure-cosmosdb-table (==1.0.6)", "azure-datalake-store (==0.0.48)", "azure-eventgrid (==1.3.0)", "azure-graphrbac (==0.40.0)", "azure-keyvault (==1.1.0)", "azure-loganalytics (==0.1.0)", "azure-mgmt-advisor (==1.0.1)", "azure-mgmt-applicationinsights (==0.1.1)", "azure-mgmt-authorization (==0.50.0)", "azure-mgmt-batch (==5.0.1)", "azure-mgmt-batchai (==2.0.0)", "azure-mgmt-billing (==0.2.0)", "azure-mgmt-cdn (==3.1.0)", "azure-mgmt-cognitiveservices (==3.0.0)", "azure-mgmt-commerce (==1.0.1)", "azure-mgmt-compute (==4.6.2)", "azure-mgmt-consumption (==2.0.0)", "azure-mgmt-containerinstance (==1.5.0)", "azure-mgmt-containerregistry (==2.8.0)", "azure-mgmt-containerservice (==4.4.0)", "azure-mgmt-cosmosdb (==0.4.1)", "azure-mgmt-datafactory (==0.6.0)", "azure-mgmt-datalake-analytics (==0.6.0)", "azure-mgmt-datalake-nspkg (==3.0.1)", "azure-mgmt-datalake-store (==0.5.0)", "azure-mgmt-datamigration (==1.0.0)", "azure-mgmt-devspaces (==0.1.0)", "azure-mgmt-devtestlabs (==2.2.0)", "azure-mgmt-dns (==2.1.0)", "azure-mgmt-eventgrid (==1.0.0)", "azure-mgmt-eventhub (==2.6.0)", "azure-mgmt-hanaonazure (==0.1.1)", "azure-mgmt-iotcentral (==0.1.0)", "azure-mgmt-iothub (==0.5.0)", "azure-mgmt-iothubprovisioningservices (==0.2.0)", "azure-mgmt-keyvault (==1.1.0)", "azure-mgmt-loganalytics (==0.2.0)", "azure-mgmt-logic (==3.0.0)", "azure-mgmt-machinelearningcompute (==0.4.1)", "azure-mgmt-managementgroups (==0.1.0)", "azure-mgmt-managementpartner (==0.1.1)", "azure-mgmt-maps (==0.1.0)", "azure-mgmt-marketplaceordering (==0.1.0)", "azure-mgmt-media (==1.0.0)", "azure-mgmt-monitor (==0.5.2)", "azure-mgmt-msi (==0.2.0)", "azure-mgmt-network (==2.7.0)", "azure-mgmt-notificationhubs (==2.1.0)", "azure-mgmt-nspkg (==3.0.2)", "azure-mgmt-policyinsights (==0.1.0)", "azure-mgmt-powerbiembedded (==2.0.0)", "azure-mgmt-rdbms (==1.9.0)", "azure-mgmt-recoveryservices (==0.3.0)", "azure-mgmt-recoveryservicesbackup (==0.3.0)", "azure-mgmt-redis (==5.0.0)", "azure-mgmt-relay (==0.1.0)", "azure-mgmt-reservations (==0.2.1)", "azure-mgmt-resource (==2.2.0)", "azure-mgmt-scheduler (==2.0.0)", "azure-mgmt-search (==2.1.0)", "azure-mgmt-servicebus (==0.5.3)", "azure-mgmt-servicefabric (==0.2.0)", "azure-mgmt-signalr (==0.1.1)", "azure-mgmt-sql (==0.9.1)", "azure-mgmt-storage (==2.0.0)", "azure-mgmt-subscription (==0.2.0)", "azure-mgmt-trafficmanager (==0.50.0)", "azure-mgmt-web (==0.35.0)", "azure-mgmt (==4.0.0)", "azure-nspkg (==3.0.2)", "azure-servicebus (==0.21.1)", "azure-servicefabric (==6.3.0.0)", "azure-servicemanagement-legacy (==0.20.6)", "azure-storage-blob (==1.5.0)", "azure-storage-common (==1.4.2)", "azure-storage-file (==1.4.0)", "azure-storage-queue (==1.4.0)", "azure (==4.0.0)", "backports.functools-lru-cache (==1.6.1)", "bcrypt (==3.1.7)", "boto3 (==1.9.196)", "boto (==2.49.0)", "botocore (==1.12.196)", "cachetools (==3.1.1)", "certifi (==2019.11.28)", "cffi (==1.13.2)", "chardet (==3.0.4)", "colorama (==0.3.9)", "cryptography (==2.8)", "deepmerge (==0.1.0)", "docutils (==0.14)", "gitdb2 (==2.0.6)", "gitpython (==3.0.5)", "google-auth (==1.8.2)", "hashmerge (==0.1)", "himl (==0.5.0)", "hvac (==0.9.3)", "idna (==2.8)", "inflection (==0.3.1)", "isodate (==0.6.0)", "jinja2 (==2.10.1)", "jmespath (==0.9.4)", "kubernetes (==9.0.0)", "lru-cache (==0.2.3)", "markupsafe (==1.1.1)", "msrest (==0.6.10)", "msrestazure (==0.6.0)", "oauthlib (==3.1.0)", "paramiko (==2.7.1)", "passgen (==1.1.1)", "pathlib2 (==2.3.5)", "pyasn1-modules (==0.2.7)", "pyasn1 (==0.4.8)", "pycparser (==2.19)", "pyjwt (==1.7.1)", "pynacl (==1.3.0)", "python-consul (==1.1.0)", "requests-oauthlib (==1.3.0)", "requests (==2.22.0)", "rsa (==3.4.2)", "s3transfer (==0.2.1)", "simpledi (==0.3)", "six (==1.13.0)", "smmap2 (==2.0.5)", "websocket-client (==0.56.0)", "pyyaml (==5.1) ; python_version != \"2.6\"", "python-dateutil (==2.8.1) ; python_version >= \"2.7\"", "urllib3 (==1.25.7) ; python_version >= \"3.4\""], "requires_python": ">=3.5", "summary": "Ops - wrapper for Terraform, Ansible, and SSH for cloud automation", "version": "2.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Ops CLI</h1>\n<p><a href=\"https://www.travis-ci.com/adobe/ops-cli\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cb2e61214eb216460758fceabc720b188922d0fe/68747470733a2f2f7777772e7472617669732d63692e636f6d2f61646f62652f6f70732d636c692e7376673f746f6b656e3d38754871666867737864764a393371574178686e266272616e63683d6d6173746572\"></a> <a href=\"https://hub.docker.com/r/adobe/ops-cli\" rel=\"nofollow\"><img alt=\"Docker pull\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4eeb11e9d89dc854d95999651402e7a4359be5ad/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f61646f62652f6f70732d636c69\"></a> <a href=\"https://microbadger.com/images/adobe/ops-cli\" rel=\"nofollow\" title=\"Get your own version badge on microbadger.com\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e29d64ce5380bae3909d815050af7698f62cd23a/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f76657273696f6e2f61646f62652f6f70732d636c692e737667\"></a> <a href=\"https://github.com/adobe/ops-cli/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3e1b9c3105cf4a59822ca47629654e44a7612a83/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f61646f62652f6f70732d636c69\"></a></p>\n<p><strong>From version 2.0 onward, <code>ops-cli</code> requires Python3.<br>\nIf you're still using Python2, use <code>ops-cli</code> version &lt;2.0</strong></p>\n<p><strong>ops-cli</strong> is a python wrapper for <a href=\"https://www.terraform.io/\" rel=\"nofollow\">Terraform</a>, <a href=\"https://www.ansible.com/\" rel=\"nofollow\">Ansible</a> and SSH for cloud automation.</p>\n<p>We use multiple tools to manage our infrastructure at Adobe. The purpose of <code>ops-cli</code> is to gather the common cluster configurations in a single place and, based on these, interact with the above mentioned tools. In this way, we can avoid duplication and can quickly spin up new clusters (either production or development ones). All we need to do is customize the cluster configuration file (<a href=\"https://github.com/adobe/ops-cli/blob/master/examples/aws-kubernetes/clusters/my-kubernetes-cluster.yaml\" rel=\"nofollow\">example here</a>).</p>\n<p><code>ops-cli</code> integrates with the Azure and AWS cli, in order to provide inventory, ssh, sync, tunnel and the possibility to run ansible playbooks on a fleet of EC2 instances.\nIt can be used to add a layer of templating (using jinja2) on top of Terraform files. This is useful for removing duplicated code when it comes to spinning up infrastructure across multiple environments (stage/sandbox/prod) and across teams. Useful for both AWS and <a href=\"https://github.com/adobe/ops-cli/tree/master/examples/aws-kubernetes\" rel=\"nofollow\">Kubernetes deployments</a>.</p>\n<h1>Table of Contents</h1>\n\n<ul>\n<li><a href=\"#how-it-works\" rel=\"nofollow\">How it works?</a></li>\n<li><a href=\"#use-cases\" rel=\"nofollow\">Use cases</a>\n<ul>\n<li><a href=\"#manage-aws-ec2-instances\" rel=\"nofollow\">Manage AWS EC2 instances</a></li>\n<li><a href=\"#terraform\" rel=\"nofollow\">Terraform</a></li>\n<li><a href=\"#run-terraform-by-using-hierarchical-configs\" rel=\"nofollow\">Run terraform by using hierarchical configs</a></li>\n<li><a href=\"#create-kubernetes-cluster-using-aws-eks\" rel=\"nofollow\">Create Kubernetes cluster (using AWS EKS)</a></li>\n</ul>\n</li>\n<li><a href=\"#installing\" rel=\"nofollow\">Installing</a>\n<ul>\n<li><a href=\"#local\" rel=\"nofollow\">Local</a>\n<ul>\n<li><a href=\"#virtualenv\" rel=\"nofollow\">Virtualenv</a></li>\n<li><a href=\"#ops-tool-installation\" rel=\"nofollow\">Ops tool installation</a>\n<ul>\n<li><a href=\"#python-3\" rel=\"nofollow\">Python 3</a></li>\n</ul>\n</li>\n<li><a href=\"#terraform-1\" rel=\"nofollow\">Terraform</a></li>\n</ul>\n</li>\n<li><a href=\"#using-docker-image\" rel=\"nofollow\">Using docker image</a></li>\n<li><a href=\"#configuring\" rel=\"nofollow\">Configuring</a>\n<ul>\n<li><a href=\"#aws\" rel=\"nofollow\">AWS</a></li>\n<li><a href=\"#azure\" rel=\"nofollow\">Azure</a></li>\n</ul>\n</li>\n<li><a href=\"#examples\" rel=\"nofollow\">Examples</a></li>\n<li><a href=\"#usage-help\" rel=\"nofollow\">Usage help</a></li>\n<li><a href=\"#more-help\" rel=\"nofollow\">More help</a></li>\n<li><a href=\"#tool-configuration-opsconfigyaml\" rel=\"nofollow\">Tool configuration: .opsconfig.yaml</a>\n<ul>\n<li><a href=\"#inventory\" rel=\"nofollow\">Inventory</a>\n<ul>\n<li><a href=\"#aws-example\" rel=\"nofollow\">AWS example</a></li>\n<li><a href=\"#azure-example\" rel=\"nofollow\">Azure example</a></li>\n<li><a href=\"#inventory-usage\" rel=\"nofollow\">Inventory usage</a></li>\n</ul>\n</li>\n<li><a href=\"#terraform-2\" rel=\"nofollow\">Terraform</a>\n<ul>\n<li><a href=\"#terraform-landscape\" rel=\"nofollow\">Terraform landscape</a></li>\n</ul>\n</li>\n<li><a href=\"#ssh\" rel=\"nofollow\">SSH</a>\n<ul>\n<li><a href=\"#sshpass\" rel=\"nofollow\">SSHPass</a></li>\n</ul>\n</li>\n<li><a href=\"#play\" rel=\"nofollow\">Play</a></li>\n<li><a href=\"#run-command\" rel=\"nofollow\">Run command</a></li>\n<li><a href=\"#sync-files\" rel=\"nofollow\">Sync files</a></li>\n<li><a href=\"#noop\" rel=\"nofollow\">Noop</a></li>\n<li><a href=\"#packer\" rel=\"nofollow\">Packer</a></li>\n</ul>\n</li>\n<li><a href=\"#secrets-management\" rel=\"nofollow\">Secrets Management</a>\n<ul>\n<li><a href=\"#vault\" rel=\"nofollow\">Vault</a></li>\n<li><a href=\"#amazon-secrets-manager-ssm\" rel=\"nofollow\">Amazon Secrets Manager (SSM)</a></li>\n</ul>\n</li>\n<li><a href=\"#using-jinja2-filters-in-playbooks-and-terraform-templates\" rel=\"nofollow\">Using jinja2 filters in playbooks and terraform templates</a></li>\n<li><a href=\"#skms\" rel=\"nofollow\">SKMS</a></li>\n</ul>\n</li>\n<li><a href=\"#development\" rel=\"nofollow\">Development</a>\n<ul>\n<li><a href=\"#install-ops-in-development-mode\" rel=\"nofollow\">Install ops in development mode</a></li>\n<li><a href=\"#running-tests\" rel=\"nofollow\">Running tests</a></li>\n</ul>\n</li>\n<li><a href=\"#troubleshooting\" rel=\"nofollow\">Troubleshooting</a></li>\n<li><a href=\"#license\" rel=\"nofollow\">License</a></li>\n</ul>\n\n\n<h1>How it works?</h1>\n<p>You define a cluster configuration, using a yaml file. The yaml file contains different kind of sections, one for each plugin. For instance, you could have a section for Terraform files, a section for AWS instructions, Kubernetes Helm charts and so forth.</p>\n<h1>Use cases</h1>\n<h2>Manage AWS EC2 instances</h2>\n<p>Once you define your cluster configuration, you can run <code>ops</code> commands such as seeing the instance inventory.</p>\n<pre><span class=\"c1\"># fetch instances from AWS and prints them</span>\nops clusters/mycluster.yaml inventory --limit webapp \n</pre>\n<p>This would output something like:\n<img alt=\"ops\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e7495241da8905b41a6fd9e476d802f357a10237/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f3935323833362f35323032313430312d39663535336338302d323466642d313165392d383032632d3135356635613065376636332e706e67\"></p>\n<p>Then you can run <code>ssh</code>, <code>play</code>, <code>run</code>, <code>sync</code> etc.</p>\n<pre><span class=\"c1\"># SSH to one of the nodes (can handle bastion as well)</span>\nops clusters/mycluster.yaml ssh webapp-01\n\n<span class=\"c1\"># run a deployment playbook via ansible</span>\nops clusters/mycluster.yaml play ansible/playbooks/task/webapp/deployment.yaml -- -e <span class=\"nv\">version</span><span class=\"o\">=</span><span class=\"m\">5</span>.36.2 -u ec2-user --limit webapp\n\n<span class=\"c1\"># run command on all selected nodes</span>\nops clusters/mycluster.yaml run <span class=\"s2\">\"sudo yum upgrade myawesomeapp; sudo service myawesomeapp restart\"</span> -- -u ec2-user --limit <span class=\"s1\">'\"aam_app_group=canary;az=us-east-1a\"'</span>\n\n<span class=\"c1\"># copy file to all servers</span>\nops clusters/mycluster.yaml sync /tmp/myfile webapp: -l ec2-user\n\n<span class=\"c1\"># create a tunnel</span>\nops clusters/stage.yaml ssh --tunnel --local <span class=\"m\">8080</span> --remote <span class=\"m\">8080</span> stage-thanos-1 -l ec2-user\n</pre>\n<p>See <a href=\"https://github.com/adobe/ops-cli/tree/master/examples/features/inventory\" rel=\"nofollow\">examples/features/inventory</a></p>\n<h2>Terraform</h2>\n<pre><span class=\"c1\"># Performs jinja templating (if any) and runs terraform plan</span>\nops clusters/mycluster.yaml terraform --path-name aws-eks plan\n\n<span class=\"c1\"># Run terraform apply, with the possibility to sync the tf state files remotely (currently, AWS S3 bucket is supported + DynamoDB for locking). </span>\nops clusters/mycluster.yaml terraform --path-name aws-eks apply\n</pre>\n<p><img alt=\"ops-terraform\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9d59907889c1c0bbb8f350a0e15a3f7101402e5f/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f3935323833362f35323032313339362d39626331623538302d323466642d313165392d396461382d3030666236386264356337322e706e67\"></p>\n<h2>Run terraform by using hierarchical configs</h2>\n<p>See <a href=\"https://github.com/adobe/ops-cli/tree/master/examples/features/terraform-hierarchical\" rel=\"nofollow\">examples/features/terraform-hierarchical</a></p>\n<h2>Create Kubernetes cluster (using AWS EKS)</h2>\n<p>See <a href=\"https://github.com/adobe/ops-cli/tree/master/examples/aws-kubernetes\" rel=\"nofollow\">examples/aws-kubernetes</a></p>\n<h1>Installing</h1>\n<h2>Local</h2>\n<h3>Virtualenv</h3>\n<p>Here is a link about how to install and use virtualenv:\n<a href=\"https://virtualenv.pypa.io/en/stable/\" rel=\"nofollow\">https://virtualenv.pypa.io/en/stable/</a></p>\n<h3>Ops tool installation</h3>\n<h4>Python 3</h4>\n<pre><span class=\"c1\"># Make sure pip is up to date</span>\ncurl https://bootstrap.pypa.io/get-pip.py <span class=\"p\">|</span> python3\n\n<span class=\"c1\"># Install virtualenv</span>\npip install --upgrade virtualenv\npip install --upgrade virtualenvwrapper\n\n<span class=\"nb\">echo</span> <span class=\"s1\">'export WORKON_HOME=$HOME/.virtualenvs'</span> &gt;&gt; ~/.bash_profile\n<span class=\"nb\">echo</span> <span class=\"s1\">'source /usr/local/bin/virtualenvwrapper.sh'</span> &gt;&gt; ~/.bash_profile\n<span class=\"nb\">source</span> ~/.bash_profile\n\n<span class=\"c1\"># create virtualenv</span>\nmkvirtualenv ops\nworkon ops\n\n<span class=\"c1\"># uninstall previous `ops` version (if you have it)</span>\npip uninstall ops --yes\n\n<span class=\"c1\"># install ops-cli v2.0.4 stable release</span>\npip install --upgrade ops-cli\n</pre>\n<h3>Terraform</h3>\n<p>Optionally, install terraform to be able to access terraform plugin. See <a href=\"https://www.terraform.io/intro/getting-started/install.html\" rel=\"nofollow\">https://www.terraform.io/intro/getting-started/install.html</a>\nAlso for pretty formatting of terraform plan output you can install <a href=\"https://github.com/coinbase/terraform-landscape\" rel=\"nofollow\">https://github.com/coinbase/terraform-landscape</a> (use gem install for MacOS)</p>\n<h2>Using docker image</h2>\n<p>You can try out <code>ops-cli</code>, by using docker. The docker image has all required prerequisites (python, terraform, helm, git, ops-cli etc).</p>\n<p>To start out a container, running the latest <code>ops-cli</code> docker image run:</p>\n<pre>docker run -it adobe/ops-cli:2.0.4 bash\n</pre>\n<p>After the container has started, you can start using <code>ops-cli</code>:</p>\n<pre>ops <span class=\"nb\">help</span>\n<span class=\"c1\"># usage: ops [-h] [--root-dir ROOT_DIR] [--verbose] [-e EXTRA_VARS]</span>\n<span class=\"c1\">#           cluster_config_path</span>\n<span class=\"c1\">#           {inventory,terraform,packer,ssh,play,run,sync,noop} ...</span>\n\ngit clone https://github.com/adobe/ops-cli.git\n<span class=\"nb\">cd</span> ops-cli\nls examples\n<span class=\"c1\"># aws-kubernetes</span>\n<span class=\"c1\"># cassandra-stress</span>\n<span class=\"c1\"># features</span>\n\n<span class=\"nb\">cd</span> examples/aws-kubernetes\nops clusters/my-kubernetes-cluster.yaml terraform --path-name aws-eks plan\n<span class=\"c1\"># in order to setup aws-kubernetes follow the steps from https://github.com/adobe/ops-cli/blob/master/examples/aws-kubernetes/README.md</span>\n</pre>\n<h2>Configuring</h2>\n<h3>AWS</h3>\n<p>If you plan to use ops with AWS, you must configure credentials for each account</p>\n<pre>$ aws configure --profile aws_account_name\n</pre>\n<h3>Azure</h3>\n<p>TBD</p>\n<h2>Examples</h2>\n<p>See <a href=\"https://github.com/adobe/ops-cli/tree/master/examples\" rel=\"nofollow\">examples/</a> folder:</p>\n<ul>\n<li>cassandra-stress - n-node cassandra cluster used for stress-testing; a basic stress profile is included</li>\n<li>spin up a Kubernetes cluster</li>\n<li>distinct <code>ops</code> features</li>\n</ul>\n<h2>Usage help</h2>\n<p>To see all commands and a short description run <code>ops --help</code></p>\n<pre><code>usage: ops [-h] [--root-dir ROOT_DIR] [--verbose] [-e EXTRA_VARS]\n           cluster_config_path\n           {inventory,terraform,packer,ssh,play,run,sync,noop} ...\n\nRun commands against a cluster definition\n\npositional arguments:\n  cluster_config_path   The cluster config path cluster.yaml\n  {inventory,terraform,packer,ssh,play,run,sync,noop}\n    inventory           Show current inventory data\n    terraform           Wrap common terraform tasks with full templated\n                        configuration support\n    packer              Wrap common packer tasks and inject variables from a\n                        cluster file\n    ssh                 SSH or create an SSH tunnel to a server in the cluster\n    play                Run an Ansible playbook\n    run                 Runs a command against hosts in the cluster\n    sync                Sync files from/to a cluster\n    noop                used to initialize the full container for api usage\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --root-dir ROOT_DIR   The root of the resource tree - it can be an absolute\n                        path or relative to the current dir\n  --verbose, -v         Get more verbose output from commands\n  -e EXTRA_VARS, --extra-vars EXTRA_VARS\n                        Extra variables to use. Eg: -e ssh_user=ssh_user\n</code></pre>\n<h2>More help</h2>\n<p>Each sub-command includes additional help information that you can get by running:\n<code>ops examples/inventory/aam.yaml sync --help</code></p>\n<h2>Tool configuration: .opsconfig.yaml</h2>\n<p>Some tool settings are available via a <a href=\"https://github.com/adobe/ops/blob/master/src/ops/opsconfig.py\" rel=\"nofollow\">.opsconfig.yaml</a> configuration file.\nThe file is looked-up in <code>/etc/opswrapper/.opsconfig.yaml</code>, then in <code>~/.opsconfig.yaml</code> and then in the project folder starting from the current dir and up to the root dir.\nAll the files found this way are merged together so that you can set some global defaults, then project defaults in the root dir of the project and\noverwrite them for individual envs. Eg: <code>~/.opsconfig.yaml</code>, <code>/project/.opsconfig.yaml</code>, <code>/project/clusters/dev/.opsconfig.yaml</code></p>\n<h3>Inventory</h3>\n<p>The <code>inventory</code> command will list all the servers in a given cluster and cache the results for further operations on them (for instance, SSHing to a given node or running an ansible playbook).</p>\n<p>You can always filter which nodes you want to display or use to run an ansible playbook on, by using the <code>--limit</code> argument (eg. <code>--limit webapp</code>). The extra filter is applied on the instance tags, which includes the instance name.</p>\n<p>The way <code>inventory</code> works is by doing a describe command in AWS/Azure. The describe command matches all the nodes that have the tag \"cluster\" equal to the cluster name you have defined.</p>\n<p>In order to configure it, you need to add the <code>inventory</code> section in your cluster configuration file (<a href=\"https://github.com/adobe/ops-cli/blob/master/examples/features/inventory/my-aws-cluster.yaml\" rel=\"nofollow\">example here</a>).</p>\n<h4>AWS example</h4>\n<pre><code>---\ninventory:\n  - plugin: cns\n    args:\n      clusters:\n        - region: us-east-1\n          boto_profile: aam-npe # make sure you have this profile in your ~/.aws/credentials file\n          names: [mycluster1] # this assumes the EC2 nodes have the Tag Name \"cluster\" with Value \"mycluster1\"\n</code></pre>\n<h4>Azure example</h4>\n<pre><code>---\ninventory:\n  - plugin: azr\n    args:\n      tags: environment=prod\n      locations: westeurope,northeurope\n</code></pre>\n<h4>Inventory usage</h4>\n<pre><code>usage: ops cluster_config_path inventory [-h] [-e EXTRA_VARS]\n                                         [--refresh-cache] [--limit LIMIT]\n                                         [--facts]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTRA_VARS, --extra-vars EXTRA_VARS\n                        Extra variables to use. Eg: -e ssh_user=ssh_user\n  --refresh-cache       Refresh the cache for the inventory\n  --limit LIMIT         Limit run to a specific server subgroup. Eg: --limit\n                        newton-dcs\n  --facts               Show inventory facts for the given hosts\n</code></pre>\n<h3>Terraform</h3>\n<pre><code>usage: ops cluster_config_path terraform [-h] [--var VAR] [--module MODULE]\n                                         [--resource RESOURCE] [--name NAME]\n                                         [--plan]\n                                         subcommand\n\npositional arguments:\n  subcommand           apply | console | destroy | import | output | plan |\n                       refresh | show | taint | template | untaint\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --var VAR             the output var to show\n  --module MODULE       for use with \"taint\", \"untaint\" and \"import\". The\n                        module to use. e.g.: vpc\n  --resource RESOURCE   for use with \"taint\", \"untaint\" and \"import\". The\n                        resource to target. e.g.: aws_instance.nat\n  --name NAME           for use with \"import\". The name or ID of the imported\n                        resource. e.g.: i-abcd1234\n  --plan                for use with \"show\", show the plan instead of the\n                        statefile\n  --skip-refresh        for use with \"plan\". Skip refresh of statefile\n  --raw-output          for use with \"plan\". Show raw plan output without piping through terraform landscape (if terraform landscape is not enabled in opsconfig.yaml this will have no impact)\n  --path-name PATH_NAME in case multiple terraform paths are defined, this\n                        allows to specify which one to use when running\n                        terraform\n\n    Examples:\n        # Create a new cluster with Terraform\n        ops clusters/qe1.yaml terraform plan\n        ops clusters/qe1.yaml terraform apply\n\n        # Update an existing cluster\n        ops clusters/qe1.yaml terraform plan\n        ops clusters/qe1.yaml terraform apply\n\n        # Get rid of a cluster and all of its components\n        ops clusters/qe1.yaml terraform destroy\n\n        # Retrieve all output from a previously created Terraform cluster\n        ops clusters/qe1.yaml terraform output\n\n        # Retrieve a specific output from a previously created Terraform cluster\n        ops clusters/qe1.yaml terraform output --var nat_public_ip\n\n        # Refresh a statefile (no longer part of plan)\n        ops clusters/qe1.yaml terraform refresh\n\n        # Taint a resource- forces a destroy, then recreate on next plan/apply\n        ops clusters/qe1.yaml terraform taint --module vpc --resource aws_instance.nat\n\n        # Untaint a resource\n        ops clusters/qe1.yaml terraform untaint --module vpc --resource aws_instance.nat\n\n        # Show the statefile in human-readable form\n        ops clusters/qe1.yaml terraform show\n\n        # Show the plan in human-readable form\n        ops clusters/qe1.yaml terraform show --plan\n\n        # View parsed jinja on the terminal\n        ops clusters/qe1.yaml terraform template\n\n        # Import an unmanaged existing resource to a statefile\n        ops clusters/qe1.yaml terraform import --module vpc --resource aws_instance.nat --name i-abcd1234\n\n        # Use the Terraform Console on a cluster\n        ops clusters/qe1.yaml terraform console\n\n        # Validate the syntax of Terraform files\n        ops clusters/qe1.yaml terraform validate\n\n        # Specify which terraform path to use\n        ops clusters/qe1.yaml terraform plan --path-name terraformFolder1\n</code></pre>\n<h4>Terraform landscape</h4>\n<p>For pretty formatting of terraform plan output you can install <a href=\"https://github.com/coinbase/terraform-landscape\" rel=\"nofollow\">https://github.com/coinbase/terraform-landscape</a> (use gem install for MacOS).\nTo make <code>ops</code> use it you need to add <code>terraform.landscape: True</code> in opsconfig.yaml file.</p>\n<h3>SSH</h3>\n<pre><code>usage: ops cluster_config_path ssh [-h] [-e EXTRA_VARS] [-l USER]\n                                   [--ssh-config SSH_CONFIG] [--index INDEX]\n                                   [--tunnel] [--ipaddress] [--local LOCAL]\n                                   [--remote REMOTE] [--proxy] [--nossh]\n                                   role [ssh_opts [ssh_opts ...]]\n\npositional arguments:\n  role                  Server role to ssh to. Eg: dcs\n  ssh_opts              Manual ssh options\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTRA_VARS, --extra-vars EXTRA_VARS\n                        Extra variables to use. Eg: -e ssh_user=ssh_user\n  -l USER, --user USER  SSH User\n  --ssh-config SSH_CONFIG\n                        Ssh config file name in the ./ansible dir\n  --index INDEX         Index of the server from the group\n  --tunnel              Use SSH tunnel, must pass --local and --remote\n  --ipaddress\n  --local LOCAL         local port for ssh proxy or ssh tunnel\n  --remote REMOTE       remote port for ssh tunnel\n  --proxy               Use SSH proxy, must pass --local\n  --nossh               Port tunnel a machine that does not have SSH. Implies\n                        --ipaddress, and --tunnel; requires --local and\n                        --remote\n\n    Examples:\n        # SSH using current username as remote username\n        ops clusters/qe1.yaml ssh nagios\n\n        # SSH using a different username\n        ops clusters/qe1.yaml ssh nagios -l ec2-user\n\n        # SSH to the second nagios instance\n        ops clusters/qe1.yaml ssh nagios --index 2\n\n        # SSH to a specific hostname, instead of the tagged role\n        ops clusters/qe1.yaml ssh full-hostname-here-1\n\n        # Create an SSH tunnel to Nagios forwarding the remote port 80 to local port 8080\n        ops clusters/qe1.yaml ssh --tunnel --remote 80 --local 8080 nagios\n\n        # Create an SSH tunnel to a host where the service is NOT listening on `localhost`\n        ops clusters/qe1.yaml ssh --tunnel --remote 80 --local 8080 nagios --ipaddress\n\n        # Create an SSH tunnel to a host with an open port which does NOT have SSH itself (Windows)\n        # Note that the connection will be made from the Bastion host\n        ops clusters/qe1.yaml ssh --tunnel --local 3389 --remote 3389 --nossh windowshost\n\n        # Create a proxy to a remote server that listens on a local port\n        ops clusters/qe1.yaml ssh --proxy --local 8080 bastion\n</code></pre>\n<h4>SSHPass</h4>\n<p>In case you want to use the OSX Keychain to store your password and reuse across multiple nodes (e.g. running a playbook on 300 nodes and not having to enter the password for every node) follow the tutorial below:</p>\n<ol>\n<li>Open <code>Keychain Access</code> app on OSX</li>\n<li>Create a new keychain (<code>File -&gt; New Keychain</code>), let's say <code>aam</code></li>\n<li>Select the <code>aam</code> keychain and add a new password entry in this (<code>File -&gt; New Password Item</code>):</li>\n</ol>\n<ul>\n<li>Name: <code>idm</code></li>\n<li>Kind: <code>application password</code></li>\n<li>Account: <code>your_ldap_account</code> (e.g. <code>johnsmith</code>)</li>\n<li>Where: <code>idm</code></li>\n</ul>\n<ol>\n<li>\n<p>Create <code>$HOME/bin</code> dir - this is where the scripts below are saved</p>\n</li>\n<li>\n<p>Create <code>~/bin/askpass</code> script and update the ldap account there:</p>\n</li>\n</ol>\n<pre>cat &gt; ~/bin/askpass  &lt;&lt;<span class=\"s2\">\"EOF\"</span>\n<span class=\"c1\">#!/usr/bin/env bash</span>\n/usr/bin/security find-generic-password -a &lt;your_ldap_account&gt; -s idm -w <span class=\"nv\">$HOME</span>/Library/Keychains/aam.keychain\nEOF\nchmod +x ~/bin/askpass\n</pre>\n<ol>\n<li>\n<p>Checkout <a href=\"https://github.com/pharaujo/notty\" rel=\"nofollow\">notty github repo</a>, build and move the binary to <code>$HOME/bin/</code></p>\n</li>\n<li>\n<p>Create <code>~/bin/sshpass</code> script:</p>\n</li>\n</ol>\n<pre>cat &gt; <span class=\"nv\">$HOME</span>/bin/sshpass &lt;&lt;<span class=\"s2\">\"EOF\"</span>\n<span class=\"c1\">#!/usr/bin/env bash</span>\n<span class=\"nb\">export</span> <span class=\"nv\">DISPLAY</span><span class=\"o\">=</span>:99\n<span class=\"nb\">export</span> <span class=\"nv\">SSH_ASKPASS</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"nv\">$HOME</span><span class=\"s2\">/bin/askpass\"</span>\n<span class=\"o\">[[</span> <span class=\"nv\">$1</span> <span class=\"o\">==</span> -d* <span class=\"o\">]]</span> <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">shift</span>\n<span class=\"nv\">$HOME</span>/bin/notty <span class=\"nv\">$@</span>\nEOF\n\nchmod +x <span class=\"nv\">$HOME</span>/bin/sshpass\n</pre>\n<ol>\n<li>Verify the setup works:</li>\n</ol>\n<pre><span class=\"c1\"># Connect to bastion</span>\n~/bin/sshpass ssh -o <span class=\"nv\">StrictHostKeyChecking</span><span class=\"o\">=</span>no -l &lt;your_ldap_account&gt; &lt;<span class=\"m\">52</span>.5.5.5&gt;\n</pre>\n<ol>\n<li>Run <code>ops</code> tool</li>\n</ol>\n<h3>Play</h3>\n<p>Run an ansible playbook.</p>\n<pre><code>usage: ops cluster_config_path play [-h] [-e EXTRA_VARS] [--ask-sudo-pass]\n                                    [--limit LIMIT]\n                                    playbook_path\n                                    [ansible_args [ansible_args ...]]\n\npositional arguments:\n  playbook_path         The playbook path\n  ansible_args          Extra ansible args\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTRA_VARS, --extra-vars EXTRA_VARS\n                        Extra variables to use. Eg: -e ssh_user=ssh_user\n  --ask-sudo-pass       Ask sudo pass for commands that need sudo\n  --limit LIMIT         Limit run to a specific server subgroup. Eg: --limit\n                        newton-dcs\n\n    Examples:\n        # Run an ansible playbook\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml\n\n        # Limit the run of a playbook to a subgroup\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml -- --limit dcs\n\n        # Overwrite or set a variable\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml -- -e city=paris\n\n        # Filter with tags\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml -- -t common\n\n        # Run a playbook and overwrite the default user\n        ops clusters/qe1.yaml play ansible/plays/cluster/configure.yaml -- -u ec2-user\n</code></pre>\n<h3>Run command</h3>\n<p>Run a bash command on the selected nodes.</p>\n<pre><code>usage: ops cluster_config_path run [-h] [--ask-sudo-pass] [--limit LIMIT]\n                                   host_pattern shell_command\n                                   [extra_args [extra_args ...]]\n\npositional arguments:\n  host_pattern     Limit the run to the following hosts\n  shell_command    Shell command you want to run\n  extra_args       Extra ansible arguments\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --ask-sudo-pass  Ask sudo pass for commands that need sudo\n  --limit LIMIT    Limit run to a specific server subgroup. Eg: --limit\n                   newton-dcs\n\n    Examples:\n        # Last 5 installed packages on each host\n        ops qe1.yaml run all 'sudo grep Installed /var/log/yum.log | tail -5'\n\n        # See nodetool status on each cassandra node\n        ops qe1.yaml run qe1-cassandra 'nodetool status'\n\n        # Complex limits\n        ops qe1.yaml run 'qe1-cassandra,!qe1-cassandra-0' 'nodetool status'\n\n        # Show how to pass other args\n</code></pre>\n<h3>Sync files</h3>\n<p>Performs <code>rsync</code> to/from a given set of nodes.</p>\n<pre><code>usage: ops cluster_config_path sync [-h] [-l USER] src dest [opts [opts ...]]\n\npositional arguments:\n  src                   Source dir\n  dest                  Dest dir\n  opts                  Rsync opts\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -l USER, --user USER  Value for remote user that will be used for ssh\n\n        rsync wrapper for ops inventory conventions\n\n        Example:\n\n        # rsync from remote dcs role\n        ops cluster.yml sync 'dcs[0]:/usr/local/demdex/conf' /tmp/configurator-data --user remote_user\n\n        # extra rsync options\n        ops cluster.yml sync 'dcs[0]:/usr/local/demdex/conf' /tmp/configurator-data -l remote_user -- --progress\n</code></pre>\n<h3>Noop</h3>\n<pre><code>usage: ops cluster_config_path noop [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>\n<h3>Packer</h3>\n<p>Runs <a href=\"https://www.packer.io/intro/\" rel=\"nofollow\">packer</a>, for creating images.</p>\n<pre><code>usage: ops cluster_config_path packer [-h] subcommand\n\npositional arguments:\n  subcommand  build | validate\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n    Examples:\n        # Validate a packer file\n        ops clusters/centos7.yaml packer validate\n\n        # Build a packer file\n        ops clusters/centos7.yaml packer build\n</code></pre>\n<h2>Secrets Management</h2>\n<p>There are cases where you need to reference sensitive data in your <code>cluster.yaml</code> file (credentials, passwords, tokens etc). Given that the cluster configuration file can be stored in a version control system (such as Git), the best practice is to not put sensitive data in the file itself. Instead, we can use <code>ops-cli</code> to fetch the desired credentials from a secrets manager such as Vault or Amazon SSM, at runtime.</p>\n<h3>Vault</h3>\n<p>Ops can manage the automatic generation of secrets and their push in Vault, without actually persisting the secrets in the cluster file.\nA cluster file will only need to use a construct like the following:</p>\n<pre><code>db_password: \"{{'secret/campaign/generated_password'|managed_vault_secret(policy=128)}}\"\n</code></pre>\n<p>Which will translate behind the scenes in :</p>\n<ul>\n<li>look up in vault the secrets at secret/campaign/generated_password in the default key 'value' (Adobe convention that can be overridden with the key parameter)</li>\n<li>if the value there is missing, generate a new secret using the engine passgen with a policy of length 128 characters</li>\n<li>return the generated value</li>\n<li>if the value at that path already exist, just return that value.\nThis allows us to just refer in cluster files a secret that actually exists in vault and make sure we only generate it once - if it was already created by os or any other system, we will just use what is already there.\nThe reference is by means of fixed form jinja call  added to the cluster file, which ends up interpreted later during the templating phase.</li>\n</ul>\n<h3>Amazon Secrets Manager (SSM)</h3>\n<p>Amazon offers the possibility to use their <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html\" rel=\"nofollow\">Secrets Manager</a> in order to manage configuration data such as credentials, passwords and license keys.</p>\n<p>We can use <code>ops-cli</code> to fetch the sensitive data from SSM, at runtime. Just define this in your cluster configuration file (eg. <code>mycluster.yaml</code>).</p>\n<pre><code>db_password: \"{{ '/my/ssm/path' | read_ssm(aws_profile='myprofile') }}\"\n</code></pre>\n<p><code>ops-cli</code> will read the SSM value by running a command similar to: <code>AWS_PROFILE=aam-npe aws ssm get-parameter --name \"/my/ssm/path\" --region us-east-1 --with-decryption</code>.\nNote that you can specify the AWS region via <code>read_ssm(aws_profile='myprofile', region_name='us-west-2')</code>.</p>\n<h2>Using jinja2 filters in playbooks and terraform templates</h2>\n<p>You can register your own jinja2 filters that you can  use in the cluster config file, terraform templates and ansible playbooks</p>\n<p>All ops commands look for filters in the following locations:</p>\n<ul>\n<li>the python path</li>\n<li>the .opsconfig.yaml <a href=\"https://github.com/adobe/ops/blob/master/src/ops/opsconfig.py#L58\" rel=\"nofollow\">ansible.filter_plugins</a> setting (defaults to plugins/filter_plugins)</li>\n</ul>\n<p>Example simple filter:</p>\n<pre><code># plugins/filter_plugin/myfilters.py\n\ndef my_filter(string):\n    return 'filtered: ' + string\n\n\nclass FilterModule(object):\n    def filters(self):\n        return {\n            'my_filter': my_filter\n        }\n\n# usage in playbook, templates, cluster config\n# test_custom_filters: \"{{ 'value' | my_filter }}\"\n</code></pre>\n<h2>SKMS</h2>\n<p>Create a file in <code>~/.skms/credentials.yaml</code> which looks like the following:</p>\n<pre><span class=\"nt\">endpoint</span><span class=\"p\">:</span> <span class=\"s\">\"api.skms.mycompany.com\"</span>\n<span class=\"nt\">username</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;username&gt;</span>\n<span class=\"nt\">password</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;password&gt;</span>\n</pre>\n<h1>Development</h1>\n<h2>Install <code>ops</code> in development mode</h2>\n<pre><code>git clone https://github.com/adobe/ops-cli.git\ncd ops\n# Install openssl\nbrew install openssl libyaml\nenv LDFLAGS=\"-L$(brew --prefix openssl)/lib\" CFLAGS=\"-I$(brew --prefix openssl)/include\" python setup.py develop\n</code></pre>\n<h2>Running tests</h2>\n<ul>\n<li>on your machine: <code>py.test tests</code></li>\n</ul>\n<h1>Troubleshooting</h1>\n<ul>\n<li>\n<p>Permission issues when installing: you should install the tool in a python virtualenv</p>\n</li>\n<li>\n<p>Exception when running: <code>ops</code>\n<code>pkg_resources._vendor.packaging.requirements.InvalidRequirement: Invalid requirement, parse error at \"'!= 2.4'\"</code></p>\n<p>Caused by a broken paramiko version, reinstall paramiko: <code>pip2 uninstall paramiko; pip2 install paramiko</code></p>\n</li>\n<li>\n<p>Exception when installing ops because the cryptography package fails to install:</p>\n</li>\n</ul>\n<p>Either install the tool in a virtualenv or:</p>\n<pre><code>    brew install libffi\n    brew link libffi --force\n    brew install openssl  \n    brew link openssl --force\n</code></pre>\n<h1>License</h1>\n<p><a href=\"/LICENSE\" rel=\"nofollow\">Apache License 2.0</a></p>\n\n          </div>"}, "last_serial": 6311346, "releases": {"1.10.0": [{"comment_text": "", "digests": {"md5": "e39cc1a509c61e5f543400c4a926949f", "sha256": "17cb9f6e2eaee866189e260d20d0a32ef9651c40580f97ed03ead561f5b9a8e1"}, "downloads": -1, "filename": "ops-cli-1.10.0.tar.gz", "has_sig": false, "md5_digest": "e39cc1a509c61e5f543400c4a926949f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 134032, "upload_time": "2019-08-27T18:19:53", "upload_time_iso_8601": "2019-08-27T18:19:53.037615Z", "url": "https://files.pythonhosted.org/packages/6f/9d/7acefdb94a3413b748c6ab300eef1ed8b0ff64684f1b92e1d64639f6e272/ops-cli-1.10.0.tar.gz", "yanked": false}], "1.10.1": [{"comment_text": "", "digests": {"md5": "e648d54def51867905fa37d4a2519a33", "sha256": "216e0aecb13022f18ebfce87198e8e3b2f9c3fcb6a67ce9ab0c8718506b6ce00"}, "downloads": -1, "filename": "ops-cli-1.10.1.tar.gz", "has_sig": false, "md5_digest": "e648d54def51867905fa37d4a2519a33", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 134081, "upload_time": "2019-08-29T14:47:48", "upload_time_iso_8601": "2019-08-29T14:47:48.959740Z", "url": "https://files.pythonhosted.org/packages/aa/d4/91af8beb326a43736e067bce566736fe695bdbb48cd30859aed5215e7ba1/ops-cli-1.10.1.tar.gz", "yanked": false}], "1.11.0": [{"comment_text": "", "digests": {"md5": "8c2ae37b5cfda9ce5db1574d9ca10899", "sha256": "f4c38c74ba72367f96cac71c5089e1b3764873ecd45e265daefc1ea3edf577be"}, "downloads": -1, "filename": "ops-cli-1.11.0.tar.gz", "has_sig": false, "md5_digest": "8c2ae37b5cfda9ce5db1574d9ca10899", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 134080, "upload_time": "2019-08-29T14:48:45", "upload_time_iso_8601": "2019-08-29T14:48:45.641310Z", "url": "https://files.pythonhosted.org/packages/53/52/5ba9dda48f05f7a1ae3028048a3cb2d730455fc03b9c3648777e01dfb5b4/ops-cli-1.11.0.tar.gz", "yanked": false}], "1.11.1": [{"comment_text": "", "digests": {"md5": "e500850597263aefc3f79c6749fd94b9", "sha256": "4642ea2c5677ed7d4f44b25f2f2b0c07273bb01a54504b5d2347c2ac31c6b297"}, "downloads": -1, "filename": "ops-cli-1.11.1.tar.gz", "has_sig": false, "md5_digest": "e500850597263aefc3f79c6749fd94b9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 134061, "upload_time": "2019-08-29T16:09:50", "upload_time_iso_8601": "2019-08-29T16:09:50.898507Z", "url": "https://files.pythonhosted.org/packages/fd/66/de4772b6346dd87b3a27a97993b3615d3516652078fbf9275fe5e00027ba/ops-cli-1.11.1.tar.gz", "yanked": false}], "1.11.10": [{"comment_text": "", "digests": {"md5": "f9f0d85823b4d747c3c84177dac1e0f4", "sha256": "4854339f1564b0a60c607621b52d249062ade872a045e98bce1a09f4aa117c26"}, "downloads": -1, "filename": "ops-cli-1.11.10.tar.gz", "has_sig": false, "md5_digest": "f9f0d85823b4d747c3c84177dac1e0f4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 133936, "upload_time": "2019-10-23T11:47:51", "upload_time_iso_8601": "2019-10-23T11:47:51.730788Z", "url": "https://files.pythonhosted.org/packages/d9/8d/87eb36da8d9558cdfa5e58753a468280dc6d506daaa600e516a6670b30b2/ops-cli-1.11.10.tar.gz", "yanked": false}], "1.11.11": [{"comment_text": "", "digests": {"md5": "3a242e96fd9fc9850c5a9327b20cbd9b", "sha256": "a1194ae37fec769e552e98f7a1c0e5a99d9ee8e76387429b021b0c7a900ea80d"}, "downloads": -1, "filename": "ops-cli-1.11.11.tar.gz", "has_sig": false, "md5_digest": "3a242e96fd9fc9850c5a9327b20cbd9b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 133936, "upload_time": "2019-10-23T13:18:34", "upload_time_iso_8601": "2019-10-23T13:18:34.621508Z", "url": "https://files.pythonhosted.org/packages/54/8b/b253e4c1a5956e0b558b5456a50c012a6e11df592f2435a43dad20e967ef/ops-cli-1.11.11.tar.gz", "yanked": false}], "1.11.12": [{"comment_text": "", "digests": {"md5": "5b563b5a4eb4c04d2ac213b048176f87", "sha256": "3b6a7bcf27f666b0bea34ad0cffeb5c330342fc41a49cc795ae54b8f5b009f46"}, "downloads": -1, "filename": "ops-cli-1.11.12.tar.gz", "has_sig": false, "md5_digest": "5b563b5a4eb4c04d2ac213b048176f87", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 133908, "upload_time": "2019-11-05T14:58:40", "upload_time_iso_8601": "2019-11-05T14:58:40.135681Z", "url": "https://files.pythonhosted.org/packages/c8/86/bf92593203e8f24cbf7cfbee5b5e81e1151128c14d75bf241f921c31e3c1/ops-cli-1.11.12.tar.gz", "yanked": false}], "1.11.2": [{"comment_text": "", "digests": {"md5": "29c4d5c43643df77d48f9073016ca7d1", "sha256": "124f6d98255fc6c104fb32e22effb65d209c7be53346bf20dcabf2ea65b2843c"}, "downloads": -1, "filename": "ops-cli-1.11.2.tar.gz", "has_sig": false, "md5_digest": "29c4d5c43643df77d48f9073016ca7d1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 134105, "upload_time": "2019-08-29T16:50:24", "upload_time_iso_8601": "2019-08-29T16:50:24.242137Z", "url": "https://files.pythonhosted.org/packages/42/11/b8ef00dadbfddf87cf7bfe9fa040b0968f970d21e88873aff44c0dc9eb59/ops-cli-1.11.2.tar.gz", "yanked": false}], "1.11.3": [{"comment_text": "", "digests": {"md5": "d274caa0a0cf08e85fa8524fad9130e0", "sha256": "1f306d7da322d9d41a67156f8f9a6719b3074fb7728444db9d40c7bb0b6866a6"}, "downloads": -1, "filename": "ops-cli-1.11.3.tar.gz", "has_sig": false, "md5_digest": "d274caa0a0cf08e85fa8524fad9130e0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 134249, "upload_time": "2019-09-10T14:31:57", "upload_time_iso_8601": "2019-09-10T14:31:57.889620Z", "url": "https://files.pythonhosted.org/packages/63/79/82fae39e6bdc710149f6ea0f3b4f630e5cfb619f9376413039cbd6c6d51e/ops-cli-1.11.3.tar.gz", "yanked": false}], "1.11.4": [{"comment_text": "", "digests": {"md5": "2b5a11ccf529c192876f3f3adaef5d21", "sha256": "14e3dd9a623e5391797c538ad0e87c5f827ae894e4a47b7886716e9e70d982e5"}, "downloads": -1, "filename": "ops-cli-1.11.4.tar.gz", "has_sig": false, "md5_digest": "2b5a11ccf529c192876f3f3adaef5d21", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 134251, "upload_time": "2019-10-04T03:36:02", "upload_time_iso_8601": "2019-10-04T03:36:02.563594Z", "url": "https://files.pythonhosted.org/packages/50/a0/0169d7e967c5a1ff21387206b72c27bc83fd46ef73a4b7891611982b18b7/ops-cli-1.11.4.tar.gz", "yanked": false}], "1.11.5": [{"comment_text": "", "digests": {"md5": "d49c93892a9cac5c9620d8f311631a33", "sha256": "eb2c70ad303023dcc09f27a174b231bf550f96ad07021b7aec7bcc5389c689c3"}, "downloads": -1, "filename": "ops-cli-1.11.5.tar.gz", "has_sig": false, "md5_digest": "d49c93892a9cac5c9620d8f311631a33", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 134429, "upload_time": "2019-10-04T17:12:36", "upload_time_iso_8601": "2019-10-04T17:12:36.643905Z", "url": "https://files.pythonhosted.org/packages/75/e7/1a985252f708a201e79143c71cfaabd8614665b199ea388b0c243a4364eb/ops-cli-1.11.5.tar.gz", "yanked": false}], "1.11.6": [{"comment_text": "", "digests": {"md5": "dee04034b0c7af648a8e2459c63bf586", "sha256": "a6b55e08fbed8849f276a0e60d02535b8357f38ab3dcaf0e6f2744a144b1318c"}, "downloads": -1, "filename": "ops-cli-1.11.6.tar.gz", "has_sig": false, "md5_digest": "dee04034b0c7af648a8e2459c63bf586", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 128878, "upload_time": "2019-10-07T15:55:04", "upload_time_iso_8601": "2019-10-07T15:55:04.430777Z", "url": "https://files.pythonhosted.org/packages/2d/a8/768c390eb9d56803cf4780a21e1a8689ff7b6cd7165964148c6e501bd412/ops-cli-1.11.6.tar.gz", "yanked": false}], "1.11.7": [{"comment_text": "", "digests": {"md5": "795cd414b150d33df71731cd4d91996e", "sha256": "cd072af8ed6ed76e1faebc414926e53d644c03d94b2ce966c8ed38e6e11b5fa7"}, "downloads": -1, "filename": "ops-cli-1.11.7.tar.gz", "has_sig": false, "md5_digest": "795cd414b150d33df71731cd4d91996e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 129696, "upload_time": "2019-10-17T14:32:34", "upload_time_iso_8601": "2019-10-17T14:32:34.770853Z", "url": "https://files.pythonhosted.org/packages/cc/41/46b89231bbfd4a5936e814679c5df8ed9407e8617a5a004115264a916769/ops-cli-1.11.7.tar.gz", "yanked": false}], "1.11.8": [{"comment_text": "", "digests": {"md5": "0fe6ec2b6de20dbf6c9079559ce5c4ea", "sha256": "54e070c80679f67053492e1c02f6ecc45e090746121dc4452cfd6d239180b6fd"}, "downloads": -1, "filename": "ops-cli-1.11.8.tar.gz", "has_sig": false, "md5_digest": "0fe6ec2b6de20dbf6c9079559ce5c4ea", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 133115, "upload_time": "2019-10-18T18:55:48", "upload_time_iso_8601": "2019-10-18T18:55:48.015215Z", "url": "https://files.pythonhosted.org/packages/0a/28/b810be1525efabcc2dd3d2bc5b90927308db8a0298bf1d931a4c8bde5db7/ops-cli-1.11.8.tar.gz", "yanked": false}], "1.11.9": [{"comment_text": "", "digests": {"md5": "0694069467e605d1b86eb8f28a460845", "sha256": "c827aa2a83a71e9a019cfd2d5b3be0cf5b989067a7dad13a22285c29db5c7769"}, "downloads": -1, "filename": "ops-cli-1.11.9.tar.gz", "has_sig": false, "md5_digest": "0694069467e605d1b86eb8f28a460845", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 134099, "upload_time": "2019-10-23T08:22:34", "upload_time_iso_8601": "2019-10-23T08:22:34.375814Z", "url": "https://files.pythonhosted.org/packages/ba/12/c027c5e28531da59dd15e23385670799abc1a1b80a798397320a1f52c373/ops-cli-1.11.9.tar.gz", "yanked": false}], "1.12.1": [{"comment_text": "", "digests": {"md5": "7d1020811e068cdb2c17bc5d1d10e8cc", "sha256": "d78a606b72f2c54bc7547df1a16de41c384a22b83bcc2a91bd35183185d69af3"}, "downloads": -1, "filename": "ops_cli-1.12.1-py3-none-any.whl", "has_sig": false, "md5_digest": "7d1020811e068cdb2c17bc5d1d10e8cc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 96712, "upload_time": "2019-11-12T12:17:00", "upload_time_iso_8601": "2019-11-12T12:17:00.206745Z", "url": "https://files.pythonhosted.org/packages/8c/10/1f504602478e347662e124ece408d5ef4fd31114f8c0b8afbfcbabb55503/ops_cli-1.12.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b12e294c3a1481cf5d32a531071dedd6", "sha256": "8a84a1c0a488e61392661a0c61bf7bf529b764ec2009e2263e07f39d6b8fb0b1"}, "downloads": -1, "filename": "ops-cli-1.12.1.tar.gz", "has_sig": false, "md5_digest": "b12e294c3a1481cf5d32a531071dedd6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 135903, "upload_time": "2019-11-12T12:17:02", "upload_time_iso_8601": "2019-11-12T12:17:02.113879Z", "url": "https://files.pythonhosted.org/packages/37/85/f3f01601cc6ccc5c4cf6bbd84e2d3f9648ea13b7e4d1ad6060555a2941b9/ops-cli-1.12.1.tar.gz", "yanked": false}], "1.12.2": [{"comment_text": "", "digests": {"md5": "27a4a5cb1dfea97ee85a0f04e455f5ac", "sha256": "092efc796c1d6ad567e77010b411534597b50df44317dbc52a71625901006433"}, "downloads": -1, "filename": "ops_cli-1.12.2-py3-none-any.whl", "has_sig": false, "md5_digest": "27a4a5cb1dfea97ee85a0f04e455f5ac", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 96715, "upload_time": "2019-11-12T14:06:14", "upload_time_iso_8601": "2019-11-12T14:06:14.439043Z", "url": "https://files.pythonhosted.org/packages/5d/85/624ac87ef967a44d5a3d6c24a73b004e917fd044d6dadf74e68c985d7e72/ops_cli-1.12.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4d5acb1f9d805b7c5ef73dfcb8ef5d91", "sha256": "28de2e99d371cb36fa18b5259f5720f6e33b1cc36ace25b8b2672aaa5b6ccadc"}, "downloads": -1, "filename": "ops-cli-1.12.2.tar.gz", "has_sig": false, "md5_digest": "4d5acb1f9d805b7c5ef73dfcb8ef5d91", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 135901, "upload_time": "2019-11-12T14:06:16", "upload_time_iso_8601": "2019-11-12T14:06:16.530377Z", "url": "https://files.pythonhosted.org/packages/34/09/53872c7700ae20da046405bd56761321a254aa78a13768dfbf17bb1b39cd/ops-cli-1.12.2.tar.gz", "yanked": false}], "2.0.3": [{"comment_text": "", "digests": {"md5": "0f20c327a3bc38f260cf4aa53330b1a8", "sha256": "9edd78f8dd62dd23168cb20c03239f9753c90a5ab30382f809742f900facd08d"}, "downloads": -1, "filename": "ops_cli-2.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "0f20c327a3bc38f260cf4aa53330b1a8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 98083, "upload_time": "2019-11-12T12:09:18", "upload_time_iso_8601": "2019-11-12T12:09:18.812975Z", "url": "https://files.pythonhosted.org/packages/20/7f/43fab6480ad7d92dd21ef8d627464216d9480050375531d1d0ea126e47ce/ops_cli-2.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a825a64d05669baaed4e0299877a70c6", "sha256": "21fc6842501b6d1a12d1a6afe87b2885c555261fcaeb4aac7e7c509732d29885"}, "downloads": -1, "filename": "ops-cli-2.0.3.tar.gz", "has_sig": false, "md5_digest": "a825a64d05669baaed4e0299877a70c6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 136274, "upload_time": "2019-11-12T12:09:20", "upload_time_iso_8601": "2019-11-12T12:09:20.368445Z", "url": "https://files.pythonhosted.org/packages/5d/81/e4c2281dbbde9295b64fc93d761de7d1ecbf7264b4e7bca8e80289131969/ops-cli-2.0.3.tar.gz", "yanked": false}], "2.0.4": [{"comment_text": "", "digests": {"md5": "0a33e64e940c0aac4f1f48044ee151fd", "sha256": "b582f270af47d5f80fc7d54234fb6929fbe2ad7aa3b7796cebd69f166f2725c2"}, "downloads": -1, "filename": "ops_cli-2.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "0a33e64e940c0aac4f1f48044ee151fd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 98091, "upload_time": "2019-12-16T13:52:01", "upload_time_iso_8601": "2019-12-16T13:52:01.335526Z", "url": "https://files.pythonhosted.org/packages/93/75/3702ae033f82d3c040cd5c0c88fa8a16f2163102571ddbd6f9285ae4e02e/ops_cli-2.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a51c642ddaf4c339086834b8c30492b0", "sha256": "3368400958c0fe4ad4fe9c8ee507083591b5423b25d187ea3f37cb511c0f56d0"}, "downloads": -1, "filename": "ops-cli-2.0.4.tar.gz", "has_sig": false, "md5_digest": "a51c642ddaf4c339086834b8c30492b0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 136321, "upload_time": "2019-12-16T13:52:02", "upload_time_iso_8601": "2019-12-16T13:52:02.991674Z", "url": "https://files.pythonhosted.org/packages/9b/f6/f5ed707a8003d1c5281a0c373f0816ed258b4069eb992286e4f754cc4bb9/ops-cli-2.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0a33e64e940c0aac4f1f48044ee151fd", "sha256": "b582f270af47d5f80fc7d54234fb6929fbe2ad7aa3b7796cebd69f166f2725c2"}, "downloads": -1, "filename": "ops_cli-2.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "0a33e64e940c0aac4f1f48044ee151fd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 98091, "upload_time": "2019-12-16T13:52:01", "upload_time_iso_8601": "2019-12-16T13:52:01.335526Z", "url": "https://files.pythonhosted.org/packages/93/75/3702ae033f82d3c040cd5c0c88fa8a16f2163102571ddbd6f9285ae4e02e/ops_cli-2.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a51c642ddaf4c339086834b8c30492b0", "sha256": "3368400958c0fe4ad4fe9c8ee507083591b5423b25d187ea3f37cb511c0f56d0"}, "downloads": -1, "filename": "ops-cli-2.0.4.tar.gz", "has_sig": false, "md5_digest": "a51c642ddaf4c339086834b8c30492b0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 136321, "upload_time": "2019-12-16T13:52:02", "upload_time_iso_8601": "2019-12-16T13:52:02.991674Z", "url": "https://files.pythonhosted.org/packages/9b/f6/f5ed707a8003d1c5281a0c373f0816ed258b4069eb992286e4f754cc4bb9/ops-cli-2.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:02:09 2020"}