{"info": {"author": "Abhijeet Kumar", "author_email": "abhijeetchar@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# finbert_embedding\nToken and sentence level embeddings from FinBERT model (Financial Domain).\n\n[BERT](https://arxiv.org/abs/1810.04805), published by Google, is conceptually simple and empirically powerful as it obtained state-of-the-art results on eleven natural language processing tasks.  \n\nThe objective of this project is to obtain the word or sentence embeddings from [FinBERT](https://github.com/ProsusAI/finBERT), pre-trained model by Dogu Tan Araci (University of Amsterdam). FinBERT, which is a BERT language model further trained on Financial news articles for adapting financial domain. It achieved the state-of-the-art on FiQA sentiment scoring and Financial PhraseBank dataset. Paper [here](https://arxiv.org/abs/1908.10063).\n\nInstead of building and do fine-tuning for an end-to-end NLP model, You can directly utilize word embeddings from Financial BERT to build NLP models for various downstream tasks eg. Financial text classification, Text clustering, Extractive summarization or Entity extraction etc.\n\n\n\n## Features\n* Creates an abstraction to remove dealing with inferencing pre-trained FinBERT model.\n* Require only two lines of code to get sentence/token-level encoding for a text sentence.\n* The package takes care of OOVs (out of vocabulary) inherently.\n* Downloads and installs FinBERT pre-trained model (first initialization, usage in next section).\n\n## Install\n(Recommended to create a conda env to have isolation and avoid dependency clashes)\n\n```\npip install finbert-embedding==0.1.4\n```\n\nNote: If you get error in installing this package (common error with Tf): <br>\n\nInstalling collected packages: wrapt, tensorflow <br>\n  Found existing installation: wrapt 1.10.11 <br>\nERROR: Cannot uninstall 'wrapt'. It is a distutils installed project....\n\nthen, just do this:\n```\npip install wrapt --upgrade --ignore-installed\npip install finbert-embedding==0.1.4\n```\n\n## Usage 1\n\nword embeddings generated are list of 768 dimensional embeddings for each word. <br>\nsentence embedding generated is 768 dimensional embedding which is average of each token.\n\n```python\nfrom finbert_embedding.embedding import FinbertEmbedding\n\ntext = \"Another PSU bank, Punjab National Bank which also reported numbers managed to see a slight improvement in asset quality.\"\n\n# Class Initialization (You can set default 'model_path=None' as your finetuned BERT model path while Initialization)\nfinbert = FinbertEmbedding()\n\nword_embeddings = finbert.word_vector(text)\nsentence_embedding = finbert.sentence_vector(text)\n\nprint(\"Text Tokens: \", finbert.tokens)\n# Text Tokens:  ['another', 'psu', 'bank', ',', 'punjab', 'national', 'bank', 'which', 'also', 'reported', 'numbers', 'managed', 'to', 'see', 'a', 'slight', 'improvement', 'in', 'asset', 'quality', '.']\n\nprint ('Shape of Word Embeddings: %d x %d' % (len(word_embeddings), len(word_embeddings[0])))\n# Shape of Word Embeddings: 21 x 768\n\nprint(\"Shape of Sentence Embedding = \",len(sentence_embedding))\n# Shape of Sentence Embedding =  768\n```\n\n## Usage 2\n\nA decent representation for a downstream task doesn't mean that it will be meaningful in terms of cosine distance. Since cosine distance is a linear space where all dimensions are weighted equally. if you want to use cosine distance anyway, then please focus on the rank not the absolute value.\n\nNamely, do not use: <br>\n  if cosine(A, B) > 0.9, then A and B are similar\n\nPlease consider the following instead: <br>\n  if cosine(A, B) > cosine(A, C), then A is more similar to B than C.\n\n```python\nfrom finbert_embedding.embedding import FinbertEmbedding\n\ntext = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\nfinbert = FinbertEmbedding()\nword_embeddings = finbert.word_vector(text)\n\nfrom scipy.spatial.distance import cosine\ndiff_bank = 1 - cosine(word_embeddings[9], word_embeddings[18])\nsame_bank = 1 - cosine(word_embeddings[9], word_embeddings[5])\n\nprint('Vector similarity for similar bank meanings (bank vault & bank robber):  %.2f' % same_bank)\nprint('Vector similarity for different bank meanings (bank robber & river bank):  %.2f' % diff_bank)\n\n# Vector similarity for similar bank meanings (bank vault & bank robber):  0.92\n# Vector similarity for different bank meanings (bank robber & river bank):  0.64\n```\n\n### Warning\n\nAccording to BERT author Jacob Devlin:\n```I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn't mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).```\n\nHowever, with the [CLS] token, it does become meaningful if the model has been fine-tuned, where the last hidden layer of this token is used as the \u201csentence vector\u201d for downstream sequence classification task. This package encode sentence in similar manner.   \n\n### To Do (Next Version)\n\n* Extend it to give word embeddings for a paragram/Document (Currently, it takes one sentence as input). Chunkize your paragraph or text document into sentences using Spacy or NLTK before using finbert_embedding.\n* Adding batch processing feature.\n* More ways of handing OOVs (Currently, uses average of all tokens of a OOV word)\n* Ingesting and extending it to more pre-trained financial models.\n\n### Future Goal\n\n* Create generic downstream framework using various FinBERT language model for any financial labelled text classifcation task like sentiment classification, Financial news classification, Financial Document classification.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/abhijeet3922/finbert_embedding/archive/v0.1.4.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/abhijeet3922/finbert_embedding", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "finbert-embedding", "package_url": "https://pypi.org/project/finbert-embedding/", "platform": "", "project_url": "https://pypi.org/project/finbert-embedding/", "project_urls": {"Download": "https://github.com/abhijeet3922/finbert_embedding/archive/v0.1.4.tar.gz", "Homepage": "https://github.com/abhijeet3922/finbert_embedding"}, "release_url": "https://pypi.org/project/finbert-embedding/0.1.4/", "requires_dist": ["torch (==1.1.0)", "pytorch-pretrained-bert (==0.6.2)", "tensorflow"], "requires_python": ">=3.6", "summary": "Embeddings from Financial BERT", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>finbert_embedding</h1>\n<p>Token and sentence level embeddings from FinBERT model (Financial Domain).</p>\n<p><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a>, published by Google, is conceptually simple and empirically powerful as it obtained state-of-the-art results on eleven natural language processing tasks.</p>\n<p>The objective of this project is to obtain the word or sentence embeddings from <a href=\"https://github.com/ProsusAI/finBERT\" rel=\"nofollow\">FinBERT</a>, pre-trained model by Dogu Tan Araci (University of Amsterdam). FinBERT, which is a BERT language model further trained on Financial news articles for adapting financial domain. It achieved the state-of-the-art on FiQA sentiment scoring and Financial PhraseBank dataset. Paper <a href=\"https://arxiv.org/abs/1908.10063\" rel=\"nofollow\">here</a>.</p>\n<p>Instead of building and do fine-tuning for an end-to-end NLP model, You can directly utilize word embeddings from Financial BERT to build NLP models for various downstream tasks eg. Financial text classification, Text clustering, Extractive summarization or Entity extraction etc.</p>\n<h2>Features</h2>\n<ul>\n<li>Creates an abstraction to remove dealing with inferencing pre-trained FinBERT model.</li>\n<li>Require only two lines of code to get sentence/token-level encoding for a text sentence.</li>\n<li>The package takes care of OOVs (out of vocabulary) inherently.</li>\n<li>Downloads and installs FinBERT pre-trained model (first initialization, usage in next section).</li>\n</ul>\n<h2>Install</h2>\n<p>(Recommended to create a conda env to have isolation and avoid dependency clashes)</p>\n<pre><code>pip install finbert-embedding==0.1.4\n</code></pre>\n<p>Note: If you get error in installing this package (common error with Tf): <br></p>\n<p>Installing collected packages: wrapt, tensorflow <br>\nFound existing installation: wrapt 1.10.11 <br>\nERROR: Cannot uninstall 'wrapt'. It is a distutils installed project....</p>\n<p>then, just do this:</p>\n<pre><code>pip install wrapt --upgrade --ignore-installed\npip install finbert-embedding==0.1.4\n</code></pre>\n<h2>Usage 1</h2>\n<p>word embeddings generated are list of 768 dimensional embeddings for each word. <br>\nsentence embedding generated is 768 dimensional embedding which is average of each token.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">finbert_embedding.embedding</span> <span class=\"kn\">import</span> <span class=\"n\">FinbertEmbedding</span>\n\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s2\">\"Another PSU bank, Punjab National Bank which also reported numbers managed to see a slight improvement in asset quality.\"</span>\n\n<span class=\"c1\"># Class Initialization (You can set default 'model_path=None' as your finetuned BERT model path while Initialization)</span>\n<span class=\"n\">finbert</span> <span class=\"o\">=</span> <span class=\"n\">FinbertEmbedding</span><span class=\"p\">()</span>\n\n<span class=\"n\">word_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">finbert</span><span class=\"o\">.</span><span class=\"n\">word_vector</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n<span class=\"n\">sentence_embedding</span> <span class=\"o\">=</span> <span class=\"n\">finbert</span><span class=\"o\">.</span><span class=\"n\">sentence_vector</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Text Tokens: \"</span><span class=\"p\">,</span> <span class=\"n\">finbert</span><span class=\"o\">.</span><span class=\"n\">tokens</span><span class=\"p\">)</span>\n<span class=\"c1\"># Text Tokens:  ['another', 'psu', 'bank', ',', 'punjab', 'national', 'bank', 'which', 'also', 'reported', 'numbers', 'managed', 'to', 'see', 'a', 'slight', 'improvement', 'in', 'asset', 'quality', '.']</span>\n\n<span class=\"nb\">print</span> <span class=\"p\">(</span><span class=\"s1\">'Shape of Word Embeddings: </span><span class=\"si\">%d</span><span class=\"s1\"> x </span><span class=\"si\">%d</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">word_embeddings</span><span class=\"p\">),</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">word_embeddings</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])))</span>\n<span class=\"c1\"># Shape of Word Embeddings: 21 x 768</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Shape of Sentence Embedding = \"</span><span class=\"p\">,</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sentence_embedding</span><span class=\"p\">))</span>\n<span class=\"c1\"># Shape of Sentence Embedding =  768</span>\n</pre>\n<h2>Usage 2</h2>\n<p>A decent representation for a downstream task doesn't mean that it will be meaningful in terms of cosine distance. Since cosine distance is a linear space where all dimensions are weighted equally. if you want to use cosine distance anyway, then please focus on the rank not the absolute value.</p>\n<p>Namely, do not use: <br>\nif cosine(A, B) &gt; 0.9, then A and B are similar</p>\n<p>Please consider the following instead: <br>\nif cosine(A, B) &gt; cosine(A, C), then A is more similar to B than C.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">finbert_embedding.embedding</span> <span class=\"kn\">import</span> <span class=\"n\">FinbertEmbedding</span>\n\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s2\">\"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"</span>\n<span class=\"n\">finbert</span> <span class=\"o\">=</span> <span class=\"n\">FinbertEmbedding</span><span class=\"p\">()</span>\n<span class=\"n\">word_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">finbert</span><span class=\"o\">.</span><span class=\"n\">word_vector</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">scipy.spatial.distance</span> <span class=\"kn\">import</span> <span class=\"n\">cosine</span>\n<span class=\"n\">diff_bank</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">cosine</span><span class=\"p\">(</span><span class=\"n\">word_embeddings</span><span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">],</span> <span class=\"n\">word_embeddings</span><span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">])</span>\n<span class=\"n\">same_bank</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">cosine</span><span class=\"p\">(</span><span class=\"n\">word_embeddings</span><span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">],</span> <span class=\"n\">word_embeddings</span><span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">])</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'Vector similarity for similar bank meanings (bank vault &amp; bank robber):  </span><span class=\"si\">%.2f</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"n\">same_bank</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'Vector similarity for different bank meanings (bank robber &amp; river bank):  </span><span class=\"si\">%.2f</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"n\">diff_bank</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Vector similarity for similar bank meanings (bank vault &amp; bank robber):  0.92</span>\n<span class=\"c1\"># Vector similarity for different bank meanings (bank robber &amp; river bank):  0.64</span>\n</pre>\n<h3>Warning</h3>\n<p>According to BERT author Jacob Devlin:\n<code>I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn't mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).</code></p>\n<p>However, with the [CLS] token, it does become meaningful if the model has been fine-tuned, where the last hidden layer of this token is used as the \u201csentence vector\u201d for downstream sequence classification task. This package encode sentence in similar manner.</p>\n<h3>To Do (Next Version)</h3>\n<ul>\n<li>Extend it to give word embeddings for a paragram/Document (Currently, it takes one sentence as input). Chunkize your paragraph or text document into sentences using Spacy or NLTK before using finbert_embedding.</li>\n<li>Adding batch processing feature.</li>\n<li>More ways of handing OOVs (Currently, uses average of all tokens of a OOV word)</li>\n<li>Ingesting and extending it to more pre-trained financial models.</li>\n</ul>\n<h3>Future Goal</h3>\n<ul>\n<li>Create generic downstream framework using various FinBERT language model for any financial labelled text classifcation task like sentiment classification, Financial news classification, Financial Document classification.</li>\n</ul>\n\n          </div>"}, "last_serial": 6364679, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "7b91c76991842b071364585f35153aa1", "sha256": "da9d0fb88b1587cd486c7fccb90510922dea50f80adce2640e978bfbaf1c3afc"}, "downloads": -1, "filename": "finbert_embedding-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "7b91c76991842b071364585f35153aa1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 5547, "upload_time": "2019-12-26T14:22:01", "upload_time_iso_8601": "2019-12-26T14:22:01.139302Z", "url": "https://files.pythonhosted.org/packages/1c/d4/1b2f565e1f42254ec8ec0e45c68e3eb915d72d2778ef92e1cd98872840a2/finbert_embedding-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6e59517e92913a237d8f574bc7f4cf3f", "sha256": "dfad112f742cb1959b0498860540e4220c0de1311cb3f5e93aae12e9dc6dbfdb"}, "downloads": -1, "filename": "finbert-embedding-0.1.0.tar.gz", "has_sig": false, "md5_digest": "6e59517e92913a237d8f574bc7f4cf3f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 3792, "upload_time": "2019-12-26T14:22:03", "upload_time_iso_8601": "2019-12-26T14:22:03.450718Z", "url": "https://files.pythonhosted.org/packages/2d/c8/c5f43c3547ffcac544d9f4c39cf8aa3cf1390afcd7e51a8e6aeae20b40cf/finbert-embedding-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "48a85d103eb4caaaf9499a7f8dfa440e", "sha256": "7855aa454ced7c44b4b1bc44eaf772997cf9ef77aa64f33bbe849f5d672b3f93"}, "downloads": -1, "filename": "finbert_embedding-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "48a85d103eb4caaaf9499a7f8dfa440e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 5526, "upload_time": "2019-12-26T15:32:54", "upload_time_iso_8601": "2019-12-26T15:32:54.911313Z", "url": "https://files.pythonhosted.org/packages/51/a7/864971852fa94ed190c80dc1a9c23c6939a4d9007787771b4e752678240c/finbert_embedding-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f5ee2070368f7914688bd4c9ab3d7480", "sha256": "c47c86f0a2850628603b760f9a6c08631e04a29fa3273b5191eee680c33b966c"}, "downloads": -1, "filename": "finbert-embedding-0.1.1.tar.gz", "has_sig": false, "md5_digest": "f5ee2070368f7914688bd4c9ab3d7480", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 3786, "upload_time": "2019-12-26T15:32:56", "upload_time_iso_8601": "2019-12-26T15:32:56.421238Z", "url": "https://files.pythonhosted.org/packages/fb/44/d18a58e74aa865c25281a2313f52ba14ca8f62cc3a5e1c398ec47dff0ac2/finbert-embedding-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "7a43b3cd3413cca47336b865b42a3953", "sha256": "f62de312c3299a88f2a8f8f53765fcb8fa79e6d2d4e2793e33852beafc72bd32"}, "downloads": -1, "filename": "finbert_embedding-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "7a43b3cd3413cca47336b865b42a3953", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 5529, "upload_time": "2019-12-26T15:48:13", "upload_time_iso_8601": "2019-12-26T15:48:13.627534Z", "url": "https://files.pythonhosted.org/packages/1a/89/65ce593e2a37f36091869332250103a1846ebc4a715bf643a0f334c6034f/finbert_embedding-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f63fe71372b910efc5d46986c07b4d9c", "sha256": "261587117433b730a83671d45854b1582782eaae80075283a4a5f76f33476c62"}, "downloads": -1, "filename": "finbert-embedding-0.1.2.tar.gz", "has_sig": false, "md5_digest": "f63fe71372b910efc5d46986c07b4d9c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 3791, "upload_time": "2019-12-26T15:48:15", "upload_time_iso_8601": "2019-12-26T15:48:15.462328Z", "url": "https://files.pythonhosted.org/packages/b1/dc/c29674c37e694625a431a0418e6ba2e909f019ae9f5b9987c2ce24dcdf6c/finbert-embedding-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "fa6aa7eeeb7310591730c956161ab385", "sha256": "307deb363a7738d5081d355dd7170acf7e718325c6517e06980b17f9566c53b8"}, "downloads": -1, "filename": "finbert_embedding-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "fa6aa7eeeb7310591730c956161ab385", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 7824, "upload_time": "2019-12-27T08:51:31", "upload_time_iso_8601": "2019-12-27T08:51:31.208503Z", "url": "https://files.pythonhosted.org/packages/70/4a/1e744c9063c5c66c98f5d86893f51b460239588d5ca8d3172767044756b2/finbert_embedding-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "498c4d91b8bd5f22b1382727b5be6c3e", "sha256": "e2d6b73afdb748af4d39d246a4c184b039b1383a6f6f627cacf6025b41e3a658"}, "downloads": -1, "filename": "finbert-embedding-0.1.3.tar.gz", "has_sig": false, "md5_digest": "498c4d91b8bd5f22b1382727b5be6c3e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6271, "upload_time": "2019-12-27T08:51:33", "upload_time_iso_8601": "2019-12-27T08:51:33.260395Z", "url": "https://files.pythonhosted.org/packages/d8/7c/dc92841e62fd21d892a3d2f9a8bb967c1d21a7080af96769e4c07dd27963/finbert-embedding-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "91fc17660c86a020241f6e7d2d7da4e3", "sha256": "753b5a368a5e5b162c657cb8b4b6952008d6bb370548cf3ebbe7ac3ce73848fc"}, "downloads": -1, "filename": "finbert_embedding-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "91fc17660c86a020241f6e7d2d7da4e3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 7826, "upload_time": "2019-12-27T09:39:52", "upload_time_iso_8601": "2019-12-27T09:39:52.890560Z", "url": "https://files.pythonhosted.org/packages/65/74/62c14e0960b5815da757e4e28d06166ddd879b4931d5a900ea97e3d9cb58/finbert_embedding-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4473286726aa571ca9d116d25eb50a77", "sha256": "b5859ddef5290c31637e8d325f869ec291c93d331fa4c9be410b01edd4317fa4"}, "downloads": -1, "filename": "finbert-embedding-0.1.4.tar.gz", "has_sig": false, "md5_digest": "4473286726aa571ca9d116d25eb50a77", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6279, "upload_time": "2019-12-27T09:39:55", "upload_time_iso_8601": "2019-12-27T09:39:55.334131Z", "url": "https://files.pythonhosted.org/packages/71/2a/01a2bbf5446670ad56e3769836c5bc7941ae3d911ffb0effe7982dee1919/finbert-embedding-0.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "91fc17660c86a020241f6e7d2d7da4e3", "sha256": "753b5a368a5e5b162c657cb8b4b6952008d6bb370548cf3ebbe7ac3ce73848fc"}, "downloads": -1, "filename": "finbert_embedding-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "91fc17660c86a020241f6e7d2d7da4e3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 7826, "upload_time": "2019-12-27T09:39:52", "upload_time_iso_8601": "2019-12-27T09:39:52.890560Z", "url": "https://files.pythonhosted.org/packages/65/74/62c14e0960b5815da757e4e28d06166ddd879b4931d5a900ea97e3d9cb58/finbert_embedding-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4473286726aa571ca9d116d25eb50a77", "sha256": "b5859ddef5290c31637e8d325f869ec291c93d331fa4c9be410b01edd4317fa4"}, "downloads": -1, "filename": "finbert-embedding-0.1.4.tar.gz", "has_sig": false, "md5_digest": "4473286726aa571ca9d116d25eb50a77", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6279, "upload_time": "2019-12-27T09:39:55", "upload_time_iso_8601": "2019-12-27T09:39:55.334131Z", "url": "https://files.pythonhosted.org/packages/71/2a/01a2bbf5446670ad56e3769836c5bc7941ae3d911ffb0effe7982dee1919/finbert-embedding-0.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:42:26 2020"}