{"info": {"author": "Shinji Watanabe", "author_email": "shinjiw@ieee.org", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Operating System :: POSIX :: Linux", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "<div align=\"left\"><img src=\"doc/image/espnet_logo1.png\" width=\"550\"/></div>\n\n# ESPnet: end-to-end speech processing toolkit\n\n[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)\n[![Build Status](https://travis-ci.org/espnet/espnet.svg?branch=master)](https://travis-ci.org/espnet/espnet)\n[![CircleCI](https://circleci.com/gh/espnet/espnet.svg?style=svg)](https://circleci.com/gh/espnet/espnet)\n[![codecov](https://codecov.io/gh/espnet/espnet/branch/master/graph/badge.svg)](https://codecov.io/gh/espnet/espnet)\n[![Gitter](https://badges.gitter.im/espnet-en/community.svg)](https://gitter.im/espnet-en/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n\n[**Docs**](https://espnet.github.io/espnet/)\n| [**Example**](https://github.com/espnet/espnet/tree/master/egs)\n| [**Docker**](https://github.com/espnet/espnet/tree/master/docker)\n| [**Notebook**](https://github.com/espnet/notebook)\n| [**Tutorial (2019)**](https://github.com/espnet/interspeech2019-tutorial)\n\nESPnet is an end-to-end speech processing toolkit, mainly focuses on end-to-end speech recognition and end-to-end text-to-speech.\nESPnet uses [chainer](https://chainer.org/) and [pytorch](http://pytorch.org/) as a main deep learning engine,\nand also follows [Kaldi](http://kaldi-asr.org/) style data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments.\n\n## Key Features\n\n### Kaldi style complete recipe\n- Support numbers of `ASR` recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, etc.)\n- Support numbers of `TTS` recipes with a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)\n- Support numbers of `ST` recipes (Fisher-CallHome Spanish, Libri-trans, IWSLT'18, How2, Must-C, Mboshi-French, etc.)\n- Support numbers of `MT` recipes (IWSLT'16, the above ST recipes etc.)\n- Support speech separation and recognition recipe (WSJ-2mix)\n- Support voice conversion recipe (VCC2020 baseline) (new!)\n\n\n### ASR: Automatic Speech Recognition\n\n- **State-of-the-art performance** in several ASR benchmarks (comparable/superior to hybrid DNN/HMM and CTC)\n- **Hybrid CTC/attention** based end-to-end ASR\n  - Fast/accurate training with CTC/attention multitask training\n  - CTC/attention joint decoding to boost monotonic alignment decoding\n  - Encoder: VGG-like CNN + BiRNN (LSTM/GRU), sub-sampling BiRNN (LSTM/GRU) or Transformer\n- Attention: Dot product, location-aware attention, variants of multihead\n- Incorporate RNNLM/LSTMLM/TransformerLM trained only with text data\n- Batch GPU decoding\n- **Transducer** based end-to-end ASR\n  - Available: RNN-Transducer, Transformer-Transducer, Transformer/RNN-Transducer\n  - Support attention extension and VGG-Transformer (encoder)\n\n### TTS: Text-to-speech\n- Tacotron2 based end-to-end TTS\n- Transformer based end-to-end TTS\n- Feed-forward Transformer (a.k.a. FastSpeech) based end-to-end TTS (new!)\n\n### ST: Speech Translation & MT: Machine Translation\n- **State-of-the-art performance** in several ST benchmarks (comparable/superior to cascaded ASR and MT)\n- Transformer based end-to-end ST (new!)\n- Transformer based end-to-end MT (new!)\n\n### VC: Voice conversion\n- End-to-end VC based on cascaded ASR+TTS (new!)\n- Baseline system for Voice Conversion Challenge 2020!\n\n### DNN Framework\n- Flexible network architecture thanks to chainer and pytorch\n- Flexible front-end processing thanks to [kaldiio](https://github.com/nttcslab-sp/kaldiio) and HDF5 support\n- Tensorboard based monitoring\n\n## Installation\nSee https://espnet.github.io/espnet/installation.html\n\n## Usage\nSee https://espnet.github.io/espnet/tutorial.html\n\n## Docker Container\n\ngo to [docker/](docker/) and follow [instructions](https://espnet.github.io/espnet/docker.html).\n\n## Contribution\nThank you for taking times for ESPnet! Any contributions to ESPNet are welcome and feel free to ask any questions or requests to [issues](https://github.com/espnet/espnet/issues).\nIf it's the first contribution to ESPnet for you,  please follow the [contribution guide](CONTRIBUTING.md).\n\n## Results and demo\n\nYou can find useful tutorials and demos in [Interspeech 2019 Tutorial](https://github.com/espnet/interspeech2019-tutorial)\n\n### ASR results\n\nWe list the character error rate (CER) and word error rate (WER) of major ASR tasks.\n\n| Task                   | CER (%) | WER (%) | Pretrained model                                                                                                                                                      |\n| -----------            | :----:  | :----:  | :----:                                                                                                                                                                |\n| Aishell dev            | 6.0     | N/A     | [link](https://github.com/espnet/espnet/blob/master/egs/aishell/asr1/RESULTS.md#transformer-result-default-transformer-with-initial-learning-rate--10-and-epochs--50) |\n| Aishell test           | 6.7     | N/A     | same as above                                                                                                                                                         |\n| Common Voice dev       | 1.7     | 2.2     | [link](https://github.com/espnet/espnet/blob/master/egs/commonvoice/asr1/RESULTS.md#first-results-default-pytorch-transformer-setting-with-bpe-100-epochs-single-gpu) |\n| Common Voice test      | 1.8     | 2.3     | same as above                                                                                                                                                         |\n| CSJ eval1              | 5.7     | N/A     | [link](https://github.com/espnet/espnet/blob/master/egs/csj/asr1/RESULTS.md#pytorch-backend-transformer-without-any-hyperparameter-tuning)                            |\n| CSJ eval2              | 3.8     | N/A     | same as above                                                                                                                                                         |\n| CSJ eval3              | 4.2     | N/A     | same as above                                                                                                                                                         |\n| HKUST dev              | 23.5    | N/A     | [link](https://github.com/espnet/espnet/blob/master/egs/hkust/asr1/RESULTS.md#transformer-only-20-epochs)                                                             |\n| Librispeech dev_clean  | N/A     | 2.1     | [link](https://github.com/espnet/espnet/blob/master/egs/librispeech/asr1/RESULTS.md#pytorch-large-transformer-with-specaug-4-gpus--transformer-lm-4-gpus)             |\n| Librispeech dev_other  | N/A     | 5.3     | same as above                                                                                                                                                         |\n| Librispeech test_clean | N/A     | 2.5     | same as above                                                                                                                                                         |\n| Librispeech test_other | N/A     | 5.5     | same as above                                                                                                                                                         |\n| TEDLIUM2 dev           | N/A     | 9.3     | [link](https://github.com/espnet/espnet/blob/master/egs/tedlium2/asr1/RESULTS.md#transformer-large-model--specaug--large-lm)                                          |\n| TEDLIUM2 test          | N/A     | 8.1     | same as above                                                                                                                                                         |\n| TEDLIUM3 dev           | N/A     | 9.7     | [link](https://github.com/espnet/espnet/blob/master/egs/tedlium3/asr1/RESULTS.md#transformer-elayers12-dlayers6-units2048-8-gpus-specaug--large-lm)                   |\n| TEDLIUM3 test          | N/A     | 8.0     | same as above                                                                                                                                                         |\n| WSJ dev93              | 3.2     | 7.0     | N/A                                                                                                                                                                   |\n| WSJ eval92             | 2.1     | 4.7     | N/A                                                                                                                                                                   |\n\nNote that the performance of the CSJ, HKUST, and Librispeech tasks was significantly improved by using the wide network (#units = 1024) and large subword units if necessary reported by [RWTH](https://arxiv.org/pdf/1805.03294.pdf).\n\nIf you want to check the results of the other recipes, please check `egs/<name_of_recipe>/asr1/RESULTS.md`.\n\n### ASR demo\n\nYou can recognize speech in a WAV file using pretrained models.\nGo to a recipe directory and run `utils/recog_wav.sh` as follows:\n```sh\ncd egs/tedlium2/asr1\n../../../utils/recog_wav.sh --models tedlium2.transformer.v1 example.wav\n```\nwhere `example.wav` is a WAV file to be recognized.\nThe sampling rate must be consistent with that of data used in training.\n\nAvailable pretrained models in the demo script are listed as below.\n\n| Model                                                                                            | Notes                                                      |\n| :------                                                                                          | :------                                                    |\n| [tedlium2.rnn.v1](https://drive.google.com/open?id=1UqIY6WJMZ4sxNxSugUqp3mrGb3j6h7xe)            | Streaming decoding based on CTC-based VAD                  |\n| [tedlium2.rnn.v2](https://drive.google.com/open?id=1cac5Uc09lJrCYfWkLQsF8eapQcxZnYdf)            | Streaming decoding based on CTC-based VAD (batch decoding) |\n| [tedlium2.transformer.v1](https://drive.google.com/open?id=1mgbiWabOSkh_oHJIDA-h7hekQ3W95Z_U)    | Joint-CTC attention Transformer trained on Tedlium 2       |\n| [tedlium3.transformer.v1](https://drive.google.com/open?id=1wYYTwgvbB7uy6agHywhQfnuVWWW_obmO)    | Joint-CTC attention Transformer trained on Tedlium 3       |\n| [librispeech.transformer.v1](https://drive.google.com/open?id=1BtQvAnsFvVi-dp_qsaFP7n4A_5cwnlR6) | Joint-CTC attention Transformer trained on Librispeech     |\n| [commonvoice.transformer.v1](https://drive.google.com/open?id=1tWccl6aYU67kbtkm8jv5H6xayqg1rzjh) | Joint-CTC attention Transformer trained on CommonVoice     |\n| [csj.transformer.v1](https://drive.google.com/open?id=120nUQcSsKeY5dpyMWw_kI33ooMRGT2uF)         | Joint-CTC attention Transformer trained on CSJ             |\n\n\n### ST results\n\nWe list 4-gram BLEU of major ST tasks.\n\n#### end-to-end system\n| Task | BLEU | Pretrained model |\n| ---- | :----: | :----: |\n| Fisher-CallHome Spanish fisher_test (Es->En)      | 48.39 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |\n| Fisher-CallHome Spanish callhome_evltest (Es->En) | 18.67 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |\n| Libri-trans test (En->Fr)                         | 16.70 | [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/st1/RESULTS.md#train_spfr_lc_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans-1) |\n| How2 dev5 (En->Pt)                                | 45.68 | [link](https://github.com/espnet/espnet/blob/master/egs/how2/st1/RESULTS.md#trainpt_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans-1) |\n| Must-C tst-COMMON (En->De)                        | 22.91 | [link](https://github.com/espnet/espnet/blob/master/egs/must_c/st1/RESULTS.md#train_spen-dede_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans) |\n| Mboshi-French dev (Fr->Mboshi)                    | 6.18  | N/A  |\n\n#### cascaded system\n| Task | BLEU | Pretrained model |\n| ---- | :----: | :----: |\n| Fisher-CallHome Spanish fisher_test (Es->En)      | 42.16 | N/A  |\n| Fisher-CallHome Spanish callhome_evltest (Es->En) | 19.82 | N/A  |\n| Libri-trans test (En->Fr)                         | 16.96 | N/A  |\n| How2 dev5 (En->Pt)                                | 44.90 | N/A  |\n| Must-C tst-COMMON (En->De)                        | 23.65 | N/A  |\n\nIf you want to check the results of the other recipes, please check `egs/<name_of_recipe>/st1/RESULTS.md`.\n\n### ST demo\n\n(**New!**) We made a new real-time E2E-ST + TTS demonstration in Google Colab.\nPlease access the notebook from the following button and enjoy the real-time speech-to-speech translation!\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb)\n\n---\n\nYou can translate speech in a WAV file using pretrained models.\nGo to a recipe directory and run `utils/translate_wav.sh` as follows:\n```sh\ncd egs/fisher_callhome_spanish/st1/\nwget -O - https://github.com/espnet/espnet/files/4100928/test.wav.tar.gz | tar zxvf - ../../../utils/translate_wav.sh --models fisher_callhome_spanish.transformer.v1.es-en test.wav\n```\nwhere `test.wav` is a WAV file to be translated.\nThe sampling rate must be consistent with that of data used in training.\n\nAvailable pretrained models in the demo script are listed as below.\n\n| Model                                                                                            | Notes                                                      |\n| :------                                                                                          | :------                                                    |\n| [fisher_callhome_spanish.transformer.v1](https://drive.google.com/open?id=1hawp5ZLw4_SIHIT3edglxbKIIkPVe8n3)            | Transformer-ST trained on Fisher-CallHome Spanish Es->En                  |\n\n\n### MT results\n\n| Task | BLEU | Pretrained model |\n| ---- | :----: | :----: |\n| Fisher-CallHome Spanish fisher_test (Es->En)      | 61.45 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |\n| Fisher-CallHome Spanish callhome_evltest (Es->En) | 29.86 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |\n| Libri-trans test (En->Fr)                         | 18.09 | [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/mt1/RESULTS.md#trainfr_lcrm_tc_pytorch_train_pytorch_transformer_bpe1000) |\n| How2 dev5 (En->Pt)                                | 58.61 | [link](https://github.com/espnet/espnet/blob/master/egs/how2/mt1/RESULTS.md#trainpt_tc_tc_pytorch_train_pytorch_transformer_bpe8000) |\n| Must-C tst-COMMON (En->De)                        | 27.63 | [link](https://github.com/espnet/espnet/blob/master/egs/must_c/mt1/RESULTS.md#summary-4-gram-bleu) |\n| IWSLT'14 test2014 (En->De)                        | 24.70 | [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result) |\n| IWSLT'14 test2014 (De->En)                        | 29.22 | [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result) |\n| IWSLT'16 test2014 (En->De)                        | 24.05 | [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result) |\n| IWSLT'16 test2014 (De->En)                        | 29.13 | [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result) |\n\n\n### TTS results\n\nYou can listen to our samples in demo HP [espnet-tts-sample](https://espnet.github.io/espnet-tts-sample/).\nHere we list some notable ones:\n\n- [Single English speaker Tacotron2](https://drive.google.com/open?id=18JgsOCWiP_JkhONasTplnHS7yaF_konr)\n- [Single Japanese speaker Tacotron2](https://drive.google.com/open?id=1fEgS4-K4dtgVxwI4Pr7uOA1h4PE-zN7f)\n- [Single other language speaker Tacotron2](https://drive.google.com/open?id=1q_66kyxVZGU99g8Xb5a0Q8yZ1YVm2tN0)\n- [Multi English speaker Tacotron2](https://drive.google.com/open?id=18S_B8Ogogij34rIfJOeNF8D--uG7amz2)\n- [Single English speaker Transformer](https://drive.google.com/open?id=14EboYVsMVcAq__dFP1p6lyoZtdobIL1X)\n- [Single English speaker FastSpeech](https://drive.google.com/open?id=1PSxs1VauIndwi8d5hJmZlppGRVu2zuy5)\n- [Multi English speaker Transformer](https://drive.google.com/open?id=1_vrdqjM43DdN1Qz7HJkvMQ6lCMmWLeGp)\n- [Single Italian speaker FastSpeech](https://drive.google.com/open?id=13I5V2w7deYFX4DlVk1-0JfaXmUR2rNOv)\n- [Single Mandarin speaker Transformer](https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD)\n- [Single Mandarin speaker FastSpeech](https://drive.google.com/open?id=1Ol_048Tuy6BgvYm1RpjhOX4HfhUeBqdK)\n- [Multi Japanese speaker Transformer](https://drive.google.com/open?id=1fFMQDF6NV5Ysz48QLFYE8fEvbAxCsMBw)\n- [Single English speaker models with Parallel WaveGAN](https://drive.google.com/open?id=1HvB0_LDf1PVinJdehiuCt5gWmXGguqtx)\n- [Single English speaker knowledge distillation-based FastSpeech (New!)](https://drive.google.com/open?id=1wG-Y0itVYalxuLAHdkAHO7w1CWFfRPF4)\n\nYou can download all of the pretrained models and generated samples:\n- [All of the pretrained E2E-TTS models](https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF)\n- [All of the generated samples](https://drive.google.com/open?id=1bQGuqH92xuxOX__reWLP4-cif0cbpMLX)\n\nNote that in the generated samples we use three vocoders: Griffin-Lim (**GL**), WaveNet vocoder (**WaveNet**), Parallel WaveGAN (**ParallelWaveGAN**), and MelGAN (**MelGAN**).\nThe neural vocoders are based on following repositories.\n- [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN): Parallel WaveGAN / MelGAN\n- [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder): 16 bit mixture of Logistics WaveNet vocoder\n- [kan-bayashi/PytorchWaveNetVocoder](https://github.com/kan-bayashi/PytorchWaveNetVocoder): 8 bit Softmax WaveNet Vocoder with the noise shaping\n\nIf you want to build your own neural vocoder, please check the above repositories.\n\nHere we list all of the pretrained neural vocoders. Please download and enjoy the generation of high quality speech!\n\n| Model link                                                                                              | Lang  | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Model type                                                              |\n| :------                                                                                                 | :---: | :----:  | :--------:     | :---------------:      | :------                                                                 |\n| [ljspeech.wavenet.softmax.ns.v1](https://drive.google.com/open?id=1eA1VcRS9jzFa-DovyTgJLQ_jmwOLIi8L)    | EN    | 22.05k  | None           | 1024 / 256 / None      | [Softmax WaveNet](https://github.com/kan-bayashi/PytorchWaveNetVocoder) |\n| [ljspeech.wavenet.mol.v1](https://drive.google.com/open?id=1sY7gEUg39QaO1szuN62-Llst9TrFno2t)           | EN    | 22.05k  | None           | 1024 / 256 / None      | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [ljspeech.parallel_wavegan.v1](https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7)      | EN    | 22.05k  | None           | 1024 / 256 / None      | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n| [ljspeech.wavenet.mol.v2](https://drive.google.com/open?id=1es2HuKUeKVtEdq6YDtAsLNpqCy4fhIXr)           | EN    | 22.05k  | 80-7600        | 1024 / 256 / None      | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [ljspeech.parallel_wavegan.v2](https://drive.google.com/open?id=1Grn7X9wD35UcDJ5F7chwdTqTa4U7DeVB)      | EN    | 22.05k  | 80-7600        | 1024 / 256 / None      | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n| [ljspeech.melgan.v1 (EXPERIMENTAL)](https://drive.google.com/open?id=1ipPWYl8FBNRlBFaKj1-i23eQpW_W_YcR) | EN    | 22.05k  | 80-7600        | 1024 / 256 / None      | [MelGAN](https://github.com/kan-bayashi/ParallelWaveGAN)                |\n| [ljspeech.melgan.v3 (EXPERIMENTAL)](https://drive.google.com/open?id=1_a8faVA5OGCzIcJNw4blQYjfG4oA9VEt) | EN    | 22.05k  | 80-7600        | 1024 / 256 / None      | [MelGAN](https://github.com/kan-bayashi/ParallelWaveGAN)                |\n| [libritts.wavenet.mol.v1](https://drive.google.com/open?id=1jHUUmQFjWiQGyDd7ZeiCThSjjpbF_B4h)           | EN    | 24k     | None           | 1024 / 256 / None      | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [jsut.wavenet.mol.v1](https://drive.google.com/open?id=187xvyNbmJVZ0EZ1XHCdyjZHTXK9EcfkK)               | JP    | 24k     | 80-7600        | 2048 / 300 / 1200      | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [jsut.parallel_wavegan.v1](https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM)          | JP    | 24k     | 80-7600        | 2048 / 300 / 1200      | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n| [csmsc.wavenet.mol.v1](https://drive.google.com/open?id=1PsjFRV5eUP0HHwBaRYya9smKy5ghXKzj)              | ZH    | 24k     | 80-7600        | 2048 / 300 / 1200      | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [csmsc.parallel_wavegan.v1](https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy)         | ZH    | 24k     | 80-7600        | 2048 / 300 / 1200      | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n\nIf you want to use the above pretrained vocoders, please exactly match the feature setting with them.\n\n\n### TTS demo\n\n(**New!**) We made a new real-time E2E-TTS demonstration in Google Colab.\nPlease access the notebook from the following button and enjoy the real-time synthesis!\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)\n\n---\n\nYou can synthesize speech in a TXT file using pretrained models.\nGo to a recipe directory and run `utils/synth_wav.sh` as follows:\n\n```sh\ncd egs/ljspeech/tts1\necho \"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\" > example.txt\n../../../utils/synth_wav.sh example.txt\n```\n\nYou can change the pretrained model as follows:\n\n```sh\n../../../utils/synth_wav.sh --models ljspeech.fastspeech.v1 example.txt\n```\n\nWaveform synthesis is performed with Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN).\nYou can change the pretrained vocoder model as follows:\n\n```\n../../../utils/synth_wav.sh --vocoder_models ljspeech.wavenet.mol.v1 example.txt\n```\n\nNote that WaveNet vocoder provides very high quality speech but it takes time to generate.\n\nAvailable pretrained models in the demo script are listed as follows:\n\n| Model link                                                                                    | Lang  | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Input  | R   | Model type                                  |\n| :------                                                                                       | :---: | :----:  | :--------:     | :---------------:      | :---:  | :-: | :------                                     |\n| [ljspeech.tacotron2.v1](https://drive.google.com/open?id=1dKzdaDpOkpx7kWZnvrvx2De7eZEdPHZs)   | EN    | 22.05k  | None           | 1024 / 256 / None      | char   | 2   | Tacotron 2                                  |\n| [ljspeech.tacotron2.v2](https://drive.google.com/open?id=11T9qw8rJlYzUdXvFjkjQjYrp3iGfQ15h)   | EN    | 22.05k  | None           | 1024 / 256 / None      | char   | 1   | Tacotron 2 + forward attention              |\n| [ljspeech.tacotron2.v3](https://drive.google.com/open?id=1hiZn14ITUDM1nkn-GkaN_M3oaTOUcn1n)   | EN    | 22.05k  | None           | 1024 / 256 / None      | char   | 1   | Tacotron 2 + guided attention loss          |\n| [ljspeech.transformer.v1](https://drive.google.com/open?id=13DR-RB5wrbMqBGx_MC655VZlsEq52DyS) | EN    | 22.05k  | None           | 1024 / 256 / None      | char   | 1   | Deep Transformer                            |\n| [ljspeech.transformer.v2](https://drive.google.com/open?id=1xxAwPuUph23RnlC5gym7qDM02ZCW9Unp) | EN    | 22.05k  | None           | 1024 / 256 / None      | char   | 3   | Shallow Transformer                         |\n| [ljspeech.transformer.v3](https://drive.google.com/open?id=1M_w7nxI6AfbtSHpMO-exILnAc_aUYvXP) | EN    | 22.05k  | None           | 1024 / 256 / None      | phn    | 1   | Deep Transformer                            |\n| [ljspeech.fastspeech.v1](https://drive.google.com/open?id=17RUNFLP4SSTbGA01xWRJo7RkR876xM0i)  | EN    | 22.05k  | None           | 1024 / 256 / None      | char   | 1   | FF-Transformer                              |\n| [ljspeech.fastspeech.v2](https://drive.google.com/open?id=1zD-2GMrWM3thaDpS3h3rkTU4jIC0wc5B)  | EN    | 22.05k  | None           | 1024 / 256 / None      | char   | 1   | FF-Transformer + CNN in FFT block           |\n| [ljspeech.fastspeech.v3](https://drive.google.com/open?id=1W86YEQ6KbuUTIvVURLqKtSNqe_eI2GDN)  | EN    | 22.05k  | None           | 1024 / 256 / None      | phn    | 1   | FF-Transformer + CNN in FFT block + postnet |\n| [libritts.tacotron2.v1](https://drive.google.com/open?id=1iAXwC0AuWusa9AcFeUVkcNLG0I-hnSr3)   | EN    | 24k     | 80-7600        | 1024 / 256 / None      | char   | 2   | Multi-speaker Tacotron 2                    |\n| [libritts.transformer.v1](https://drive.google.com/open?id=1Xj73mDPuuPH8GsyNO8GnOC3mn0_OK4g3) | EN    | 24k     | 80-7600        | 1024 / 256 / None      | char   | 2   | Multi-speaker Transformer                   |\n| [jsut.tacotron2](https://drive.google.com/open?id=1kp5M4VvmagDmYckFJa78WGqh1drb_P9t)          | JP    | 24k     | 80-7600        | 2048 / 300 / 1200      | phn    | 2   | Tacotron 2                                  |\n| [jsut.transformer](https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD)        | JP    | 24k     | 80-7600        | 2048 / 300 / 1200      | phn    | 3   | Shallow Transformer                         |\n| [csmsc.transformer.v1](https://drive.google.com/open?id=1bTSygvonv5TS6-iuYsOIUWpN2atGnyhZ)    | ZH    | 24k     | 80-7600        | 2048 / 300 / 1200      | pinyin | 1   | Deep Transformer                            |\n| [csmsc.fastspeech.v3](https://drive.google.com/open?id=1T8thxkAxjGFPXPWPTcKLvHnd6lG0-82R)     | ZH    | 24k     | 80-7600        | 2048 / 300 / 1200      | pinyin | 1   | FF-Transformer + CNN in FFT block + postnet |\n\nAvailable pretrained vocoder models in the demo script are listed as follows:\n\n| Model link                                                                                           | Lang  | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Model type                                                              |\n| :------                                                                                              | :---: | :----:  | :--------:     | :---------------:      | :------                                                                 |\n| [ljspeech.wavenet.softmax.ns.v1](https://drive.google.com/open?id=1eA1VcRS9jzFa-DovyTgJLQ_jmwOLIi8L) | EN    | 22.05k  | None           | 1024 / 256 / None      | [Softmax WaveNet](https://github.com/kan-bayashi/PytorchWaveNetVocoder) |\n| [ljspeech.wavenet.mol.v1](https://drive.google.com/open?id=1sY7gEUg39QaO1szuN62-Llst9TrFno2t)        | EN    | 22.05k  | None           | 1024 / 256 / None      | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [ljspeech.parallel_wavegan.v1](https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7)   | EN    | 22.05k  | None           | 1024 / 256 / None      | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n| [libritts.wavenet.mol.v1](https://drive.google.com/open?id=1jHUUmQFjWiQGyDd7ZeiCThSjjpbF_B4h)        | EN    | 24k     | None           | 1024 / 256 / None      | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [jsut.wavenet.mol.v1](https://drive.google.com/open?id=187xvyNbmJVZ0EZ1XHCdyjZHTXK9EcfkK)            | JP    | 24k     | 80-7600        | 2048 / 300 / 1200      | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [jsut.parallel_wavegan.v1](https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM)       | JP    | 24k     | 80-7600        | 2048 / 300 / 1200      | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n| [csmsc.wavenet.mol.v1](https://drive.google.com/open?id=1PsjFRV5eUP0HHwBaRYya9smKy5ghXKzj)           | ZH    | 24k     | 80-7600        | 2048 / 300 / 1200      | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [csmsc.parallel_wavegan.v1](https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy)      | ZH    | 24k     | 80-7600        | 2048 / 300 / 1200      | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n\n### VC results\n\nThe [Voice Conversion Challenge 2020](http://www.vc-challenge.org/) (VCC2020) adopts ESPnet to build an end-to-end based baseline system. In VCC2020, the objective is intra/cross lingual nonparallel VC. A cascade method of ASR+TTS is developed.  \nYou can download converted samples [here](https://drive.google.com/drive/folders/1oeZo83GrOgtqxGwF7KagzIrfjr8X59Ue?usp=sharing).\n\n## References\n\n[1] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai, \"ESPnet: End-to-End Speech Processing Toolkit,\" *Proc. Interspeech'18*, pp. 2207-2211 (2018)\n\n[2] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, \"Joint CTC-attention based end-to-end speech recognition using multi-task learning,\" *Proc. ICASSP'17*, pp. 4835--4839 (2017)\n\n[3] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey and Tomoki Hayashi, \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition,\" *IEEE Journal of Selected Topics in Signal Processing*, vol. 11, no. 8, pp. 1240-1253, Dec. 2017\n\n## Citations\n\n```\n@inproceedings{watanabe2018espnet,\n  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},\n  title={ESPnet: End-to-End Speech Processing Toolkit},\n  year=2018,\n  booktitle={Interspeech},\n  pages={2207--2211},\n  doi={10.21437/Interspeech.2018-1456},\n  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}\n}\n@misc{hayashi2019espnettts,\n    title={ESPnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit},\n    author={Tomoki Hayashi and Ryuichi Yamamoto and Katsuki Inoue and Takenori Yoshimura and Shinji Watanabe and Tomoki Toda and Kazuya Takeda and Yu Zhang and Xu Tan},\n    year={2019},\n    eprint={1910.10909},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/espnet/espnet", "keywords": "", "license": "Apache Software License", "maintainer": "", "maintainer_email": "", "name": "espnet", "package_url": "https://pypi.org/project/espnet/", "platform": "", "project_url": "https://pypi.org/project/espnet/", "project_urls": {"Homepage": "http://github.com/espnet/espnet"}, "release_url": "https://pypi.org/project/espnet/0.6.3/", "requires_dist": ["chainer (==6.0.0)", "setuptools (>=38.5.1)", "scipy (<=1.3.3)", "librosa (>=0.7.0)", "soundfile (>=0.10.2)", "inflect (>=1.0.0)", "unidecode (>=1.0.22)", "editdistance (==0.5.2)", "h5py (==2.9.0)", "tensorboardX (>=1.8)", "pillow (>=6.1.0)", "nara-wpe (>=0.0.5)", "museval (>=0.2.1)", "pystoi (>=0.2.2)", "kaldiio (>=2.13.8)", "matplotlib (>=3.1.0)", "funcsigs (>=1.0.2)", "configargparse (==1.1)", "PyYAML (>=5.1.2)", "sentencepiece (>=0.1.82)", "pysptk (>=0.1.17)", "nltk (>=3.4.5)", "nnmnkwii", "jaconv", "g2p-en", "torch-complex", "pytorch-wpe", "Sphinx (==2.1.2) ; extra == 'doc'", "sphinx-rtd-theme (>=0.2.4) ; extra == 'doc'", "sphinx-argparse (>=0.2.5) ; extra == 'doc'", "commonmark (==0.8.1) ; extra == 'doc'", "recommonmark (>=0.4.0) ; extra == 'doc'", "travis-sphinx (>=2.0.1) ; extra == 'doc'", "nbsphinx (>=0.4.2) ; extra == 'doc'", "sphinx-markdown-tables (>=0.0.12) ; extra == 'doc'", "pytest (>=3.3.0) ; extra == 'test'", "pytest-pythonpath (>=0.7.3) ; extra == 'test'", "pytest-cov (>=2.7.1) ; extra == 'test'", "hacking (>=1.1.0) ; extra == 'test'", "mock (>=2.0.0) ; extra == 'test'", "autopep8 (>=1.3.3) ; extra == 'test'", "jsondiff (>=1.2.0) ; extra == 'test'", "flake8 (>=3.7.8) ; extra == 'test'", "flake8-docstrings (>=1.3.1) ; extra == 'test'"], "requires_python": "", "summary": "ESPnet: end-to-end speech processing toolkit", "version": "0.6.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/eedd7e915a8573422dbd7eb3283e4a249b1170f8/646f632f696d6167652f6573706e65745f6c6f676f312e706e67\" width=\"550\"></div>\n<h1>ESPnet: end-to-end speech processing toolkit</h1>\n<p><a href=\"https://github.com/espnet/espnet/actions\" rel=\"nofollow\"><img alt=\"Github Actions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0a391f1cd2330813ab9de5b27f0de0abc60760ce/68747470733a2f2f6769746875622e636f6d2f6573706e65742f6573706e65742f776f726b666c6f77732f43492f62616467652e737667\"></a>\n<a href=\"https://travis-ci.org/espnet/espnet\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3472aa13bb086a45e80664794cb536cee2520782/68747470733a2f2f7472617669732d63692e6f72672f6573706e65742f6573706e65742e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://circleci.com/gh/espnet/espnet\" rel=\"nofollow\"><img alt=\"CircleCI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bc1eaed5bdbed725c344e5131f31951ccdab5e86/68747470733a2f2f636972636c6563692e636f6d2f67682f6573706e65742f6573706e65742e7376673f7374796c653d737667\"></a>\n<a href=\"https://codecov.io/gh/espnet/espnet\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0f36132cf72d2ce2fef7f50aa4ea7fd6c533d549/68747470733a2f2f636f6465636f762e696f2f67682f6573706e65742f6573706e65742f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<a href=\"https://gitter.im/espnet-en/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge\" rel=\"nofollow\"><img alt=\"Gitter\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9ad717e1e0c225f9aa962b1f5b036edae1688d15/68747470733a2f2f6261646765732e6769747465722e696d2f6573706e65742d656e2f636f6d6d756e6974792e737667\"></a></p>\n<p><a href=\"https://espnet.github.io/espnet/\" rel=\"nofollow\"><strong>Docs</strong></a>\n| <a href=\"https://github.com/espnet/espnet/tree/master/egs\" rel=\"nofollow\"><strong>Example</strong></a>\n| <a href=\"https://github.com/espnet/espnet/tree/master/docker\" rel=\"nofollow\"><strong>Docker</strong></a>\n| <a href=\"https://github.com/espnet/notebook\" rel=\"nofollow\"><strong>Notebook</strong></a>\n| <a href=\"https://github.com/espnet/interspeech2019-tutorial\" rel=\"nofollow\"><strong>Tutorial (2019)</strong></a></p>\n<p>ESPnet is an end-to-end speech processing toolkit, mainly focuses on end-to-end speech recognition and end-to-end text-to-speech.\nESPnet uses <a href=\"https://chainer.org/\" rel=\"nofollow\">chainer</a> and <a href=\"http://pytorch.org/\" rel=\"nofollow\">pytorch</a> as a main deep learning engine,\nand also follows <a href=\"http://kaldi-asr.org/\" rel=\"nofollow\">Kaldi</a> style data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments.</p>\n<h2>Key Features</h2>\n<h3>Kaldi style complete recipe</h3>\n<ul>\n<li>Support numbers of <code>ASR</code> recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, etc.)</li>\n<li>Support numbers of <code>TTS</code> recipes with a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)</li>\n<li>Support numbers of <code>ST</code> recipes (Fisher-CallHome Spanish, Libri-trans, IWSLT'18, How2, Must-C, Mboshi-French, etc.)</li>\n<li>Support numbers of <code>MT</code> recipes (IWSLT'16, the above ST recipes etc.)</li>\n<li>Support speech separation and recognition recipe (WSJ-2mix)</li>\n<li>Support voice conversion recipe (VCC2020 baseline) (new!)</li>\n</ul>\n<h3>ASR: Automatic Speech Recognition</h3>\n<ul>\n<li><strong>State-of-the-art performance</strong> in several ASR benchmarks (comparable/superior to hybrid DNN/HMM and CTC)</li>\n<li><strong>Hybrid CTC/attention</strong> based end-to-end ASR\n<ul>\n<li>Fast/accurate training with CTC/attention multitask training</li>\n<li>CTC/attention joint decoding to boost monotonic alignment decoding</li>\n<li>Encoder: VGG-like CNN + BiRNN (LSTM/GRU), sub-sampling BiRNN (LSTM/GRU) or Transformer</li>\n</ul>\n</li>\n<li>Attention: Dot product, location-aware attention, variants of multihead</li>\n<li>Incorporate RNNLM/LSTMLM/TransformerLM trained only with text data</li>\n<li>Batch GPU decoding</li>\n<li><strong>Transducer</strong> based end-to-end ASR\n<ul>\n<li>Available: RNN-Transducer, Transformer-Transducer, Transformer/RNN-Transducer</li>\n<li>Support attention extension and VGG-Transformer (encoder)</li>\n</ul>\n</li>\n</ul>\n<h3>TTS: Text-to-speech</h3>\n<ul>\n<li>Tacotron2 based end-to-end TTS</li>\n<li>Transformer based end-to-end TTS</li>\n<li>Feed-forward Transformer (a.k.a. FastSpeech) based end-to-end TTS (new!)</li>\n</ul>\n<h3>ST: Speech Translation &amp; MT: Machine Translation</h3>\n<ul>\n<li><strong>State-of-the-art performance</strong> in several ST benchmarks (comparable/superior to cascaded ASR and MT)</li>\n<li>Transformer based end-to-end ST (new!)</li>\n<li>Transformer based end-to-end MT (new!)</li>\n</ul>\n<h3>VC: Voice conversion</h3>\n<ul>\n<li>End-to-end VC based on cascaded ASR+TTS (new!)</li>\n<li>Baseline system for Voice Conversion Challenge 2020!</li>\n</ul>\n<h3>DNN Framework</h3>\n<ul>\n<li>Flexible network architecture thanks to chainer and pytorch</li>\n<li>Flexible front-end processing thanks to <a href=\"https://github.com/nttcslab-sp/kaldiio\" rel=\"nofollow\">kaldiio</a> and HDF5 support</li>\n<li>Tensorboard based monitoring</li>\n</ul>\n<h2>Installation</h2>\n<p>See <a href=\"https://espnet.github.io/espnet/installation.html\" rel=\"nofollow\">https://espnet.github.io/espnet/installation.html</a></p>\n<h2>Usage</h2>\n<p>See <a href=\"https://espnet.github.io/espnet/tutorial.html\" rel=\"nofollow\">https://espnet.github.io/espnet/tutorial.html</a></p>\n<h2>Docker Container</h2>\n<p>go to <a href=\"docker/\" rel=\"nofollow\">docker/</a> and follow <a href=\"https://espnet.github.io/espnet/docker.html\" rel=\"nofollow\">instructions</a>.</p>\n<h2>Contribution</h2>\n<p>Thank you for taking times for ESPnet! Any contributions to ESPNet are welcome and feel free to ask any questions or requests to <a href=\"https://github.com/espnet/espnet/issues\" rel=\"nofollow\">issues</a>.\nIf it's the first contribution to ESPnet for you,  please follow the <a href=\"CONTRIBUTING.md\" rel=\"nofollow\">contribution guide</a>.</p>\n<h2>Results and demo</h2>\n<p>You can find useful tutorials and demos in <a href=\"https://github.com/espnet/interspeech2019-tutorial\" rel=\"nofollow\">Interspeech 2019 Tutorial</a></p>\n<h3>ASR results</h3>\n<p>We list the character error rate (CER) and word error rate (WER) of major ASR tasks.</p>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th align=\"center\">CER (%)</th>\n<th align=\"center\">WER (%)</th>\n<th align=\"center\">Pretrained model</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Aishell dev</td>\n<td align=\"center\">6.0</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/aishell/asr1/RESULTS.md#transformer-result-default-transformer-with-initial-learning-rate--10-and-epochs--50\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Aishell test</td>\n<td align=\"center\">6.7</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">same as above</td>\n</tr>\n<tr>\n<td>Common Voice dev</td>\n<td align=\"center\">1.7</td>\n<td align=\"center\">2.2</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/commonvoice/asr1/RESULTS.md#first-results-default-pytorch-transformer-setting-with-bpe-100-epochs-single-gpu\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Common Voice test</td>\n<td align=\"center\">1.8</td>\n<td align=\"center\">2.3</td>\n<td align=\"center\">same as above</td>\n</tr>\n<tr>\n<td>CSJ eval1</td>\n<td align=\"center\">5.7</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/csj/asr1/RESULTS.md#pytorch-backend-transformer-without-any-hyperparameter-tuning\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>CSJ eval2</td>\n<td align=\"center\">3.8</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">same as above</td>\n</tr>\n<tr>\n<td>CSJ eval3</td>\n<td align=\"center\">4.2</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">same as above</td>\n</tr>\n<tr>\n<td>HKUST dev</td>\n<td align=\"center\">23.5</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/hkust/asr1/RESULTS.md#transformer-only-20-epochs\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Librispeech dev_clean</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">2.1</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/librispeech/asr1/RESULTS.md#pytorch-large-transformer-with-specaug-4-gpus--transformer-lm-4-gpus\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Librispeech dev_other</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">5.3</td>\n<td align=\"center\">same as above</td>\n</tr>\n<tr>\n<td>Librispeech test_clean</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">2.5</td>\n<td align=\"center\">same as above</td>\n</tr>\n<tr>\n<td>Librispeech test_other</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">5.5</td>\n<td align=\"center\">same as above</td>\n</tr>\n<tr>\n<td>TEDLIUM2 dev</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">9.3</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/tedlium2/asr1/RESULTS.md#transformer-large-model--specaug--large-lm\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>TEDLIUM2 test</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">8.1</td>\n<td align=\"center\">same as above</td>\n</tr>\n<tr>\n<td>TEDLIUM3 dev</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">9.7</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/tedlium3/asr1/RESULTS.md#transformer-elayers12-dlayers6-units2048-8-gpus-specaug--large-lm\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>TEDLIUM3 test</td>\n<td align=\"center\">N/A</td>\n<td align=\"center\">8.0</td>\n<td align=\"center\">same as above</td>\n</tr>\n<tr>\n<td>WSJ dev93</td>\n<td align=\"center\">3.2</td>\n<td align=\"center\">7.0</td>\n<td align=\"center\">N/A</td>\n</tr>\n<tr>\n<td>WSJ eval92</td>\n<td align=\"center\">2.1</td>\n<td align=\"center\">4.7</td>\n<td align=\"center\">N/A</td>\n</tr></tbody></table>\n<p>Note that the performance of the CSJ, HKUST, and Librispeech tasks was significantly improved by using the wide network (#units = 1024) and large subword units if necessary reported by <a href=\"https://arxiv.org/pdf/1805.03294.pdf\" rel=\"nofollow\">RWTH</a>.</p>\n<p>If you want to check the results of the other recipes, please check <code>egs/&lt;name_of_recipe&gt;/asr1/RESULTS.md</code>.</p>\n<h3>ASR demo</h3>\n<p>You can recognize speech in a WAV file using pretrained models.\nGo to a recipe directory and run <code>utils/recog_wav.sh</code> as follows:</p>\n<pre><span class=\"nb\">cd</span> egs/tedlium2/asr1\n../../../utils/recog_wav.sh --models tedlium2.transformer.v1 example.wav\n</pre>\n<p>where <code>example.wav</code> is a WAV file to be recognized.\nThe sampling rate must be consistent with that of data used in training.</p>\n<p>Available pretrained models in the demo script are listed as below.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Model</th>\n<th align=\"left\">Notes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1UqIY6WJMZ4sxNxSugUqp3mrGb3j6h7xe\" rel=\"nofollow\">tedlium2.rnn.v1</a></td>\n<td align=\"left\">Streaming decoding based on CTC-based VAD</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1cac5Uc09lJrCYfWkLQsF8eapQcxZnYdf\" rel=\"nofollow\">tedlium2.rnn.v2</a></td>\n<td align=\"left\">Streaming decoding based on CTC-based VAD (batch decoding)</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1mgbiWabOSkh_oHJIDA-h7hekQ3W95Z_U\" rel=\"nofollow\">tedlium2.transformer.v1</a></td>\n<td align=\"left\">Joint-CTC attention Transformer trained on Tedlium 2</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1wYYTwgvbB7uy6agHywhQfnuVWWW_obmO\" rel=\"nofollow\">tedlium3.transformer.v1</a></td>\n<td align=\"left\">Joint-CTC attention Transformer trained on Tedlium 3</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1BtQvAnsFvVi-dp_qsaFP7n4A_5cwnlR6\" rel=\"nofollow\">librispeech.transformer.v1</a></td>\n<td align=\"left\">Joint-CTC attention Transformer trained on Librispeech</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1tWccl6aYU67kbtkm8jv5H6xayqg1rzjh\" rel=\"nofollow\">commonvoice.transformer.v1</a></td>\n<td align=\"left\">Joint-CTC attention Transformer trained on CommonVoice</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=120nUQcSsKeY5dpyMWw_kI33ooMRGT2uF\" rel=\"nofollow\">csj.transformer.v1</a></td>\n<td align=\"left\">Joint-CTC attention Transformer trained on CSJ</td>\n</tr></tbody></table>\n<h3>ST results</h3>\n<p>We list 4-gram BLEU of major ST tasks.</p>\n<h4>end-to-end system</h4>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th align=\"center\">BLEU</th>\n<th align=\"center\">Pretrained model</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Fisher-CallHome Spanish fisher_test (Es-&gt;En)</td>\n<td align=\"center\">48.39</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Fisher-CallHome Spanish callhome_evltest (Es-&gt;En)</td>\n<td align=\"center\">18.67</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Libri-trans test (En-&gt;Fr)</td>\n<td align=\"center\">16.70</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/libri_trans/st1/RESULTS.md#train_spfr_lc_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans-1\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>How2 dev5 (En-&gt;Pt)</td>\n<td align=\"center\">45.68</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/how2/st1/RESULTS.md#trainpt_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans-1\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Must-C tst-COMMON (En-&gt;De)</td>\n<td align=\"center\">22.91</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/must_c/st1/RESULTS.md#train_spen-dede_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Mboshi-French dev (Fr-&gt;Mboshi)</td>\n<td align=\"center\">6.18</td>\n<td align=\"center\">N/A</td>\n</tr></tbody></table>\n<h4>cascaded system</h4>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th align=\"center\">BLEU</th>\n<th align=\"center\">Pretrained model</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Fisher-CallHome Spanish fisher_test (Es-&gt;En)</td>\n<td align=\"center\">42.16</td>\n<td align=\"center\">N/A</td>\n</tr>\n<tr>\n<td>Fisher-CallHome Spanish callhome_evltest (Es-&gt;En)</td>\n<td align=\"center\">19.82</td>\n<td align=\"center\">N/A</td>\n</tr>\n<tr>\n<td>Libri-trans test (En-&gt;Fr)</td>\n<td align=\"center\">16.96</td>\n<td align=\"center\">N/A</td>\n</tr>\n<tr>\n<td>How2 dev5 (En-&gt;Pt)</td>\n<td align=\"center\">44.90</td>\n<td align=\"center\">N/A</td>\n</tr>\n<tr>\n<td>Must-C tst-COMMON (En-&gt;De)</td>\n<td align=\"center\">23.65</td>\n<td align=\"center\">N/A</td>\n</tr></tbody></table>\n<p>If you want to check the results of the other recipes, please check <code>egs/&lt;name_of_recipe&gt;/st1/RESULTS.md</code>.</p>\n<h3>ST demo</h3>\n<p>(<strong>New!</strong>) We made a new real-time E2E-ST + TTS demonstration in Google Colab.\nPlease access the notebook from the following button and enjoy the real-time speech-to-speech translation!</p>\n<p><a href=\"https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></p>\n<hr>\n<p>You can translate speech in a WAV file using pretrained models.\nGo to a recipe directory and run <code>utils/translate_wav.sh</code> as follows:</p>\n<pre><span class=\"nb\">cd</span> egs/fisher_callhome_spanish/st1/\nwget -O - https://github.com/espnet/espnet/files/4100928/test.wav.tar.gz <span class=\"p\">|</span> tar zxvf - ../../../utils/translate_wav.sh --models fisher_callhome_spanish.transformer.v1.es-en test.wav\n</pre>\n<p>where <code>test.wav</code> is a WAV file to be translated.\nThe sampling rate must be consistent with that of data used in training.</p>\n<p>Available pretrained models in the demo script are listed as below.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Model</th>\n<th align=\"left\">Notes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1hawp5ZLw4_SIHIT3edglxbKIIkPVe8n3\" rel=\"nofollow\">fisher_callhome_spanish.transformer.v1</a></td>\n<td align=\"left\">Transformer-ST trained on Fisher-CallHome Spanish Es-&gt;En</td>\n</tr></tbody></table>\n<h3>MT results</h3>\n<table>\n<thead>\n<tr>\n<th>Task</th>\n<th align=\"center\">BLEU</th>\n<th align=\"center\">Pretrained model</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Fisher-CallHome Spanish fisher_test (Es-&gt;En)</td>\n<td align=\"center\">61.45</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Fisher-CallHome Spanish callhome_evltest (Es-&gt;En)</td>\n<td align=\"center\">29.86</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Libri-trans test (En-&gt;Fr)</td>\n<td align=\"center\">18.09</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/libri_trans/mt1/RESULTS.md#trainfr_lcrm_tc_pytorch_train_pytorch_transformer_bpe1000\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>How2 dev5 (En-&gt;Pt)</td>\n<td align=\"center\">58.61</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/how2/mt1/RESULTS.md#trainpt_tc_tc_pytorch_train_pytorch_transformer_bpe8000\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>Must-C tst-COMMON (En-&gt;De)</td>\n<td align=\"center\">27.63</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/must_c/mt1/RESULTS.md#summary-4-gram-bleu\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>IWSLT'14 test2014 (En-&gt;De)</td>\n<td align=\"center\">24.70</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>IWSLT'14 test2014 (De-&gt;En)</td>\n<td align=\"center\">29.22</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>IWSLT'16 test2014 (En-&gt;De)</td>\n<td align=\"center\">24.05</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result\" rel=\"nofollow\">link</a></td>\n</tr>\n<tr>\n<td>IWSLT'16 test2014 (De-&gt;En)</td>\n<td align=\"center\">29.13</td>\n<td align=\"center\"><a href=\"https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result\" rel=\"nofollow\">link</a></td>\n</tr></tbody></table>\n<h3>TTS results</h3>\n<p>You can listen to our samples in demo HP <a href=\"https://espnet.github.io/espnet-tts-sample/\" rel=\"nofollow\">espnet-tts-sample</a>.\nHere we list some notable ones:</p>\n<ul>\n<li><a href=\"https://drive.google.com/open?id=18JgsOCWiP_JkhONasTplnHS7yaF_konr\" rel=\"nofollow\">Single English speaker Tacotron2</a></li>\n<li><a href=\"https://drive.google.com/open?id=1fEgS4-K4dtgVxwI4Pr7uOA1h4PE-zN7f\" rel=\"nofollow\">Single Japanese speaker Tacotron2</a></li>\n<li><a href=\"https://drive.google.com/open?id=1q_66kyxVZGU99g8Xb5a0Q8yZ1YVm2tN0\" rel=\"nofollow\">Single other language speaker Tacotron2</a></li>\n<li><a href=\"https://drive.google.com/open?id=18S_B8Ogogij34rIfJOeNF8D--uG7amz2\" rel=\"nofollow\">Multi English speaker Tacotron2</a></li>\n<li><a href=\"https://drive.google.com/open?id=14EboYVsMVcAq__dFP1p6lyoZtdobIL1X\" rel=\"nofollow\">Single English speaker Transformer</a></li>\n<li><a href=\"https://drive.google.com/open?id=1PSxs1VauIndwi8d5hJmZlppGRVu2zuy5\" rel=\"nofollow\">Single English speaker FastSpeech</a></li>\n<li><a href=\"https://drive.google.com/open?id=1_vrdqjM43DdN1Qz7HJkvMQ6lCMmWLeGp\" rel=\"nofollow\">Multi English speaker Transformer</a></li>\n<li><a href=\"https://drive.google.com/open?id=13I5V2w7deYFX4DlVk1-0JfaXmUR2rNOv\" rel=\"nofollow\">Single Italian speaker FastSpeech</a></li>\n<li><a href=\"https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD\" rel=\"nofollow\">Single Mandarin speaker Transformer</a></li>\n<li><a href=\"https://drive.google.com/open?id=1Ol_048Tuy6BgvYm1RpjhOX4HfhUeBqdK\" rel=\"nofollow\">Single Mandarin speaker FastSpeech</a></li>\n<li><a href=\"https://drive.google.com/open?id=1fFMQDF6NV5Ysz48QLFYE8fEvbAxCsMBw\" rel=\"nofollow\">Multi Japanese speaker Transformer</a></li>\n<li><a href=\"https://drive.google.com/open?id=1HvB0_LDf1PVinJdehiuCt5gWmXGguqtx\" rel=\"nofollow\">Single English speaker models with Parallel WaveGAN</a></li>\n<li><a href=\"https://drive.google.com/open?id=1wG-Y0itVYalxuLAHdkAHO7w1CWFfRPF4\" rel=\"nofollow\">Single English speaker knowledge distillation-based FastSpeech (New!)</a></li>\n</ul>\n<p>You can download all of the pretrained models and generated samples:</p>\n<ul>\n<li><a href=\"https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF\" rel=\"nofollow\">All of the pretrained E2E-TTS models</a></li>\n<li><a href=\"https://drive.google.com/open?id=1bQGuqH92xuxOX__reWLP4-cif0cbpMLX\" rel=\"nofollow\">All of the generated samples</a></li>\n</ul>\n<p>Note that in the generated samples we use three vocoders: Griffin-Lim (<strong>GL</strong>), WaveNet vocoder (<strong>WaveNet</strong>), Parallel WaveGAN (<strong>ParallelWaveGAN</strong>), and MelGAN (<strong>MelGAN</strong>).\nThe neural vocoders are based on following repositories.</p>\n<ul>\n<li><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">kan-bayashi/ParallelWaveGAN</a>: Parallel WaveGAN / MelGAN</li>\n<li><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">r9y9/wavenet_vocoder</a>: 16 bit mixture of Logistics WaveNet vocoder</li>\n<li><a href=\"https://github.com/kan-bayashi/PytorchWaveNetVocoder\" rel=\"nofollow\">kan-bayashi/PytorchWaveNetVocoder</a>: 8 bit Softmax WaveNet Vocoder with the noise shaping</li>\n</ul>\n<p>If you want to build your own neural vocoder, please check the above repositories.</p>\n<p>Here we list all of the pretrained neural vocoders. Please download and enjoy the generation of high quality speech!</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Model link</th>\n<th align=\"center\">Lang</th>\n<th align=\"center\">Fs [Hz]</th>\n<th align=\"center\">Mel range [Hz]</th>\n<th align=\"center\">FFT / Shift / Win [pt]</th>\n<th align=\"left\">Model type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1eA1VcRS9jzFa-DovyTgJLQ_jmwOLIi8L\" rel=\"nofollow\">ljspeech.wavenet.softmax.ns.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/PytorchWaveNetVocoder\" rel=\"nofollow\">Softmax WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1sY7gEUg39QaO1szuN62-Llst9TrFno2t\" rel=\"nofollow\">ljspeech.wavenet.mol.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">MoL WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7\" rel=\"nofollow\">ljspeech.parallel_wavegan.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">Parallel WaveGAN</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1es2HuKUeKVtEdq6YDtAsLNpqCy4fhIXr\" rel=\"nofollow\">ljspeech.wavenet.mol.v2</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">MoL WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1Grn7X9wD35UcDJ5F7chwdTqTa4U7DeVB\" rel=\"nofollow\">ljspeech.parallel_wavegan.v2</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">Parallel WaveGAN</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1ipPWYl8FBNRlBFaKj1-i23eQpW_W_YcR\" rel=\"nofollow\">ljspeech.melgan.v1 (EXPERIMENTAL)</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">MelGAN</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1_a8faVA5OGCzIcJNw4blQYjfG4oA9VEt\" rel=\"nofollow\">ljspeech.melgan.v3 (EXPERIMENTAL)</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">MelGAN</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1jHUUmQFjWiQGyDd7ZeiCThSjjpbF_B4h\" rel=\"nofollow\">libritts.wavenet.mol.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">MoL WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=187xvyNbmJVZ0EZ1XHCdyjZHTXK9EcfkK\" rel=\"nofollow\">jsut.wavenet.mol.v1</a></td>\n<td align=\"center\">JP</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"left\"><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">MoL WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM\" rel=\"nofollow\">jsut.parallel_wavegan.v1</a></td>\n<td align=\"center\">JP</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">Parallel WaveGAN</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1PsjFRV5eUP0HHwBaRYya9smKy5ghXKzj\" rel=\"nofollow\">csmsc.wavenet.mol.v1</a></td>\n<td align=\"center\">ZH</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"left\"><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">MoL WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy\" rel=\"nofollow\">csmsc.parallel_wavegan.v1</a></td>\n<td align=\"center\">ZH</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">Parallel WaveGAN</a></td>\n</tr></tbody></table>\n<p>If you want to use the above pretrained vocoders, please exactly match the feature setting with them.</p>\n<h3>TTS demo</h3>\n<p>(<strong>New!</strong>) We made a new real-time E2E-TTS demonstration in Google Colab.\nPlease access the notebook from the following button and enjoy the real-time synthesis!</p>\n<p><a href=\"https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></p>\n<hr>\n<p>You can synthesize speech in a TXT file using pretrained models.\nGo to a recipe directory and run <code>utils/synth_wav.sh</code> as follows:</p>\n<pre><span class=\"nb\">cd</span> egs/ljspeech/tts1\n<span class=\"nb\">echo</span> <span class=\"s2\">\"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\"</span> &gt; example.txt\n../../../utils/synth_wav.sh example.txt\n</pre>\n<p>You can change the pretrained model as follows:</p>\n<pre>../../../utils/synth_wav.sh --models ljspeech.fastspeech.v1 example.txt\n</pre>\n<p>Waveform synthesis is performed with Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN).\nYou can change the pretrained vocoder model as follows:</p>\n<pre><code>../../../utils/synth_wav.sh --vocoder_models ljspeech.wavenet.mol.v1 example.txt\n</code></pre>\n<p>Note that WaveNet vocoder provides very high quality speech but it takes time to generate.</p>\n<p>Available pretrained models in the demo script are listed as follows:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Model link</th>\n<th align=\"center\">Lang</th>\n<th align=\"center\">Fs [Hz]</th>\n<th align=\"center\">Mel range [Hz]</th>\n<th align=\"center\">FFT / Shift / Win [pt]</th>\n<th align=\"center\">Input</th>\n<th align=\"center\">R</th>\n<th align=\"left\">Model type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1dKzdaDpOkpx7kWZnvrvx2De7eZEdPHZs\" rel=\"nofollow\">ljspeech.tacotron2.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">char</td>\n<td align=\"center\">2</td>\n<td align=\"left\">Tacotron 2</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=11T9qw8rJlYzUdXvFjkjQjYrp3iGfQ15h\" rel=\"nofollow\">ljspeech.tacotron2.v2</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">char</td>\n<td align=\"center\">1</td>\n<td align=\"left\">Tacotron 2 + forward attention</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1hiZn14ITUDM1nkn-GkaN_M3oaTOUcn1n\" rel=\"nofollow\">ljspeech.tacotron2.v3</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">char</td>\n<td align=\"center\">1</td>\n<td align=\"left\">Tacotron 2 + guided attention loss</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=13DR-RB5wrbMqBGx_MC655VZlsEq52DyS\" rel=\"nofollow\">ljspeech.transformer.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">char</td>\n<td align=\"center\">1</td>\n<td align=\"left\">Deep Transformer</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1xxAwPuUph23RnlC5gym7qDM02ZCW9Unp\" rel=\"nofollow\">ljspeech.transformer.v2</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">char</td>\n<td align=\"center\">3</td>\n<td align=\"left\">Shallow Transformer</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1M_w7nxI6AfbtSHpMO-exILnAc_aUYvXP\" rel=\"nofollow\">ljspeech.transformer.v3</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">phn</td>\n<td align=\"center\">1</td>\n<td align=\"left\">Deep Transformer</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=17RUNFLP4SSTbGA01xWRJo7RkR876xM0i\" rel=\"nofollow\">ljspeech.fastspeech.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">char</td>\n<td align=\"center\">1</td>\n<td align=\"left\">FF-Transformer</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1zD-2GMrWM3thaDpS3h3rkTU4jIC0wc5B\" rel=\"nofollow\">ljspeech.fastspeech.v2</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">char</td>\n<td align=\"center\">1</td>\n<td align=\"left\">FF-Transformer + CNN in FFT block</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1W86YEQ6KbuUTIvVURLqKtSNqe_eI2GDN\" rel=\"nofollow\">ljspeech.fastspeech.v3</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">phn</td>\n<td align=\"center\">1</td>\n<td align=\"left\">FF-Transformer + CNN in FFT block + postnet</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1iAXwC0AuWusa9AcFeUVkcNLG0I-hnSr3\" rel=\"nofollow\">libritts.tacotron2.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">char</td>\n<td align=\"center\">2</td>\n<td align=\"left\">Multi-speaker Tacotron 2</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1Xj73mDPuuPH8GsyNO8GnOC3mn0_OK4g3\" rel=\"nofollow\">libritts.transformer.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">char</td>\n<td align=\"center\">2</td>\n<td align=\"left\">Multi-speaker Transformer</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1kp5M4VvmagDmYckFJa78WGqh1drb_P9t\" rel=\"nofollow\">jsut.tacotron2</a></td>\n<td align=\"center\">JP</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">phn</td>\n<td align=\"center\">2</td>\n<td align=\"left\">Tacotron 2</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD\" rel=\"nofollow\">jsut.transformer</a></td>\n<td align=\"center\">JP</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">phn</td>\n<td align=\"center\">3</td>\n<td align=\"left\">Shallow Transformer</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1bTSygvonv5TS6-iuYsOIUWpN2atGnyhZ\" rel=\"nofollow\">csmsc.transformer.v1</a></td>\n<td align=\"center\">ZH</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">pinyin</td>\n<td align=\"center\">1</td>\n<td align=\"left\">Deep Transformer</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1T8thxkAxjGFPXPWPTcKLvHnd6lG0-82R\" rel=\"nofollow\">csmsc.fastspeech.v3</a></td>\n<td align=\"center\">ZH</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">pinyin</td>\n<td align=\"center\">1</td>\n<td align=\"left\">FF-Transformer + CNN in FFT block + postnet</td>\n</tr></tbody></table>\n<p>Available pretrained vocoder models in the demo script are listed as follows:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Model link</th>\n<th align=\"center\">Lang</th>\n<th align=\"center\">Fs [Hz]</th>\n<th align=\"center\">Mel range [Hz]</th>\n<th align=\"center\">FFT / Shift / Win [pt]</th>\n<th align=\"left\">Model type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1eA1VcRS9jzFa-DovyTgJLQ_jmwOLIi8L\" rel=\"nofollow\">ljspeech.wavenet.softmax.ns.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/PytorchWaveNetVocoder\" rel=\"nofollow\">Softmax WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1sY7gEUg39QaO1szuN62-Llst9TrFno2t\" rel=\"nofollow\">ljspeech.wavenet.mol.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">MoL WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7\" rel=\"nofollow\">ljspeech.parallel_wavegan.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">Parallel WaveGAN</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1jHUUmQFjWiQGyDd7ZeiCThSjjpbF_B4h\" rel=\"nofollow\">libritts.wavenet.mol.v1</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"left\"><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">MoL WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=187xvyNbmJVZ0EZ1XHCdyjZHTXK9EcfkK\" rel=\"nofollow\">jsut.wavenet.mol.v1</a></td>\n<td align=\"center\">JP</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"left\"><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">MoL WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM\" rel=\"nofollow\">jsut.parallel_wavegan.v1</a></td>\n<td align=\"center\">JP</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">Parallel WaveGAN</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1PsjFRV5eUP0HHwBaRYya9smKy5ghXKzj\" rel=\"nofollow\">csmsc.wavenet.mol.v1</a></td>\n<td align=\"center\">ZH</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"left\"><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">MoL WaveNet</a></td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy\" rel=\"nofollow\">csmsc.parallel_wavegan.v1</a></td>\n<td align=\"center\">ZH</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"left\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN\" rel=\"nofollow\">Parallel WaveGAN</a></td>\n</tr></tbody></table>\n<h3>VC results</h3>\n<p>The <a href=\"http://www.vc-challenge.org/\" rel=\"nofollow\">Voice Conversion Challenge 2020</a> (VCC2020) adopts ESPnet to build an end-to-end based baseline system. In VCC2020, the objective is intra/cross lingual nonparallel VC. A cascade method of ASR+TTS is developed.<br>\nYou can download converted samples <a href=\"https://drive.google.com/drive/folders/1oeZo83GrOgtqxGwF7KagzIrfjr8X59Ue?usp=sharing\" rel=\"nofollow\">here</a>.</p>\n<h2>References</h2>\n<p>[1] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai, \"ESPnet: End-to-End Speech Processing Toolkit,\" <em>Proc. Interspeech'18</em>, pp. 2207-2211 (2018)</p>\n<p>[2] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, \"Joint CTC-attention based end-to-end speech recognition using multi-task learning,\" <em>Proc. ICASSP'17</em>, pp. 4835--4839 (2017)</p>\n<p>[3] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey and Tomoki Hayashi, \"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition,\" <em>IEEE Journal of Selected Topics in Signal Processing</em>, vol. 11, no. 8, pp. 1240-1253, Dec. 2017</p>\n<h2>Citations</h2>\n<pre><code>@inproceedings{watanabe2018espnet,\n  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},\n  title={ESPnet: End-to-End Speech Processing Toolkit},\n  year=2018,\n  booktitle={Interspeech},\n  pages={2207--2211},\n  doi={10.21437/Interspeech.2018-1456},\n  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}\n}\n@misc{hayashi2019espnettts,\n    title={ESPnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit},\n    author={Tomoki Hayashi and Ryuichi Yamamoto and Katsuki Inoue and Takenori Yoshimura and Shinji Watanabe and Tomoki Toda and Kazuya Takeda and Yu Zhang and Xu Tan},\n    year={2019},\n    eprint={1910.10909},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n</code></pre>\n\n          </div>"}, "last_serial": 7153885, "releases": {"0.6.0": [{"comment_text": "", "digests": {"md5": "701a0c157ec603802e29e5e2aaa361ab", "sha256": "6a656bae5a69aee3d02db667836550cc566f6f1f38c3ee8b903002764892513c"}, "downloads": -1, "filename": "espnet-0.6.0-py3-none-any.whl", "has_sig": false, "md5_digest": "701a0c157ec603802e29e5e2aaa361ab", "packagetype": "bdist_wheel", "python_version": "3.7", "requires_python": null, "size": 6224, "upload_time": "2020-05-02T05:52:41", "upload_time_iso_8601": "2020-05-02T05:52:41.171193Z", "url": "https://files.pythonhosted.org/packages/57/5b/b97ef096df6824ed12919d6bdf538d50a53b5db572c222b3b2413b08bbbd/espnet-0.6.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4adedd9618f848b112ce5eeba1d82b36", "sha256": "7af6bdb51e784e3d7a336096b4229a8b3341f44d6a6233ac0c558fed98a36a58"}, "downloads": -1, "filename": "espnet-0.6.0.tar.gz", "has_sig": false, "md5_digest": "4adedd9618f848b112ce5eeba1d82b36", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 48283, "upload_time": "2020-05-02T05:52:37", "upload_time_iso_8601": "2020-05-02T05:52:37.975076Z", "url": "https://files.pythonhosted.org/packages/d9/20/398d29176aa679c436f75541c6c06830f0f997afe318c90880fdfddadd89/espnet-0.6.0.tar.gz", "yanked": false}], "0.6.0.post1": [{"comment_text": "", "digests": {"md5": "a15a3d5362b2cac1d25f656f214574e1", "sha256": "b28a399f490f1f4bbda11363db3f24a1b7c25fd44d21c0e10f61e33adae49199"}, "downloads": -1, "filename": "espnet-0.6.0.post1-py3-none-any.whl", "has_sig": false, "md5_digest": "a15a3d5362b2cac1d25f656f214574e1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 359599, "upload_time": "2020-05-02T09:40:15", "upload_time_iso_8601": "2020-05-02T09:40:15.566809Z", "url": "https://files.pythonhosted.org/packages/43/dd/ebe863e1b9e56f9e1975b3a120cc71f8f9d52b217a34435ca7e4cc4277b4/espnet-0.6.0.post1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "996060065184bb0945629bc6771a4abb", "sha256": "a2d702a086c05cec4ae65cd26b30ffc0700ebe6e31e829395b1b5cc60aef16e0"}, "downloads": -1, "filename": "espnet-0.6.0.post1.tar.gz", "has_sig": false, "md5_digest": "996060065184bb0945629bc6771a4abb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 302606, "upload_time": "2020-05-02T09:40:17", "upload_time_iso_8601": "2020-05-02T09:40:17.983252Z", "url": "https://files.pythonhosted.org/packages/59/03/3f653c7df4b04969a1999fa75a66113ec55310030f6acfda037d949187a8/espnet-0.6.0.post1.tar.gz", "yanked": false}], "0.6.1": [{"comment_text": "", "digests": {"md5": "b12f00313079bc802fff2ab8adce4aad", "sha256": "e7fa82ed61e70d6c37c85dc3c76f7ccebdd585b23d2bb33de0d5419ebbb3c328"}, "downloads": -1, "filename": "espnet-0.6.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b12f00313079bc802fff2ab8adce4aad", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 376042, "upload_time": "2020-05-02T09:44:28", "upload_time_iso_8601": "2020-05-02T09:44:28.531119Z", "url": "https://files.pythonhosted.org/packages/69/39/1a21dc763bc40d0aefb49286fb3573c0a9e6bb4c20c8148426f9eda28d3d/espnet-0.6.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f1fb4fa642dc11b852306252f3f9889c", "sha256": "e31d70a4e67a36fa6faf2bcae8f451c1fbb13b3d36c663f7b3787a069f0c1e6e"}, "downloads": -1, "filename": "espnet-0.6.1.tar.gz", "has_sig": false, "md5_digest": "f1fb4fa642dc11b852306252f3f9889c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 318352, "upload_time": "2020-05-02T09:44:31", "upload_time_iso_8601": "2020-05-02T09:44:31.047319Z", "url": "https://files.pythonhosted.org/packages/34/dc/f04020216478cd6adda3ad51b08853e6e6c162f36b3732194fd8a2177e54/espnet-0.6.1.tar.gz", "yanked": false}], "0.6.2": [{"comment_text": "", "digests": {"md5": "9cd0139793b5564660c9188a07d6ba95", "sha256": "404f56ec24e739e28fcc4918e13bf0145d0d5a5e99045fcc538d19e7d63dce8c"}, "downloads": -1, "filename": "espnet-0.6.2-py3-none-any.whl", "has_sig": false, "md5_digest": "9cd0139793b5564660c9188a07d6ba95", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 373745, "upload_time": "2020-05-02T09:46:15", "upload_time_iso_8601": "2020-05-02T09:46:15.757894Z", "url": "https://files.pythonhosted.org/packages/3c/de/44127dbd53d300af27fa80ebca405b7eb17fbde091c9dccf97db138f3139/espnet-0.6.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ae882f7537a4ba5faa4725ec2734e592", "sha256": "bbec96830e7af36d3219c9657e22bcb97710008ac8eadfd0d26b117665fa274b"}, "downloads": -1, "filename": "espnet-0.6.2.tar.gz", "has_sig": false, "md5_digest": "ae882f7537a4ba5faa4725ec2734e592", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 305228, "upload_time": "2020-05-02T09:46:19", "upload_time_iso_8601": "2020-05-02T09:46:19.175261Z", "url": "https://files.pythonhosted.org/packages/e3/13/12bb7a9580418847a974678d35745d4857035d15147acabe3a79a0e553c7/espnet-0.6.2.tar.gz", "yanked": false}], "0.6.3": [{"comment_text": "", "digests": {"md5": "3156eec623798555391c1c951e66108d", "sha256": "547d647b45cc5f6fca5f248a9de3ad4deab0d5457b58de5565c265119d97ac5d"}, "downloads": -1, "filename": "espnet-0.6.3-py3-none-any.whl", "has_sig": false, "md5_digest": "3156eec623798555391c1c951e66108d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 375772, "upload_time": "2020-05-02T09:47:35", "upload_time_iso_8601": "2020-05-02T09:47:35.777864Z", "url": "https://files.pythonhosted.org/packages/5f/0f/f8678e88f71532b10abc9400e8b9e46f8cd1bf2bbad32334c1caf7c8339f/espnet-0.6.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3c2d3ad7a5a7a3e1ef1b58a0df2f8606", "sha256": "de51faf43a26132d51c6c278106ea52b34eed25292d48f123e831752b7b963f6"}, "downloads": -1, "filename": "espnet-0.6.3.tar.gz", "has_sig": false, "md5_digest": "3c2d3ad7a5a7a3e1ef1b58a0df2f8606", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 308094, "upload_time": "2020-05-02T09:47:40", "upload_time_iso_8601": "2020-05-02T09:47:40.583522Z", "url": "https://files.pythonhosted.org/packages/a9/81/4abfd674a03e7191d7609a6d71736d3e670c94661de11fd2c6626d20664b/espnet-0.6.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3156eec623798555391c1c951e66108d", "sha256": "547d647b45cc5f6fca5f248a9de3ad4deab0d5457b58de5565c265119d97ac5d"}, "downloads": -1, "filename": "espnet-0.6.3-py3-none-any.whl", "has_sig": false, "md5_digest": "3156eec623798555391c1c951e66108d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 375772, "upload_time": "2020-05-02T09:47:35", "upload_time_iso_8601": "2020-05-02T09:47:35.777864Z", "url": "https://files.pythonhosted.org/packages/5f/0f/f8678e88f71532b10abc9400e8b9e46f8cd1bf2bbad32334c1caf7c8339f/espnet-0.6.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3c2d3ad7a5a7a3e1ef1b58a0df2f8606", "sha256": "de51faf43a26132d51c6c278106ea52b34eed25292d48f123e831752b7b963f6"}, "downloads": -1, "filename": "espnet-0.6.3.tar.gz", "has_sig": false, "md5_digest": "3c2d3ad7a5a7a3e1ef1b58a0df2f8606", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 308094, "upload_time": "2020-05-02T09:47:40", "upload_time_iso_8601": "2020-05-02T09:47:40.583522Z", "url": "https://files.pythonhosted.org/packages/a9/81/4abfd674a03e7191d7609a6d71736d3e670c94661de11fd2c6626d20664b/espnet-0.6.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:37 2020"}