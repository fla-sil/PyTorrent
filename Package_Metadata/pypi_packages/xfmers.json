{"info": {"author": "Timothy Liu", "author_email": "tlkh.xms@gmail.com", "bugtrack_url": null, "classifiers": ["Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "<div align=\"center\">\n<h1>xfmers</h1>\n  <p>Quickly initialize bespoke Transformer models</p>\n</div>\n\n## About\n\nThe goal of the xfmers library is to provide a simple API to quickly initialise Transformers with specified hyperparameters and features. The library generates standard TF 2.0 Keras models that can be used with `model.fit()`, Automatic Mixed Precision (AMP), XLA, Horovod and tf.distribute APIs etc.\n\n**Included layers/features:**\n\n* Multi-head attention\n  * Encoder or Decoder (causal) mode\n* Transformer layers\n  * Spatial convolution\n  * Reversible layers\n* Transformer Stack (Encoder/Decoder)\n  * Weight sharing (ALBERT-like)\n* Embedding Layer\n  * Learnable positional embeddings\n  * Factorized embedding parameterization (ALBERT-like)\n* Misc\n  * Training schedules\n  * Activation functions\n\n## Usage\n\n**Creating an ALBERT-like Transformer**\n\nModels can be created using Keras layers and trained using `model.fit()` or Gradient Tape.\n\n```python\ninputs = tf.keras.Input(shape=(None, ), name=\"inputs\")\npadding_mask = layers.PaddingMaskGenerator()(inputs)\nembeddings = layers.TokenPosEmbedding(d_vocab=vocab_size, d_model=128, pos_length=512,\n                                      # project embedding from 128 -> 512\n                                      d_projection=512)(inputs)\nencoder_block = layers.TransformerStack(layers=3,\n                                        ff_units=2048,\n                                        d_model=512,\n                                        num_heads=8,\n                                        dropout=0.1,\n                                        causal=False,        # attend pair-wise between all positons\n                                        activation=ops.gelu,\n                                        weight_sharing=True, # share weights between all encoder layers\n                                        name=\"EncoderBlock\")\nenc_outputs = encoder_block({\"token_inputs\": embeddings,\n                             \"mask_inputs\": padding_mask})\npreds = layers.LMHead(vocab_size=vocab_size, name=\"outputs\")(enc_outputs)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=preds, name=name)\n```\n\n## Installing Xfmers\n\n**Install from Pip**\n\n```shell\npip install xfmers\n```\n\n## Support\n\n* Core Maintainer: [Timothy Liu (tlkh)](https://github.com/tlkh)\n* Please open an issue if you encounter problems or have a feature request\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/tlkh/xfmers", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "xfmers", "package_url": "https://pypi.org/project/xfmers/", "platform": "", "project_url": "https://pypi.org/project/xfmers/", "project_urls": {"Homepage": "https://github.com/tlkh/xfmers"}, "release_url": "https://pypi.org/project/xfmers/0.0.3/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Quickly initialize bespoke Transformer models", "version": "0.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div>\n<h1>xfmers</h1>\n  <p>Quickly initialize bespoke Transformer models</p>\n</div>\n<h2>About</h2>\n<p>The goal of the xfmers library is to provide a simple API to quickly initialise Transformers with specified hyperparameters and features. The library generates standard TF 2.0 Keras models that can be used with <code>model.fit()</code>, Automatic Mixed Precision (AMP), XLA, Horovod and tf.distribute APIs etc.</p>\n<p><strong>Included layers/features:</strong></p>\n<ul>\n<li>Multi-head attention\n<ul>\n<li>Encoder or Decoder (causal) mode</li>\n</ul>\n</li>\n<li>Transformer layers\n<ul>\n<li>Spatial convolution</li>\n<li>Reversible layers</li>\n</ul>\n</li>\n<li>Transformer Stack (Encoder/Decoder)\n<ul>\n<li>Weight sharing (ALBERT-like)</li>\n</ul>\n</li>\n<li>Embedding Layer\n<ul>\n<li>Learnable positional embeddings</li>\n<li>Factorized embedding parameterization (ALBERT-like)</li>\n</ul>\n</li>\n<li>Misc\n<ul>\n<li>Training schedules</li>\n<li>Activation functions</li>\n</ul>\n</li>\n</ul>\n<h2>Usage</h2>\n<p><strong>Creating an ALBERT-like Transformer</strong></p>\n<p>Models can be created using Keras layers and trained using <code>model.fit()</code> or Gradient Tape.</p>\n<pre><span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"inputs\"</span><span class=\"p\">)</span>\n<span class=\"n\">padding_mask</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">PaddingMaskGenerator</span><span class=\"p\">()(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n<span class=\"n\">embeddings</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">TokenPosEmbedding</span><span class=\"p\">(</span><span class=\"n\">d_vocab</span><span class=\"o\">=</span><span class=\"n\">vocab_size</span><span class=\"p\">,</span> <span class=\"n\">d_model</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">pos_length</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span>\n                                      <span class=\"c1\"># project embedding from 128 -&gt; 512</span>\n                                      <span class=\"n\">d_projection</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">)(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n<span class=\"n\">encoder_block</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">TransformerStack</span><span class=\"p\">(</span><span class=\"n\">layers</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span>\n                                        <span class=\"n\">ff_units</span><span class=\"o\">=</span><span class=\"mi\">2048</span><span class=\"p\">,</span>\n                                        <span class=\"n\">d_model</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span>\n                                        <span class=\"n\">num_heads</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n                                        <span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>\n                                        <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>        <span class=\"c1\"># attend pair-wise between all positons</span>\n                                        <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">gelu</span><span class=\"p\">,</span>\n                                        <span class=\"n\">weight_sharing</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"c1\"># share weights between all encoder layers</span>\n                                        <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"EncoderBlock\"</span><span class=\"p\">)</span>\n<span class=\"n\">enc_outputs</span> <span class=\"o\">=</span> <span class=\"n\">encoder_block</span><span class=\"p\">({</span><span class=\"s2\">\"token_inputs\"</span><span class=\"p\">:</span> <span class=\"n\">embeddings</span><span class=\"p\">,</span>\n                             <span class=\"s2\">\"mask_inputs\"</span><span class=\"p\">:</span> <span class=\"n\">padding_mask</span><span class=\"p\">})</span>\n<span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">LMHead</span><span class=\"p\">(</span><span class=\"n\">vocab_size</span><span class=\"o\">=</span><span class=\"n\">vocab_size</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"outputs\"</span><span class=\"p\">)(</span><span class=\"n\">enc_outputs</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">preds</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">name</span><span class=\"p\">)</span>\n</pre>\n<h2>Installing Xfmers</h2>\n<p><strong>Install from Pip</strong></p>\n<pre>pip install xfmers\n</pre>\n<h2>Support</h2>\n<ul>\n<li>Core Maintainer: <a href=\"https://github.com/tlkh\" rel=\"nofollow\">Timothy Liu (tlkh)</a></li>\n<li>Please open an issue if you encounter problems or have a feature request</li>\n</ul>\n\n          </div>"}, "last_serial": 6553830, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "ce9cb76ee1cc951ae600114df44518cf", "sha256": "ca06e9e521d56ae37e0b6b53b7e646480684ddb06c0ce7ec8ee3ed54221401c7"}, "downloads": -1, "filename": "xfmers-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "ce9cb76ee1cc951ae600114df44518cf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 6673, "upload_time": "2020-01-02T03:30:40", "upload_time_iso_8601": "2020-01-02T03:30:40.725509Z", "url": "https://files.pythonhosted.org/packages/33/aa/4b070932959ac5127eda900620fdc1ff2db1a458930ae4c7a2a5849e206d/xfmers-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1347919e722bd52b7ff1734b96b71564", "sha256": "9efe0725274e497bca701772c136662f631e7173d2c1a417b251d4dee229729d"}, "downloads": -1, "filename": "xfmers-0.0.1.tar.gz", "has_sig": false, "md5_digest": "1347919e722bd52b7ff1734b96b71564", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 2765, "upload_time": "2020-01-02T03:30:43", "upload_time_iso_8601": "2020-01-02T03:30:43.225980Z", "url": "https://files.pythonhosted.org/packages/aa/9e/1f3890c00a1c51e8fe449c40b0e7951f7ecf343240c2c6da31e8300fa539/xfmers-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "e9668be452516f2d57b53b084f5170b9", "sha256": "5426d773c7eb8ec4043f105ad7b179b2abe6df4a173b057fb39f299f4823ee3f"}, "downloads": -1, "filename": "xfmers-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "e9668be452516f2d57b53b084f5170b9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 10954, "upload_time": "2020-01-02T03:34:41", "upload_time_iso_8601": "2020-01-02T03:34:41.318179Z", "url": "https://files.pythonhosted.org/packages/15/80/9780e5b3462ba61e4ec443ffad588f4d6f11bfb34926f401d1b3692953ec/xfmers-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9c33fd8ac7f0cab5b68dd6abfa6d48ac", "sha256": "7fa6a0b7ac8208120ab92cb1cefc85d16b1b56b628419cd12fad0c90c92ed20c"}, "downloads": -1, "filename": "xfmers-0.0.2.tar.gz", "has_sig": false, "md5_digest": "9c33fd8ac7f0cab5b68dd6abfa6d48ac", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6144, "upload_time": "2020-01-02T03:34:43", "upload_time_iso_8601": "2020-01-02T03:34:43.155683Z", "url": "https://files.pythonhosted.org/packages/fb/ae/f556af4d937c3eea439f57b636ee4391af2e46cf82f6dfba33c9ad6bc1dc/xfmers-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "f28bfc365c66ae1dd6cdcc693e7b227a", "sha256": "67611dca99bada91c996ebe091c16ece69ae3708ab5a029e7aa2ebe5154da4cb"}, "downloads": -1, "filename": "xfmers-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "f28bfc365c66ae1dd6cdcc693e7b227a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 8982, "upload_time": "2020-02-01T10:27:00", "upload_time_iso_8601": "2020-02-01T10:27:00.327095Z", "url": "https://files.pythonhosted.org/packages/d6/c6/4d48c88cdd49aab48fdf9e6c7386a2039f385fca6dca2201913638e3dc63/xfmers-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a226ecdcbf965eccfc1a427850e91c44", "sha256": "a179bd4ca9c1f96807250f5db711f4571dbed7dc3df63c334e54ed99af6c5613"}, "downloads": -1, "filename": "xfmers-0.0.3.tar.gz", "has_sig": false, "md5_digest": "a226ecdcbf965eccfc1a427850e91c44", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7800, "upload_time": "2020-02-01T10:27:02", "upload_time_iso_8601": "2020-02-01T10:27:02.114016Z", "url": "https://files.pythonhosted.org/packages/83/ab/76b804450d98167fe8fa3f806b1720d9eae6e2de28fd6cbbf10b947398c9/xfmers-0.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f28bfc365c66ae1dd6cdcc693e7b227a", "sha256": "67611dca99bada91c996ebe091c16ece69ae3708ab5a029e7aa2ebe5154da4cb"}, "downloads": -1, "filename": "xfmers-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "f28bfc365c66ae1dd6cdcc693e7b227a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 8982, "upload_time": "2020-02-01T10:27:00", "upload_time_iso_8601": "2020-02-01T10:27:00.327095Z", "url": "https://files.pythonhosted.org/packages/d6/c6/4d48c88cdd49aab48fdf9e6c7386a2039f385fca6dca2201913638e3dc63/xfmers-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a226ecdcbf965eccfc1a427850e91c44", "sha256": "a179bd4ca9c1f96807250f5db711f4571dbed7dc3df63c334e54ed99af6c5613"}, "downloads": -1, "filename": "xfmers-0.0.3.tar.gz", "has_sig": false, "md5_digest": "a226ecdcbf965eccfc1a427850e91c44", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7800, "upload_time": "2020-02-01T10:27:02", "upload_time_iso_8601": "2020-02-01T10:27:02.114016Z", "url": "https://files.pythonhosted.org/packages/83/ab/76b804450d98167fe8fa3f806b1720d9eae6e2de28fd6cbbf10b947398c9/xfmers-0.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:25:37 2020"}