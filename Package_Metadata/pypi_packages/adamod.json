{"info": {"author": "Jianbang Ding", "author_email": "jianbangding@pku.edu.cn", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "\ufeff# AdaMod\n\nAn optimizer which exerts adaptive momental upper bounds on individual learning rates to prevent them becoming undesirably lager than what the historical statistics suggest and avoid the non-convergence issue, thus to a better performance. Strong empirical results on many deep learning applications demonstrate the effectiveness of our proposed method especially on complex networks such as DenseNet and Transformer.\n\n<p align='center'><img src='img/Loss.bmp' width=\"100%\"/></p>\n\n## Installation\n\nAdaMod requires Python 3.6.0 or later.\n\n### Installing via pip\n\nThe preferred way to install AdaMod is via `pip` with a virtual environment.\nJust run \n```bash\npip install adamod\n```\nin your Python environment and you are ready to go!\n\n### Using source code\n\nAs AdaMod is a Python class with only 100+ lines, an alternative way is directly downloading\n[adamod.py](./adamod/adamod.py) and copying it to your project.\n\n## Usage\n\nYou can use AdaMod just like any other PyTorch optimizers.\n\n```python3\noptimizer = adamod.AdaMod(model.parameters(), lr=1e-3, beta3=0.999)\n```\nAs described in the paper, AdaMod can smooths out unexpected large learning rates throughout the training process. The `beta3` parameter is the smoothing coefficient for actual learning rate, which controls the average range. In common cases, a `beta3` in `{0.999,0.9999}` can achieve relatively good and stable results. See the paper for more details.\n\n## Demos\n\nFor the full list of demos, please refer to [this page](./demos).\n\n## Contributors\n\n[@luoruixuan](https://github.com/luoruixuan)\n\n\n\n\n\n\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/karrynest/AdaMod", "keywords": "machine learning,deep learning", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "adamod", "package_url": "https://pypi.org/project/adamod/", "platform": "", "project_url": "https://pypi.org/project/adamod/", "project_urls": {"Homepage": "https://github.com/karrynest/AdaMod"}, "release_url": "https://pypi.org/project/adamod/0.0.3/", "requires_dist": ["torch (>=0.4.0)"], "requires_python": ">=3.6.0", "summary": "AdaMod optimization algorithm, build on PyTorch.", "version": "0.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>\ufeff# AdaMod</p>\n<p>An optimizer which exerts adaptive momental upper bounds on individual learning rates to prevent them becoming undesirably lager than what the historical statistics suggest and avoid the non-convergence issue, thus to a better performance. Strong empirical results on many deep learning applications demonstrate the effectiveness of our proposed method especially on complex networks such as DenseNet and Transformer.</p>\n<p>&lt;p align=\u2019center\u2019&gt;&lt;img src=\u2019img/Loss.bmp\u2019 width=\u201d100%\u201d/&gt;&lt;/p&gt;</p>\n<p>## Installation</p>\n<p>AdaMod requires Python 3.6.0 or later.</p>\n<p>### Installing via pip</p>\n<p>The preferred way to install AdaMod is via <cite>pip</cite> with a virtual environment.\nJust run\n<tt>`bash\npip install adamod\n`</tt>\nin your Python environment and you are ready to go!</p>\n<p>### Using source code</p>\n<p>As AdaMod is a Python class with only 100+ lines, an alternative way is directly downloading\n[adamod.py](./adamod/adamod.py) and copying it to your project.</p>\n<p>## Usage</p>\n<p>You can use AdaMod just like any other PyTorch optimizers.</p>\n<p><tt>`python3\noptimizer = <span class=\"pre\">adamod.AdaMod(model.parameters(),</span> <span class=\"pre\">lr=1e-3,</span> beta3=0.999)\n`</tt>\nAs described in the paper, AdaMod can smooths out unexpected large learning rates throughout the training process. The <cite>beta3</cite> parameter is the smoothing coefficient for actual learning rate, which controls the average range. In common cases, a <cite>beta3</cite> in <cite>{0.999,0.9999}</cite> can achieve relatively good and stable results. See the paper for more details.</p>\n<p>## Demos</p>\n<p>For the full list of demos, please refer to [this page](./demos).</p>\n<p>## Contributors</p>\n<p>[@luoruixuan](<a href=\"https://github.com/luoruixuan\" rel=\"nofollow\">https://github.com/luoruixuan</a>)</p>\n\n          </div>"}, "last_serial": 6036268, "releases": {"0.0.3": [{"comment_text": "", "digests": {"md5": "7e91c66e891d4df1aaa702085f956524", "sha256": "fb77ec37f93d930e13adfa9646d4dfb63e91d8311df5b4dd826bdb677340fcc8"}, "downloads": -1, "filename": "adamod-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "7e91c66e891d4df1aaa702085f956524", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 5694, "upload_time": "2019-10-27T09:10:43", "upload_time_iso_8601": "2019-10-27T09:10:43.173368Z", "url": "https://files.pythonhosted.org/packages/08/84/36499b6bd4b0bd06670210b636216780699488f6c7666f766dc6f164db7d/adamod-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c420903e9a8f8d89b8175b7f481b817f", "sha256": "6e542b2982ad183ea7879671661d19acb581f8037adcb879464df6887d6df108"}, "downloads": -1, "filename": "adamod-0.0.3.tar.gz", "has_sig": false, "md5_digest": "c420903e9a8f8d89b8175b7f481b817f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 3342, "upload_time": "2019-10-27T09:10:44", "upload_time_iso_8601": "2019-10-27T09:10:44.527385Z", "url": "https://files.pythonhosted.org/packages/23/38/42505c02b17039057ea63b5047ab8ce7348ecf840161d0c865fb40543326/adamod-0.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "7e91c66e891d4df1aaa702085f956524", "sha256": "fb77ec37f93d930e13adfa9646d4dfb63e91d8311df5b4dd826bdb677340fcc8"}, "downloads": -1, "filename": "adamod-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "7e91c66e891d4df1aaa702085f956524", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 5694, "upload_time": "2019-10-27T09:10:43", "upload_time_iso_8601": "2019-10-27T09:10:43.173368Z", "url": "https://files.pythonhosted.org/packages/08/84/36499b6bd4b0bd06670210b636216780699488f6c7666f766dc6f164db7d/adamod-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c420903e9a8f8d89b8175b7f481b817f", "sha256": "6e542b2982ad183ea7879671661d19acb581f8037adcb879464df6887d6df108"}, "downloads": -1, "filename": "adamod-0.0.3.tar.gz", "has_sig": false, "md5_digest": "c420903e9a8f8d89b8175b7f481b817f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 3342, "upload_time": "2019-10-27T09:10:44", "upload_time_iso_8601": "2019-10-27T09:10:44.527385Z", "url": "https://files.pythonhosted.org/packages/23/38/42505c02b17039057ea63b5047ab8ce7348ecf840161d0c865fb40543326/adamod-0.0.3.tar.gz", "yanked": false}], "timestamp": "Thu May  7 16:23:44 2020"}