{"info": {"author": "Jonathan Schnabel", "author_email": "jonathan.schnabel31@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Financial and Insurance Industry", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Natural Language :: English", "Operating System :: Unix", "Programming Language :: Python :: 3.6"], "description": "# Finance Scraper\n\n## Table of contents\n1. [Description](README.md#description)\n2. [AWS infrastructure](README.md#aws-infrastructure)\n3. [General prerequisites](README.md#general-prerequisites)\n4. [Local installation](README.md#local-installation)\n5. [Cloud installation](README.md#cloud-installation)\n\n## Description\nThis repository contains a Extract-Transform-Load (ETL) pipeline which extracts security data details from pages on morningstar.fr (see [an example here](http://tools.morningstar.fr/fr/stockreport/default.aspx?Site=fr&id=0P0001A178&LanguageId=fr-FR&SecurityToken=0P0001A178]3]0]E0WWE$$ALL,DREXG$XHEL,DREXG$XLON,DREXG$XNYS)). The data is saved in a SQL database and feeds a web dashboard displaying graphs and tables.\nThe steps of the pipeline are:\n* extract: scrape web pages and save them raw on AWS S3.\n* transform: parse raw web pages and store the results in a CSV file, then store in S3.\n* load: copy the CSV file into a PostgresSQL database.\n\nThe pipeline can be run locally and load data in a database provided by the user, or in the Amazon Web Services cloud on an infrastructure built automatically by this software.\n\n## AWS infrastructure\nThe AWS infrastructure provisioned here is compatible with the free-tier, and should cost you a minimal amount of money once you finish your free-tier allowance.\nThe infrastructure is managed with Terraform and composed of the following elements (see picture below):\n* an EC2 instance running Apache Airflow with a Local Executor, belonging to an autoscaling group maintains a single running Airflow instance at all times.\n* a data warehouse consisting of an RDS instance running the PostgresSQL engine\n* a metadata database to store Airflow tasks status on a PostgresSQL RDS instance.\n* S3 buckets to store the Terraform state files, the scraped data, and the Airflow logs\n* EC2 instances serving the data dashboard and running a Dash + gunicorn + nginx stack, belonging to an autoscaling group which scales in and out based on CPU usage\n\n<img src=\"docs/images/finance_scraper.png\" width=\"800\" alt=\"architecture\" >\n\n## General prerequisites\n* this software is designed to work on a Linux distribution.\n* Python 3.6 or a superior version is required.\n* you need to provide a file containing a list of URLs to scrape (one URL per line, see an example URL in the description paragraph). Name this file `scraping_links.txt` and place it in folder `finance_scraping` (where the `main.py` module is located).\n\n## Local installation\n### Prerequisites\nThe following packages are will be installed along with `finance-scraping`:\n* requests >= 2.22\n* BeautifulSoup4 >= 4.7.1\n* psycopg2 >= 2.8\n\nYou need to install PostgresSQL, and create a database to load data into. Take note of the user name and password with write privileges to this database, as you will need these during the configuration of `finance-scraping`.\n\n### Installation and configuration\n* Run `pip install finance-scraping`.\n* Run `finance-scraper --local-config` to set the configuration of the scraper. You will be prompted to enter configuration values, which are saved in `~/.bashrc`.\n\n### How to run\nTo run the entire ETL pipeline, run `finance-scraper -etl`. You can also run `finance-scraper` with any of the optional arguments `-e`, `-t` or `-l` to run an individual step. It is possible to specify the date parameter `-d` with `-t` and/or `-l` to process data scraped on a specific date. For more information run help with `finance-scraper -h`.\n\n### Schedule runs with Apache Airflow\n* Read the [Airflow documentation](https://airflow.apache.org/index.html) to install and configure Airflow for your system.\n* Amend the file `finance_scraping_dag.py` from the `finance_scraping` folder with your scheduling preferences and copy it into the `dags` folder of your Airflow installation before starting the scheduler.\n\n## Cloud installation\n### Prerequisites\nIf you do not have an AWS account, create one [here](https://aws.amazon.com/console). Create a user with administrator permissions that Terraform will use to build the infrastructure by following these [instructions](https://docs.aws.amazon.com/mediapackage/latest/ug/setting-up-create-iam-user.html). Install the AWS command-line interface by running `pip install aws-cli` and configure it according to the [documentation](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).\n\nInstall [Terraform 0.12](https://learn.hashicorp.com/terraform/getting-started/install.html).\n\nYou should also have a registered domain name where to host your website. You will provide the domain name during the configuration of the infrastructure.\n\n### Installation and configuration\nRun `pip install finance-scraping` to install the software. Run `finance-scraper --configure` to start the configuration wizard, your input will be required on the command line. Parameters will be displayed one by one with the current value shown between brackets (empty brackets indicated the absence of value). It is required to provide a value for all parameters. The parameters are:\n* aws_profile: the AWS user Terraform should assume to build the infrastructure.\n* region: the AWS region where to build the infrastructure.\n* user_agent: the User-Agent to use when scraping web pages. It is recommended to provide your email address so that the website administrator can contact you if there is a problem. If you want to include spaces, put the value to this parameter between quotes.\n* max_retries: the maximum number of times the web scraper should retry a page when the request fails. 10 is an acceptable value.\n* backoff_factor: factor to calculate delay times to apply when the request receives an error. 0.3 is an acceptable value. Learn more [here](https://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html#module-urllib3.util.retry).\n* retry_on: comma-separated HTTP status codes that trigger a new request. An acceptable value is 500,502,503,504.\n* timeout: timeout for the request in seconds. An acceptable value is 20.\n* db_username: username to use when setting up the database.\n* db_password: password to use when setting up the database.\n* domain_name: the name of your website domain\n\nThen run `finance-scraper --build` to build the infrastructure in AWS. Provisioning of the AWS infrastructure will start and takes between 10 and 15 minutes.\n\nThe server hosting the dashboard will not be available until the ETL pipeline has run at least one time, so you will find an error if you try to access your website before ETL has run.\n\n### Monitoring your infrastructure\nFinance scraper does not provide any special tools to monitor your infrastructure, a good next step for this repository is to install the AWS Cloudwatch agent on every instance and send system logs to Cloudwatch Logs. When provisioning is done, you can take note of the public IP of the EC2 instance running Airflow on the AWS console and check the Airflow dashboard on port 8080. You can SSH into instance using the key `airflow-instance-ssh` located in your `~/.ssh` folder.\n\n### Destroying the infrastructure\nThe whole infrastructure can be destroyed by running `finance-scraper --nuke`.\n\n### Warning\nDo not modify the configuration files manually (`.terraform` folders, `terraform.tfstate` files and `~/.terraform_env_vars` file). This will break your installation and require painful cleaning.\n\nBe responsible when web scraping, and do not put a high burden on web servers by requesting a lot of pages in a short amount of time. It makes the job of maintaining webservers hard and you could be banned from accessing the website.", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jschnab/finance-scraping", "keywords": "security web scraping etl database", "license": "GNU General Public License v3.0", "maintainer": "", "maintainer_email": "", "name": "finance-scraping", "package_url": "https://pypi.org/project/finance-scraping/", "platform": "", "project_url": "https://pypi.org/project/finance-scraping/", "project_urls": {"Homepage": "https://github.com/jschnab/finance-scraping"}, "release_url": "https://pypi.org/project/finance-scraping/0.8.0/", "requires_dist": null, "requires_python": ">=3.6.8", "summary": "Finance Scraping ETL Pipeline", "version": "0.8.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p># Finance Scraper</p>\n<p>## Table of contents\n1. [Description](README.md#description)\n2. [AWS infrastructure](README.md#aws-infrastructure)\n3. [General prerequisites](README.md#general-prerequisites)\n4. [Local installation](README.md#local-installation)\n5. [Cloud installation](README.md#cloud-installation)</p>\n<p>## Description\nThis repository contains a Extract-Transform-Load (ETL) pipeline which extracts security data details from pages on morningstar.fr (see [an example here](<a href=\"http://tools.morningstar.fr/fr/stockreport/default.aspx?Site=fr&amp;id=0P0001A178&amp;LanguageId=fr-FR&amp;SecurityToken=0P0001A178%5D3%5D0%5DE0WWE%24%24ALL,DREXG%24XHEL,DREXG%24XLON,DREXG%24XNYS\" rel=\"nofollow\">http://tools.morningstar.fr/fr/stockreport/default.aspx?Site=fr&amp;id=0P0001A178&amp;LanguageId=fr-FR&amp;SecurityToken=0P0001A178]3]0]E0WWE$$ALL,DREXG$XHEL,DREXG$XLON,DREXG$XNYS</a>)). The data is saved in a SQL database and feeds a web dashboard displaying graphs and tables.\nThe steps of the pipeline are:\n* extract: scrape web pages and save them raw on AWS S3.\n* transform: parse raw web pages and store the results in a CSV file, then store in S3.\n* load: copy the CSV file into a PostgresSQL database.</p>\n<p>The pipeline can be run locally and load data in a database provided by the user, or in the Amazon Web Services cloud on an infrastructure built automatically by this software.</p>\n<p>## AWS infrastructure\nThe AWS infrastructure provisioned here is compatible with the free-tier, and should cost you a minimal amount of money once you finish your free-tier allowance.\nThe infrastructure is managed with Terraform and composed of the following elements (see picture below):\n* an EC2 instance running Apache Airflow with a Local Executor, belonging to an autoscaling group maintains a single running Airflow instance at all times.\n* a data warehouse consisting of an RDS instance running the PostgresSQL engine\n* a metadata database to store Airflow tasks status on a PostgresSQL RDS instance.\n* S3 buckets to store the Terraform state files, the scraped data, and the Airflow logs\n* EC2 instances serving the data dashboard and running a Dash + gunicorn + nginx stack, belonging to an autoscaling group which scales in and out based on CPU usage</p>\n<p>&lt;img src=\u201ddocs/images/finance_scraper.png\u201d width=\u201d800\u201d alt=\u201darchitecture\u201d &gt;</p>\n<p>## General prerequisites\n* this software is designed to work on a Linux distribution.\n* Python 3.6 or a superior version is required.\n* you need to provide a file containing a list of URLs to scrape (one URL per line, see an example URL in the description paragraph). Name this file <cite>scraping_links.txt</cite> and place it in folder <cite>finance_scraping</cite> (where the <cite>main.py</cite> module is located).</p>\n<p>## Local installation\n### Prerequisites\nThe following packages are will be installed along with <cite>finance-scraping</cite>:\n* requests &gt;= 2.22\n* BeautifulSoup4 &gt;= 4.7.1\n* psycopg2 &gt;= 2.8</p>\n<p>You need to install PostgresSQL, and create a database to load data into. Take note of the user name and password with write privileges to this database, as you will need these during the configuration of <cite>finance-scraping</cite>.</p>\n<p>### Installation and configuration\n* Run <cite>pip install finance-scraping</cite>.\n* Run <cite>finance-scraper \u2013local-config</cite> to set the configuration of the scraper. You will be prompted to enter configuration values, which are saved in <cite>~/.bashrc</cite>.</p>\n<p>### How to run\nTo run the entire ETL pipeline, run <cite>finance-scraper -etl</cite>. You can also run <cite>finance-scraper</cite> with any of the optional arguments <cite>-e</cite>, <cite>-t</cite> or <cite>-l</cite> to run an individual step. It is possible to specify the date parameter <cite>-d</cite> with <cite>-t</cite> and/or <cite>-l</cite> to process data scraped on a specific date. For more information run help with <cite>finance-scraper -h</cite>.</p>\n<p>### Schedule runs with Apache Airflow\n* Read the [Airflow documentation](<a href=\"https://airflow.apache.org/index.html\" rel=\"nofollow\">https://airflow.apache.org/index.html</a>) to install and configure Airflow for your system.\n* Amend the file <cite>finance_scraping_dag.py</cite> from the <cite>finance_scraping</cite> folder with your scheduling preferences and copy it into the <cite>dags</cite> folder of your Airflow installation before starting the scheduler.</p>\n<p>## Cloud installation\n### Prerequisites\nIf you do not have an AWS account, create one [here](<a href=\"https://aws.amazon.com/console\" rel=\"nofollow\">https://aws.amazon.com/console</a>). Create a user with administrator permissions that Terraform will use to build the infrastructure by following these [instructions](<a href=\"https://docs.aws.amazon.com/mediapackage/latest/ug/setting-up-create-iam-user.html\" rel=\"nofollow\">https://docs.aws.amazon.com/mediapackage/latest/ug/setting-up-create-iam-user.html</a>). Install the AWS command-line interface by running <cite>pip install aws-cli</cite> and configure it according to the [documentation](<a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\" rel=\"nofollow\">https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html</a>).</p>\n<p>Install [Terraform 0.12](<a href=\"https://learn.hashicorp.com/terraform/getting-started/install.html\" rel=\"nofollow\">https://learn.hashicorp.com/terraform/getting-started/install.html</a>).</p>\n<p>You should also have a registered domain name where to host your website. You will provide the domain name during the configuration of the infrastructure.</p>\n<p>### Installation and configuration\nRun <cite>pip install finance-scraping</cite> to install the software. Run <cite>finance-scraper \u2013configure</cite> to start the configuration wizard, your input will be required on the command line. Parameters will be displayed one by one with the current value shown between brackets (empty brackets indicated the absence of value). It is required to provide a value for all parameters. The parameters are:\n* aws_profile: the AWS user Terraform should assume to build the infrastructure.\n* region: the AWS region where to build the infrastructure.\n* user_agent: the User-Agent to use when scraping web pages. It is recommended to provide your email address so that the website administrator can contact you if there is a problem. If you want to include spaces, put the value to this parameter between quotes.\n* max_retries: the maximum number of times the web scraper should retry a page when the request fails. 10 is an acceptable value.\n* backoff_factor: factor to calculate delay times to apply when the request receives an error. 0.3 is an acceptable value. Learn more [here](<a href=\"https://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html#module-urllib3.util.retry\" rel=\"nofollow\">https://urllib3.readthedocs.io/en/latest/reference/urllib3.util.html#module-urllib3.util.retry</a>).\n* retry_on: comma-separated HTTP status codes that trigger a new request. An acceptable value is 500,502,503,504.\n* timeout: timeout for the request in seconds. An acceptable value is 20.\n* db_username: username to use when setting up the database.\n* db_password: password to use when setting up the database.\n* domain_name: the name of your website domain</p>\n<p>Then run <cite>finance-scraper \u2013build</cite> to build the infrastructure in AWS. Provisioning of the AWS infrastructure will start and takes between 10 and 15 minutes.</p>\n<p>The server hosting the dashboard will not be available until the ETL pipeline has run at least one time, so you will find an error if you try to access your website before ETL has run.</p>\n<p>### Monitoring your infrastructure\nFinance scraper does not provide any special tools to monitor your infrastructure, a good next step for this repository is to install the AWS Cloudwatch agent on every instance and send system logs to Cloudwatch Logs. When provisioning is done, you can take note of the public IP of the EC2 instance running Airflow on the AWS console and check the Airflow dashboard on port 8080. You can SSH into instance using the key <cite>airflow-instance-ssh</cite> located in your <cite>~/.ssh</cite> folder.</p>\n<p>### Destroying the infrastructure\nThe whole infrastructure can be destroyed by running <cite>finance-scraper \u2013nuke</cite>.</p>\n<p>### Warning\nDo not modify the configuration files manually (<cite>.terraform</cite> folders, <cite>terraform.tfstate</cite> files and <cite>~/.terraform_env_vars</cite> file). This will break your installation and require painful cleaning.</p>\n<p>Be responsible when web scraping, and do not put a high burden on web servers by requesting a lot of pages in a short amount of time. It makes the job of maintaining webservers hard and you could be banned from accessing the website.</p>\n\n          </div>"}, "last_serial": 7047997, "releases": {"0.2.5": [{"comment_text": "", "digests": {"md5": "af677583b5ad769819b031275d299c30", "sha256": "935a46cbbe13e0c29cb3421cb41c94ad6c3b4409ec879359877c771984b35fe4"}, "downloads": -1, "filename": "finance-scraping-0.2.5.tar.gz", "has_sig": false, "md5_digest": "af677583b5ad769819b031275d299c30", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 9971, "upload_time": "2019-09-02T00:27:24", "upload_time_iso_8601": "2019-09-02T00:27:24.326422Z", "url": "https://files.pythonhosted.org/packages/bd/2a/f64c8c5d251a237be6e24b532d962f39b47b4719cc6015e450b8628c6aaf/finance-scraping-0.2.5.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "2378c9bab361c263eb76141f0d570633", "sha256": "b6418d6d1b755252944c13c8bb2e8c0a94bb00daa1da1fb44b741df1874a2c47"}, "downloads": -1, "filename": "finance-scraping-0.3.1.tar.gz", "has_sig": false, "md5_digest": "2378c9bab361c263eb76141f0d570633", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 10912, "upload_time": "2019-09-03T00:51:10", "upload_time_iso_8601": "2019-09-03T00:51:10.920532Z", "url": "https://files.pythonhosted.org/packages/79/16/5875692214d6a64f298cde4a68b0566acc46f1dbe45ff5fa00cdb4c08061/finance-scraping-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "92f9a95b2bd1078eb6057e32c61caf97", "sha256": "044dcb22acadf0f03e300f5787597859d92f201e6d3e581b2082de44cebdfacd"}, "downloads": -1, "filename": "finance-scraping-0.3.2.tar.gz", "has_sig": false, "md5_digest": "92f9a95b2bd1078eb6057e32c61caf97", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 10923, "upload_time": "2019-09-03T02:11:26", "upload_time_iso_8601": "2019-09-03T02:11:26.818793Z", "url": "https://files.pythonhosted.org/packages/cc/56/80aef5bb67b6c53d1b6ca3116afeed9c0a8e9c2f2b471f652b7803e9ea52/finance-scraping-0.3.2.tar.gz", "yanked": false}], "0.3.5": [{"comment_text": "", "digests": {"md5": "4cb661c47bb30f01737e577bffd5acc9", "sha256": "35f1378eabd534c0680a8e150ce9320d1c0598caca8d1162a776320dcb5b9431"}, "downloads": -1, "filename": "finance-scraping-0.3.5.tar.gz", "has_sig": false, "md5_digest": "4cb661c47bb30f01737e577bffd5acc9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 11090, "upload_time": "2019-09-05T11:38:38", "upload_time_iso_8601": "2019-09-05T11:38:38.254786Z", "url": "https://files.pythonhosted.org/packages/89/dc/37cdf4b2887c8a600b31e686466b5e2075124d86a41cc3076633dde3da47/finance-scraping-0.3.5.tar.gz", "yanked": false}], "0.3.6": [{"comment_text": "", "digests": {"md5": "6cecb226a6f31a7071ffbdd409f46818", "sha256": "3ec733117808a62e83f4811468ae58f8b41bdb70963c118666d57e49074ddfef"}, "downloads": -1, "filename": "finance-scraping-0.3.6.tar.gz", "has_sig": false, "md5_digest": "6cecb226a6f31a7071ffbdd409f46818", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 11021, "upload_time": "2019-09-06T00:13:37", "upload_time_iso_8601": "2019-09-06T00:13:37.449617Z", "url": "https://files.pythonhosted.org/packages/7e/29/ea0c26097235ae565b17cfeb8d74e70fdfd42918b58d7792b36936ca4fa0/finance-scraping-0.3.6.tar.gz", "yanked": false}], "0.3.7": [{"comment_text": "", "digests": {"md5": "95421a8f61cc0be73ff800fc430b300e", "sha256": "d2f5ddc249588e12e3c71a0160abb0259d5b94c06059f13cf0af116412ea12df"}, "downloads": -1, "filename": "finance-scraping-0.3.7.tar.gz", "has_sig": false, "md5_digest": "95421a8f61cc0be73ff800fc430b300e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 11057, "upload_time": "2019-09-06T00:35:37", "upload_time_iso_8601": "2019-09-06T00:35:37.254236Z", "url": "https://files.pythonhosted.org/packages/5d/75/b662f6f3c8a78d18422871aefd557a9cba06f628b1b0561db05c42856eb6/finance-scraping-0.3.7.tar.gz", "yanked": false}], "0.3.8": [{"comment_text": "", "digests": {"md5": "03bd781aff117e737620ca1c0f328adb", "sha256": "07cdbf75e6fca68145b452033edf9c4fc30487431f84b78b57056212134423c7"}, "downloads": -1, "filename": "finance-scraping-0.3.8.tar.gz", "has_sig": false, "md5_digest": "03bd781aff117e737620ca1c0f328adb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 11586, "upload_time": "2019-09-06T02:34:22", "upload_time_iso_8601": "2019-09-06T02:34:22.107775Z", "url": "https://files.pythonhosted.org/packages/56/20/90e2289f36ada2c64f3db449c34739d4fc2077d11610eac956417f007fe6/finance-scraping-0.3.8.tar.gz", "yanked": false}], "0.3.9": [{"comment_text": "", "digests": {"md5": "56adab47dce3cd0ceb5d2907fd26a2a3", "sha256": "2f35b5bac1b95317dd1e5e612f6232ed3e53e237f527e2d87004cd0da7d060da"}, "downloads": -1, "filename": "finance-scraping-0.3.9.tar.gz", "has_sig": false, "md5_digest": "56adab47dce3cd0ceb5d2907fd26a2a3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 11594, "upload_time": "2019-09-07T13:25:00", "upload_time_iso_8601": "2019-09-07T13:25:00.140503Z", "url": "https://files.pythonhosted.org/packages/77/7a/26b1d4bffe4bfeb7c17f817b73b505ea6d5840d8e77552e45615ebe3a5b4/finance-scraping-0.3.9.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "9da399ed6233eb0da58c8818863184c5", "sha256": "8c122764a6e5d444585d3139dd2506c48473d88e84742dc01a85e23304e462f3"}, "downloads": -1, "filename": "finance-scraping-0.4.0.tar.gz", "has_sig": false, "md5_digest": "9da399ed6233eb0da58c8818863184c5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 13064, "upload_time": "2019-09-22T03:30:16", "upload_time_iso_8601": "2019-09-22T03:30:16.594555Z", "url": "https://files.pythonhosted.org/packages/8c/74/197559be91d34d5be56f277b23ad49be7181733fe407bcdf3790cb4a3eb0/finance-scraping-0.4.0.tar.gz", "yanked": false}], "0.5.0": [{"comment_text": "", "digests": {"md5": "fde97e671323a28584f8d2d510f2f0fa", "sha256": "af7c0234779dc11bdb507a29d0f8f9e13a6c1a7af71c08d29263dff837808e12"}, "downloads": -1, "filename": "finance-scraping-0.5.0.tar.gz", "has_sig": false, "md5_digest": "fde97e671323a28584f8d2d510f2f0fa", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 13845, "upload_time": "2019-09-28T20:43:32", "upload_time_iso_8601": "2019-09-28T20:43:32.607842Z", "url": "https://files.pythonhosted.org/packages/47/bb/8761bc8ecac1bb36f8cddd127b2e8e29fcc304fb0579cc0a50b66c0ada2e/finance-scraping-0.5.0.tar.gz", "yanked": false}], "0.5.1": [{"comment_text": "", "digests": {"md5": "c1faee064ab513b7e6880c7d15b3cfa7", "sha256": "68f309c8386903bf07f4a26f078eedf15ef913ee025fec35631fa72bd2c3a550"}, "downloads": -1, "filename": "finance-scraping-0.5.1.tar.gz", "has_sig": false, "md5_digest": "c1faee064ab513b7e6880c7d15b3cfa7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 13850, "upload_time": "2019-09-29T20:20:08", "upload_time_iso_8601": "2019-09-29T20:20:08.850801Z", "url": "https://files.pythonhosted.org/packages/11/d6/c84996efa01d64489f68e39f43ab3e73447886450bbb2bd0fab84c02f4cc/finance-scraping-0.5.1.tar.gz", "yanked": false}], "0.6.0": [{"comment_text": "", "digests": {"md5": "8b2a0b83d045ec32d55de7358944d118", "sha256": "ab42002e30db2812fab4548bb024e5d38ef87a7d4b76c656b8686344c9aea442"}, "downloads": -1, "filename": "finance-scraping-0.6.0.tar.gz", "has_sig": false, "md5_digest": "8b2a0b83d045ec32d55de7358944d118", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 13909, "upload_time": "2019-10-12T23:15:50", "upload_time_iso_8601": "2019-10-12T23:15:50.025581Z", "url": "https://files.pythonhosted.org/packages/93/b8/7ec8b816c25eee1b86fafce06000228bcc87461a041a73720082886c6429/finance-scraping-0.6.0.tar.gz", "yanked": false}], "0.6.1": [{"comment_text": "", "digests": {"md5": "683e7b1ac8ea86436595564ecfa55a10", "sha256": "0584bfdfc67f25eda2a82c65fdf6a0209dc5cec43178032a31fc15976ff73a6f"}, "downloads": -1, "filename": "finance-scraping-0.6.1.tar.gz", "has_sig": false, "md5_digest": "683e7b1ac8ea86436595564ecfa55a10", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 13962, "upload_time": "2019-12-01T17:54:07", "upload_time_iso_8601": "2019-12-01T17:54:07.106296Z", "url": "https://files.pythonhosted.org/packages/9f/03/ac81d1eac12860a7161a1f7450ea77d74e9924886fb501c007045779f2e8/finance-scraping-0.6.1.tar.gz", "yanked": false}], "0.6.2": [{"comment_text": "", "digests": {"md5": "b6a239455b8ae3f4e27e9d5e5c7128fb", "sha256": "750e2a85622bb8fdb89ede23c2eeabd896ee8f839e9c2235fe9eb34c46ef2a3d"}, "downloads": -1, "filename": "finance-scraping-0.6.2.tar.gz", "has_sig": false, "md5_digest": "b6a239455b8ae3f4e27e9d5e5c7128fb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 16315, "upload_time": "2019-12-18T03:57:07", "upload_time_iso_8601": "2019-12-18T03:57:07.227322Z", "url": "https://files.pythonhosted.org/packages/99/68/2554eb52c50dd5f608c1035a7477a61d4aa01e4ab8e9a9ca9fc58181721d/finance-scraping-0.6.2.tar.gz", "yanked": false}], "0.6.3": [{"comment_text": "", "digests": {"md5": "bf4cfc5e5f5a09418b8cccda12b3b6cf", "sha256": "4d1ea070c8ea5645885126a6a1a7f86035da49c718340549abee9fc4ab7ac5b6"}, "downloads": -1, "filename": "finance-scraping-0.6.3.tar.gz", "has_sig": false, "md5_digest": "bf4cfc5e5f5a09418b8cccda12b3b6cf", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 16327, "upload_time": "2019-12-18T13:28:42", "upload_time_iso_8601": "2019-12-18T13:28:42.083222Z", "url": "https://files.pythonhosted.org/packages/cc/82/c8974f05697ab5eec7a2cf4cc1de05359efd2afeb16b571e4073e1607f9a/finance-scraping-0.6.3.tar.gz", "yanked": false}], "0.6.4": [{"comment_text": "", "digests": {"md5": "0f6f0eec8db468e5a5ab77e6289fc333", "sha256": "e3584f077fa2531768cdf7146fe0e0857c4a6ae0efb99335b159bd8e04be7090"}, "downloads": -1, "filename": "finance-scraping-0.6.4.tar.gz", "has_sig": false, "md5_digest": "0f6f0eec8db468e5a5ab77e6289fc333", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 16348, "upload_time": "2020-01-20T16:26:31", "upload_time_iso_8601": "2020-01-20T16:26:31.702355Z", "url": "https://files.pythonhosted.org/packages/c5/07/8df112a07ad6a18f5f95ccc1313f3b31b2f2540ebef1314dd9c9f6482fb8/finance-scraping-0.6.4.tar.gz", "yanked": false}], "0.6.5": [{"comment_text": "", "digests": {"md5": "595a6400daac1904b3eecaa90ed1100a", "sha256": "97ef680f9e42cf420309bf2704b24fac4046564b6128bd1f9098c39c48bdc3d0"}, "downloads": -1, "filename": "finance-scraping-0.6.5.tar.gz", "has_sig": false, "md5_digest": "595a6400daac1904b3eecaa90ed1100a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 18348, "upload_time": "2020-01-20T16:42:03", "upload_time_iso_8601": "2020-01-20T16:42:03.104903Z", "url": "https://files.pythonhosted.org/packages/be/06/6432150717862759581d8cdc68a8655fe99bc742de3542eb211c889497b4/finance-scraping-0.6.5.tar.gz", "yanked": false}], "0.6.6": [{"comment_text": "", "digests": {"md5": "aa1a0a81752a9a9ed1a6ed7dca81ea05", "sha256": "f96ee11512548843892bbfb962c0b15d017e9737c629980e5f0df6708894fb5c"}, "downloads": -1, "filename": "finance-scraping-0.6.6.tar.gz", "has_sig": false, "md5_digest": "aa1a0a81752a9a9ed1a6ed7dca81ea05", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 18498, "upload_time": "2020-02-08T17:26:25", "upload_time_iso_8601": "2020-02-08T17:26:25.328880Z", "url": "https://files.pythonhosted.org/packages/88/44/3c2f903f6edb6622aa7b27b8ebbf06a761f13418374f340b8575c3fb2084/finance-scraping-0.6.6.tar.gz", "yanked": false}], "0.6.7": [{"comment_text": "", "digests": {"md5": "5c9c5562c973606746541437c0fe53aa", "sha256": "4f7a280aadefc22e3e15fe0642e34307b21a4ea2d23f88d5b31e72c94172850e"}, "downloads": -1, "filename": "finance-scraping-0.6.7.tar.gz", "has_sig": false, "md5_digest": "5c9c5562c973606746541437c0fe53aa", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 18531, "upload_time": "2020-02-08T21:24:11", "upload_time_iso_8601": "2020-02-08T21:24:11.281683Z", "url": "https://files.pythonhosted.org/packages/18/5d/f714006ffff6f49751db2c20b86de047938ff9452f0c0def723bc2600604/finance-scraping-0.6.7.tar.gz", "yanked": false}], "0.7.1": [{"comment_text": "", "digests": {"md5": "478cf811a82d22c438542d375c161c5f", "sha256": "26ccf5f40fb5abaf4aaa7ccc649ada92de73ee52803ac3934939a65af69f5722"}, "downloads": -1, "filename": "finance-scraping-0.7.1.tar.gz", "has_sig": false, "md5_digest": "478cf811a82d22c438542d375c161c5f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 18650, "upload_time": "2020-03-28T14:12:28", "upload_time_iso_8601": "2020-03-28T14:12:28.010851Z", "url": "https://files.pythonhosted.org/packages/1b/64/457c51f0cdb822255a7a989d2d82713f7f803862939d5ca0a2aee852b132/finance-scraping-0.7.1.tar.gz", "yanked": false}], "0.8.0": [{"comment_text": "", "digests": {"md5": "b6a96dddeabbfdf21fff34ea09a3b80c", "sha256": "4f821bfd629e6d96583517accb0730f04a2f2759f014aea10c4c532676c139b8"}, "downloads": -1, "filename": "finance-scraping-0.8.0.tar.gz", "has_sig": false, "md5_digest": "b6a96dddeabbfdf21fff34ea09a3b80c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 18866, "upload_time": "2020-04-18T13:56:28", "upload_time_iso_8601": "2020-04-18T13:56:28.663447Z", "url": "https://files.pythonhosted.org/packages/32/4e/31a242f71f35e26a1b79498f5bdc0ddc094e84159f581a6376009b252bb0/finance-scraping-0.8.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b6a96dddeabbfdf21fff34ea09a3b80c", "sha256": "4f821bfd629e6d96583517accb0730f04a2f2759f014aea10c4c532676c139b8"}, "downloads": -1, "filename": "finance-scraping-0.8.0.tar.gz", "has_sig": false, "md5_digest": "b6a96dddeabbfdf21fff34ea09a3b80c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.8", "size": 18866, "upload_time": "2020-04-18T13:56:28", "upload_time_iso_8601": "2020-04-18T13:56:28.663447Z", "url": "https://files.pythonhosted.org/packages/32/4e/31a242f71f35e26a1b79498f5bdc0ddc094e84159f581a6376009b252bb0/finance-scraping-0.8.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:42:26 2020"}