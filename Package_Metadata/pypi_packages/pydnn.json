{"info": {"author": "Isaac Kriegman", "author_email": "zackriegman@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: End Users/Desktop", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.4", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Image Recognition", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Software Development :: Libraries"], "description": "********************************************\npydnn: deep neural network library in Python\n********************************************\n\npydnn is a deep neural network library written in Python using `Theano <http://deeplearning.net/software/theano/>`_ (symbolic math and optimizing compiler package).  I wrote it as a learning project while competing in `Kaggle's National Data Science Bowl <http://www.datasciencebowl.com/>`_ in March 2015 (where it produced an entry finishing in the `top 6% <http://www.kaggle.com/c/datasciencebowl/leaderboard/private>`_) and plan to continue developing it by adding support for the most important deep learning techniques (including RNNs).\n\n============\nDesign Goals\n============\n\n* **Simplicity**\n    Wherever possible simplify code to make it a clear expression of underlying deep learning algorithms.  Minimize cognitive overhead, so that it is easy for someone who has completed the `deeplearning.net tutorials <http://deeplearning.net/tutorial/>`_ to pickup this library as a next step and easily start learning about, using, and coding more advanced techniques.\n\n* **Completeness**\n    Include all the important and popular techniques for effective deep learning and **not** techniques with more marginal or ambiguous benefit.\n\n* **Ease of use**\n    Make preparing a dataset, building a model and training a deep network only a few lines of code; enable users to work with NumPy rather than Theano.\n\n* **Performance**\n    Should be roughly on par with other Theano neural net libraries so that pydnn is a viable choice for computationally intensive deep learning.\n\n========\nFeatures\n========\n\n* High performance GPU training (courtesy of Theano)\n* Quick start tools to instantly get started training on `inexpensive <http://aws.amazon.com/ec2/pricing/>`_ Amazon EC2 GPU instances.\n* Implementations of important new techniques recently reported in the literature:\n    * `Batch Normalization <http://arxiv.org/pdf/1502.03167v3.pdf>`_\n    * `Parametric ReLU <http://arxiv.org/pdf/1502.01852.pdf>`_ activation function,\n    * `Adam <http://arxiv.org/pdf/1412.6980v4.pdf>`_ optimization\n    * `AdaDelta <http://arxiv.org/pdf/1212.5701v1.pdf>`_ optimization\n    * etc.\n* Implementations of standard deep learning techniques:\n    * Stochastic Gradient Descent with Momentum\n    * Dropout\n    * convolutions with max-pooling using overlapping windows\n    * ReLU/Tanh/sigmoid activation functions\n    * etc.\n\n=====\nUsage\n=====\n\nFirst download and unzip raw image data from somewhere (e.g. Kaggle). Then::\n\n    import pydnn\n    import numpy as np\n    rng = np.random.RandomState(e.rng_seed)\n\n    # build data, split into training/validation sets, preprocess\n    train_dir = 'home\\ubuntu\\train'\n    data = pydnn.data.DirectoryLabeledImageSet(train_dir).build()\n    data = pydnn.preprocess.split_training_data(data, 64, 80, 15, 5)\n    resizer = pydnn.preprocess.StretchResizer()\n    pre = pydnn.preprocess.Rotator360(data, (64, 64), resizer, rng)\n\n    # build the neural network\n    net = pydnn.nn.NN(pre, 'images', 121, 64, rng, pydnn.nn.relu)\n    net.add_convolution(72, (7, 7), (2, 2))\n    net.add_dropout()\n    net.add_convolution(128, (5, 5), (2, 2))\n    net.add_dropout()\n    net.add_convolution(128, (3, 3), (2, 2))\n    net.add_dropout()\n    net.add_hidden(3072)\n    net.add_dropout()\n    net.add_hidden(3072)\n    net.add_dropout()\n    net.add_logistic()\n\n    # train the network\n    lr = pydnn.nn.Adam(learning_rate=pydnn.nn.LearningRateDecay(\n                learning_rate=0.006,\n                decay=.1))\n    net.train(lr)\n\nFrom raw data to trained network (including specifying\nnetwork architecture) in 25 lines of code.\n\n\n================\nShort Term Goals\n================\n\n* Implement popular RNN techniques.\n* Integrate with Amazon EC2 clustering software (such as `StarCluster <http://star.mit.edu/cluster/>`_).\n* Integrate with hyper-parameter optimization frameworks (such as `Spearmint <https://github.com/JasperSnoek/spearmint>`_ and `hyperopt <https://github.com/hyperopt/hyperopt>`_).\n\n=======\nAuthors\n=======\n\nIsaac Kriegman", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/zackriegman/pydnn", "keywords": "neural network deep learning AI machine learning", "license": "MIT", "maintainer": null, "maintainer_email": null, "name": "pydnn", "package_url": "https://pypi.org/project/pydnn/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/pydnn/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/zackriegman/pydnn"}, "release_url": "https://pypi.org/project/pydnn/0.0.dev/", "requires_dist": null, "requires_python": null, "summary": "deep neural network library in Python", "version": "0.0.dev", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>pydnn is a deep neural network library written in Python using <a href=\"http://deeplearning.net/software/theano/\" rel=\"nofollow\">Theano</a> (symbolic math and optimizing compiler package).  I wrote it as a learning project while competing in <a href=\"http://www.datasciencebowl.com/\" rel=\"nofollow\">Kaggle\u2019s National Data Science Bowl</a> in March 2015 (where it produced an entry finishing in the <a href=\"http://www.kaggle.com/c/datasciencebowl/leaderboard/private\" rel=\"nofollow\">top 6%</a>) and plan to continue developing it by adding support for the most important deep learning techniques (including RNNs).</p>\n<div id=\"design-goals\">\n<h2>Design Goals</h2>\n<ul>\n<li><dl>\n<dt><strong>Simplicity</strong></dt>\n<dd>Wherever possible simplify code to make it a clear expression of underlying deep learning algorithms.  Minimize cognitive overhead, so that it is easy for someone who has completed the <a href=\"http://deeplearning.net/tutorial/\" rel=\"nofollow\">deeplearning.net tutorials</a> to pickup this library as a next step and easily start learning about, using, and coding more advanced techniques.</dd>\n</dl>\n</li>\n<li><dl>\n<dt><strong>Completeness</strong></dt>\n<dd>Include all the important and popular techniques for effective deep learning and <strong>not</strong> techniques with more marginal or ambiguous benefit.</dd>\n</dl>\n</li>\n<li><dl>\n<dt><strong>Ease of use</strong></dt>\n<dd>Make preparing a dataset, building a model and training a deep network only a few lines of code; enable users to work with NumPy rather than Theano.</dd>\n</dl>\n</li>\n<li><dl>\n<dt><strong>Performance</strong></dt>\n<dd>Should be roughly on par with other Theano neural net libraries so that pydnn is a viable choice for computationally intensive deep learning.</dd>\n</dl>\n</li>\n</ul>\n</div>\n<div id=\"features\">\n<h2>Features</h2>\n<ul>\n<li>High performance GPU training (courtesy of Theano)</li>\n<li>Quick start tools to instantly get started training on <a href=\"http://aws.amazon.com/ec2/pricing/\" rel=\"nofollow\">inexpensive</a> Amazon EC2 GPU instances.</li>\n<li><dl>\n<dt>Implementations of important new techniques recently reported in the literature:</dt>\n<dd><ul>\n<li><a href=\"http://arxiv.org/pdf/1502.03167v3.pdf\" rel=\"nofollow\">Batch Normalization</a></li>\n<li><a href=\"http://arxiv.org/pdf/1502.01852.pdf\" rel=\"nofollow\">Parametric ReLU</a> activation function,</li>\n<li><a href=\"http://arxiv.org/pdf/1412.6980v4.pdf\" rel=\"nofollow\">Adam</a> optimization</li>\n<li><a href=\"http://arxiv.org/pdf/1212.5701v1.pdf\" rel=\"nofollow\">AdaDelta</a> optimization</li>\n<li>etc.</li>\n</ul>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt>Implementations of standard deep learning techniques:</dt>\n<dd><ul>\n<li>Stochastic Gradient Descent with Momentum</li>\n<li>Dropout</li>\n<li>convolutions with max-pooling using overlapping windows</li>\n<li>ReLU/Tanh/sigmoid activation functions</li>\n<li>etc.</li>\n</ul>\n</dd>\n</dl>\n</li>\n</ul>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>First download and unzip raw image data from somewhere (e.g. Kaggle). Then:</p>\n<pre>import pydnn\nimport numpy as np\nrng = np.random.RandomState(e.rng_seed)\n\n# build data, split into training/validation sets, preprocess\ntrain_dir = 'home\\ubuntu\\train'\ndata = pydnn.data.DirectoryLabeledImageSet(train_dir).build()\ndata = pydnn.preprocess.split_training_data(data, 64, 80, 15, 5)\nresizer = pydnn.preprocess.StretchResizer()\npre = pydnn.preprocess.Rotator360(data, (64, 64), resizer, rng)\n\n# build the neural network\nnet = pydnn.nn.NN(pre, 'images', 121, 64, rng, pydnn.nn.relu)\nnet.add_convolution(72, (7, 7), (2, 2))\nnet.add_dropout()\nnet.add_convolution(128, (5, 5), (2, 2))\nnet.add_dropout()\nnet.add_convolution(128, (3, 3), (2, 2))\nnet.add_dropout()\nnet.add_hidden(3072)\nnet.add_dropout()\nnet.add_hidden(3072)\nnet.add_dropout()\nnet.add_logistic()\n\n# train the network\nlr = pydnn.nn.Adam(learning_rate=pydnn.nn.LearningRateDecay(\n            learning_rate=0.006,\n            decay=.1))\nnet.train(lr)\n</pre>\n<p>From raw data to trained network (including specifying\nnetwork architecture) in 25 lines of code.</p>\n</div>\n<div id=\"short-term-goals\">\n<h2>Short Term Goals</h2>\n<ul>\n<li>Implement popular RNN techniques.</li>\n<li>Integrate with Amazon EC2 clustering software (such as <a href=\"http://star.mit.edu/cluster/\" rel=\"nofollow\">StarCluster</a>).</li>\n<li>Integrate with hyper-parameter optimization frameworks (such as <a href=\"https://github.com/JasperSnoek/spearmint\" rel=\"nofollow\">Spearmint</a> and <a href=\"https://github.com/hyperopt/hyperopt\" rel=\"nofollow\">hyperopt</a>).</li>\n</ul>\n</div>\n<div id=\"authors\">\n<h2>Authors</h2>\n<p>Isaac Kriegman</p>\n</div>\n\n          </div>"}, "last_serial": 1478176, "releases": {"0.0.dev": [{"comment_text": "", "digests": {"md5": "6fbed30f08d8c0a780a3e3fb60de7a95", "sha256": "bf5b7a799fe8d6124b15bb18ea5cf5182a05d411db28b1aa21bd16369f5de113"}, "downloads": -1, "filename": "pydnn-0.0.dev-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "6fbed30f08d8c0a780a3e3fb60de7a95", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 75067, "upload_time": "2015-03-26T04:42:20", "upload_time_iso_8601": "2015-03-26T04:42:20.564704Z", "url": "https://files.pythonhosted.org/packages/ab/8b/fde2a005c42035569835ae962a49e958f75e41000bed86ee89f10536fa67/pydnn-0.0.dev-py2.py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "6fbed30f08d8c0a780a3e3fb60de7a95", "sha256": "bf5b7a799fe8d6124b15bb18ea5cf5182a05d411db28b1aa21bd16369f5de113"}, "downloads": -1, "filename": "pydnn-0.0.dev-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "6fbed30f08d8c0a780a3e3fb60de7a95", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 75067, "upload_time": "2015-03-26T04:42:20", "upload_time_iso_8601": "2015-03-26T04:42:20.564704Z", "url": "https://files.pythonhosted.org/packages/ab/8b/fde2a005c42035569835ae962a49e958f75e41000bed86ee89f10536fa67/pydnn-0.0.dev-py2.py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:06:54 2020"}