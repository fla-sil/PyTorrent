{"info": {"author": "Matteo Tomassetti (Data Scientist at Your.MD)", "author_email": "matteo.tomassetti@your.md", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Education", "Intended Audience :: Information Technology", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.4", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "Markov Autocomplete\n=======================\n\nHidden Markov Model to generate autocomplete suggestion.\n\nHow To Use\n=======================\n\nThe model can be trained with your own list of sentences.\n\nFor instance, if we want to train using the first two paragraphs of Robinson Crusoe\n\n\n``from markov_autocomplete.autocomplete import Autocomplete``\n\n``sentences = [\"I WAS born in the year 1632, in the city of York, of a good family, though not of that country, my father being a foreigner of Bremen, who settled first at Hull. He got a good estate by merchandise, and leaving off his trade, lived afterwards at York, from whence he had married my mother, whose relations were named Robinson, a very good family in that country, and from whom I was called Robinson Kreutznaer; but, by the usual corruption of words in England, we are now called - nay we call ourselves and write our name - Crusoe; and so my companions always called me.\", \"I had two elder brothers, one of whom was lieutenant-colonel to an English regiment of foot in Flanders, formerly commanded by the famous Colonel Lockhart, and was killed at the battle near Dunkirk against the Spaniards. What became of my second brother I never knew, any more than my father or mother knew what became of me.\"]``\n\n``ac = Autocomplete(model_path = \"ngram\", sentences = sentences, n_model=3, n_candidates=10, match_model=\"middle\", min_freq=0, punctuations=\"\", lowercase=True)``\n\n``ac.predictions(\"country\")``\n\n\n\nHow it works\n=======================\nGiven an input string that consists of `N` words `w_1, ..., w_N`, the model predicts the following word, `w_{N+1}`, from the language model.\n\nThe most probable candidate for `w_{N+1}` is computed by maximazing\n\n\n`P (w_{N+1} | w_N, ..., w_{N - O + 2})`\n\n\nwhere `O` is the order of the model.\n\nOnce the best candidate is computed the probability of the whole sentence is approximated with an n-gram model\n\n`P (w_1, ..., w_N, w_{N+1}) = PROD_i P (w_i | w_{i-N-1}, ..., w_{i-1})`\n\nFor instance, for a 2-gram model we have\n\n`P( w1, w2, w3, w4) = P(w1) P(w2|w1) P(w3|w2) P(w4|w3)`\n\nOn the other hand, for a 3-gram model we have\n\n`P( w1, w2, w3, w4) = P(w1) P(w2|w1) P(w3|w1, w2) P(w4|w2, w3)`\n\nHigher-order model will be more precise, but at the expense of generating a large list of n-grams, which may negatively impact on storage space and computational time.\n\nIf the input string contains less words than the order of the model, the autocomplete will compute the most probably n-gram of the same order of the model.", "description_content_type": null, "docs_url": null, "download_url": "https://github.com/YourMD/markov-autocomplete/tarball/autocomplete", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/YourMD/markov-autocomplete", "keywords": "autocomplete hidden-markov model nlp natural language processing", "license": "UNKNOWN", "maintainer": null, "maintainer_email": null, "name": "markov_autocomplete", "package_url": "https://pypi.org/project/markov_autocomplete/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/markov_autocomplete/", "project_urls": {"Download": "https://github.com/YourMD/markov-autocomplete/tarball/autocomplete", "Homepage": "https://github.com/YourMD/markov-autocomplete"}, "release_url": "https://pypi.org/project/markov_autocomplete/1.0.1/", "requires_dist": null, "requires_python": null, "summary": "Autocomplete model easy to integrate with Flask apps", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"markov-autocomplete\">\n<h2>Markov Autocomplete</h2>\n<p>Hidden Markov Model to generate autocomplete suggestion.</p>\n</div>\n<div id=\"how-to-use\">\n<h2>How To Use</h2>\n<p>The model can be trained with your own list of sentences.</p>\n<p>For instance, if we want to train using the first two paragraphs of Robinson Crusoe</p>\n<p><tt>from markov_autocomplete.autocomplete import Autocomplete</tt></p>\n<p><tt>sentences = [\"I WAS born in the year 1632, in the city of York, of a good family, though not of that country, my father being a foreigner of Bremen, who settled first at Hull. He got a good estate by merchandise, and leaving off his trade, lived afterwards at York, from whence he had married my mother, whose relations were named Robinson, a very good family in that country, and from whom I was called Robinson Kreutznaer; but, by the usual corruption of words in England, we are now called - nay we call ourselves and write our name - Crusoe; and so my companions always called <span class=\"pre\">me.\",</span> \"I had two elder brothers, one of whom was <span class=\"pre\">lieutenant-colonel</span> to an English regiment of foot in Flanders, formerly commanded by the famous Colonel Lockhart, and was killed at the battle near Dunkirk against the Spaniards. What became of my second brother I never knew, any more than my father or mother knew what became of <span class=\"pre\">me.\"]</span></tt></p>\n<p><tt>ac = Autocomplete(model_path = \"ngram\", sentences = sentences, n_model=3, n_candidates=10, <span class=\"pre\">match_model=\"middle\",</span> min_freq=0, <span class=\"pre\">punctuations=\"\",</span> lowercase=True)</tt></p>\n<p><tt><span class=\"pre\">ac.predictions(\"country\")</span></tt></p>\n</div>\n<div id=\"how-it-works\">\n<h2>How it works</h2>\n<p>Given an input string that consists of <cite>N</cite> words <cite>w_1, \u2026, w_N</cite>, the model predicts the following word, <cite>w_{N+1}</cite>, from the language model.</p>\n<p>The most probable candidate for <cite>w_{N+1}</cite> is computed by maximazing</p>\n<p><cite>P (w_{N+1} | w_N, \u2026, w_{N - O + 2})</cite></p>\n<p>where <cite>O</cite> is the order of the model.</p>\n<p>Once the best candidate is computed the probability of the whole sentence is approximated with an n-gram model</p>\n<p><cite>P (w_1, \u2026, w_N, w_{N+1}) = PROD_i P (w_i | w_{i-N-1}, \u2026, w_{i-1})</cite></p>\n<p>For instance, for a 2-gram model we have</p>\n<p><cite>P( w1, w2, w3, w4) = P(w1) P(w2|w1) P(w3|w2) P(w4|w3)</cite></p>\n<p>On the other hand, for a 3-gram model we have</p>\n<p><cite>P( w1, w2, w3, w4) = P(w1) P(w2|w1) P(w3|w1, w2) P(w4|w2, w3)</cite></p>\n<p>Higher-order model will be more precise, but at the expense of generating a large list of n-grams, which may negatively impact on storage space and computational time.</p>\n<p>If the input string contains less words than the order of the model, the autocomplete will compute the most probably n-gram of the same order of the model.</p>\n</div>\n\n          </div>"}, "last_serial": 2480516, "releases": {"1.0.0": [], "1.0.1": [{"comment_text": "", "digests": {"md5": "cd319533c4a86f15ed3ba086dc5881b5", "sha256": "eb8843dccaef26321adc15da8d8ecb8ce4558c0f4d961d02880d4b80b865226c"}, "downloads": -1, "filename": "markov_autocomplete-1.0.1.tar.gz", "has_sig": false, "md5_digest": "cd319533c4a86f15ed3ba086dc5881b5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5504, "upload_time": "2016-11-24T10:16:44", "upload_time_iso_8601": "2016-11-24T10:16:44.064276Z", "url": "https://files.pythonhosted.org/packages/7a/ae/e15dd8042a99783391749ba10738c1088606efccf0602e433fa9cf47e194/markov_autocomplete-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cd319533c4a86f15ed3ba086dc5881b5", "sha256": "eb8843dccaef26321adc15da8d8ecb8ce4558c0f4d961d02880d4b80b865226c"}, "downloads": -1, "filename": "markov_autocomplete-1.0.1.tar.gz", "has_sig": false, "md5_digest": "cd319533c4a86f15ed3ba086dc5881b5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5504, "upload_time": "2016-11-24T10:16:44", "upload_time_iso_8601": "2016-11-24T10:16:44.064276Z", "url": "https://files.pythonhosted.org/packages/7a/ae/e15dd8042a99783391749ba10738c1088606efccf0602e433fa9cf47e194/markov_autocomplete-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:57:41 2020"}