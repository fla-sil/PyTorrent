{"info": {"author": "Hunter Blanks", "author_email": "hblanks@artifex.org", "bugtrack_url": null, "classifiers": [], "description": "======\nLogjam\n======\n\nlogjam 0.0.4\n\nReleased: 14-May-2015\n\n.. image:: https://travis-ci.org/hblanks/logjam.png?branch=develop\n        :target: https://travis-ci.org/hblanks/logjam\n\n\nLogjam handles the (relatively) simple problem of compressing and archiving\nISO8601 logfiles. It works like this:\n\n    1. Write all your logs into hourly files with an ISO8601 in their filenames.\n    2. Run **logjam-compress** and **logjam-upload** on your log directories,\n       either via cron or as a daemon.\n    3. Your completed logfiles will automatically be compressed and uploaded to S3.\n\nHow to use it\n-------------\n\nthe logfile format\n~~~~~~~~~~~~~~~~~~\n\nYour logfiles need two things:\n\n\t#. They must contain an **UTC ISO8601 timestamp**. In\n\t   ``haproxy-20130704T0000Z-us-west-2-i-ae23fega.log``, for instance,\n\t   the timestamp is ``20130704T0000Z``.\n\t#. They need to be written hourly or more frequent than hourly. Not daily.\n\nIf you use *rsyslog* or *syslog-ng*, then chances are you already use hourly\nfiles. If not, they're very easy to configure.\n\nlogjam-compress\n~~~~~~~~~~~~~~~\n\nSample entry in ``/etc/cron.d/``::\n\n\t10 * * * * * root logjam-compress --once /var/log/my-log-dir/\n\nSample command to put in an **upstart** config file or **runit** run script::\n\n\tlogjam-compress /var/log/my-log-dir\n\n\nlogjam-upload\n~~~~~~~~~~~~~~~\n\nSample entry in ``/etc/cron.d/``::\n\n\t10 * * * * * root logjam-upload --once /var/log/my-log-dir/archive/ s3://YOUR_BUCKET/{prefix}/{year}/{month}{/{day}/{filename}\n\nSample command to put in an **upstart** config file or **runit** run script::\n\n\t logjam-upload /var/log/my-log-dir/archive/ s3://YOUR_BUCKET/{prefix}/{year}/{month}{/{day}/{filename}\n\n**A note on authentication** ``logjam-upload`` looks for the standard boto\nenvironment variables **$AWS_ACCESS_KEY_ID**, **AWS_SECRET_ACCESS_KEY**, plus\n**$AWS_DEFAULT_REGION** to figure out which S3 region to use, and what creds\nto use when connecting.\n\nIf those variables are not present, and you happen to be running\n``logjam-upload`` from an instance with an IAM role, ``logjam-upload``\nwill parse its AWS credential from that, and connect to the local S3\nregion unless told otherwise with **$AWS_DEFAULT_REGION**.\n\n\nWhat you need to get started\n----------------------------\n\nJust boto, and a bucket in S3.\n\n\nWhy is this useful?\n-------------------\n\nYou may be right to think that don't need this, because if you have any\nsignificant amount of logs, you're going to want some sort of online log\naggregation system, such as Logstash.\n\nAnd if you have a really significant amount of logs, you're going to want\na really robust, distributed system for collecting *and* storing logs, such as\nScribe.\n\nIf you're big enough to need the latter, then this is not the tool for you.\n\nBut, if you're small enough that don't want to maintain your own distributed\nsystem for storing logs, then you have two choices:\n\n    1. Implement the online log aggregation solution, and then deal with log\n       persistence, probably by setting up some sort S3 output to run on\n       your log aggregation server. This will work, although you will have a\n       SPOF where you can lose all your logs for a given time period.\n\n    2. Implement a log persistence solution whose primary machinism is\n       decoupled from your log aggregation solution.\n\nIn the second case, you no longer have a SPOF that can lose all your logs for a\ngiven time, although when you lose an individual server, you *will lose its logs\nfrom its last, partial hour.*\n\nI've been very happy with the second case, and indeed when I have to\nchoose which to have first, I always choose persistence over\naggregation. Unfortunately, I'm always writing code to take care of the\npersistence -- i.e. of compressing and uploading logfiles. So, finally,\nhere's a small, open source tool for it.\n\nRunning tests\n-------------\n\nUnit tests run with::\n\n    python setup.py test\n\nIntegration tests run with:\n\n    export SENTRY_DSN=\"https:// SOME SENTRY DSN\"\n    python tests/integration/test_all.py\n\nOr run them all with:\n\n    ./test_all.sh", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hblanks/logjam", "keywords": null, "license": "MIT License", "maintainer": null, "maintainer_email": null, "name": "logjam", "package_url": "https://pypi.org/project/logjam/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/logjam/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/hblanks/logjam"}, "release_url": "https://pypi.org/project/logjam/0.0.4/", "requires_dist": null, "requires_python": null, "summary": "Small tools for archiving ISO8601 logfiles", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>logjam 0.0.4</p>\n<p>Released: 14-May-2015</p>\n<a href=\"https://travis-ci.org/hblanks/logjam\" rel=\"nofollow\"><img alt=\"https://travis-ci.org/hblanks/logjam.png?branch=develop\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5d8b276108ad05d95851e424d1694011ad49a2a2/68747470733a2f2f7472617669732d63692e6f72672f68626c616e6b732f6c6f676a616d2e706e673f6272616e63683d646576656c6f70\"></a>\n<p>Logjam handles the (relatively) simple problem of compressing and archiving\nISO8601 logfiles. It works like this:</p>\n<blockquote>\n<ol>\n<li>Write all your logs into hourly files with an ISO8601 in their filenames.</li>\n<li>Run <strong>logjam-compress</strong> and <strong>logjam-upload</strong> on your log directories,\neither via cron or as a daemon.</li>\n<li>Your completed logfiles will automatically be compressed and uploaded to S3.</li>\n</ol>\n</blockquote>\n<div id=\"how-to-use-it\">\n<h2>How to use it</h2>\n<div id=\"the-logfile-format\">\n<h3>the logfile format</h3>\n<p>Your logfiles need two things:</p>\n<blockquote>\n<ol>\n<li>They must contain an <strong>UTC ISO8601 timestamp</strong>. In\n<tt><span class=\"pre\">haproxy-20130704T0000Z-us-west-2-i-ae23fega.log</span></tt>, for instance,\nthe timestamp is <tt>20130704T0000Z</tt>.</li>\n<li>They need to be written hourly or more frequent than hourly. Not daily.</li>\n</ol>\n</blockquote>\n<p>If you use <em>rsyslog</em> or <em>syslog-ng</em>, then chances are you already use hourly\nfiles. If not, they\u2019re very easy to configure.</p>\n</div>\n<div id=\"logjam-compress\">\n<h3>logjam-compress</h3>\n<p>Sample entry in <tt>/etc/cron.d/</tt>:</p>\n<pre>10 * * * * * root logjam-compress --once /var/log/my-log-dir/\n</pre>\n<p>Sample command to put in an <strong>upstart</strong> config file or <strong>runit</strong> run script:</p>\n<pre>logjam-compress /var/log/my-log-dir\n</pre>\n</div>\n<div id=\"logjam-upload\">\n<h3>logjam-upload</h3>\n<p>Sample entry in <tt>/etc/cron.d/</tt>:</p>\n<pre>10 * * * * * root logjam-upload --once /var/log/my-log-dir/archive/ s3://YOUR_BUCKET/{prefix}/{year}/{month}{/{day}/{filename}\n</pre>\n<p>Sample command to put in an <strong>upstart</strong> config file or <strong>runit</strong> run script:</p>\n<pre>logjam-upload /var/log/my-log-dir/archive/ s3://YOUR_BUCKET/{prefix}/{year}/{month}{/{day}/{filename}\n</pre>\n<p><strong>A note on authentication</strong> <tt><span class=\"pre\">logjam-upload</span></tt> looks for the standard boto\nenvironment variables <strong>$AWS_ACCESS_KEY_ID</strong>, <strong>AWS_SECRET_ACCESS_KEY</strong>, plus\n<strong>$AWS_DEFAULT_REGION</strong> to figure out which S3 region to use, and what creds\nto use when connecting.</p>\n<p>If those variables are not present, and you happen to be running\n<tt><span class=\"pre\">logjam-upload</span></tt> from an instance with an IAM role, <tt><span class=\"pre\">logjam-upload</span></tt>\nwill parse its AWS credential from that, and connect to the local S3\nregion unless told otherwise with <strong>$AWS_DEFAULT_REGION</strong>.</p>\n</div>\n</div>\n<div id=\"what-you-need-to-get-started\">\n<h2>What you need to get started</h2>\n<p>Just boto, and a bucket in S3.</p>\n</div>\n<div id=\"why-is-this-useful\">\n<h2>Why is this useful?</h2>\n<p>You may be right to think that don\u2019t need this, because if you have any\nsignificant amount of logs, you\u2019re going to want some sort of online log\naggregation system, such as Logstash.</p>\n<p>And if you have a really significant amount of logs, you\u2019re going to want\na really robust, distributed system for collecting <em>and</em> storing logs, such as\nScribe.</p>\n<p>If you\u2019re big enough to need the latter, then this is not the tool for you.</p>\n<p>But, if you\u2019re small enough that don\u2019t want to maintain your own distributed\nsystem for storing logs, then you have two choices:</p>\n<blockquote>\n<ol>\n<li>Implement the online log aggregation solution, and then deal with log\npersistence, probably by setting up some sort S3 output to run on\nyour log aggregation server. This will work, although you will have a\nSPOF where you can lose all your logs for a given time period.</li>\n<li>Implement a log persistence solution whose primary machinism is\ndecoupled from your log aggregation solution.</li>\n</ol>\n</blockquote>\n<p>In the second case, you no longer have a SPOF that can lose all your logs for a\ngiven time, although when you lose an individual server, you <em>will lose its logs\nfrom its last, partial hour.</em></p>\n<p>I\u2019ve been very happy with the second case, and indeed when I have to\nchoose which to have first, I always choose persistence over\naggregation. Unfortunately, I\u2019m always writing code to take care of the\npersistence \u2013 i.e. of compressing and uploading logfiles. So, finally,\nhere\u2019s a small, open source tool for it.</p>\n</div>\n<div id=\"running-tests\">\n<h2>Running tests</h2>\n<p>Unit tests run with:</p>\n<pre>python setup.py test\n</pre>\n<p>Integration tests run with:</p>\n<blockquote>\nexport SENTRY_DSN=\u201d<a href=\"https://\" rel=\"nofollow\">https://</a> SOME SENTRY DSN\u201d\npython tests/integration/test_all.py</blockquote>\n<p>Or run them all with:</p>\n<blockquote>\n./test_all.sh</blockquote>\n</div>\n\n          </div>"}, "last_serial": 1547606, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "679d074e3acc98edf5a136fe0c9ceb7a", "sha256": "3f0b7cc7ca48178547496517e70a49b3e0f1f4b8acb8bc35527426ad6318c386"}, "downloads": -1, "filename": "logjam-0.0.1.tar.gz", "has_sig": false, "md5_digest": "679d074e3acc98edf5a136fe0c9ceb7a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9438, "upload_time": "2013-09-25T00:06:36", "upload_time_iso_8601": "2013-09-25T00:06:36.056803Z", "url": "https://files.pythonhosted.org/packages/67/cd/f77fca1b600746b9e360d593ee94a99e709c05f339712c61604378a441dc/logjam-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "990ba9dbbe48c4be30bfd0a2f3136e60", "sha256": "71afd8cee69b35624efb27797de0319b0af2a5bf0477609440e52653d14da966"}, "downloads": -1, "filename": "logjam-0.0.2.tar.gz", "has_sig": false, "md5_digest": "990ba9dbbe48c4be30bfd0a2f3136e60", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9799, "upload_time": "2013-09-25T01:21:36", "upload_time_iso_8601": "2013-09-25T01:21:36.896908Z", "url": "https://files.pythonhosted.org/packages/53/44/6afd57fc70562c9391e8bbb23e4a9baab1aac8dd1d7e2f59febcfdb14d27/logjam-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "c9752b7f27bafe3a4c1335088df3d6aa", "sha256": "d867dc03ba8eaf970b84f55d467e164ca00e8f25c42616fc1c912c67ad9b276a"}, "downloads": -1, "filename": "logjam-0.0.3.tar.gz", "has_sig": false, "md5_digest": "c9752b7f27bafe3a4c1335088df3d6aa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9903, "upload_time": "2013-09-27T22:19:07", "upload_time_iso_8601": "2013-09-27T22:19:07.792291Z", "url": "https://files.pythonhosted.org/packages/d3/70/b305314446d55cf3e6d72d5f8782e74d7d938e8683f83864ea84cda501b2/logjam-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "693f7af4a9c42c7f163217a57832713f", "sha256": "e36da378e6d050d48d6c20d3bb2c17b58c9f24dc1c14c3b83c553b9a1763603e"}, "downloads": -1, "filename": "logjam-0.0.4.tar.gz", "has_sig": false, "md5_digest": "693f7af4a9c42c7f163217a57832713f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11209, "upload_time": "2015-05-14T19:37:27", "upload_time_iso_8601": "2015-05-14T19:37:27.442450Z", "url": "https://files.pythonhosted.org/packages/e0/fb/725f666a6203b233b26fc0847eb9528ee8cee3c1f86e9c43e1c9bdcb9182/logjam-0.0.4.tar.gz", "yanked": false}], "0.0.4-beta": [{"comment_text": "", "digests": {"md5": "ec8fb410fa5a7c9e41c14743c7dd90ae", "sha256": "7be5af817d4b62b911620de9ec675447e91adabd641db36313bb5e2dfe0b8b4d"}, "downloads": -1, "filename": "logjam-0.0.4-beta.tar.gz", "has_sig": false, "md5_digest": "ec8fb410fa5a7c9e41c14743c7dd90ae", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11264, "upload_time": "2015-03-25T01:08:16", "upload_time_iso_8601": "2015-03-25T01:08:16.443384Z", "url": "https://files.pythonhosted.org/packages/04/b0/5f168e108ead7282d9dec294744338a174e3f5cec488d913a6136558f6c1/logjam-0.0.4-beta.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "693f7af4a9c42c7f163217a57832713f", "sha256": "e36da378e6d050d48d6c20d3bb2c17b58c9f24dc1c14c3b83c553b9a1763603e"}, "downloads": -1, "filename": "logjam-0.0.4.tar.gz", "has_sig": false, "md5_digest": "693f7af4a9c42c7f163217a57832713f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11209, "upload_time": "2015-05-14T19:37:27", "upload_time_iso_8601": "2015-05-14T19:37:27.442450Z", "url": "https://files.pythonhosted.org/packages/e0/fb/725f666a6203b233b26fc0847eb9528ee8cee3c1f86e9c43e1c9bdcb9182/logjam-0.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:44:11 2020"}