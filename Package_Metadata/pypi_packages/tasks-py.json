{"info": {"author": "Vivek Narayanan", "author_email": "vivek@vivekn.com", "bugtrack_url": null, "classifiers": [], "description": "tasks\n===\n\n__tasks.py__ is a simple and fast task queue for executing multiple tasks in parallel. All you need to do is specify the task as a simple function that takes an argument and you get instant parallelism.\n\nBased on eventlet, multiprocessing and redis.\n\nIt is ideal for executing multiple network bound tasks in parallel from a single node, without going through the pain of setting up a map reduce cluster. It uses both processes and green threads to extract the maximum out of a single node setup.\n\nInstallation\n-----------\n\n1. Install redis and start the server, **tasks** uses redis for queueing jobs. If you already have a redis server setup, call `tasks.set_redis` and pass a redis connection object with a different database/namespace from what you normally use in your application. \n\n2. Install the redis-py and eventlet libraries.\n\t\n    `pip install redis eventlet`\n\n3. Install tasks or copy this package to your source code.\n\n    `pip install tasks-py`\n\nUsage\n-----\nImport `tasks` and call eventlet's monkey patch function in the first line of your module. Call `tasks.set_func` to register your function. This function will be receiving a string as an argument and its return value will be ignored. To indicate failure of the task, raise an error or exception within the function. Call `tasks.main()` to get the interactive command line options.  \t\n\n    import eventlet\n    eventlet.monkey_patch()\n    import tasks\n    \n    from urllib2 import urlopen\n    \n    def fetch(url):\n    \tf = open('/tmp/download', 'w')\n    \tbody = urlopen(url).read()\n    \tf.write(body)\n    \tf.close()\n    \t\n    tasks.set_func(fetch)\n    tasks.main()\n    \nNow to add jobs, create a file with one argument per line and use this command.\n\n`$ python yourfile.py add <list_of_jobs.txt>`\n\nTo start (or restart) the job processing (do this in a **screen** session or close the input stream):\n\n`$ python yourfile.py run`\n\n**tasks** has resume support, so it will start where you left off the last time.\n\nTo view the current status while it is running: \n\n`$ python yourfile.py status`\n\nOnce you are done, you can clear the logs and the completed tasks by calling reset.\n\n`$ python yourfile.py reset`\n\nSee the code or the test.py file for more information. Feel free to fork and modify this.\n\n----\n\n**Author** : Vivek Narayanan <<vivek@vivekn.com>>\n\n**License** : BSD\n\n(C) Vivek Narayanan (2014)", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/vivekn/tasks", "keywords": "redis,multiprocessing,task queue", "license": "BSD", "maintainer": null, "maintainer_email": null, "name": "tasks-py", "package_url": "https://pypi.org/project/tasks-py/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/tasks-py/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/vivekn/tasks"}, "release_url": "https://pypi.org/project/tasks-py/1.0.3/", "requires_dist": null, "requires_python": null, "summary": "A simple and fast task queue for executing multiple tasks in parallel.", "version": "1.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            tasks<br>===<br><br>__tasks.py__ is a simple and fast task queue for executing multiple tasks in parallel. All you need to do is specify the task as a simple function that takes an argument and you get instant parallelism.<br><br>Based on eventlet, multiprocessing and redis.<br><br>It is ideal for executing multiple network bound tasks in parallel from a single node, without going through the pain of setting up a map reduce cluster. It uses both processes and green threads to extract the maximum out of a single node setup.<br><br>Installation<br>-----------<br><br>1. Install redis and start the server, **tasks** uses redis for queueing jobs. If you already have a redis server setup, call `tasks.set_redis` and pass a redis connection object with a different database/namespace from what you normally use in your application. <br><br>2. Install the redis-py and eventlet libraries.<br>\t<br>    `pip install redis eventlet`<br><br>3. Install tasks or copy this package to your source code.<br><br>    `pip install tasks-py`<br><br>Usage<br>-----<br>Import `tasks` and call eventlet's monkey patch function in the first line of your module. Call `tasks.set_func` to register your function. This function will be receiving a string as an argument and its return value will be ignored. To indicate failure of the task, raise an error or exception within the function. Call `tasks.main()` to get the interactive command line options.  \t<br><br>    import eventlet<br>    eventlet.monkey_patch()<br>    import tasks<br>    <br>    from urllib2 import urlopen<br>    <br>    def fetch(url):<br>    \tf = open('/tmp/download', 'w')<br>    \tbody = urlopen(url).read()<br>    \tf.write(body)<br>    \tf.close()<br>    \t<br>    tasks.set_func(fetch)<br>    tasks.main()<br>    <br>Now to add jobs, create a file with one argument per line and use this command.<br><br>`$ python yourfile.py add &lt;list_of_jobs.txt&gt;`<br><br>To start (or restart) the job processing (do this in a **screen** session or close the input stream):<br><br>`$ python yourfile.py run`<br><br>**tasks** has resume support, so it will start where you left off the last time.<br><br>To view the current status while it is running: <br><br>`$ python yourfile.py status`<br><br>Once you are done, you can clear the logs and the completed tasks by calling reset.<br><br>`$ python yourfile.py reset`<br><br>See the code or the test.py file for more information. Feel free to fork and modify this.<br><br>----<br><br>**Author** : Vivek Narayanan &lt;&lt;vivek@vivekn.com&gt;&gt;<br><br>**License** : BSD<br><br>(C) Vivek Narayanan (2014)\n          </div>"}, "last_serial": 1342890, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "d1f0705750dafe62719d2b2fd4bda6d3", "sha256": "aff7251f65e75650bc391dd8fca38b434554bc70b44d61f65b114cc8e20f911f"}, "downloads": -1, "filename": "tasks-py-1.0.1.tar.gz", "has_sig": false, "md5_digest": "d1f0705750dafe62719d2b2fd4bda6d3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3486, "upload_time": "2014-12-06T19:12:21", "upload_time_iso_8601": "2014-12-06T19:12:21.666492Z", "url": "https://files.pythonhosted.org/packages/c9/64/4763c6b6914784a88e19a57ee2f3b8271e2de2073d00d4aec20aceff9095/tasks-py-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "6cd3c8e67e89f2832b2286d6ae7bce83", "sha256": "a67e51e984a4249784465ba4443d1af4222f3396ccfc059551c51887da5606ec"}, "downloads": -1, "filename": "tasks-py-1.0.2.tar.gz", "has_sig": false, "md5_digest": "6cd3c8e67e89f2832b2286d6ae7bce83", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3484, "upload_time": "2014-12-07T08:30:46", "upload_time_iso_8601": "2014-12-07T08:30:46.800514Z", "url": "https://files.pythonhosted.org/packages/65/a4/2bf010c7e0269df2d493feb8d0748d4e1635208d53cd434d2c838379b0e4/tasks-py-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "built for Darwin-14.0.0", "digests": {"md5": "9f4481283f7d07ad447fc3c79632d1e8", "sha256": "c9c2d68c1be8933fa0e7fbe7c329c4c5dbd5b36038beca7f3f8180eb58eee6bf"}, "downloads": -1, "filename": "tasks-py-1.0.3.macosx-10.10-intel.tar.gz", "has_sig": false, "md5_digest": "9f4481283f7d07ad447fc3c79632d1e8", "packagetype": "bdist_dumb", "python_version": "any", "requires_python": null, "size": 4811, "upload_time": "2014-12-14T04:41:01", "upload_time_iso_8601": "2014-12-14T04:41:01.658968Z", "url": "https://files.pythonhosted.org/packages/4a/bf/c7b87d36fdaab6fd28111d80a626b4d2baf09e214cac6892634e451f2be5/tasks-py-1.0.3.macosx-10.10-intel.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "765ddeab387b1ab5624f9685808726c6", "sha256": "c2c162c7e20afcaddb0bb24012d4b58182f1ebfa36f3bc2e4d1f02484dc22784"}, "downloads": -1, "filename": "tasks-py-1.0.3.tar.gz", "has_sig": false, "md5_digest": "765ddeab387b1ab5624f9685808726c6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3487, "upload_time": "2014-12-14T04:40:57", "upload_time_iso_8601": "2014-12-14T04:40:57.591536Z", "url": "https://files.pythonhosted.org/packages/f3/1b/43e081533932f54d09e2c1e1b7efb3b04d658bf28654d8132b276f4482d0/tasks-py-1.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "built for Darwin-14.0.0", "digests": {"md5": "9f4481283f7d07ad447fc3c79632d1e8", "sha256": "c9c2d68c1be8933fa0e7fbe7c329c4c5dbd5b36038beca7f3f8180eb58eee6bf"}, "downloads": -1, "filename": "tasks-py-1.0.3.macosx-10.10-intel.tar.gz", "has_sig": false, "md5_digest": "9f4481283f7d07ad447fc3c79632d1e8", "packagetype": "bdist_dumb", "python_version": "any", "requires_python": null, "size": 4811, "upload_time": "2014-12-14T04:41:01", "upload_time_iso_8601": "2014-12-14T04:41:01.658968Z", "url": "https://files.pythonhosted.org/packages/4a/bf/c7b87d36fdaab6fd28111d80a626b4d2baf09e214cac6892634e451f2be5/tasks-py-1.0.3.macosx-10.10-intel.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "765ddeab387b1ab5624f9685808726c6", "sha256": "c2c162c7e20afcaddb0bb24012d4b58182f1ebfa36f3bc2e4d1f02484dc22784"}, "downloads": -1, "filename": "tasks-py-1.0.3.tar.gz", "has_sig": false, "md5_digest": "765ddeab387b1ab5624f9685808726c6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3487, "upload_time": "2014-12-14T04:40:57", "upload_time_iso_8601": "2014-12-14T04:40:57.591536Z", "url": "https://files.pythonhosted.org/packages/f3/1b/43e081533932f54d09e2c1e1b7efb3b04d658bf28654d8132b276f4482d0/tasks-py-1.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:57:44 2020"}