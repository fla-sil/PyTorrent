{"info": {"author": "hsz", "author_email": "hsz1273327@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Documentation :: Sphinx"], "description": "\r\nkeras-attention-block\r\n===============================\r\n\r\n* version: 0.0.2\r\n\r\n* status: dev\r\n\r\n* author: hsz\r\n\r\n* email: hsz1273327@gmail.com\r\n\r\nDesc\r\n--------------------------------\r\n\r\nkeras-attention-block is an extension for keras to add attention. It was born from lack of existing function to add attention inside keras.\r\nThe module itself is pure Python with no dependencies on modules or packages outside the standard Python distribution and keras.\r\n\r\n\r\n\r\nkeywords:keras,deeplearning,attention\r\n\r\n\r\nFeature\r\n----------------------\r\n\r\n* support one dimensional attention, that is to take in inputs whose dimensions are batch_size * time_step * hidden_size\r\n* support two dimensional attention, that is to take in inputs of dimensions are batch_size * X * Y * hidden_size\r\n* support self-attention, that is to take in tensors. Four well defined calculations are included : additive, multiplicative, dot-product based and  as well as linear.\r\n* support attention, that is to take in two tensors. Three well defined calculations are included : additive, multiplicative and dot product based.\r\n* support attention. Three well defined calculations are included : additive, multiplicative and dot product based.\r\n* support multihead attention\r\n* support customized calculations of similarity between Key and Query\r\n* support customized calculations of Value\r\n\r\nExample\r\n-------------------------------\r\n\r\n.. code:: python\r\n\r\n    from keras.layers import merge\r\n    from keras.layers.core import *\r\n    from keras.layers.recurrent import LSTM\r\n    from keras.layers import Convolution2D\r\n    from keras.models import *\r\n    from keras.layers.normalization import BatchNormalization\r\n    from keras_attention_block import *\r\n\r\n    INPUT_DIM = 32\r\n    TIME_STEPS = 20\r\n    SINGLE_ATTENTION_VECTOR = False\r\n    APPLY_ATTENTION_BEFORE_LSTM = False\r\n\r\n    inputs = Input(shape=(TIME_STEPS, INPUT_DIM))\r\n    attention_mul =  SelfAttention1DLayer(similarity=\"linear\",dropout_rate=0.2)(inputs)#MyLayer((20,32))(inputs)#\r\n    lstm_units = 32\r\n    #attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\r\n    attention_mul = Flatten()(attention_mul)\r\n    output = Dense(1, activation='sigmoid')(attention_mul)\r\n    m = Model(inputs=[inputs], outputs=output)\r\n\r\n    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\r\n    print(m.summary())\r\n\r\n    train_data = np.random.random((1000,20,32))\r\n    train_lab = np.random.randint(0,2,1000)\r\n    m.fit(train_data,train_lab , epochs=1, batch_size=100 )\r\n\r\n\r\n\r\n\r\nInstall\r\n--------------------------------\r\n\r\n- ``python -m pip install keras_attention_block``\r\n\r\n\r\nDocumentation\r\n--------------------------------\r\n\r\n`Documentation on Readthedocs <https://github.com/NLP-Deeplearning-Club/keras_attention_block/>`_.\r\n\r\n\r\n\r\nTODO\r\n-----------------------------------\r\n* 3D attention\r\n\r\n\r\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/NLP-Deeplearning-Club/keras_attention_block/", "keywords": "keras", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "keras_attention_block", "package_url": "https://pypi.org/project/keras_attention_block/", "platform": "", "project_url": "https://pypi.org/project/keras_attention_block/", "project_urls": {"Homepage": "https://github.com/NLP-Deeplearning-Club/keras_attention_block/"}, "release_url": "https://pypi.org/project/keras_attention_block/0.0.2/", "requires_dist": null, "requires_python": "", "summary": "simple tools", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <ul>\n<li>version: 0.0.2</li>\n<li>status: dev</li>\n<li>author: hsz</li>\n<li>email: <a href=\"mailto:hsz1273327%40gmail.com\">hsz1273327<span>@</span>gmail<span>.</span>com</a></li>\n</ul>\n<div id=\"desc\">\n<h2>Desc</h2>\n<p>keras-attention-block is an extension for keras to add attention. It was born from lack of existing function to add attention inside keras.\nThe module itself is pure Python with no dependencies on modules or packages outside the standard Python distribution and keras.</p>\n<p>keywords:keras,deeplearning,attention</p>\n</div>\n<div id=\"feature\">\n<h2>Feature</h2>\n<ul>\n<li>support one dimensional attention, that is to take in inputs whose dimensions are batch_size * time_step * hidden_size</li>\n<li>support two dimensional attention, that is to take in inputs of dimensions are batch_size * X * Y * hidden_size</li>\n<li>support self-attention, that is to take in tensors. Four well defined calculations are included : additive, multiplicative, dot-product based and  as well as linear.</li>\n<li>support attention, that is to take in two tensors. Three well defined calculations are included : additive, multiplicative and dot product based.</li>\n<li>support attention. Three well defined calculations are included : additive, multiplicative and dot product based.</li>\n<li>support multihead attention</li>\n<li>support customized calculations of similarity between Key and Query</li>\n<li>support customized calculations of Value</li>\n</ul>\n</div>\n<div id=\"example\">\n<h2>Example</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">merge</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers.core</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers.recurrent</span> <span class=\"kn\">import</span> <span class=\"n\">LSTM</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Convolution2D</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.models</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers.normalization</span> <span class=\"kn\">import</span> <span class=\"n\">BatchNormalization</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras_attention_block</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n\n<span class=\"n\">INPUT_DIM</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n<span class=\"n\">TIME_STEPS</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n<span class=\"n\">SINGLE_ATTENTION_VECTOR</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n<span class=\"n\">APPLY_ATTENTION_BEFORE_LSTM</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">TIME_STEPS</span><span class=\"p\">,</span> <span class=\"n\">INPUT_DIM</span><span class=\"p\">))</span>\n<span class=\"n\">attention_mul</span> <span class=\"o\">=</span>  <span class=\"n\">SelfAttention1DLayer</span><span class=\"p\">(</span><span class=\"n\">similarity</span><span class=\"o\">=</span><span class=\"s2\">\"linear\"</span><span class=\"p\">,</span><span class=\"n\">dropout_rate</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)(</span><span class=\"n\">inputs</span><span class=\"p\">)</span><span class=\"c1\">#MyLayer((20,32))(inputs)#</span>\n<span class=\"n\">lstm_units</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n<span class=\"c1\">#attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)</span>\n<span class=\"n\">attention_mul</span> <span class=\"o\">=</span> <span class=\"n\">Flatten</span><span class=\"p\">()(</span><span class=\"n\">attention_mul</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'sigmoid'</span><span class=\"p\">)(</span><span class=\"n\">attention_mul</span><span class=\"p\">)</span>\n<span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">inputs</span><span class=\"p\">],</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">output</span><span class=\"p\">)</span>\n\n<span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s1\">'adam'</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s1\">'binary_crossentropy'</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'accuracy'</span><span class=\"p\">])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">())</span>\n\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"p\">((</span><span class=\"mi\">1000</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">32</span><span class=\"p\">))</span>\n<span class=\"n\">train_lab</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">1000</span><span class=\"p\">)</span>\n<span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">,</span><span class=\"n\">train_lab</span> <span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">100</span> <span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"install\">\n<h2>Install</h2>\n<ul>\n<li><tt>python <span class=\"pre\">-m</span> pip install keras_attention_block</tt></li>\n</ul>\n</div>\n<div id=\"documentation\">\n<h2>Documentation</h2>\n<p><a href=\"https://github.com/NLP-Deeplearning-Club/keras_attention_block/\" rel=\"nofollow\">Documentation on Readthedocs</a>.</p>\n</div>\n<div id=\"todo\">\n<h2>TODO</h2>\n<ul>\n<li>3D attention</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 3440074, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "395e29d94d484900c7455ead54ab8d66", "sha256": "8a0240aabc09b10f5b4466995e4f06a45b4c668d705147c006ec95a96c8f116d"}, "downloads": -1, "filename": "keras_attention_block-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "395e29d94d484900c7455ead54ab8d66", "packagetype": "bdist_wheel", "python_version": "3.6", "requires_python": null, "size": 12555, "upload_time": "2017-12-21T15:01:03", "upload_time_iso_8601": "2017-12-21T15:01:03.148994Z", "url": "https://files.pythonhosted.org/packages/a7/a4/828a700a5eab28b1c5f8da9d7d25f8252f0f46e483f5f5f8279a27cc0cd3/keras_attention_block-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "77385300bbb1b96b15c4808a0aaf87df", "sha256": "2dc8c12cf0fade431c254c4d0a9c75076c3f569cf9693ab92a194a419308889f"}, "downloads": -1, "filename": "keras_attention_block-0.0.1.tar.gz", "has_sig": false, "md5_digest": "77385300bbb1b96b15c4808a0aaf87df", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7231, "upload_time": "2017-12-21T15:00:59", "upload_time_iso_8601": "2017-12-21T15:00:59.431144Z", "url": "https://files.pythonhosted.org/packages/46/b1/53c34fea258ee96a754fc828a9d07fa4294413126a7b0f35046b344f724a/keras_attention_block-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "7dbfe2466d12d702b3677a05d770ddec", "sha256": "0fed55018ba5c51523d257c5a3a383020879ad42b48687a22be3fe9d9b8ac370"}, "downloads": -1, "filename": "keras_attention_block-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "7dbfe2466d12d702b3677a05d770ddec", "packagetype": "bdist_wheel", "python_version": "3.6", "requires_python": null, "size": 18881, "upload_time": "2017-12-24T06:06:43", "upload_time_iso_8601": "2017-12-24T06:06:43.727156Z", "url": "https://files.pythonhosted.org/packages/7f/ba/e2869c827348e609a5bff3eb9b2078ff5c52e5fa7a1f33af96800c5d6778/keras_attention_block-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1f7b29f95d3b314e0e52bcf775d41c28", "sha256": "eefc3dbe925bc4faac225405c3b29daf0699f51174011fac9dd3e2e097a70338"}, "downloads": -1, "filename": "keras_attention_block-0.0.2.tar.gz", "has_sig": false, "md5_digest": "1f7b29f95d3b314e0e52bcf775d41c28", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9704, "upload_time": "2017-12-24T06:06:40", "upload_time_iso_8601": "2017-12-24T06:06:40.495802Z", "url": "https://files.pythonhosted.org/packages/33/d1/1115a726281f49e3bf5c38b65b788485a133cdcb78d1230918e766738b89/keras_attention_block-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "7dbfe2466d12d702b3677a05d770ddec", "sha256": "0fed55018ba5c51523d257c5a3a383020879ad42b48687a22be3fe9d9b8ac370"}, "downloads": -1, "filename": "keras_attention_block-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "7dbfe2466d12d702b3677a05d770ddec", "packagetype": "bdist_wheel", "python_version": "3.6", "requires_python": null, "size": 18881, "upload_time": "2017-12-24T06:06:43", "upload_time_iso_8601": "2017-12-24T06:06:43.727156Z", "url": "https://files.pythonhosted.org/packages/7f/ba/e2869c827348e609a5bff3eb9b2078ff5c52e5fa7a1f33af96800c5d6778/keras_attention_block-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1f7b29f95d3b314e0e52bcf775d41c28", "sha256": "eefc3dbe925bc4faac225405c3b29daf0699f51174011fac9dd3e2e097a70338"}, "downloads": -1, "filename": "keras_attention_block-0.0.2.tar.gz", "has_sig": false, "md5_digest": "1f7b29f95d3b314e0e52bcf775d41c28", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9704, "upload_time": "2017-12-24T06:06:40", "upload_time_iso_8601": "2017-12-24T06:06:40.495802Z", "url": "https://files.pythonhosted.org/packages/33/d1/1115a726281f49e3bf5c38b65b788485a133cdcb78d1230918e766738b89/keras_attention_block-0.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:14 2020"}