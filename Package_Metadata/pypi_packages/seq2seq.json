{"info": {"author": "Yukio Fukuzawa", "author_email": "y.fukuzawa@massey.ac.nz", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python"], "description": "Real valued sequence to sequence autoencoder\n---\nMost sequence to sequence autoencoders I can find are suitable for categorical sequences,\n such as translation.\n \nThis auto encoder is for real valued sequences.\n\nThe input and output can be multi dimensional, have different dimensions and could even be totally different\n\n\n### Compatibility\n- This package is compatible with both Python 2 and 3\n- Example scripts in `main.py` and `main.ipynb` are also compatible with both\n- I have no plan to use tensorflow 2.0, so all the nonsense about future obsolescence is muted\n\n\n### Installation\n\n```bash\npip install seq2seq\n```\n\n\n### Usage\nFirst create a factory:\n```python\nfrom models import  NDS2SAEFactory\n\nn_iterations = 3000\nfactory = NDS2SAEFactory()\nfactory.set_output('toy.zip')\nfactory.input_dim = 2 #  Input is 2 dimensiona;\nfactory.output_dim = 1 #  Output is one dimensional\nfactory.layer_sizes = [50, 30]\n\n# Use exponential decaying learning rate, in here it starts at 0.02 then decreases exponentially to 0.00001 \nfactory.lrtype = 'expdecay'\nfactory.lrargs = dict(start_lr=0.02, finish_lr=0.00001, decay_steps=n_iterations)\n\n# Alternatively, set a constant rate is also possible, e.g. 0.001\n# factory.lrtype = 'constant'\n# factory.lrargs = dict(lr=0.001)\n\n# For the dropout layer. Default is None. If None, the dropout layer is not used\nfactory.keep_prob = 0.7\n\n# The hidden layer will be symmetric (in this case: 50:30:30:50)\n# otherwise it'll be repeated (50:30:50:30)\nfactory.symmetric = True\n\n# Save or load (and resume) from this zip file\nencoder = factory.build()\n```\n\nCreate a training sample generator and a validation sample generator. Both should have the same signature:\n```python\ndef generate_samples(batch_size):\n    \"\"\"\n    :return in_seq: a list of input sequences. Each sequence must be a np.ndarray\n            out_seq: a list of output sequences. Each sequence must be a np.ndarray\n                These sequences don't need to be the same length and don't need any padding\n                The encoder will take care of that\n            last_batch: True if this batch is the last of the iteration.\n                E.g. if you have 70000 samples and the batch size is 20000, you'll have 4 batches\n                     the last batch contains 10000 samples. You should return False for the first 3\n                     batches and True for the last one\n    \"\"\"\n    ...\n    return in_seq, out_seq, last_batch\n```\n\n#### Train\n\n```python\nencoder.train(train_generator, valid_generator, n_iterations=3000, batch_size=100, display_step=100)\n```\n\n#### Predict\n\n```python\n# test_seq is a list of np.ndarrays\npredicted = encoder.predict(test_seq)\n\n# predicted is a list of np.ndarrays. Each sequence will have the same length (due to padding)\n# Look for the stop token to truncate the padding out\n```\n\n#### Encode\n\n```python\n# test_seq is a list of np.ndarrays\nencoded = encoder.encode(test_seq)\n\n# encoded is a list of hidden-layer states corresponding to each input sequence\n```\n\n### Jupyter notebook\nOpen [main.ipynb](main.ipynb) to run the example\n\n### Licence\nMIT\n\nPRs are welcome", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/fzyukio/multidimensional-variable-length-seq2seq-autoencoder", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "seq2seq", "package_url": "https://pypi.org/project/seq2seq/", "platform": "", "project_url": "https://pypi.org/project/seq2seq/", "project_urls": {"Homepage": "https://github.com/fzyukio/multidimensional-variable-length-seq2seq-autoencoder"}, "release_url": "https://pypi.org/project/seq2seq/0.0.5/", "requires_dist": null, "requires_python": "", "summary": "Real valued sequence to sequence autoencoder", "version": "0.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h2>Real valued sequence to sequence autoencoder</h2>\n<p>Most sequence to sequence autoencoders I can find are suitable for categorical sequences,\nsuch as translation.</p>\n<p>This auto encoder is for real valued sequences.</p>\n<p>The input and output can be multi dimensional, have different dimensions and could even be totally different</p>\n<h3>Compatibility</h3>\n<ul>\n<li>This package is compatible with both Python 2 and 3</li>\n<li>Example scripts in <code>main.py</code> and <code>main.ipynb</code> are also compatible with both</li>\n<li>I have no plan to use tensorflow 2.0, so all the nonsense about future obsolescence is muted</li>\n</ul>\n<h3>Installation</h3>\n<pre>pip install seq2seq\n</pre>\n<h3>Usage</h3>\n<p>First create a factory:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">models</span> <span class=\"kn\">import</span>  <span class=\"n\">NDS2SAEFactory</span>\n\n<span class=\"n\">n_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">3000</span>\n<span class=\"n\">factory</span> <span class=\"o\">=</span> <span class=\"n\">NDS2SAEFactory</span><span class=\"p\">()</span>\n<span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">set_output</span><span class=\"p\">(</span><span class=\"s1\">'toy.zip'</span><span class=\"p\">)</span>\n<span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">input_dim</span> <span class=\"o\">=</span> <span class=\"mi\">2</span> <span class=\"c1\">#  Input is 2 dimensiona;</span>\n<span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">output_dim</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"c1\">#  Output is one dimensional</span>\n<span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">layer_sizes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Use exponential decaying learning rate, in here it starts at 0.02 then decreases exponentially to 0.00001 </span>\n<span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">lrtype</span> <span class=\"o\">=</span> <span class=\"s1\">'expdecay'</span>\n<span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">lrargs</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">start_lr</span><span class=\"o\">=</span><span class=\"mf\">0.02</span><span class=\"p\">,</span> <span class=\"n\">finish_lr</span><span class=\"o\">=</span><span class=\"mf\">0.00001</span><span class=\"p\">,</span> <span class=\"n\">decay_steps</span><span class=\"o\">=</span><span class=\"n\">n_iterations</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Alternatively, set a constant rate is also possible, e.g. 0.001</span>\n<span class=\"c1\"># factory.lrtype = 'constant'</span>\n<span class=\"c1\"># factory.lrargs = dict(lr=0.001)</span>\n\n<span class=\"c1\"># For the dropout layer. Default is None. If None, the dropout layer is not used</span>\n<span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">keep_prob</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>\n\n<span class=\"c1\"># The hidden layer will be symmetric (in this case: 50:30:30:50)</span>\n<span class=\"c1\"># otherwise it'll be repeated (50:30:50:30)</span>\n<span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">symmetric</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n\n<span class=\"c1\"># Save or load (and resume) from this zip file</span>\n<span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">()</span>\n</pre>\n<p>Create a training sample generator and a validation sample generator. Both should have the same signature:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">generate_samples</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">    :return in_seq: a list of input sequences. Each sequence must be a np.ndarray</span>\n<span class=\"sd\">            out_seq: a list of output sequences. Each sequence must be a np.ndarray</span>\n<span class=\"sd\">                These sequences don't need to be the same length and don't need any padding</span>\n<span class=\"sd\">                The encoder will take care of that</span>\n<span class=\"sd\">            last_batch: True if this batch is the last of the iteration.</span>\n<span class=\"sd\">                E.g. if you have 70000 samples and the batch size is 20000, you'll have 4 batches</span>\n<span class=\"sd\">                     the last batch contains 10000 samples. You should return False for the first 3</span>\n<span class=\"sd\">                     batches and True for the last one</span>\n<span class=\"sd\">    \"\"\"</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">in_seq</span><span class=\"p\">,</span> <span class=\"n\">out_seq</span><span class=\"p\">,</span> <span class=\"n\">last_batch</span>\n</pre>\n<h4>Train</h4>\n<pre><span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">train_generator</span><span class=\"p\">,</span> <span class=\"n\">valid_generator</span><span class=\"p\">,</span> <span class=\"n\">n_iterations</span><span class=\"o\">=</span><span class=\"mi\">3000</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">display_step</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n</pre>\n<h4>Predict</h4>\n<pre><span class=\"c1\"># test_seq is a list of np.ndarrays</span>\n<span class=\"n\">predicted</span> <span class=\"o\">=</span> <span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">test_seq</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># predicted is a list of np.ndarrays. Each sequence will have the same length (due to padding)</span>\n<span class=\"c1\"># Look for the stop token to truncate the padding out</span>\n</pre>\n<h4>Encode</h4>\n<pre><span class=\"c1\"># test_seq is a list of np.ndarrays</span>\n<span class=\"n\">encoded</span> <span class=\"o\">=</span> <span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">test_seq</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># encoded is a list of hidden-layer states corresponding to each input sequence</span>\n</pre>\n<h3>Jupyter notebook</h3>\n<p>Open <a href=\"main.ipynb\" rel=\"nofollow\">main.ipynb</a> to run the example</p>\n<h3>Licence</h3>\n<p>MIT</p>\n<p>PRs are welcome</p>\n\n          </div>"}, "last_serial": 5436000, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "7d0792f49849fc6e2fea2f79aa387824", "sha256": "3283b04c9d659a19517a3b15e49e7b7040edb32cdc466486634dcbcc20d2904b"}, "downloads": -1, "filename": "seq2seq-0.0.1.tar.gz", "has_sig": false, "md5_digest": "7d0792f49849fc6e2fea2f79aa387824", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2109, "upload_time": "2019-06-11T08:36:52", "upload_time_iso_8601": "2019-06-11T08:36:52.219539Z", "url": "https://files.pythonhosted.org/packages/34/be/2fb3f89cc0504f0cddb77f7bd7aa39355a733e8717365290421bbcd4d22b/seq2seq-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "cbc2b9dfdb09ba1c02a6ae71069c39da", "sha256": "f70674e093183ce3718c988cfcb1603e865c7fd9925dda9724db0fcc4dd86416"}, "downloads": -1, "filename": "seq2seq-0.0.2.tar.gz", "has_sig": false, "md5_digest": "cbc2b9dfdb09ba1c02a6ae71069c39da", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2115, "upload_time": "2019-06-12T08:47:18", "upload_time_iso_8601": "2019-06-12T08:47:18.674383Z", "url": "https://files.pythonhosted.org/packages/bf/40/c3a38b1ac832f66113fb0046df1e6ac40a4e4058fe4d4ab20d17cbed4f45/seq2seq-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "ffc1b1e86e24496bcaebe5fb76f90c04", "sha256": "816d9d3a0af05d70608fba51d73a522ee4ddbc0f3d0b7b33e70548fff98c1bd9"}, "downloads": -1, "filename": "seq2seq-0.0.3.tar.gz", "has_sig": false, "md5_digest": "ffc1b1e86e24496bcaebe5fb76f90c04", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2111, "upload_time": "2019-06-19T11:09:16", "upload_time_iso_8601": "2019-06-19T11:09:16.071231Z", "url": "https://files.pythonhosted.org/packages/34/f6/178779839e83de09b2cb9d201e588617483e020a517bb932d6a14038a81a/seq2seq-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "365569760c3b5d27d73b664e01eaf236", "sha256": "4546ded6d4f37204f7ecfac1983515571a0c845d004c2090a99e221158e668a1"}, "downloads": -1, "filename": "seq2seq-0.0.4.tar.gz", "has_sig": false, "md5_digest": "365569760c3b5d27d73b664e01eaf236", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2110, "upload_time": "2019-06-19T22:05:36", "upload_time_iso_8601": "2019-06-19T22:05:36.412053Z", "url": "https://files.pythonhosted.org/packages/4a/93/e10bf4e0a44cbab2b91fb96cef498f15493d9354e6e3b22ae97f8f8a076a/seq2seq-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "c0bba6ac44925776b16b68b75f8e2a97", "sha256": "17709120dd208de7c275d737d294707e12e84cef99f62e57595be12fb5b33ddc"}, "downloads": -1, "filename": "seq2seq-0.0.5.tar.gz", "has_sig": false, "md5_digest": "c0bba6ac44925776b16b68b75f8e2a97", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2790, "upload_time": "2019-06-23T00:53:26", "upload_time_iso_8601": "2019-06-23T00:53:26.518491Z", "url": "https://files.pythonhosted.org/packages/7a/e3/42b85fdaf1885de998c212afd9dda06aee4f5bded209551f5fa1b1945551/seq2seq-0.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c0bba6ac44925776b16b68b75f8e2a97", "sha256": "17709120dd208de7c275d737d294707e12e84cef99f62e57595be12fb5b33ddc"}, "downloads": -1, "filename": "seq2seq-0.0.5.tar.gz", "has_sig": false, "md5_digest": "c0bba6ac44925776b16b68b75f8e2a97", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2790, "upload_time": "2019-06-23T00:53:26", "upload_time_iso_8601": "2019-06-23T00:53:26.518491Z", "url": "https://files.pythonhosted.org/packages/7a/e3/42b85fdaf1885de998c212afd9dda06aee4f5bded209551f5fa1b1945551/seq2seq-0.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:54:54 2020"}