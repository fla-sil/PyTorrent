{"info": {"author": "Levi Borodenko", "author_email": "Levi.borodenko@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Programming Language :: Python"], "description": "# GARNN [TensorFlow]\nTensorFlow implementation of _Graphical Attention Recurrent Neural Networks_ based on work by [Cirstea et al., 2019](https://milets19.github.io/papers/milets19_paper_8.pdf).\n\nMoreover, we offer stand-alone implementations of the _Graph Attention Mechanism_ [(Veli\u010dkovi\u0107 et al., 2017)](https://arxiv.org/abs/1710.10903) and _Diffusional Graph Convolution_ [(Li et al., 2017)](https://arxiv.org/pdf/1707.01926.pdf).\n\n### Installation\nSimply run `pip install garnn`. Dependencies are `numpy; tensorflow`.\n\n### Usage\n\nThe core data structure is the _graph signal_. If we have N nodes in a graph each having F observed features then the graph signal is the tensor with shape (batch, N, F) corresponding to the data produced by all nodes. Often we have sequences of graph signals in a time series. We will call them _temporal_ graph signals and assume a shape of (batch, time steps, N, F). We also need to know the adjacency matrix E of the underlying graph with shape (N, N).\n\n#### Non-Temporal Data (batch, N, F)\nAll but the recurrent layers work with non-temporal data, i.e. the data points are individual graph signals and not sequences of graph signals.\n\nThe `AttentionMechanism` found in `garnn.components.attention` will take a graph signal and return an attention matrix as described in [Veli\u010dkovi\u0107 et al., 2017](https://arxiv.org/abs/1710.10903).\n\nThe layer is initiated with the following parameters:\n\n| Parameter | Function |\n|:------------- | :--------|\n|`F` (required) | Dimension of internal embedding.|\n|`adjacency_matrix` (required) | Adjacency matrix of the graph.|\n|`num_heads` (default: 1) | Number of attention matrices that are averaged to return the output attention.|\n|`use_reverse_diffusion` (default: True) | Whether or not to calculate A_in and A_out as done by [Cirstea et al., 2019](https://milets19.github.io/papers/milets19_paper_8.pdf). If E is symmetric then the value will be set to False.|\n\nThe output is of shape (batch, N, N). If `use_reverse_diffusion` is true then we obtain 2 attention matrices and thus the shape is (batch, 2, N, N).\n\nThe `GraphDiffusionConvolution` layer in `garnn.layers.diffconv` offers diffusion graph convolution as described by [Li et al., 2017](https://arxiv.org/pdf/1707.01926.pdf). It operates on a tuple containing a graph signal X and a transition matrix A (usually an attention matrix returned by an attention mechanism) and is initiated with the following parameters\n\n| Parameter | Function |\n|:------------- | :--------|\n|`features` (required) | Number output features. Q in the paper.|\n|`num_diffusion_steps` (required) | Number of hops done by the diffusion process. K in the paper. |\n\nThere are more specialised parameters like regularisers and initialisers -- those can be found in the doc string. The convolutional layer returns a diffused graph signal of shape (batch, N, Q).\n\nThus, if we have 10 nodes with 5 features each and we would like to apply diffusion graph convolution with 20 features using a 5-head attention mechanism with an internal embedding of 64 units then we would need to run\n\n```python\nfrom garnn.components.attention import AttentionMechanism\nfrom garnn.layers.diffconv import GraphDiffusionConvolution\nfrom tensorflow.keras.layers import Input\n\n# input of 10 by 5 graph signal\ninputs = Input(shape=(10, 5))\n\n# Initiating attention mechanism. Make sure you define E\nAttn = AttentionMechanism(64, adjacency_matrix=E, num_heads=5)(inputs)\n\n# Now the convolutional layer. Make sure you use the correct order in the\n# input-tuple: Graph signal is always first!\noutput = GraphDiffusionConvolution(\n    features=10, num_diffusion_steps=5)((inputs, Attn))\n```\n\n\n#### Temporal Data (batch, time steps, N, F)\n\nBoth `AttentionMechanism` and `DiffusionGraphConvolution` naturally extend to temporal graph signals. The output now simply has an additional time steps dimension.\n\nThe `garnn_gru` layer found in `garnn.components.garnn_gru` is the diffusional & attention-based GRU introduced by [Cirstea et al., 2019](https://milets19.github.io/papers/milets19_paper_8.pdf). It operates on temporal graph signals and an attention mechanism. Initiate with\n\n| Parameter | Function |\n|:------------- | :--------|\n|`num_hidden_features` (required) | Number of features in the hidden state. (Q in the paper)|\n|`num_diffusion_steps` (default: 5) | Number of hops done by the diffusion process in all internal convolutions. K in the paper.|\n|`return_sequence` (default: False) | Whether or not the RNN should return the hidden state at each time step or only at the final time step. Set it to `True` if you stack another RNN layer on top of this one.|\n\nHence, if we would like to rebuild a model similar to the one used by [Cirstea et al., 2019](https://milets19.github.io/papers/milets19_paper_8.pdf) then one needs to run\n\n```python\nimport numpy as np\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input\n\nfrom garnn.components.attention import AttentionMechanism\nfrom garnn.components.garnn_gru import garnn_gru\n\n# We assume we have 207 nodes with 10 features each.\n\n# For simplicity we create a random adjacency matrix E\nE = np.random.randint(0, 2, size=(207, 207))\n\n# Input of the temporal graph signals. Note that \"None\" allows\n# us to pass in variable length time series.\nX = Input(shape=(None, 207, 10))\n\n# creating attention mechanism with 3 heads and an\n# embedding-size of 16\nA = AttentionMechanism(16, adjacency_matrix=E, num_heads=3)(X)\n\n# Piping X and A into the 2 stacked GRU layers.\n# First layer streches the features into 64 diffused features.\n# We assume that we are using 6 hop diffusions.\ngru_1 = garnn_gru(num_hidden_features=64, num_diffusion_steps=6, return_sequences=True)(\n    (X, A)\n)\n\n# And then we use the 2nd GRU to shrink it back to 1 feature - the feature\n# that we are predicting. We use the same attention for this layer, but note that\n# we could've also introduce a new attention mechanism for this GRU.\noutput = garnn_gru(num_hidden_features=1, num_diffusion_steps=6)((gru_1, A))\n\ngarnn_model = Model(inputs=X, outputs=output)\n```\n\n### Contribute\nBug reports, fixes and additional features are always welcome! Make sure to run the tests with `python setup.py test` and write your own for new features. Thanks.", "description_content_type": "text/markdown; charset=UTF-8", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/LeviBorodenko/garnn", "keywords": "", "license": "mit", "maintainer": "", "maintainer_email": "", "name": "garnn", "package_url": "https://pypi.org/project/garnn/", "platform": "any", "project_url": "https://pypi.org/project/garnn/", "project_urls": {"Homepage": "https://github.com/LeviBorodenko/garnn"}, "release_url": "https://pypi.org/project/garnn/1.0.3/", "requires_dist": null, "requires_python": "", "summary": "TensorFlow implementation of the Graphical Attention Recurrent Neural Net Model", "version": "1.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>GARNN [TensorFlow]</h1>\n<p>TensorFlow implementation of <em>Graphical Attention Recurrent Neural Networks</em> based on work by <a href=\"https://milets19.github.io/papers/milets19_paper_8.pdf\" rel=\"nofollow\">Cirstea et al., 2019</a>.</p>\n<p>Moreover, we offer stand-alone implementations of the <em>Graph Attention Mechanism</em> <a href=\"https://arxiv.org/abs/1710.10903\" rel=\"nofollow\">(Veli\u010dkovi\u0107 et al., 2017)</a> and <em>Diffusional Graph Convolution</em> <a href=\"https://arxiv.org/pdf/1707.01926.pdf\" rel=\"nofollow\">(Li et al., 2017)</a>.</p>\n<h3>Installation</h3>\n<p>Simply run <code>pip install garnn</code>. Dependencies are <code>numpy; tensorflow</code>.</p>\n<h3>Usage</h3>\n<p>The core data structure is the <em>graph signal</em>. If we have N nodes in a graph each having F observed features then the graph signal is the tensor with shape (batch, N, F) corresponding to the data produced by all nodes. Often we have sequences of graph signals in a time series. We will call them <em>temporal</em> graph signals and assume a shape of (batch, time steps, N, F). We also need to know the adjacency matrix E of the underlying graph with shape (N, N).</p>\n<h4>Non-Temporal Data (batch, N, F)</h4>\n<p>All but the recurrent layers work with non-temporal data, i.e. the data points are individual graph signals and not sequences of graph signals.</p>\n<p>The <code>AttentionMechanism</code> found in <code>garnn.components.attention</code> will take a graph signal and return an attention matrix as described in <a href=\"https://arxiv.org/abs/1710.10903\" rel=\"nofollow\">Veli\u010dkovi\u0107 et al., 2017</a>.</p>\n<p>The layer is initiated with the following parameters:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Parameter</th>\n<th align=\"left\">Function</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><code>F</code> (required)</td>\n<td align=\"left\">Dimension of internal embedding.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>adjacency_matrix</code> (required)</td>\n<td align=\"left\">Adjacency matrix of the graph.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>num_heads</code> (default: 1)</td>\n<td align=\"left\">Number of attention matrices that are averaged to return the output attention.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>use_reverse_diffusion</code> (default: True)</td>\n<td align=\"left\">Whether or not to calculate A_in and A_out as done by <a href=\"https://milets19.github.io/papers/milets19_paper_8.pdf\" rel=\"nofollow\">Cirstea et al., 2019</a>. If E is symmetric then the value will be set to False.</td>\n</tr></tbody></table>\n<p>The output is of shape (batch, N, N). If <code>use_reverse_diffusion</code> is true then we obtain 2 attention matrices and thus the shape is (batch, 2, N, N).</p>\n<p>The <code>GraphDiffusionConvolution</code> layer in <code>garnn.layers.diffconv</code> offers diffusion graph convolution as described by <a href=\"https://arxiv.org/pdf/1707.01926.pdf\" rel=\"nofollow\">Li et al., 2017</a>. It operates on a tuple containing a graph signal X and a transition matrix A (usually an attention matrix returned by an attention mechanism) and is initiated with the following parameters</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Parameter</th>\n<th align=\"left\">Function</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><code>features</code> (required)</td>\n<td align=\"left\">Number output features. Q in the paper.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>num_diffusion_steps</code> (required)</td>\n<td align=\"left\">Number of hops done by the diffusion process. K in the paper.</td>\n</tr></tbody></table>\n<p>There are more specialised parameters like regularisers and initialisers -- those can be found in the doc string. The convolutional layer returns a diffused graph signal of shape (batch, N, Q).</p>\n<p>Thus, if we have 10 nodes with 5 features each and we would like to apply diffusion graph convolution with 20 features using a 5-head attention mechanism with an internal embedding of 64 units then we would need to run</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">garnn.components.attention</span> <span class=\"kn\">import</span> <span class=\"n\">AttentionMechanism</span>\n<span class=\"kn\">from</span> <span class=\"nn\">garnn.layers.diffconv</span> <span class=\"kn\">import</span> <span class=\"n\">GraphDiffusionConvolution</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Input</span>\n\n<span class=\"c1\"># input of 10 by 5 graph signal</span>\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Initiating attention mechanism. Make sure you define E</span>\n<span class=\"n\">Attn</span> <span class=\"o\">=</span> <span class=\"n\">AttentionMechanism</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">adjacency_matrix</span><span class=\"o\">=</span><span class=\"n\">E</span><span class=\"p\">,</span> <span class=\"n\">num_heads</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Now the convolutional layer. Make sure you use the correct order in the</span>\n<span class=\"c1\"># input-tuple: Graph signal is always first!</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">GraphDiffusionConvolution</span><span class=\"p\">(</span>\n    <span class=\"n\">features</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">num_diffusion_steps</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)((</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">Attn</span><span class=\"p\">))</span>\n</pre>\n<h4>Temporal Data (batch, time steps, N, F)</h4>\n<p>Both <code>AttentionMechanism</code> and <code>DiffusionGraphConvolution</code> naturally extend to temporal graph signals. The output now simply has an additional time steps dimension.</p>\n<p>The <code>garnn_gru</code> layer found in <code>garnn.components.garnn_gru</code> is the diffusional &amp; attention-based GRU introduced by <a href=\"https://milets19.github.io/papers/milets19_paper_8.pdf\" rel=\"nofollow\">Cirstea et al., 2019</a>. It operates on temporal graph signals and an attention mechanism. Initiate with</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Parameter</th>\n<th align=\"left\">Function</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><code>num_hidden_features</code> (required)</td>\n<td align=\"left\">Number of features in the hidden state. (Q in the paper)</td>\n</tr>\n<tr>\n<td align=\"left\"><code>num_diffusion_steps</code> (default: 5)</td>\n<td align=\"left\">Number of hops done by the diffusion process in all internal convolutions. K in the paper.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>return_sequence</code> (default: False)</td>\n<td align=\"left\">Whether or not the RNN should return the hidden state at each time step or only at the final time step. Set it to <code>True</code> if you stack another RNN layer on top of this one.</td>\n</tr></tbody></table>\n<p>Hence, if we would like to rebuild a model similar to the one used by <a href=\"https://milets19.github.io/papers/milets19_paper_8.pdf\" rel=\"nofollow\">Cirstea et al., 2019</a> then one needs to run</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras</span> <span class=\"kn\">import</span> <span class=\"n\">Model</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Input</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">garnn.components.attention</span> <span class=\"kn\">import</span> <span class=\"n\">AttentionMechanism</span>\n<span class=\"kn\">from</span> <span class=\"nn\">garnn.components.garnn_gru</span> <span class=\"kn\">import</span> <span class=\"n\">garnn_gru</span>\n\n<span class=\"c1\"># We assume we have 207 nodes with 10 features each.</span>\n\n<span class=\"c1\"># For simplicity we create a random adjacency matrix E</span>\n<span class=\"n\">E</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">207</span><span class=\"p\">,</span> <span class=\"mi\">207</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Input of the temporal graph signals. Note that \"None\" allows</span>\n<span class=\"c1\"># us to pass in variable length time series.</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">207</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># creating attention mechanism with 3 heads and an</span>\n<span class=\"c1\"># embedding-size of 16</span>\n<span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">AttentionMechanism</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"n\">adjacency_matrix</span><span class=\"o\">=</span><span class=\"n\">E</span><span class=\"p\">,</span> <span class=\"n\">num_heads</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Piping X and A into the 2 stacked GRU layers.</span>\n<span class=\"c1\"># First layer streches the features into 64 diffused features.</span>\n<span class=\"c1\"># We assume that we are using 6 hop diffusions.</span>\n<span class=\"n\">gru_1</span> <span class=\"o\">=</span> <span class=\"n\">garnn_gru</span><span class=\"p\">(</span><span class=\"n\">num_hidden_features</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">num_diffusion_steps</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)(</span>\n    <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">A</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># And then we use the 2nd GRU to shrink it back to 1 feature - the feature</span>\n<span class=\"c1\"># that we are predicting. We use the same attention for this layer, but note that</span>\n<span class=\"c1\"># we could've also introduce a new attention mechanism for this GRU.</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">garnn_gru</span><span class=\"p\">(</span><span class=\"n\">num_hidden_features</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">num_diffusion_steps</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">)((</span><span class=\"n\">gru_1</span><span class=\"p\">,</span> <span class=\"n\">A</span><span class=\"p\">))</span>\n\n<span class=\"n\">garnn_model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">output</span><span class=\"p\">)</span>\n</pre>\n<h3>Contribute</h3>\n<p>Bug reports, fixes and additional features are always welcome! Make sure to run the tests with <code>python setup.py test</code> and write your own for new features. Thanks.</p>\n\n          </div>"}, "last_serial": 6386946, "releases": {"0.9.2": [{"comment_text": "", "digests": {"md5": "00120e44b3981476f5de2502a9478ac3", "sha256": "ed078c97f4f8cccc3ad535a29e0d62fb0b987d83d0e6cad067ea32e99402debd"}, "downloads": -1, "filename": "garnn-0.9.2.tar.gz", "has_sig": false, "md5_digest": "00120e44b3981476f5de2502a9478ac3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22893, "upload_time": "2019-12-18T22:24:08", "upload_time_iso_8601": "2019-12-18T22:24:08.074982Z", "url": "https://files.pythonhosted.org/packages/29/af/3d48f503b7628f4895c4751551df289fae2f0b68c10c461ab1db3960feac/garnn-0.9.2.tar.gz", "yanked": false}], "0.9.3": [{"comment_text": "", "digests": {"md5": "312a26c4ccf6441b18396349fe286fd2", "sha256": "2d8889d627c404e29ee3abce4962442ccb066fa8c5e31e085243e7ebd7a8889a"}, "downloads": -1, "filename": "garnn-0.9.3.tar.gz", "has_sig": false, "md5_digest": "312a26c4ccf6441b18396349fe286fd2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22457, "upload_time": "2019-12-18T22:24:10", "upload_time_iso_8601": "2019-12-18T22:24:10.884714Z", "url": "https://files.pythonhosted.org/packages/f6/ed/991ada9cd9d034e0cba930fd0ac3c233ecd30b96891525090906d777e3e1/garnn-0.9.3.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "af323ca75431e5a7c8f7f7c1ef0f9e5a", "sha256": "e3a6f8e82c6bdb30fcbd2571ac934b64a9eb75ce96123c567389054a706d1e80"}, "downloads": -1, "filename": "garnn-1.0.0.tar.gz", "has_sig": false, "md5_digest": "af323ca75431e5a7c8f7f7c1ef0f9e5a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22473, "upload_time": "2019-12-18T22:29:53", "upload_time_iso_8601": "2019-12-18T22:29:53.016538Z", "url": "https://files.pythonhosted.org/packages/14/19/8d19de9cfa5920468b5a574c8550d32e425332912d800286eb34792778af/garnn-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "3dccd1b08834b0045dc8573fdb35674a", "sha256": "912d5e9fe1394b5000dba682a3b69be52f63d03f220ea5e954fcc4c5cd1e328b"}, "downloads": -1, "filename": "garnn-1.0.1.tar.gz", "has_sig": false, "md5_digest": "3dccd1b08834b0045dc8573fdb35674a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22588, "upload_time": "2019-12-29T17:49:40", "upload_time_iso_8601": "2019-12-29T17:49:40.701692Z", "url": "https://files.pythonhosted.org/packages/83/f6/d00dc7a63ab50c166c934f4dfdba58f04071cc1bf8a469b6b042887f382f/garnn-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "aaf39cbb87ab94c5dd6334f00d7c6539", "sha256": "44e587b80eae30ccfe01a48fa33f438d550888b90530aa6270c49a0d01122f27"}, "downloads": -1, "filename": "garnn-1.0.2.tar.gz", "has_sig": false, "md5_digest": "aaf39cbb87ab94c5dd6334f00d7c6539", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24215, "upload_time": "2020-01-01T15:13:55", "upload_time_iso_8601": "2020-01-01T15:13:55.262760Z", "url": "https://files.pythonhosted.org/packages/2a/56/be32aaec90c71978bfe32515763fd7b2cce95325f9457986b1bc9da63bd8/garnn-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "1c6ec201d9f453d0b4790ce23d2797ea", "sha256": "81139a6f1252871da1c788a43485f0599e605291590de2a2670f808b756f9ae6"}, "downloads": -1, "filename": "garnn-1.0.3.tar.gz", "has_sig": false, "md5_digest": "1c6ec201d9f453d0b4790ce23d2797ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24335, "upload_time": "2020-01-02T17:08:34", "upload_time_iso_8601": "2020-01-02T17:08:34.008590Z", "url": "https://files.pythonhosted.org/packages/0b/05/ca4c0a2a6e489586c38d0feec42bdab8f40bb65a2f37bb37b6aaef9bea35/garnn-1.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1c6ec201d9f453d0b4790ce23d2797ea", "sha256": "81139a6f1252871da1c788a43485f0599e605291590de2a2670f808b756f9ae6"}, "downloads": -1, "filename": "garnn-1.0.3.tar.gz", "has_sig": false, "md5_digest": "1c6ec201d9f453d0b4790ce23d2797ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24335, "upload_time": "2020-01-02T17:08:34", "upload_time_iso_8601": "2020-01-02T17:08:34.008590Z", "url": "https://files.pythonhosted.org/packages/0b/05/ca4c0a2a6e489586c38d0feec42bdab8f40bb65a2f37bb37b6aaef9bea35/garnn-1.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:59:02 2020"}