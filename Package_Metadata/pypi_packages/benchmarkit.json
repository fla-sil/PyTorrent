{"info": {"author": "Vitaliy Grabovets", "author_email": "v.grabovets@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# Benchmarkit\n\n[![PyPI version](https://badge.fury.io/py/benchmarkit.svg)](https://badge.fury.io/py/benchmarkit)\n[![Build Status](https://travis-ci.org/vgrabovets/benchmarkit.svg?branch=master)](https://travis-ci.org/vgrabovets/benchmarkit)\n[![codecov](https://codecov.io/gh/vgrabovets/benchmarkit/branch/master/graph/badge.svg)](https://codecov.io/gh/vgrabovets/benchmarkit)\n[![CodeFactor](https://www.codefactor.io/repository/github/vgrabovets/benchmarkit/badge)](https://www.codefactor.io/repository/github/vgrabovets/benchmarkit)\n[![Dependabot Status](https://api.dependabot.com/badges/status?host=github&repo=vgrabovets/benchmarkit)](https://dependabot.com)\n\nBenchmark and analyze functions' time execution and results over the course of development. \n\n## Features\n\n- No boilerplate code\n- Saves history and additional info\n- Saves function output and parameters to benchmark data science tasks\n- Easy to analyze results\n- Disables garbage collector during benchmarking\n\n## Motivation\n\n- I need to benchmark execution time of my function\n- I don't want to memorize and write boilerplate code\n- I want to compare results with previous runs before some changes were introduced\n- I don't want to manually write down results somewhere\n- I want to know exact commits of my previous runs months ago\n- I want to benchmark accuracy, precision, recall of my models and keep track of hyperparameters \n\n## Installation\n\n```text\npip install benchmarkit\n```\n\n## Usage\n### Benchmark execution times\n\nPut `@benchmark` decorator over function with piece of code that should be timed\n\n```python\nfrom benchmarkit import benchmark, benchmark_run\n\nN = 10000\nseq_list = list(range(N))\nseq_set = set(range(N))\n\nSAVE_PATH = '/tmp/benchmark_time.jsonl'\n\n\n@benchmark(num_iters=100, save_params=True, save_output=False)\ndef search_in_list(num_items=N):\n    return num_items - 1 in seq_list\n\n\n@benchmark(num_iters=100, save_params=True, save_output=False)\ndef search_in_set(num_items=N):\n    return num_items - 1 in seq_set\n```\n\n- __num_iters__ - how many times to repeat benchmarked function. Default _1_\n- __save_params__ - save parameters passed to the benchmarked function in the file with benchmark results. In the example above `num_items` will be saved. Default _False_\n- __save_output__ - save benchmarked function output. Should return dict `{'name': value}`. Default _False_. See example how to benchmark model results.\n\nRun benchmark:\n\n```python\nbenchmark_results = benchmark_run(\n    [search_in_list, search_in_set],\n    SAVE_PATH,\n    comment='initial benchmark search',\n    rows_limit=10,\n    extra_fields=['num_items'],\n    metric='mean_time',\n    bigger_is_better=False,\n)  \n```\n\n- __functions__ - function or list of functions with `benchmark` decorator\n- __save_file__ - path to file where to save results\n- __comment__ - comment to save alongside the results\n- __rows_limit__ - limit table rows in console output. Default _10_\n- __extra_fields__ - extra fields to include in console output\n- __metric__ - metric which is used for comparison. Default `mean_time`\n- __bigger_is_better__ - whether bigger value of metric indicates that result is better. For time benchmarks should be `False`, for model accuracy should be `True`. Default _False_\n\nPrints to terminal and returns list of dictionaries with data for the last run.\n\n![Benchmark time output1](https://raw.githubusercontent.com/vgrabovets/benchmarkit/master/img/benchmark_time1.jpg)\n\nChange `N=1000000` and rerun\n\n![Benchmark time output2](https://raw.githubusercontent.com/vgrabovets/benchmarkit/master/img/benchmark_time2.jpg)\n\nThe same can be run from command line:\n```text\nbenchmark_run test_data/time/benchmark_functions.py --save_dir /tmp/ --comment \"million items\" --extra_fields num_items\n```\n\n### Benchmark model results\n\n```python\nfrom benchmarkit import benchmark, benchmark_run\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\nMODEL_BENCHMARK_SAVE_FILE = '/tmp/benchmark_model.jsonl'\n\nx, y = load_iris(return_X_y=True)\n\n@benchmark(save_params=True, save_output=True)\ndef log_regression(C=1.0, fit_intercept=True):\n    clf = LogisticRegression(\n        random_state=0, \n        solver='lbfgs', \n        multi_class='multinomial', \n        C=C,\n        fit_intercept=fit_intercept,\n    )\n    clf.fit(x, y)\n    score = clf.score(x, y)\n    return {'score': score}\n\nmodel_benchmark_results = benchmark_run(\n    log_regression,\n    MODEL_BENCHMARK_SAVE_FILE,\n    comment='baseline model',\n    extra_fields=['C', 'fit_intercept'],\n    metric='score',\n    bigger_is_better=True,\n)\n```\n\n![Benchmark model1](https://raw.githubusercontent.com/vgrabovets/benchmarkit/master/img/benchmark_model1.jpg)\n\nChange hyperparameter `C=0.5` and rerun. Output:\n\n![Benchmark model2](https://raw.githubusercontent.com/vgrabovets/benchmarkit/master/img/benchmark_model2.jpg)\n\nThe same can be run from command line:\n```text\nbenchmark_run file_with_benchmark.py --save_dir /tmp/ --comment \"stronger regularization\" --extra_fields C fit_intercept --metric score --bigger_is_better\n```\n\n### Analyze results from the file\n\n```python\nfrom benchmarkit import benchmark_analyze\n\nSAVE_PATH = '/tmp/benchmark_time.jsonl'\n\nbenchmark_df = benchmark_analyze(\n    SAVE_PATH,\n    func_name=None, \n    rows_limit=10,\n    metric='mean_time',\n    bigger_is_better=False,\n    extra_fields=['num_items'],\n)\n```\n\n- __input_path__ - path to `.jsonl` file or directory with `.jsonl` files with benchmark results \n- __func_name__ - display statistics for particular function. If `None` then all functions, stored in file, are displayed. Default _None_\n- __rows_limit__ - limit table rows in console output. Default _10_\n- __metric__ - metric which is used for comparison. Default `mean_time`\n- __bigger_is_better__ - whether bigger value of metric indicates that result is better. For time benchmarks should be `False`, for model accuracy should be `True`. Default _False_\n- __extra_fields__ - extra fields to include in console output\n\nPrints to terminal and returns pandas `DataFrame`.\n\n![Benchmark analyze](https://raw.githubusercontent.com/vgrabovets/benchmarkit/master/img/benchmark_analyze.jpg)\n\nThe same can be run from command line:\n```text\nbenchmark_analyze /tmp/benchmark_time.jsonl --extra_fields num_items\n```\n\n[Other examples](https://nbviewer.jupyter.org/github/vgrabovets/benchmarkit/blob/master/notebooks/benchmark_examples.ipynb)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/vgrabovets/benchmarkit", "keywords": "benchmark,timeit,time", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "benchmarkit", "package_url": "https://pypi.org/project/benchmarkit/", "platform": "", "project_url": "https://pypi.org/project/benchmarkit/", "project_urls": {"Homepage": "https://github.com/vgrabovets/benchmarkit"}, "release_url": "https://pypi.org/project/benchmarkit/0.0.4/", "requires_dist": ["colorama (==0.4.1)", "GitPython (==2.1.15)", "pandas (==0.24.2)", "path.py (==12.0.1)"], "requires_python": ">=3.6.0", "summary": "Benchmark and analyze functions' time execution and results over the course of development", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Benchmarkit</h1>\n<p><a href=\"https://badge.fury.io/py/benchmarkit\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b3754435ef1e6640b51c20ef83dd79740ae86070/68747470733a2f2f62616467652e667572792e696f2f70792f62656e63686d61726b69742e737667\"></a>\n<a href=\"https://travis-ci.org/vgrabovets/benchmarkit\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a6fd0eb4b11fd010184368de76a66c15affd9ce6/68747470733a2f2f7472617669732d63692e6f72672f76677261626f766574732f62656e63686d61726b69742e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://codecov.io/gh/vgrabovets/benchmarkit\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/518f7031373c2d256cfd7f118b844c3e0986ea5f/68747470733a2f2f636f6465636f762e696f2f67682f76677261626f766574732f62656e63686d61726b69742f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<a href=\"https://www.codefactor.io/repository/github/vgrabovets/benchmarkit\" rel=\"nofollow\"><img alt=\"CodeFactor\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7da88f800a3d548a33ba06b945b15c4304b62f5c/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f76677261626f766574732f62656e63686d61726b69742f6261646765\"></a>\n<a href=\"https://dependabot.com\" rel=\"nofollow\"><img alt=\"Dependabot Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ce928ce64e7029428f0bf0c544fa888de4adf5b3/68747470733a2f2f6170692e646570656e6461626f742e636f6d2f6261646765732f7374617475733f686f73743d676974687562267265706f3d76677261626f766574732f62656e63686d61726b6974\"></a></p>\n<p>Benchmark and analyze functions' time execution and results over the course of development.</p>\n<h2>Features</h2>\n<ul>\n<li>No boilerplate code</li>\n<li>Saves history and additional info</li>\n<li>Saves function output and parameters to benchmark data science tasks</li>\n<li>Easy to analyze results</li>\n<li>Disables garbage collector during benchmarking</li>\n</ul>\n<h2>Motivation</h2>\n<ul>\n<li>I need to benchmark execution time of my function</li>\n<li>I don't want to memorize and write boilerplate code</li>\n<li>I want to compare results with previous runs before some changes were introduced</li>\n<li>I don't want to manually write down results somewhere</li>\n<li>I want to know exact commits of my previous runs months ago</li>\n<li>I want to benchmark accuracy, precision, recall of my models and keep track of hyperparameters</li>\n</ul>\n<h2>Installation</h2>\n<pre>pip install benchmarkit\n</pre>\n<h2>Usage</h2>\n<h3>Benchmark execution times</h3>\n<p>Put <code>@benchmark</code> decorator over function with piece of code that should be timed</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">benchmarkit</span> <span class=\"kn\">import</span> <span class=\"n\">benchmark</span><span class=\"p\">,</span> <span class=\"n\">benchmark_run</span>\n\n<span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span>\n<span class=\"n\">seq_list</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">))</span>\n<span class=\"n\">seq_set</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">))</span>\n\n<span class=\"n\">SAVE_PATH</span> <span class=\"o\">=</span> <span class=\"s1\">'/tmp/benchmark_time.jsonl'</span>\n\n\n<span class=\"nd\">@benchmark</span><span class=\"p\">(</span><span class=\"n\">num_iters</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">save_params</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">save_output</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">search_in_list</span><span class=\"p\">(</span><span class=\"n\">num_items</span><span class=\"o\">=</span><span class=\"n\">N</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">num_items</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"ow\">in</span> <span class=\"n\">seq_list</span>\n\n\n<span class=\"nd\">@benchmark</span><span class=\"p\">(</span><span class=\"n\">num_iters</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">save_params</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">save_output</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">search_in_set</span><span class=\"p\">(</span><span class=\"n\">num_items</span><span class=\"o\">=</span><span class=\"n\">N</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">num_items</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"ow\">in</span> <span class=\"n\">seq_set</span>\n</pre>\n<ul>\n<li><strong>num_iters</strong> - how many times to repeat benchmarked function. Default <em>1</em></li>\n<li><strong>save_params</strong> - save parameters passed to the benchmarked function in the file with benchmark results. In the example above <code>num_items</code> will be saved. Default <em>False</em></li>\n<li><strong>save_output</strong> - save benchmarked function output. Should return dict <code>{'name': value}</code>. Default <em>False</em>. See example how to benchmark model results.</li>\n</ul>\n<p>Run benchmark:</p>\n<pre><span class=\"n\">benchmark_results</span> <span class=\"o\">=</span> <span class=\"n\">benchmark_run</span><span class=\"p\">(</span>\n    <span class=\"p\">[</span><span class=\"n\">search_in_list</span><span class=\"p\">,</span> <span class=\"n\">search_in_set</span><span class=\"p\">],</span>\n    <span class=\"n\">SAVE_PATH</span><span class=\"p\">,</span>\n    <span class=\"n\">comment</span><span class=\"o\">=</span><span class=\"s1\">'initial benchmark search'</span><span class=\"p\">,</span>\n    <span class=\"n\">rows_limit</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n    <span class=\"n\">extra_fields</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'num_items'</span><span class=\"p\">],</span>\n    <span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"s1\">'mean_time'</span><span class=\"p\">,</span>\n    <span class=\"n\">bigger_is_better</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>  \n</pre>\n<ul>\n<li><strong>functions</strong> - function or list of functions with <code>benchmark</code> decorator</li>\n<li><strong>save_file</strong> - path to file where to save results</li>\n<li><strong>comment</strong> - comment to save alongside the results</li>\n<li><strong>rows_limit</strong> - limit table rows in console output. Default <em>10</em></li>\n<li><strong>extra_fields</strong> - extra fields to include in console output</li>\n<li><strong>metric</strong> - metric which is used for comparison. Default <code>mean_time</code></li>\n<li><strong>bigger_is_better</strong> - whether bigger value of metric indicates that result is better. For time benchmarks should be <code>False</code>, for model accuracy should be <code>True</code>. Default <em>False</em></li>\n</ul>\n<p>Prints to terminal and returns list of dictionaries with data for the last run.</p>\n<p><img alt=\"Benchmark time output1\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c25fac73d9ffe5148062de3e7aea748b8c66b933/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f76677261626f766574732f62656e63686d61726b69742f6d61737465722f696d672f62656e63686d61726b5f74696d65312e6a7067\"></p>\n<p>Change <code>N=1000000</code> and rerun</p>\n<p><img alt=\"Benchmark time output2\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/be908666a7926a0e7986b3799303b87179c18e85/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f76677261626f766574732f62656e63686d61726b69742f6d61737465722f696d672f62656e63686d61726b5f74696d65322e6a7067\"></p>\n<p>The same can be run from command line:</p>\n<pre>benchmark_run test_data/time/benchmark_functions.py --save_dir /tmp/ --comment \"million items\" --extra_fields num_items\n</pre>\n<h3>Benchmark model results</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">benchmarkit</span> <span class=\"kn\">import</span> <span class=\"n\">benchmark</span><span class=\"p\">,</span> <span class=\"n\">benchmark_run</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_iris</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">LogisticRegression</span>\n\n<span class=\"n\">MODEL_BENCHMARK_SAVE_FILE</span> <span class=\"o\">=</span> <span class=\"s1\">'/tmp/benchmark_model.jsonl'</span>\n\n<span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">load_iris</span><span class=\"p\">(</span><span class=\"n\">return_X_y</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"nd\">@benchmark</span><span class=\"p\">(</span><span class=\"n\">save_params</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">save_output</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">log_regression</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">):</span>\n    <span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">(</span>\n        <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> \n        <span class=\"n\">solver</span><span class=\"o\">=</span><span class=\"s1\">'lbfgs'</span><span class=\"p\">,</span> \n        <span class=\"n\">multi_class</span><span class=\"o\">=</span><span class=\"s1\">'multinomial'</span><span class=\"p\">,</span> \n        <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"n\">C</span><span class=\"p\">,</span>\n        <span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"n\">fit_intercept</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n    <span class=\"n\">score</span> <span class=\"o\">=</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s1\">'score'</span><span class=\"p\">:</span> <span class=\"n\">score</span><span class=\"p\">}</span>\n\n<span class=\"n\">model_benchmark_results</span> <span class=\"o\">=</span> <span class=\"n\">benchmark_run</span><span class=\"p\">(</span>\n    <span class=\"n\">log_regression</span><span class=\"p\">,</span>\n    <span class=\"n\">MODEL_BENCHMARK_SAVE_FILE</span><span class=\"p\">,</span>\n    <span class=\"n\">comment</span><span class=\"o\">=</span><span class=\"s1\">'baseline model'</span><span class=\"p\">,</span>\n    <span class=\"n\">extra_fields</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'C'</span><span class=\"p\">,</span> <span class=\"s1\">'fit_intercept'</span><span class=\"p\">],</span>\n    <span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"s1\">'score'</span><span class=\"p\">,</span>\n    <span class=\"n\">bigger_is_better</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre>\n<p><img alt=\"Benchmark model1\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/048f58aaec18010643ad5d7451a4d5095b913013/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f76677261626f766574732f62656e63686d61726b69742f6d61737465722f696d672f62656e63686d61726b5f6d6f64656c312e6a7067\"></p>\n<p>Change hyperparameter <code>C=0.5</code> and rerun. Output:</p>\n<p><img alt=\"Benchmark model2\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1ddba47e211472736fef473a6d4e5eae43cc558a/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f76677261626f766574732f62656e63686d61726b69742f6d61737465722f696d672f62656e63686d61726b5f6d6f64656c322e6a7067\"></p>\n<p>The same can be run from command line:</p>\n<pre>benchmark_run file_with_benchmark.py --save_dir /tmp/ --comment \"stronger regularization\" --extra_fields C fit_intercept --metric score --bigger_is_better\n</pre>\n<h3>Analyze results from the file</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">benchmarkit</span> <span class=\"kn\">import</span> <span class=\"n\">benchmark_analyze</span>\n\n<span class=\"n\">SAVE_PATH</span> <span class=\"o\">=</span> <span class=\"s1\">'/tmp/benchmark_time.jsonl'</span>\n\n<span class=\"n\">benchmark_df</span> <span class=\"o\">=</span> <span class=\"n\">benchmark_analyze</span><span class=\"p\">(</span>\n    <span class=\"n\">SAVE_PATH</span><span class=\"p\">,</span>\n    <span class=\"n\">func_name</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> \n    <span class=\"n\">rows_limit</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n    <span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"s1\">'mean_time'</span><span class=\"p\">,</span>\n    <span class=\"n\">bigger_is_better</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">extra_fields</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'num_items'</span><span class=\"p\">],</span>\n<span class=\"p\">)</span>\n</pre>\n<ul>\n<li><strong>input_path</strong> - path to <code>.jsonl</code> file or directory with <code>.jsonl</code> files with benchmark results</li>\n<li><strong>func_name</strong> - display statistics for particular function. If <code>None</code> then all functions, stored in file, are displayed. Default <em>None</em></li>\n<li><strong>rows_limit</strong> - limit table rows in console output. Default <em>10</em></li>\n<li><strong>metric</strong> - metric which is used for comparison. Default <code>mean_time</code></li>\n<li><strong>bigger_is_better</strong> - whether bigger value of metric indicates that result is better. For time benchmarks should be <code>False</code>, for model accuracy should be <code>True</code>. Default <em>False</em></li>\n<li><strong>extra_fields</strong> - extra fields to include in console output</li>\n</ul>\n<p>Prints to terminal and returns pandas <code>DataFrame</code>.</p>\n<p><img alt=\"Benchmark analyze\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/633969010ba082310b9d13779f696851e25cb519/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f76677261626f766574732f62656e63686d61726b69742f6d61737465722f696d672f62656e63686d61726b5f616e616c797a652e6a7067\"></p>\n<p>The same can be run from command line:</p>\n<pre>benchmark_analyze /tmp/benchmark_time.jsonl --extra_fields num_items\n</pre>\n<p><a href=\"https://nbviewer.jupyter.org/github/vgrabovets/benchmarkit/blob/master/notebooks/benchmark_examples.ipynb\" rel=\"nofollow\">Other examples</a></p>\n\n          </div>"}, "last_serial": 6786829, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "84dd4a97db59c14c06df31d02ab7f2e5", "sha256": "33f27ec2b926d788b3fd7fb8dab35e8d687b14251b3cefc195680fbec94597d7"}, "downloads": -1, "filename": "benchmarkit-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "84dd4a97db59c14c06df31d02ab7f2e5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 9229, "upload_time": "2019-05-26T10:14:17", "upload_time_iso_8601": "2019-05-26T10:14:17.422324Z", "url": "https://files.pythonhosted.org/packages/5d/fe/0bbde1c725e8cc822d650d265aee2400f08b959dc490e43e68657ef8ea53/benchmarkit-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "456a8ef5ef8a5dd510d824c5972f9910", "sha256": "fc588cb7582babcbec4d670c35c475dae844696f9f0231bad740b5561cc4e7b9"}, "downloads": -1, "filename": "benchmarkit-0.0.2.tar.gz", "has_sig": false, "md5_digest": "456a8ef5ef8a5dd510d824c5972f9910", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 7412, "upload_time": "2019-05-26T10:14:19", "upload_time_iso_8601": "2019-05-26T10:14:19.490353Z", "url": "https://files.pythonhosted.org/packages/6a/1c/11402b289ef81d54f8824b8bd6e503d4eb8dca746c0f34f05ee3597a14fb/benchmarkit-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "d43d0ab97e19aac9bf50f69350ff3037", "sha256": "c56048a178cee300fc3be90a39bb3cbd4654936ccac30f4fd056add34097ef69"}, "downloads": -1, "filename": "benchmarkit-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "d43d0ab97e19aac9bf50f69350ff3037", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 9802, "upload_time": "2019-05-29T18:17:25", "upload_time_iso_8601": "2019-05-29T18:17:25.856404Z", "url": "https://files.pythonhosted.org/packages/a2/df/1b5ad6a2606b5131ae5cd07ce29625d5b34ed02cdcf11caede664e430454/benchmarkit-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f717147888011b29a12171e8babef648", "sha256": "dc769c51869efb923cc35a96cbca16fa6cc7aaababee9112494f49f8f663332d"}, "downloads": -1, "filename": "benchmarkit-0.0.3.tar.gz", "has_sig": false, "md5_digest": "f717147888011b29a12171e8babef648", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 7613, "upload_time": "2019-05-29T18:17:27", "upload_time_iso_8601": "2019-05-29T18:17:27.062301Z", "url": "https://files.pythonhosted.org/packages/27/04/e2c0c1a51a2d71a217a347056d7c68d56027a8b98b70a1857f6cd8abb5a7/benchmarkit-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "dda11d6061b9459721eac4968e60435a", "sha256": "9c54194e368d676214db48adcce57ae4ce8b10dd6d3cc3ab87169ce054d1b17e"}, "downloads": -1, "filename": "benchmarkit-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "dda11d6061b9459721eac4968e60435a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 9805, "upload_time": "2020-03-10T18:21:32", "upload_time_iso_8601": "2020-03-10T18:21:32.864207Z", "url": "https://files.pythonhosted.org/packages/a1/89/549542a5454fe84d2edcf60af705fec2fd5a3cc7f1006ed9d080e69c032f/benchmarkit-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cc7006357d83288b01aeb1c7a4ad6887", "sha256": "c81a28942c38dbe93ff4606828f729c23bb73faee4f0c02041032a397b766505"}, "downloads": -1, "filename": "benchmarkit-0.0.4.tar.gz", "has_sig": false, "md5_digest": "cc7006357d83288b01aeb1c7a4ad6887", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 7614, "upload_time": "2020-03-10T18:21:33", "upload_time_iso_8601": "2020-03-10T18:21:33.968034Z", "url": "https://files.pythonhosted.org/packages/c4/50/d5d6779e82149dde26432f60827ad1e48c33c1bcc615458f663986cafeea/benchmarkit-0.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "dda11d6061b9459721eac4968e60435a", "sha256": "9c54194e368d676214db48adcce57ae4ce8b10dd6d3cc3ab87169ce054d1b17e"}, "downloads": -1, "filename": "benchmarkit-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "dda11d6061b9459721eac4968e60435a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 9805, "upload_time": "2020-03-10T18:21:32", "upload_time_iso_8601": "2020-03-10T18:21:32.864207Z", "url": "https://files.pythonhosted.org/packages/a1/89/549542a5454fe84d2edcf60af705fec2fd5a3cc7f1006ed9d080e69c032f/benchmarkit-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cc7006357d83288b01aeb1c7a4ad6887", "sha256": "c81a28942c38dbe93ff4606828f729c23bb73faee4f0c02041032a397b766505"}, "downloads": -1, "filename": "benchmarkit-0.0.4.tar.gz", "has_sig": false, "md5_digest": "cc7006357d83288b01aeb1c7a4ad6887", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 7614, "upload_time": "2020-03-10T18:21:33", "upload_time_iso_8601": "2020-03-10T18:21:33.968034Z", "url": "https://files.pythonhosted.org/packages/c4/50/d5d6779e82149dde26432f60827ad1e48c33c1bcc615458f663986cafeea/benchmarkit-0.0.4.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:48 2020"}