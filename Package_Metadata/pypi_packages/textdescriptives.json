{"info": {"author": "Lasse Hansen", "author_email": "placeholder@placeholder.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Text Processing"], "description": "# TextDescriptives\n\nA Python package for calculating a large variety of statistics from text(s).\n\n## Installation\nClone the Github directory, navigate to it in a terminal, and call\n`pip install .`\n\n ## Usage\n\nTo calculate all possible metrics:\n```\nimport textdescriptives\n\n# Input can be either a string, list of strings, or pandas Series \nen_test = ['The world is changed. I feel it in the water. I feel it in the earth. I smell it in the air. Much that once was is lost, for none now live who remember it.',\n            'He felt that his whole life was some kind of dream and he sometimes wondered whose it was and whether they were enjoying it.']\n\ntextdescriptives.all_metrics(en_test, lang = 'en', snlp_path = snlp_path)\n```\n|    | Text                                                                                                                                                        |   avg_word_length |   median_word_length |   std_word_length |   avg_sentence_length |   median_sentence_length |   std_sentence_length |   avg_syl_per_word |   median_syl_per_word |   std_syl_per_word |   type_token_ratio |     lix |   rix |   n_types |   n_sentences |   n_tokens |   n_chars |   gunning_fog |    smog |   flesch_reading_ease |   flesch_kincaid_grade |   automated_readability_index |   coleman_liau_index |   Germanic |   Latinate |   Latinate/Germanic |   mean_dependency_distance |   std_dependency_distance |   mean_prop_adjacent_dependency_relation |   std_prop_adjacent_dependency_relation |\n|---:|:------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------:|---------------------:|------------------:|----------------------:|-------------------------:|----------------------:|-------------------:|----------------------:|-------------------:|-------------------:|--------:|------:|----------:|--------------:|-----------:|----------:|--------------:|--------:|----------------------:|-----------------------:|------------------------------:|---------------------:|-----------:|-----------:|--------------------:|---------------------------:|--------------------------:|-----------------------------------------:|----------------------------------------:|\n|  0 | The world is changed.(...) |           3.28571 |                    3 |           1.54127 |                     7 |                        6 |               3.09839 |            1.08571 |                     1 |           0.368117 |           0.657143 | 12.7143 |   0.4 |        24 |             5 |         35 |       121 |       3.94286 | 5.68392 |               107.879 |             -0.0485714 |                      -2.45429 |            -0.708571 |    75      |    25      |            0.333333 |                    1.60381 |                   0.36493 |                                 0.695238 |                               0.0481871 |\n|  1 | He felt that his whole (...)                                |           4.16667 |                    4 |           1.97203 |                    24 |                       24 |               0       |            1.16667 |                     1 |           0.471405 |           0.833333 | 40.6667 |   4   |        21 |             1 |         24 |       101 |      11.2667  | 0       |                83.775 |              7.53667   |                      10.195   |             7.46667  |    83.3333 |    16.6667 |            0.2      |                    2.16    |                   0       |                                 0.64     |                               0         |\n\n\nTo calculate one category at a time:\n```\ntextdescriptives.basic_stats(texts, lang = 'en', metrics = 'all')\ntextdescriptives.readability(texts, lang = 'en')\ntextdescriptives.etymology(texts, lang = 'en')\ntextdescriptives.dependency_distance(texsts, lang = 'en', snlp_path = None)\n```\nTextdescriptives works for most languages, simply change the country code:\n```\nda_test = pd.Series(['Da jeg var atten, tog jeg patent p\u00e5 ild. Det skulle senere vise sig at blive en meget indbringende forretning',\n            \"Spis skovsneglen, Mulle. Du vil jo gerne v\u00e6re med i hulen, ikk'?\"])\n\ntextdescriptives.all_metrics(da_test, lang = 'da', snlp_path=snlp_path)\n```\n\nIf you only want a subset of the basic statistics\n```\ntextdescriptives.basic_stats(en_test, lang = 'en', metrics=['avg_word_length', 'n_chars'])\n```\n|    | Text                                                                                                                                                        |   avg_word_length |   n_chars |\n|---:|:------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------:|----------:|\n|  0 | The world is changed.(...) |           3.28571 |       121 |\n|  1 | He felt that his whole (...) |           4.16667 |       101 |\n\n### Readability\n\nThe readability measures are largely derived from the [textstat](https://github.com/shivam5992/textstat) library and are thoroughly defined there.\n\n### Etymology\nThe etymology measures are calculated using [macroetym](https://github.com/JonathanReeve/macro-etym) only slightly rewritten to be called from a script. They are calculated since in English, a greater frequency of words with a Latinate origin tends to indicate a more formal language register. \n\n### Dependency Distance\nMean dependency distance can be used as a way of measuring the average syntactic complexity of a text. Requres the `snlp` library. \nThe dependency distance function requires stanfordnlp, and their language models. If you have already downloaded these models, the path to the folder can be specified in the snlp_path paramter. Otherwise, the models will be downloaded to your working directory + /snlp_resources.\n\n\n## Dependencies\nDepending on which measures you want to calculate, the dependencies differ.\n * Basic and readability: numpy, pandas, pyphen, pycountry\n * Etymology: nltk and the following models \n`python3 -c \"import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('averaged_perceptron_tagger'); nltk.download('wordnet')\"`\n * Depedency distance: snlp\n\n\n## Metrics\nMetrics currently implemented:\n\n1. Basic descriptive statistics - mean, median, standard deviation of the following:\n  * Word length\n  * Sentence length, words\n  * Sentence length, characters (TODO)\n  * Syllables per word\n  * Number of characters\n  * Number of sentences\n  * Number of types (unique words)\n  * Number of tokens (total words)\n  * Type/to\u1e31en ratio\n  * Lix\n  * Rix\n\n2. Readability metrics:\n  * Gunning-Fog\n  * SMOG\n  * Flesch reading ease\n  * Flesch-Kincaid grade\n  * Automated readability index\n  * Coleman-Liau index\n\n 3. Etymology-related metrics:\n  * Percentage words with Germanic origin\n  * Percentage words with Latinate origin\n  * Latinate/Germanic origin ratio\n\n 4. Dependency distance metrics:\n  * Mean dependency distance, sentence level (mean, standard deviation)\n  * Mean proportion adjacent dependency relations, sentence level (mean, standard devaiation)\n\n\n  Developed by Lasse Hansen at the [Center for Humanities Computing Aarhus](chcaa.io)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/HLasse/TextDescriptives", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "textdescriptives", "package_url": "https://pypi.org/project/textdescriptives/", "platform": "", "project_url": "https://pypi.org/project/textdescriptives/", "project_urls": {"Homepage": "https://github.com/HLasse/TextDescriptives"}, "release_url": "https://pypi.org/project/textdescriptives/0.1.1/", "requires_dist": ["pandas", "numpy", "pyphen", "pycountry", "stanfordnlp", "nltk", "iso-639"], "requires_python": ">=3.6", "summary": "A package for calculating a wide variety of features from text", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>TextDescriptives</h1>\n<p>A Python package for calculating a large variety of statistics from text(s).</p>\n<h2>Installation</h2>\n<p>Clone the Github directory, navigate to it in a terminal, and call\n<code>pip install .</code></p>\n<h2>Usage</h2>\n<p>To calculate all possible metrics:</p>\n<pre><code>import textdescriptives\n\n# Input can be either a string, list of strings, or pandas Series \nen_test = ['The world is changed. I feel it in the water. I feel it in the earth. I smell it in the air. Much that once was is lost, for none now live who remember it.',\n            'He felt that his whole life was some kind of dream and he sometimes wondered whose it was and whether they were enjoying it.']\n\ntextdescriptives.all_metrics(en_test, lang = 'en', snlp_path = snlp_path)\n</code></pre>\n<table>\n<thead>\n<tr>\n<th align=\"right\"></th>\n<th align=\"left\">Text</th>\n<th align=\"right\">avg_word_length</th>\n<th align=\"right\">median_word_length</th>\n<th align=\"right\">std_word_length</th>\n<th align=\"right\">avg_sentence_length</th>\n<th align=\"right\">median_sentence_length</th>\n<th align=\"right\">std_sentence_length</th>\n<th align=\"right\">avg_syl_per_word</th>\n<th align=\"right\">median_syl_per_word</th>\n<th align=\"right\">std_syl_per_word</th>\n<th align=\"right\">type_token_ratio</th>\n<th align=\"right\">lix</th>\n<th align=\"right\">rix</th>\n<th align=\"right\">n_types</th>\n<th align=\"right\">n_sentences</th>\n<th align=\"right\">n_tokens</th>\n<th align=\"right\">n_chars</th>\n<th align=\"right\">gunning_fog</th>\n<th align=\"right\">smog</th>\n<th align=\"right\">flesch_reading_ease</th>\n<th align=\"right\">flesch_kincaid_grade</th>\n<th align=\"right\">automated_readability_index</th>\n<th align=\"right\">coleman_liau_index</th>\n<th align=\"right\">Germanic</th>\n<th align=\"right\">Latinate</th>\n<th align=\"right\">Latinate/Germanic</th>\n<th align=\"right\">mean_dependency_distance</th>\n<th align=\"right\">std_dependency_distance</th>\n<th align=\"right\">mean_prop_adjacent_dependency_relation</th>\n<th align=\"right\">std_prop_adjacent_dependency_relation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"right\">0</td>\n<td align=\"left\">The world is changed.(...)</td>\n<td align=\"right\">3.28571</td>\n<td align=\"right\">3</td>\n<td align=\"right\">1.54127</td>\n<td align=\"right\">7</td>\n<td align=\"right\">6</td>\n<td align=\"right\">3.09839</td>\n<td align=\"right\">1.08571</td>\n<td align=\"right\">1</td>\n<td align=\"right\">0.368117</td>\n<td align=\"right\">0.657143</td>\n<td align=\"right\">12.7143</td>\n<td align=\"right\">0.4</td>\n<td align=\"right\">24</td>\n<td align=\"right\">5</td>\n<td align=\"right\">35</td>\n<td align=\"right\">121</td>\n<td align=\"right\">3.94286</td>\n<td align=\"right\">5.68392</td>\n<td align=\"right\">107.879</td>\n<td align=\"right\">-0.0485714</td>\n<td align=\"right\">-2.45429</td>\n<td align=\"right\">-0.708571</td>\n<td align=\"right\">75</td>\n<td align=\"right\">25</td>\n<td align=\"right\">0.333333</td>\n<td align=\"right\">1.60381</td>\n<td align=\"right\">0.36493</td>\n<td align=\"right\">0.695238</td>\n<td align=\"right\">0.0481871</td>\n</tr>\n<tr>\n<td align=\"right\">1</td>\n<td align=\"left\">He felt that his whole (...)</td>\n<td align=\"right\">4.16667</td>\n<td align=\"right\">4</td>\n<td align=\"right\">1.97203</td>\n<td align=\"right\">24</td>\n<td align=\"right\">24</td>\n<td align=\"right\">0</td>\n<td align=\"right\">1.16667</td>\n<td align=\"right\">1</td>\n<td align=\"right\">0.471405</td>\n<td align=\"right\">0.833333</td>\n<td align=\"right\">40.6667</td>\n<td align=\"right\">4</td>\n<td align=\"right\">21</td>\n<td align=\"right\">1</td>\n<td align=\"right\">24</td>\n<td align=\"right\">101</td>\n<td align=\"right\">11.2667</td>\n<td align=\"right\">0</td>\n<td align=\"right\">83.775</td>\n<td align=\"right\">7.53667</td>\n<td align=\"right\">10.195</td>\n<td align=\"right\">7.46667</td>\n<td align=\"right\">83.3333</td>\n<td align=\"right\">16.6667</td>\n<td align=\"right\">0.2</td>\n<td align=\"right\">2.16</td>\n<td align=\"right\">0</td>\n<td align=\"right\">0.64</td>\n<td align=\"right\">0</td>\n</tr></tbody></table>\n<p>To calculate one category at a time:</p>\n<pre><code>textdescriptives.basic_stats(texts, lang = 'en', metrics = 'all')\ntextdescriptives.readability(texts, lang = 'en')\ntextdescriptives.etymology(texts, lang = 'en')\ntextdescriptives.dependency_distance(texsts, lang = 'en', snlp_path = None)\n</code></pre>\n<p>Textdescriptives works for most languages, simply change the country code:</p>\n<pre><code>da_test = pd.Series(['Da jeg var atten, tog jeg patent p\u00e5 ild. Det skulle senere vise sig at blive en meget indbringende forretning',\n            \"Spis skovsneglen, Mulle. Du vil jo gerne v\u00e6re med i hulen, ikk'?\"])\n\ntextdescriptives.all_metrics(da_test, lang = 'da', snlp_path=snlp_path)\n</code></pre>\n<p>If you only want a subset of the basic statistics</p>\n<pre><code>textdescriptives.basic_stats(en_test, lang = 'en', metrics=['avg_word_length', 'n_chars'])\n</code></pre>\n<table>\n<thead>\n<tr>\n<th align=\"right\"></th>\n<th align=\"left\">Text</th>\n<th align=\"right\">avg_word_length</th>\n<th align=\"right\">n_chars</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"right\">0</td>\n<td align=\"left\">The world is changed.(...)</td>\n<td align=\"right\">3.28571</td>\n<td align=\"right\">121</td>\n</tr>\n<tr>\n<td align=\"right\">1</td>\n<td align=\"left\">He felt that his whole (...)</td>\n<td align=\"right\">4.16667</td>\n<td align=\"right\">101</td>\n</tr></tbody></table>\n<h3>Readability</h3>\n<p>The readability measures are largely derived from the <a href=\"https://github.com/shivam5992/textstat\" rel=\"nofollow\">textstat</a> library and are thoroughly defined there.</p>\n<h3>Etymology</h3>\n<p>The etymology measures are calculated using <a href=\"https://github.com/JonathanReeve/macro-etym\" rel=\"nofollow\">macroetym</a> only slightly rewritten to be called from a script. They are calculated since in English, a greater frequency of words with a Latinate origin tends to indicate a more formal language register.</p>\n<h3>Dependency Distance</h3>\n<p>Mean dependency distance can be used as a way of measuring the average syntactic complexity of a text. Requres the <code>snlp</code> library.\nThe dependency distance function requires stanfordnlp, and their language models. If you have already downloaded these models, the path to the folder can be specified in the snlp_path paramter. Otherwise, the models will be downloaded to your working directory + /snlp_resources.</p>\n<h2>Dependencies</h2>\n<p>Depending on which measures you want to calculate, the dependencies differ.</p>\n<ul>\n<li>Basic and readability: numpy, pandas, pyphen, pycountry</li>\n<li>Etymology: nltk and the following models\n<code>python3 -c \"import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('averaged_perceptron_tagger'); nltk.download('wordnet')\"</code></li>\n<li>Depedency distance: snlp</li>\n</ul>\n<h2>Metrics</h2>\n<p>Metrics currently implemented:</p>\n<ol>\n<li>Basic descriptive statistics - mean, median, standard deviation of the following:</li>\n</ol>\n<ul>\n<li>Word length</li>\n<li>Sentence length, words</li>\n<li>Sentence length, characters (TODO)</li>\n<li>Syllables per word</li>\n<li>Number of characters</li>\n<li>Number of sentences</li>\n<li>Number of types (unique words)</li>\n<li>Number of tokens (total words)</li>\n<li>Type/to\u1e31en ratio</li>\n<li>Lix</li>\n<li>Rix</li>\n</ul>\n<ol>\n<li>Readability metrics:</li>\n</ol>\n<ul>\n<li>Gunning-Fog</li>\n<li>SMOG</li>\n<li>Flesch reading ease</li>\n<li>Flesch-Kincaid grade</li>\n<li>Automated readability index</li>\n<li>Coleman-Liau index</li>\n</ul>\n<ol>\n<li>Etymology-related metrics:</li>\n</ol>\n<ul>\n<li>Percentage words with Germanic origin</li>\n<li>Percentage words with Latinate origin</li>\n<li>Latinate/Germanic origin ratio</li>\n</ul>\n<ol>\n<li>Dependency distance metrics:</li>\n</ol>\n<ul>\n<li>Mean dependency distance, sentence level (mean, standard deviation)</li>\n<li>Mean proportion adjacent dependency relations, sentence level (mean, standard devaiation)</li>\n</ul>\n<p>Developed by Lasse Hansen at the <a href=\"chcaa.io\" rel=\"nofollow\">Center for Humanities Computing Aarhus</a></p>\n\n          </div>"}, "last_serial": 6761732, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "b4edda59c285b05a925d35fcd2b6ec10", "sha256": "54d3c692cdc9db3c4165d9abde93114363c66c396dca02ad2f298795cd50d7f5"}, "downloads": -1, "filename": "textdescriptives-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b4edda59c285b05a925d35fcd2b6ec10", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 11347915, "upload_time": "2020-03-06T12:54:39", "upload_time_iso_8601": "2020-03-06T12:54:39.224655Z", "url": "https://files.pythonhosted.org/packages/39/49/cf074d626273d71c07eec75a1137576e6209adf14c577a328c765f62f3bb/textdescriptives-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d7439baa908e4f01c5c777e649e4dcd9", "sha256": "780462ba5dcfcde236f0166eb1b190eab80e1984ff56c60041ff392aa24b0962"}, "downloads": -1, "filename": "textdescriptives-0.1.1.tar.gz", "has_sig": false, "md5_digest": "d7439baa908e4f01c5c777e649e4dcd9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11160032, "upload_time": "2020-03-06T12:54:49", "upload_time_iso_8601": "2020-03-06T12:54:49.621473Z", "url": "https://files.pythonhosted.org/packages/28/59/ac7972714ac8b094a39e6002541ce13d45dca52e72532c33cecc904cb664/textdescriptives-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b4edda59c285b05a925d35fcd2b6ec10", "sha256": "54d3c692cdc9db3c4165d9abde93114363c66c396dca02ad2f298795cd50d7f5"}, "downloads": -1, "filename": "textdescriptives-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b4edda59c285b05a925d35fcd2b6ec10", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 11347915, "upload_time": "2020-03-06T12:54:39", "upload_time_iso_8601": "2020-03-06T12:54:39.224655Z", "url": "https://files.pythonhosted.org/packages/39/49/cf074d626273d71c07eec75a1137576e6209adf14c577a328c765f62f3bb/textdescriptives-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d7439baa908e4f01c5c777e649e4dcd9", "sha256": "780462ba5dcfcde236f0166eb1b190eab80e1984ff56c60041ff392aa24b0962"}, "downloads": -1, "filename": "textdescriptives-0.1.1.tar.gz", "has_sig": false, "md5_digest": "d7439baa908e4f01c5c777e649e4dcd9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11160032, "upload_time": "2020-03-06T12:54:49", "upload_time_iso_8601": "2020-03-06T12:54:49.621473Z", "url": "https://files.pythonhosted.org/packages/28/59/ac7972714ac8b094a39e6002541ce13d45dca52e72532c33cecc904cb664/textdescriptives-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:04 2020"}