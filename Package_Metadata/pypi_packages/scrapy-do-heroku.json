{"info": {"author": "Ahmed Rafik", "author_email": "djerahahmedrafik@mail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Environment :: No Input/Output (Daemon)", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.7", "Topic :: Internet :: WWW/HTTP"], "description": "\n=========\nScrapy Do\n=========\n\n.. image:: https://api.travis-ci.org/ljanyst/scrapy-do.svg?branch=master\n   :target: https://travis-ci.org/ljanyst/scrapy-do\n\n.. image:: https://coveralls.io/repos/github/ljanyst/scrapy-do/badge.svg?branch=master\n   :target: https://coveralls.io/github/ljanyst/scrapy-do?branch=master\n\n.. image:: https://img.shields.io/pypi/v/scrapy-do.svg\n   :target: https://pypi.python.org/pypi/scrapy-do\n   :alt: PyPI Version\n\n\nScrapy Do is a daemon that provides a convenient way to run `Scrapy\n<https://scrapy.org/>`_ spiders. It can either do it once - immediately; or it\ncan run them periodically, at specified time intervals. It's been inspired by\n`scrapyd <https://github.com/scrapy/scrapyd>`_ but written from scratch. It\ncomes with a REST API, a command line client, and an interactive web interface.\n\n * Homepage: `https://jany.st/scrapy-do.html <https://jany.st/scrapy-do.html>`_\n * Documentation: `https://scrapy-do.readthedocs.io/en/latest/ <https://scrapy-do.readthedocs.io/en/latest/>`_\n\n-----------\nQuick Start\n-----------\n\n* Install ``scrapy-do`` using ``pip``:\n\n  .. code-block:: console\n\n       $ pip install scrapy-do\n\n* Start the daemon in the foreground:\n\n  .. code-block:: console\n\n       $ scrapy-do -n scrapy-do\n\n* Open another terminal window, download the Scrapy's Quotesbot example, and\n  push the code to the server:\n\n  .. code-block:: console\n\n       $ git clone https://github.com/scrapy/quotesbot.git\n       $ cd quotesbot\n       $ scrapy-do-cl push-project\n       +----------------+\n       | quotesbot      |\n       |----------------|\n       | toscrape-css   |\n       | toscrape-xpath |\n       +----------------+\n\n* Schedule some jobs:\n\n  .. code-block:: console\n\n       $ scrapy-do-cl schedule-job --project quotesbot \\\n           --spider toscrape-css --when 'every 5 to 15 minutes'\n       +--------------------------------------+\n       | identifier                           |\n       |--------------------------------------|\n       | 0a3db618-d8e1-48dc-a557-4e8d705d599c |\n       +--------------------------------------+\n\n       $ scrapy-do-cl schedule-job --project quotesbot --spider toscrape-css\n       +--------------------------------------+\n       | identifier                           |\n       |--------------------------------------|\n       | b3a61347-92ef-4095-bb68-0702270a52b8 |\n       +--------------------------------------+\n\n* See what's going on:\n\n  .. figure:: https://github.com/ljanyst/scrapy-do/raw/master/docs/_static/jobs-active.png\n     :scale: 50 %\n     :alt: Active Jobs\n\n     The web interface is available at http://localhost:7654 by default.\n\n--------------------\nBuilding from source\n--------------------\n\nBoth of the steps below require `nodejs` to be installed.\n\n* Check if things work fine:\n\n  .. code-block:: console\n\n       $ pip install -rrequirements-dev.txt\n       $ tox\n\n* Build the wheel:\n\n  .. code-block:: console\n\n       $ python setup.py bdist_wheel\n\n---------\nChangeLog\n---------\n\nVersion 0.4.0\n-------------\n\n* Migration to the Bootstrap 4 UI\n* Make it possible to add a short description to jobs\n* Make it possible to specify user-defined payload in each job that is passed\n  on as a parameter to the python crawler\n* UI updates to support the above\n* New log viewers in the web UI\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "BSD License", "maintainer": "", "maintainer_email": "", "name": "scrapy-do-heroku", "package_url": "https://pypi.org/project/scrapy-do-heroku/", "platform": "", "project_url": "https://pypi.org/project/scrapy-do-heroku/", "project_urls": null, "release_url": "https://pypi.org/project/scrapy-do-heroku/0.4.1/", "requires_dist": ["autobahn", "pem", "psutil", "pyOpenSSL", "python-dateutil", "requests", "schedule", "scrapy", "tabulate", "twisted", "tzlocal"], "requires_python": "", "summary": "A daemon for scheduling Scrapy spiders", "version": "0.4.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://travis-ci.org/ljanyst/scrapy-do\" rel=\"nofollow\"><img alt=\"https://api.travis-ci.org/ljanyst/scrapy-do.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8b91370cfc03b1882f24f26a6b102b045ff385f2/68747470733a2f2f6170692e7472617669732d63692e6f72672f6c6a616e7973742f7363726170792d646f2e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/ljanyst/scrapy-do?branch=master\" rel=\"nofollow\"><img alt=\"https://coveralls.io/repos/github/ljanyst/scrapy-do/badge.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/17aa45bc6f7c8d74a06e6ed61dbc3594a6fa5c50/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6c6a616e7973742f7363726170792d646f2f62616467652e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://pypi.python.org/pypi/scrapy-do\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e9e6298f74d988953a958f14d9d37174fc4113fb/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363726170792d646f2e737667\"></a>\n<p>Scrapy Do is a daemon that provides a convenient way to run <a href=\"https://scrapy.org/\" rel=\"nofollow\">Scrapy</a> spiders. It can either do it once - immediately; or it\ncan run them periodically, at specified time intervals. It\u2019s been inspired by\n<a href=\"https://github.com/scrapy/scrapyd\" rel=\"nofollow\">scrapyd</a> but written from scratch. It\ncomes with a REST API, a command line client, and an interactive web interface.</p>\n<blockquote>\n<ul>\n<li>Homepage: <a href=\"https://jany.st/scrapy-do.html\" rel=\"nofollow\">https://jany.st/scrapy-do.html</a></li>\n<li>Documentation: <a href=\"https://scrapy-do.readthedocs.io/en/latest/\" rel=\"nofollow\">https://scrapy-do.readthedocs.io/en/latest/</a></li>\n</ul>\n</blockquote>\n<div id=\"quick-start\">\n<h2>Quick Start</h2>\n<ul>\n<li><p>Install <tt><span class=\"pre\">scrapy-do</span></tt> using <tt>pip</tt>:</p>\n<pre><span class=\"gp\">$</span> pip install scrapy-do\n</pre>\n</li>\n<li><p>Start the daemon in the foreground:</p>\n<pre><span class=\"gp\">$</span> scrapy-do -n scrapy-do\n</pre>\n</li>\n<li><p>Open another terminal window, download the Scrapy\u2019s Quotesbot example, and\npush the code to the server:</p>\n<pre><span class=\"gp\">$</span> git clone https://github.com/scrapy/quotesbot.git\n<span class=\"gp\">$</span> <span class=\"nb\">cd</span> quotesbot\n<span class=\"gp\">$</span> scrapy-do-cl push-project\n<span class=\"go\">+----------------+\n| quotesbot      |\n|----------------|\n| toscrape-css   |\n| toscrape-xpath |\n+----------------+</span>\n</pre>\n</li>\n<li><p>Schedule some jobs:</p>\n<pre><span class=\"gp\">$</span> scrapy-do-cl schedule-job --project quotesbot <span class=\"se\">\\\n</span>    --spider toscrape-css --when <span class=\"s1\">'every 5 to 15 minutes'</span>\n<span class=\"go\">+--------------------------------------+\n| identifier                           |\n|--------------------------------------|\n| 0a3db618-d8e1-48dc-a557-4e8d705d599c |\n+--------------------------------------+\n\n</span><span class=\"gp\">$</span> scrapy-do-cl schedule-job --project quotesbot --spider toscrape-css\n<span class=\"go\">+--------------------------------------+\n| identifier                           |\n|--------------------------------------|\n| b3a61347-92ef-4095-bb68-0702270a52b8 |\n+--------------------------------------+</span>\n</pre>\n</li>\n<li><p>See what\u2019s going on:</p>\n<div>\n<img alt=\"Active Jobs\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d377d13b37d2981e6dad0bbf10b75a1627ce3cd4/68747470733a2f2f6769746875622e636f6d2f6c6a616e7973742f7363726170792d646f2f7261772f6d61737465722f646f63732f5f7374617469632f6a6f62732d6163746976652e706e67\">\n<p>The web interface is available at <a href=\"http://localhost:7654\" rel=\"nofollow\">http://localhost:7654</a> by default.</p>\n</div>\n</li>\n</ul>\n</div>\n<div id=\"building-from-source\">\n<h2>Building from source</h2>\n<p>Both of the steps below require <cite>nodejs</cite> to be installed.</p>\n<ul>\n<li><p>Check if things work fine:</p>\n<pre><span class=\"gp\">$</span> pip install -rrequirements-dev.txt\n<span class=\"gp\">$</span> tox\n</pre>\n</li>\n<li><p>Build the wheel:</p>\n<pre><span class=\"gp\">$</span> python setup.py bdist_wheel\n</pre>\n</li>\n</ul>\n</div>\n<div id=\"changelog\">\n<h2>ChangeLog</h2>\n<h2 id=\"version-0-4-0\"><span class=\"section-subtitle\">Version 0.4.0</span></h2>\n<ul>\n<li>Migration to the Bootstrap 4 UI</li>\n<li>Make it possible to add a short description to jobs</li>\n<li>Make it possible to specify user-defined payload in each job that is passed\non as a parameter to the python crawler</li>\n<li>UI updates to support the above</li>\n<li>New log viewers in the web UI</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 7124412, "releases": {"0.4.1": [{"comment_text": "", "digests": {"md5": "757e537a363c68580327e717202c3ea6", "sha256": "56c829eb9d0cfcf580cd4c453e3f8779de136991215ad0debcbd9366cbaaa2ee"}, "downloads": -1, "filename": "scrapy_do_heroku-0.4.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "757e537a363c68580327e717202c3ea6", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 2075480, "upload_time": "2020-04-28T23:51:19", "upload_time_iso_8601": "2020-04-28T23:51:19.585777Z", "url": "https://files.pythonhosted.org/packages/ba/4a/56e782bfa76dacaf906367d1a42744da3c0dba22e17c6c754a14de674a91/scrapy_do_heroku-0.4.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2402bddabd858ce1fa39de9fcf4bac2c", "sha256": "52c121b45c8e3b080d30308e41575efc1c06227a27f992d9d9277d0920fa7a2a"}, "downloads": -1, "filename": "scrapy-do-heroku-0.4.1.tar.gz", "has_sig": false, "md5_digest": "2402bddabd858ce1fa39de9fcf4bac2c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 61983, "upload_time": "2020-04-28T23:51:22", "upload_time_iso_8601": "2020-04-28T23:51:22.004481Z", "url": "https://files.pythonhosted.org/packages/fa/6e/8e1cd199b457d3eef5f5577bd07ccb8f40167bc664d3a8130fd289b6eddb/scrapy-do-heroku-0.4.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "757e537a363c68580327e717202c3ea6", "sha256": "56c829eb9d0cfcf580cd4c453e3f8779de136991215ad0debcbd9366cbaaa2ee"}, "downloads": -1, "filename": "scrapy_do_heroku-0.4.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "757e537a363c68580327e717202c3ea6", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 2075480, "upload_time": "2020-04-28T23:51:19", "upload_time_iso_8601": "2020-04-28T23:51:19.585777Z", "url": "https://files.pythonhosted.org/packages/ba/4a/56e782bfa76dacaf906367d1a42744da3c0dba22e17c6c754a14de674a91/scrapy_do_heroku-0.4.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2402bddabd858ce1fa39de9fcf4bac2c", "sha256": "52c121b45c8e3b080d30308e41575efc1c06227a27f992d9d9277d0920fa7a2a"}, "downloads": -1, "filename": "scrapy-do-heroku-0.4.1.tar.gz", "has_sig": false, "md5_digest": "2402bddabd858ce1fa39de9fcf4bac2c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 61983, "upload_time": "2020-04-28T23:51:22", "upload_time_iso_8601": "2020-04-28T23:51:22.004481Z", "url": "https://files.pythonhosted.org/packages/fa/6e/8e1cd199b457d3eef5f5577bd07ccb8f40167bc664d3a8130fd289b6eddb/scrapy-do-heroku-0.4.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:47 2020"}