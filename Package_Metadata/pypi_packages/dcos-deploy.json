{"info": {"author": "MaibornWolff", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "<p align=\"center\"><img src=\"img/dcos-deploy-logo.png\" alt=\"dcos-deploy\" width=\"256\"></p>\n\ndcos-deploy is a command line tool that helps you to deploy and manage groups of services and apps on [DC/OS](https://dcos.io). It acts as an orchestration engine on top of the existing DC/OS tools and APIs to install, configure and update frameworks, marathon apps and metronome jobs. It is based on a yaml-configuration file which describes the services that you want. It will read this file and execute any changes necessary so that your DC/OS cluster reflects your desired configuration.\n\nFor example: To deploy a complete elasticsearch stack on your cluster you would typically need to install the elasticsearch framework from the DC/OS universe, a kibana app, expose both to your loadbalancer and add some regular jobs for backups and cleanup. Additionally if you run elasticsearch with x-pack or searchguard installed you also need to create a public-private keypair, service-account and secret for your framework. This amounts to quite a number of steps. With dcos-deploy you just describe your entire stack in one simple yaml file and let dcos-deploy do the rest. See the examples folder for more.\n\n(!) This tool is under heavy development. Use in production environments at your own risk.\n\n## Features\n\n* Handles the following \"entities\":\n  * DC/OS packages\n  * Marathon apps\n  * Metronome jobs\n  * secrets\n  * serviceaccounts\n  * public-private-keypairs (for use in secrets)\n  * [Edge-LB](https://docs.mesosphere.com/services/edge-lb/)\n  * [S3](https://aws.amazon.com/s3/) files\n* For DC/OS packages it supports version updates and configuration changes.\n* Handles install and update dependencies between entities (e.g. a framework is only installed after its serviceaccount is created, an app is restarted if an attached secret changes).\n* Parameterise your configuration using variables (e.g. to support different instances of a service).\n* Stateless / no backend: There is no local or remote state. Everything needed is pulled directly from the cluster: dcos-deploy is a client-only tool that directly uses the existing DC/OS APIs to access your DC/OS cluster and does not need its own backend service.\n* Uses your installed dcos-cli to figure out how to talk to your cluster. No extra configuration needed.\n* dry-run mode: Check which changes would be done.\n* partial deployment: Choose the entities to be deployed.\n* Can be extended with extra modules (still in development).\n\n\n### Limitations\n* Deleting packages/apps/jobs is not supported: Since dcos-deploy does not keep a state it cannot detect if you remove a service/app/job from its configuration. Therefore you are responsible to delete any no longer wanted entities yourself.\n* Frameworks/packages with more complicated update procedures (like Edge-LB) are at the moment not fully supported.\n\n\n## Requirements\n* A DC/OS cluster with version >= 1.11 (for features like secrets an EE cluster is needed)\n* dcos-cli installed  and connected to your cluster (to verify it works run `dcos node` and it should display a list of nodes in your cluster)\n\nIf you want to run it from source, aditionally you need:\n* Python >= 3.5\n* Python modules from `requirements.txt`\n\n## Installation\nThere are several ways to install dcos-deploy:\n* Binary\n  * Download the binary for your system from the Releases page\n  * Make the file executable and copy it into a folder inside your path\n* Install from pypi (`pip install dcos-deploy`)\n* Run from source\n  * Clone this github repository to your system (for a stable release checkout a release tag)\n  * Install all requirements from `requirements.txt` (optional: use a virtualenv to keep your system clean)\n  * Optional: Create a symlink of `dcos-deploy` to a folder in your exectuable path (e.g. `ln -s $(pwd)/dcos-deploy ~/usr/bin/dcos-deploy`)\n\n## Usage\n* Create your `dcos.yml` (start from scratch or use one of the examples). You should separate your stack into groups and create a `dcos.yml` for each of them (e.g. one for all hdfs related services, one for all elastic related and so on) to keep the complexity manageable.\n* Run `dcos-deploy apply`.\n* See `dcos-deploy apply --help` for all options.\n\n### Credentials\n\ndcos-deploy has several ways to retrieve connection credentials for your DC/OS cluster:\n\n* dcos-cli: By default dcos-deploy uses the authentication information from the dcos-cli, so make sure you are logged in (verify by running `dcos node`, if it works you should see a list of nodes in your cluster).\n* Static token via environment variables: `DCOS_BASE_URL` (set this to the public URL of your master) and `DCOS_AUTH_TOKEN` (set this to a valid auth token). This way is handy for automation situations where you do not want to expose the admin username and password by adding them to your automation tool (e.g. Gitlab CI secrets). Instead you can provide the automation job with short-lived credentials.\n* DC/OS serviceaccount secret: If you are running dcos-deploy from inside your DC/OS cluster (e.g. in a metronome job) you can also provide the credentials via a serviceaccount secret. To do that create a serviceaccount with superuser rights and expose its credentials secret to your service/job via the environment variable `DCOS_SERVICE_ACCOUNT_CREDENTIAL`. Also provide the variable `DCOS_BASE_URL` and set it to the internal URL of your master (should be `https://leader.mesos` in most cases).\n* Username and Password via environment variables: `DCOS_BASE_URL` (set this to the public URL of your master), `DCOS_USERNAME` (username of a DC/OS admin user) and `DCOS_PASSWORD` (password of a DC/OS admin user).\n\n## Config file syntax\nThe config file is written as a yaml file. The root level consists of key/value-pairs (a dictionary). Each key represents the unique name for one entity, the value is again a dictionary with all the options for that entity.\n\n### Meta fields\nThere are some meta fields for further configuation:\n* `variables`: Define variables to be used in the rest of the file and in app definitions and package options. See the [Variables](#variables) section for more info.\n* `includes`: Structure your config further by separating parts into different files and including them. Provide a list of filenames. The include files must be structured the same way as the main file. Each entity name must be unique over the base file and all included files.\n* `modules`: Extend the features of dcos-deploy using external modules (still in development, not documented yet).\n\n### Variables\nTo be more generic with your services you can use variables in certain places. All variables must be defined in the `variables` section of your config file.\n```\nvariables:\n  var1:\n    required: True  # Required variables must be provided when calling dcos-deploy using the -e option (e.g. -e var1=foo)\n  var2:\n    required: False\n    default: foobar  # If a variable is not provided by the user the default value is used. Variables without a default are treated as empty values\n  var3:\n    values:  # You can restrict the allowed values for a variable\n      - foo\n      - bar\n      - baz\n    default: bar\n  var4:\n    env: MY_VAR4  # Variables can also be read from the environment\n    required: True\n  var5:\n    file: myvalue.txt  # The value of the variable will be the content of the file. Can be useful to provide longer values\n    encode: base64  # This will encode the value using base64. Some DC/OS frameworks have config options that require base64-encoded values. Using this option the value can be kept in clear-text and will be automatically encoded by dcos-deploy when rendering the options file for the framework.\n```\n\nVariables can be used via [Mustache](http://mustache.github.io/) templates in most options and in marathon app definitions and package options. See `examples/elastic` for usage examples. You can not use variables in entity names or outside option strings (ground rule: the yaml file must be syntactically correct without mustache template rendering).\n\nSome variables are automatically provided by `dcos-deploy` based on your cluster. These are:\n\n* `_cluster_version`: The DC/OS version of your cluster, for example `1.12.2`\n* `_cluster_variant`: The DC/OS variant of your cluster, for example `enterprise`\n* `_num_masters`: The number of masters in your cluster\n* `_num_private_agents`: The number of private agents in your cluster (useful if you want to tailor the size of an app or framework node to the cluster size)\n* `_num_public_agents`: The number of public agents in your cluster\n* `_num_all_agents`: The number of public and private agents in your cluster\n\n### Global config\nTo specifiy an attribute for all entities of a specific type you can define it in the global config. The config for a specific entity will be merged with the global config for the corresponding type.\n\nExample:\n```\nglobal:\n  s3file:\n    server:\n      endpoint: \"{{s3_endpoint}}\"\n      access_key: \"{{s3_access_key}}\"\n      secret_key: \"{{s3_secret_key}}\"\n```\n\n### Encryption\ndcos-deploy supports encrypting files so that sensitive information is not stored unencrypted. Files are symmetricly encrypted using [Fernet](https://cryptography.io/en/latest/fernet/) (AES-128) from the python [cryptography](https://cryptography.io/en/latest/) library.\nIn most places where you provide a filename (at the moment not possible with `s3file`) you can use encrypted files by using the following syntax as filename: `vault:<encryption-key>:<filename-to-encrypted-file>`.\nYou should use a variable for the encryption key.\n\nExample:\n```\nvariables:\n  encryption_key:\n    env: ENCRYPTION_KEY\n    required: True\n\nservicepasswords:\n  type: secret\n  path: /myservice/passwords\n  file: \"vault:{{encryption_key}}:servicepasswords.encrypted\"\n```\n\nYou can also specify the encryption key in the global config to avoid writing it for every occurence:\n```\nvariables:\n  encryption_key:\n    env: ENCRYPTION_KEY\n    required: True\n\nglobal:\n  vault:\n    key: \"{{encryption_key}}\"\n\nservicepasswords:\n  type: secret\n  path: /myservice/passwords\n  file: vault::servicepasswords.encrypted\n```\n\n\ndcos-deploy has several util commands that help you in creating encrypted files:\n* `dcos-deploy vault generate-key`: Generates a new key that you can use for encrypting files\n* `dcos-deploy vault encrypt`: Encrypts a file using a given key\n* `dcos-deploy vault decrypt`: Decrypts a file using a given key\n\nExample usage:\n```\n$ echo \"supersecret\" > servicepasswords\n$ dcos-deploy vault generate-key\nYour new key: 6htLemxXcEXnahVl1aZI6Aa3TXIHmbE8abtibC2iO6c=\nKeep this safe.\n$ export ENCRYPTION_KEY=\"6htLemxXcEXnahVl1aZI6Aa3TXIHmbE8abtibC2iO6c=\"\n$ dcos-deploy vault encrypt -i servicepasswords -o servicepasswords.encrypted -e ENCRYPTION_KEY\n$ cat servicepasswords.encrypted\ngAAAAABb9rkSdEqT1xn0w5G5ipjPH6Vd5TsUfDLAWnN7I8FXTOPK6t1tMUstm8nA_yiA6SV5B1Blxj1h-xqCl8VqqbH7D7RF9Q==\n$ dcos-deploy vault decrypt -i servicepasswords.encrypted -o servicepasswords.clear -e ENCRYPTION_KEY\n$ cat servicepasswords.clear\nsupersecret\n```\n\nTo use encrypted files with includes make sure that the variable for the encryption key is either defined in the file itsself or in an included file that is read before the encrypted one.\n\n\n### Entity\nEach entity has the same structure and some common options:\n```\nentityname:\n  type: foo\n  only:\n    var1: foo\n  except:\n    var2: bar\n  state: removed\n  dependencies:\n    - otherentity\n  loop:\n    var3:\n      - a\n      - b\n```\n\n`type` defines what kind of entity this is. Currently implemented are\n* `app`\n* `framework`\n* `job`\n* `account`\n* `secret`\n* `cert`\n* `repository`\n* `edgelb`\n* `s3file`\n* `taskexec`\nSee their respective sections below for details.\n\n`only` and `except` take key/value-pairs of variable names and values. These are evaluated when reading the config file based on all provided and default variables. The entity is excluded if one of the variables in the `only` section does not have the value specified or if one of the variables in the `except` section has the specified value. In the example above the entity is only included if `var1 == foo` and `var2 != bar`. If a variable in the `only` section is not defined (no default value) the condition is treated as false and the entity is ignored.\n\n`state` can be used to define a specific state for an entity. Currently only none (default, option is ignored) and `removed` are supported. By specifying `state: removed` the entity will be deleted if it exists. This can be useful in several ways. For example in air-gapped clusters the normally configured universe repository is not reachable and must be removed before other frameworks can be installed from local universes / package registries. This can be accomplished by defining the universe repo as an entity with `state: removed`.\n\n`dependencies` takes a list of entity names that this entity depends on. Optionally the dependency type can be provided. Currently supported are `create` (default) and `update`. They can be defined by adding a colon after the entity name and then the type (e.g. `otherentity:create`). A `create` dependency is only honored during creation time of an entity and means that the entity will only be created after all its dependencies have been successfully created (e.g. a service account for a framework). An update dependency extends the `create` dependency and is currently only honored by the `marathon` module. If during an apply-operation a dependency of a marathon app is changed (e.g. a secret) and the app has no changes it will be restarted.\n\n`loop` is very useful for describing multiple entities that are very similar. The values of the variables defined under `loop` will be extended into a cross product and for each combination an entity with these extra variables will be created. These extra variables can be used like normal variables to parametrize the entity. By default the entity name will be created by concatenating the given name with the values of all loop variables (in the example above these would be `entityname-a` and `entityname-b`). This can be overridden by using an entityname that has template parameters (for example `{{var3}}-myentity`).\n\n### Marathon app\n`type: app` defines a marathon app. It has the following specific options:\n* `path`: id of the app. If not specified the `id` field of the marathon app definition is used. Variables can be used.\n* `marathon`: path to the marathon app definition json file. Required. Variables can be used in the path and in the json file itsself.\n* `extra_vars`: key/value-pairs of of extra variables and values to be used when rendering the app definition file.\n\nIf you want to start several apps from the same basic app definition, there is a meta option to allow this:\n\n```yaml\nmymultiapp:\n  type: app\n  _template: marathon.json\n  _vars: mymultiapp.yml\n```\n\nThis option and `loop` can not be used at the same time.\n\nThe `marathon.json` file is your normal app definition json file, paramerised with mustache variables. The `mymultiapp.yml` file has the following structure:\n\n```yaml\ndefaults: # optional, key/value-pairs of variables, can get overwritten in the instances section\n  var1: foo\n  var2: bar\ninstances:\n  instance1:\n    var1: baz\n    var3: hello\n  instance2:\n    var3: world\n```\n\nFor each key under `instances` dcos-deploy will create a marathon app from the template file specialized with the variables provided.\n\nIn both the `extra_vars` and the variables in the `instances` you can define dependent variables. The variable name must be in the form `<existing variable name>:<value to compare>`, and the value must be key-value pairs of variables. If `<existing variable name>` has value `<value to compare>` the variables defined under that key will be added to the variables for that entity, otherwise they will be discarded. A simple example:\n\n```yaml\nvariables:\n  env:\n    required: True\n  foo: something\nmyapp:\n  type: app\n  marathon: myapp.json\n  extra_vars:\n    env:test:\n      foo: bar\n    env:int:\n      foo: baz\n```\n\nIf `env` has value `test`, the variable `foo` will have value `bar`, if `env` has value `int`, the variable `foo` will have value `baz`, in neither of these cases `foo` will have value `something`.\n\nIf not defined marathon will add a number of default fields to an app definition. dcos-deploy tries to detect these defaults and exclude them when checking for changes between the local definition and the one known to marathon. This is rather complex as these default values partly depend on several modes (network, container type, etc.). If you find a case where dcos-deploy falesly reports a change please open an issue at the github project and attach your app definition and the definition reported by marathon (via `dcos marathon app show <app-id>`).\n\n### Framework\n`type: framework` defines a DC/OS framework. It has the following specific options:\n* `path`: name of the framework. If not specified the `service.name` field of the package options are used. Variables can be used.\n* `package`:\n  * `name`: name of the package in the DC/OS universe. This is the same as used when installing a package via the cli (`dcos package install <packagename>`). Required. Variables can be used.\n  * `version`: version of the package. This is the same as used when installing a package via the cli (`dcos package install <packagename> --package-version=<version>`). Required. Variables can be used.\n  * `options`: Path to the json options file to configure the package. Required. Variables can be used in the path and in the json file itsself.\n\nThese options correspond to the parameters provided when installing a package via the dcos-cli: `dcos package install <packagename> --package-version=<version> --options=<options.json>`.\n\nDuring installation of a package dcos-deploy will wait until the framework is installed. If the framework exposes the standard plan API (like all frameworks based on the [Mesosphere SDK](https://github.com/mesosphere/dcos-commons/), e.g. elastic, hdfs or kafka) dcos-deploy will also wait (with a timeout of 10 minutes) until the deploy-plan is complete.\nAny change in the options file or in the package version will trigger an update (the same as doing `dcos <framework> update start --package-version=<version> --options=<options.json>`). dcos-deploy will not wait for the completion of the update as it assumes that any updates are done in a rolling-restart fashion.\n\n### Metronome job\n\n`type: job` defines a metronome job. It has the following specific options:\n\n* `path`: id of the job. If not specified the `id` field of the job definition is used. Variables can be used.\n* `definition`: path to the metronome job definition json file. Required. Variables can be used in the path and in the json file itsself.\n\nThe job definition is expected to be in the format used starting with DC/S 1.13. It is described in the [DC/OS 1.13 release notes](https://docs.d2iq.com/mesosphere/dcos/1.13/release-notes/1.13.0/#using-separate-json-files-for-job-scheduling).\n\n### Secret\n`type: secret` defines a secret. This can only be used on EE clusters. It has the following specific options:\n* `path`: path for the secret. Required. Variables can be used.\n* `value`: Value for the secret. Variables can be used. Either this or `file` is required.\n* `file`: Path to a file. The content of the file will be used as value for the secret. Either this or `value` is required.\n* `render`: Wether to render the file content with mustache. Use if your file contains variables. Boolean. Defaults to False.\n\n### Serviceaccount\n`type: serviceaccount` defines a serviceaccount. This can only be used on EE clusters. It has the following specific options:\n* `name`: Name of the serviceaccount. Required. Variables can be used.\n* `secret`: Path to use for the secret that contains the private key associated with the account. Required. Variables can be used.\n* `groups`: List of groups the account should be added to. Optional. Variables can be used.\n* `permissions`: Permissions to give to the account. Dictionary. Key is the name of the permission (rid), value is a list of actions to allow. If a permission does not yet exist, it will be created. Variables are not supported.\n\nThis entity equates to the steps required to create a serviceaccount for a framework as described in the [DC/OS documentation](https://docs.mesosphere.com/1.12/security/ent/service-auth/custom-service-auth/).\nAny groups or permissions not specified in the config are removed from the account during the update process.\n\nExample:\n```\ntype: serviceaccount\nname: hdfs-service-account\nsecret: hdfs/serviceaccount-secret\npermissions:\n  \"dcos:mesos:master:framework:role:hdfs-role\":\n    - create\n  \"dcos:mesos:master:reservation:role:hdfs-role\":\n    - create\n    - delete\n```\n\n### X.509 certificate\n\n`type: cert` is a special type that uses the [DC/OS CA API](https://docs.mesosphere.com/1.11/security/ent/tls-ssl/ca-api/) to create a public-private keypair, sign it with the internal DC/OS CA and provide the key and certificate as secrets. This can only be used on EE clusters. It has the following specific options:\n\n* `cert_secret`: secret path to store the certificate. Required. Variables can be used.\n* `key_secret`: secret path to store the key. Required. Variables can be used.\n* `dn`: distinguished name to be used for the certificate. Requied. Variables can be used.\n* `hostnames`: List of hostnames to put in the certificate. Optional.\n* `encoding`: Encoding to use for the private key (PEM, DER), default is PEM. Optional.\n* `format`: Format to use for the private key (PKCS1, PKCS8), default is PKCS1. Optional.\n* `algorithm`: Algorithm to use for the private key (RSA, ECDSA), default is RSA. Optional.\n* `key_size`: Key size in bits to use for the private key. Default is 2048 for RSA and 256 for ECDSA. Optional.\n\nOne example usecase for this is a service secured with TLS client certificate authentication (e.g. elasticsearch with x-pack or searchguard). You configure the service to accept client certificates signed by the cluster-internal DC/OS CA. Then using the `cert` entity provide appropriate certificates as secrets to your client services. The keys and certificates are securely kept inside the cluster and using the [path restrictions](https://docs.mesosphere.com/1.11/security/ent/#spaces-for-secrets) for secrets can only be accessed by authorized services.\n\n### Package repository\n`type: repository` defines a package repository. It has the following specific options:\n* `name`: name of the repository. Required. Variables can be used.\n* `uri`: uri for the repositroy. Required. Variables can be used.\n* `index`: at what index to place the repository in the repository list. 0 for beginning. Do not set for end of list.\n\nWith this type you can add additional package repositories to DC/OS. You can for example use it to add the Edge-LB repositories to your EE cluster. Set the repository as a dependency for any frameworks/packages installed from it.\n\n### Edge-LB pool\n\n`type: edgelb` defines an edgelb pool. This can only be used on EE clusters. It has the following specific options:\n\n* `api_server`: path to the edgelb-api server if not installed at default location. Variables can be used. Optional. E.g. if you installed Edge-LB at `infra/edgelb` then use this for `api_server`.\n* `name`: name of the pool. Taken from pool config if not present. Variables can be used.\n* `pool`: filename to the yaml file for configuring the pool. Required. The filename itsself and the yaml file can contain variables.\n\nFor configuring a pool see the [Edge-LB configuration](https://docs.mesosphere.com/services/edge-lb/1.2/pool-configuration/).\n\nFor this to work you must have the edgelb package installed via the repository URLs provided by Mesosphere (use the `repository` and `package` types, there is a [complete example](examples/edgelb) available).\nAt the moment dcos-deploy can not safely detect if the pool config was changed so it will always apply it. Be aware that changing certain options in the pool config (like ports or secrets) will result in a restart of the haproxy instances. Make sure you have a HA setup so that there is no downtime.\n\n### S3 File\n`type: s3file` defines a file to be uploaded to S3-compatible storage. If you are not running on AWS you can use [Minio](https://minio.io/) or any other storage service with an s3-compatible interface. This entitiy is designed to provide files for apps / services (via the fetch mechanism) that are either too big or otherwise not suited for storage as secrets (for example plugins for a service). It has the following specific options:\n* `server`:\n  * `endpoint`: endpoint of your s3-compatible storage. If you use AWS S3 set this to your region endpoint (e.g. `s3.eu-central-1.amazonaws.com`). Required. Variables can be used.\n  * `access_key`: your access key. Required. Variables can be used. For security reasons you should provide the value via environment variable. Make sure the iam user associated with these credentials has all rights for retrieving and uploading objects (`s3:Get*` and `s3:Put*`).\n  * `secret_key`: your secret access key. Required. Variables can be used. For security reasons you should provide the value via environment variable.\n  * `ssl_verify`: Set to false to disable ssl verifcation for s3 connection. Defaults to true. Optional.\n  * `secure`: Set to false to use insecure http connections for s3 connection. Defaults to true. Optional.\n* `source`: path to your file or folder to be uploaded to s3. Should be relative to the `dcos.yml` file. Required. Variables can be used.\n* `destination`:\n  * `bucket`: name of the bucket to use. Required. Variables can be used.\n  * `key`: key to store the file(s) under. Required. Variables can be used.\n  * `create_bucket`: Set to true to create bucket if it does not exist. Defaults to false. Optional.\n  * `bucket_policy`: S3 bucket policy content. When bucket will be created because of `create_bucket: true` the policy will be applied. Optional.\n* `compress`: type of compressed archive to combine the files in. Currently only supported is `zip`. Optional. Variables can be used.\n\nThis entity has several modes of operation:\n\n* Upload a single file: `source` points to a file, `compress` is not set. The file is uploaded and `destination.key` is used as name.\n* Upload a folder: `source` points to a folder, `compress` is not set. All files and folders under `source` are recursively uploaded. `destination.key` is used as prefix.\n* Upload a folder as a zip file: `source` points to a folder, `compress: zip`. The files and folders under `source` are compressed into a zip file and uploaded. `destiation.key` is used as name of the zip file.\n\nChanges to the uploaded files are detected by comparing md5 hashsums, which are stored as metadata with the object in S3. Only changed files will be uploaded. If you try to manage files that were already uploaded some other way, on the first run they will be detected as changed (due to the missing hash value metadata) and reuploaded. For comparing hashsums for files to be uploaded in compressed form the tool will temporarily create the zip file to calculate the hashsum.\n\nFor multi-file upload: If `source` does not end in a slash, the last part of the name will be used as base folder: For `source: files/foo` and `destination.key: bar`, all files will under foo be uploaded as `bar/foo/<filename>`. In contrast if `source: files/foo/`, all files will be uploaded as `bar/<filename>`.\n\nTo use your S3 bucket to serve files to apps and services you must configure it for anonymous read access. For security reasons you should restrict that access to the IP range of your cluster. See the [AWS S3 documentation](https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html) for details.\nCreating and configuring the bucket is outside the scope of this tool.\n\nTo make sure a marathon app that uses s3 files is made aware of changes to the uploaded files, you should set the `s3file` entity as an `update` dependency to the `app` entity. dcos-deploy will restart the app whenever the uploaded file changes.\n\nIf you run `apply` with `--debug` dcosdeploy will download already existing files from s3 and print the differences between the local and remote version in the unified diff format. So only use `--debug` for textual files.\n\n### Task exec\n`type: taskexec` allows to execute commands inside tasks. This is primarily meant to trigger configuration reloads on services that can do some sort of hot-reload as to avoid restarting a service. It has the following specific options:\n* `task`: task identifier to uniquely identify the task. Can be part of a task name. Required. Variables can be used.\n* `command`: command to execute in the task. Required. Variables can be used.\n* `print`: boolean. Wether to print the output of the executed command. Optional. Defaults to false.\n\nThis has the same effect as running `dcos task exec <task> <command>`.\n\nAs dcos-deploy has no state it cannnot detect if this command has been run before. As such it will run this command every time `apply` is called. You must either make the command idempotent so that running it multiple times is possible without changing the result, or (recommended) add dependencies (with `:update`) to it and add `when: dependencies-changed`. That way the command will only be executed when one of its dependencies has been changed.\n\nExample for this is: Uploading configuration files to S3 and triggering a redownload of the files into the service by a command. See [examples](examples/demo) for details.\n\n### HttpCall\n`type: httpcall` allows to make HTTP calls as part of deployments. This is similar to `taskexec` and can be used to upload configuration to services as part of deployments. It has the following specific options:\n\n* `url`: HTTP(s) URL to call. Required. Variables can be used.\n* `method`: HTTP method to use for the call, defaults to `GET`.\n* `ignore_errors`: If set to true a non-200 result status code of the call will not be treated as a failure. Defaults to false.\n* `body`: Allows to send a request body along. Optional. Has the following sub options:\n  * `content`: Specifies the content of the request body.\n  * `file`: Contents of this file will be used as request body. Is mutually exclusive with `content`. Variables can be used in the filename.\n  * `render`: If set to true the request body from `file` or `content` will be rendered using mustache.\n\nIf the URL is neither a http nor a https url it wil be treated as a path behind the DC/OS adminrouter. When executing the call dcos-deploy will use its DC/OS credentials to authenticate against the adminrouter. So for example if a DC/OS cluster is reachable under `https://dcos.mycluster` and `url` is defined as `/service/myservice/reload` then dcos-deploy will call the URL `https://dcos.mycluster/service/myservice/reload`.\n\nThe same restrictions from `taskexec` in regards to state apply. As such this entity will run every time `apply` is called unless `when` and dependencies are used (see description for `taskexec` above for details).\n\n## Deployment process\nWhen running the `apply` command dcos-deploy will first check all entities if they have changed. To do this it will first render all options and files using the provided variables, retrieve the currently running configurations from the DC/OS cluster using the specific APIs (e.g. get the app definition from marathon) and compare them. It will print a list of changes and ask for confirmation (unless `--yes` is used). If an entity needs to be created it will first recursively create any dependencies.\nThere is no guaranteed order of execution. Only that any defined dependencies will be created before the entity itsself is created.\n\nThe deployment process has some specific restrictions:\n* Names/paths/ids may not be changed.\n* Options files for frameworks, apps and jobs are not verified for structural correctness. If the new file is not accepted by the API an error will be thrown.\n* For frameworks dcos-deploy does not verify beforehand if a version update is supported by the framework. If the framework does not accept the new version an error will be thrown.\n* An already created `cert` entity will not be changed if the `dn` or `hostnames` fields change.\n* Dependencies must be explicitly defined in the `dcos.yml`. Implicit dependencies (like a secret referenced in a marathon app) that are not explicitly stated are not honored by dcos-deploy.\n\n### Deleting entities\n\ndcos-deploy has support for deleting entities. You can use it to delete one or all entities defined (for example to clean up after tests). Do so use the command `dcos-deploy delete`. It will delete all entities defined in your configuration, honoring the dependencies (e.g. deleting a service before deleting the secret associated with it). If you only want to delete a specific entity use `--only <entity-name>`. All entities that have this entity as a dependency will also be deleted (e.g. if you delete a secret a marathon app depending on it will also be deleted). Check the dry-run output to make sure you don't unintentionally delete the wrong entity. The command is idempotent, so deleting an already deleted entity has no effect.\nThe delete command will not modify your configuration files. So to make sure that the deleted entity will not be recreated during the next `apply`-run, remove the entity definition from your yaml files.  \n\n\n## Roadmap\n\n* Support for creating and configuring kubernetes clusters\n* Add as package to the Mesosphere universe for easy installation as a module for the dcos-cli\n* Better and documented plugin support\n* Provide some sort of verification for configurations\n\n\n## Contributing\nIf you found a bug or have a feature request, please open an issue in Github.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/MaibornWolff/dcos-deploy/", "keywords": "dcos marathon mesos", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "dcos-deploy", "package_url": "https://pypi.org/project/dcos-deploy/", "platform": "", "project_url": "https://pypi.org/project/dcos-deploy/", "project_urls": {"Homepage": "https://github.com/MaibornWolff/dcos-deploy/"}, "release_url": "https://pypi.org/project/dcos-deploy/0.3.0/", "requires_dist": null, "requires_python": ">=3.5", "summary": "Deploy and orchestrate DC/OS services and apps", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p align=\"center\"><img alt=\"dcos-deploy\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/567dbc629ae0f66b5b00d5b383c7774f137238be/696d672f64636f732d6465706c6f792d6c6f676f2e706e67\" width=\"256\"></p>\n<p>dcos-deploy is a command line tool that helps you to deploy and manage groups of services and apps on <a href=\"https://dcos.io\" rel=\"nofollow\">DC/OS</a>. It acts as an orchestration engine on top of the existing DC/OS tools and APIs to install, configure and update frameworks, marathon apps and metronome jobs. It is based on a yaml-configuration file which describes the services that you want. It will read this file and execute any changes necessary so that your DC/OS cluster reflects your desired configuration.</p>\n<p>For example: To deploy a complete elasticsearch stack on your cluster you would typically need to install the elasticsearch framework from the DC/OS universe, a kibana app, expose both to your loadbalancer and add some regular jobs for backups and cleanup. Additionally if you run elasticsearch with x-pack or searchguard installed you also need to create a public-private keypair, service-account and secret for your framework. This amounts to quite a number of steps. With dcos-deploy you just describe your entire stack in one simple yaml file and let dcos-deploy do the rest. See the examples folder for more.</p>\n<p>(!) This tool is under heavy development. Use in production environments at your own risk.</p>\n<h2>Features</h2>\n<ul>\n<li>Handles the following \"entities\":\n<ul>\n<li>DC/OS packages</li>\n<li>Marathon apps</li>\n<li>Metronome jobs</li>\n<li>secrets</li>\n<li>serviceaccounts</li>\n<li>public-private-keypairs (for use in secrets)</li>\n<li><a href=\"https://docs.mesosphere.com/services/edge-lb/\" rel=\"nofollow\">Edge-LB</a></li>\n<li><a href=\"https://aws.amazon.com/s3/\" rel=\"nofollow\">S3</a> files</li>\n</ul>\n</li>\n<li>For DC/OS packages it supports version updates and configuration changes.</li>\n<li>Handles install and update dependencies between entities (e.g. a framework is only installed after its serviceaccount is created, an app is restarted if an attached secret changes).</li>\n<li>Parameterise your configuration using variables (e.g. to support different instances of a service).</li>\n<li>Stateless / no backend: There is no local or remote state. Everything needed is pulled directly from the cluster: dcos-deploy is a client-only tool that directly uses the existing DC/OS APIs to access your DC/OS cluster and does not need its own backend service.</li>\n<li>Uses your installed dcos-cli to figure out how to talk to your cluster. No extra configuration needed.</li>\n<li>dry-run mode: Check which changes would be done.</li>\n<li>partial deployment: Choose the entities to be deployed.</li>\n<li>Can be extended with extra modules (still in development).</li>\n</ul>\n<h3>Limitations</h3>\n<ul>\n<li>Deleting packages/apps/jobs is not supported: Since dcos-deploy does not keep a state it cannot detect if you remove a service/app/job from its configuration. Therefore you are responsible to delete any no longer wanted entities yourself.</li>\n<li>Frameworks/packages with more complicated update procedures (like Edge-LB) are at the moment not fully supported.</li>\n</ul>\n<h2>Requirements</h2>\n<ul>\n<li>A DC/OS cluster with version &gt;= 1.11 (for features like secrets an EE cluster is needed)</li>\n<li>dcos-cli installed  and connected to your cluster (to verify it works run <code>dcos node</code> and it should display a list of nodes in your cluster)</li>\n</ul>\n<p>If you want to run it from source, aditionally you need:</p>\n<ul>\n<li>Python &gt;= 3.5</li>\n<li>Python modules from <code>requirements.txt</code></li>\n</ul>\n<h2>Installation</h2>\n<p>There are several ways to install dcos-deploy:</p>\n<ul>\n<li>Binary\n<ul>\n<li>Download the binary for your system from the Releases page</li>\n<li>Make the file executable and copy it into a folder inside your path</li>\n</ul>\n</li>\n<li>Install from pypi (<code>pip install dcos-deploy</code>)</li>\n<li>Run from source\n<ul>\n<li>Clone this github repository to your system (for a stable release checkout a release tag)</li>\n<li>Install all requirements from <code>requirements.txt</code> (optional: use a virtualenv to keep your system clean)</li>\n<li>Optional: Create a symlink of <code>dcos-deploy</code> to a folder in your exectuable path (e.g. <code>ln -s $(pwd)/dcos-deploy ~/usr/bin/dcos-deploy</code>)</li>\n</ul>\n</li>\n</ul>\n<h2>Usage</h2>\n<ul>\n<li>Create your <code>dcos.yml</code> (start from scratch or use one of the examples). You should separate your stack into groups and create a <code>dcos.yml</code> for each of them (e.g. one for all hdfs related services, one for all elastic related and so on) to keep the complexity manageable.</li>\n<li>Run <code>dcos-deploy apply</code>.</li>\n<li>See <code>dcos-deploy apply --help</code> for all options.</li>\n</ul>\n<h3>Credentials</h3>\n<p>dcos-deploy has several ways to retrieve connection credentials for your DC/OS cluster:</p>\n<ul>\n<li>dcos-cli: By default dcos-deploy uses the authentication information from the dcos-cli, so make sure you are logged in (verify by running <code>dcos node</code>, if it works you should see a list of nodes in your cluster).</li>\n<li>Static token via environment variables: <code>DCOS_BASE_URL</code> (set this to the public URL of your master) and <code>DCOS_AUTH_TOKEN</code> (set this to a valid auth token). This way is handy for automation situations where you do not want to expose the admin username and password by adding them to your automation tool (e.g. Gitlab CI secrets). Instead you can provide the automation job with short-lived credentials.</li>\n<li>DC/OS serviceaccount secret: If you are running dcos-deploy from inside your DC/OS cluster (e.g. in a metronome job) you can also provide the credentials via a serviceaccount secret. To do that create a serviceaccount with superuser rights and expose its credentials secret to your service/job via the environment variable <code>DCOS_SERVICE_ACCOUNT_CREDENTIAL</code>. Also provide the variable <code>DCOS_BASE_URL</code> and set it to the internal URL of your master (should be <code>https://leader.mesos</code> in most cases).</li>\n<li>Username and Password via environment variables: <code>DCOS_BASE_URL</code> (set this to the public URL of your master), <code>DCOS_USERNAME</code> (username of a DC/OS admin user) and <code>DCOS_PASSWORD</code> (password of a DC/OS admin user).</li>\n</ul>\n<h2>Config file syntax</h2>\n<p>The config file is written as a yaml file. The root level consists of key/value-pairs (a dictionary). Each key represents the unique name for one entity, the value is again a dictionary with all the options for that entity.</p>\n<h3>Meta fields</h3>\n<p>There are some meta fields for further configuation:</p>\n<ul>\n<li><code>variables</code>: Define variables to be used in the rest of the file and in app definitions and package options. See the <a href=\"#variables\" rel=\"nofollow\">Variables</a> section for more info.</li>\n<li><code>includes</code>: Structure your config further by separating parts into different files and including them. Provide a list of filenames. The include files must be structured the same way as the main file. Each entity name must be unique over the base file and all included files.</li>\n<li><code>modules</code>: Extend the features of dcos-deploy using external modules (still in development, not documented yet).</li>\n</ul>\n<h3>Variables</h3>\n<p>To be more generic with your services you can use variables in certain places. All variables must be defined in the <code>variables</code> section of your config file.</p>\n<pre><code>variables:\n  var1:\n    required: True  # Required variables must be provided when calling dcos-deploy using the -e option (e.g. -e var1=foo)\n  var2:\n    required: False\n    default: foobar  # If a variable is not provided by the user the default value is used. Variables without a default are treated as empty values\n  var3:\n    values:  # You can restrict the allowed values for a variable\n      - foo\n      - bar\n      - baz\n    default: bar\n  var4:\n    env: MY_VAR4  # Variables can also be read from the environment\n    required: True\n  var5:\n    file: myvalue.txt  # The value of the variable will be the content of the file. Can be useful to provide longer values\n    encode: base64  # This will encode the value using base64. Some DC/OS frameworks have config options that require base64-encoded values. Using this option the value can be kept in clear-text and will be automatically encoded by dcos-deploy when rendering the options file for the framework.\n</code></pre>\n<p>Variables can be used via <a href=\"http://mustache.github.io/\" rel=\"nofollow\">Mustache</a> templates in most options and in marathon app definitions and package options. See <code>examples/elastic</code> for usage examples. You can not use variables in entity names or outside option strings (ground rule: the yaml file must be syntactically correct without mustache template rendering).</p>\n<p>Some variables are automatically provided by <code>dcos-deploy</code> based on your cluster. These are:</p>\n<ul>\n<li><code>_cluster_version</code>: The DC/OS version of your cluster, for example <code>1.12.2</code></li>\n<li><code>_cluster_variant</code>: The DC/OS variant of your cluster, for example <code>enterprise</code></li>\n<li><code>_num_masters</code>: The number of masters in your cluster</li>\n<li><code>_num_private_agents</code>: The number of private agents in your cluster (useful if you want to tailor the size of an app or framework node to the cluster size)</li>\n<li><code>_num_public_agents</code>: The number of public agents in your cluster</li>\n<li><code>_num_all_agents</code>: The number of public and private agents in your cluster</li>\n</ul>\n<h3>Global config</h3>\n<p>To specifiy an attribute for all entities of a specific type you can define it in the global config. The config for a specific entity will be merged with the global config for the corresponding type.</p>\n<p>Example:</p>\n<pre><code>global:\n  s3file:\n    server:\n      endpoint: \"{{s3_endpoint}}\"\n      access_key: \"{{s3_access_key}}\"\n      secret_key: \"{{s3_secret_key}}\"\n</code></pre>\n<h3>Encryption</h3>\n<p>dcos-deploy supports encrypting files so that sensitive information is not stored unencrypted. Files are symmetricly encrypted using <a href=\"https://cryptography.io/en/latest/fernet/\" rel=\"nofollow\">Fernet</a> (AES-128) from the python <a href=\"https://cryptography.io/en/latest/\" rel=\"nofollow\">cryptography</a> library.\nIn most places where you provide a filename (at the moment not possible with <code>s3file</code>) you can use encrypted files by using the following syntax as filename: <code>vault:&lt;encryption-key&gt;:&lt;filename-to-encrypted-file&gt;</code>.\nYou should use a variable for the encryption key.</p>\n<p>Example:</p>\n<pre><code>variables:\n  encryption_key:\n    env: ENCRYPTION_KEY\n    required: True\n\nservicepasswords:\n  type: secret\n  path: /myservice/passwords\n  file: \"vault:{{encryption_key}}:servicepasswords.encrypted\"\n</code></pre>\n<p>You can also specify the encryption key in the global config to avoid writing it for every occurence:</p>\n<pre><code>variables:\n  encryption_key:\n    env: ENCRYPTION_KEY\n    required: True\n\nglobal:\n  vault:\n    key: \"{{encryption_key}}\"\n\nservicepasswords:\n  type: secret\n  path: /myservice/passwords\n  file: vault::servicepasswords.encrypted\n</code></pre>\n<p>dcos-deploy has several util commands that help you in creating encrypted files:</p>\n<ul>\n<li><code>dcos-deploy vault generate-key</code>: Generates a new key that you can use for encrypting files</li>\n<li><code>dcos-deploy vault encrypt</code>: Encrypts a file using a given key</li>\n<li><code>dcos-deploy vault decrypt</code>: Decrypts a file using a given key</li>\n</ul>\n<p>Example usage:</p>\n<pre><code>$ echo \"supersecret\" &gt; servicepasswords\n$ dcos-deploy vault generate-key\nYour new key: 6htLemxXcEXnahVl1aZI6Aa3TXIHmbE8abtibC2iO6c=\nKeep this safe.\n$ export ENCRYPTION_KEY=\"6htLemxXcEXnahVl1aZI6Aa3TXIHmbE8abtibC2iO6c=\"\n$ dcos-deploy vault encrypt -i servicepasswords -o servicepasswords.encrypted -e ENCRYPTION_KEY\n$ cat servicepasswords.encrypted\ngAAAAABb9rkSdEqT1xn0w5G5ipjPH6Vd5TsUfDLAWnN7I8FXTOPK6t1tMUstm8nA_yiA6SV5B1Blxj1h-xqCl8VqqbH7D7RF9Q==\n$ dcos-deploy vault decrypt -i servicepasswords.encrypted -o servicepasswords.clear -e ENCRYPTION_KEY\n$ cat servicepasswords.clear\nsupersecret\n</code></pre>\n<p>To use encrypted files with includes make sure that the variable for the encryption key is either defined in the file itsself or in an included file that is read before the encrypted one.</p>\n<h3>Entity</h3>\n<p>Each entity has the same structure and some common options:</p>\n<pre><code>entityname:\n  type: foo\n  only:\n    var1: foo\n  except:\n    var2: bar\n  state: removed\n  dependencies:\n    - otherentity\n  loop:\n    var3:\n      - a\n      - b\n</code></pre>\n<p><code>type</code> defines what kind of entity this is. Currently implemented are</p>\n<ul>\n<li><code>app</code></li>\n<li><code>framework</code></li>\n<li><code>job</code></li>\n<li><code>account</code></li>\n<li><code>secret</code></li>\n<li><code>cert</code></li>\n<li><code>repository</code></li>\n<li><code>edgelb</code></li>\n<li><code>s3file</code></li>\n<li><code>taskexec</code>\nSee their respective sections below for details.</li>\n</ul>\n<p><code>only</code> and <code>except</code> take key/value-pairs of variable names and values. These are evaluated when reading the config file based on all provided and default variables. The entity is excluded if one of the variables in the <code>only</code> section does not have the value specified or if one of the variables in the <code>except</code> section has the specified value. In the example above the entity is only included if <code>var1 == foo</code> and <code>var2 != bar</code>. If a variable in the <code>only</code> section is not defined (no default value) the condition is treated as false and the entity is ignored.</p>\n<p><code>state</code> can be used to define a specific state for an entity. Currently only none (default, option is ignored) and <code>removed</code> are supported. By specifying <code>state: removed</code> the entity will be deleted if it exists. This can be useful in several ways. For example in air-gapped clusters the normally configured universe repository is not reachable and must be removed before other frameworks can be installed from local universes / package registries. This can be accomplished by defining the universe repo as an entity with <code>state: removed</code>.</p>\n<p><code>dependencies</code> takes a list of entity names that this entity depends on. Optionally the dependency type can be provided. Currently supported are <code>create</code> (default) and <code>update</code>. They can be defined by adding a colon after the entity name and then the type (e.g. <code>otherentity:create</code>). A <code>create</code> dependency is only honored during creation time of an entity and means that the entity will only be created after all its dependencies have been successfully created (e.g. a service account for a framework). An update dependency extends the <code>create</code> dependency and is currently only honored by the <code>marathon</code> module. If during an apply-operation a dependency of a marathon app is changed (e.g. a secret) and the app has no changes it will be restarted.</p>\n<p><code>loop</code> is very useful for describing multiple entities that are very similar. The values of the variables defined under <code>loop</code> will be extended into a cross product and for each combination an entity with these extra variables will be created. These extra variables can be used like normal variables to parametrize the entity. By default the entity name will be created by concatenating the given name with the values of all loop variables (in the example above these would be <code>entityname-a</code> and <code>entityname-b</code>). This can be overridden by using an entityname that has template parameters (for example <code>{{var3}}-myentity</code>).</p>\n<h3>Marathon app</h3>\n<p><code>type: app</code> defines a marathon app. It has the following specific options:</p>\n<ul>\n<li><code>path</code>: id of the app. If not specified the <code>id</code> field of the marathon app definition is used. Variables can be used.</li>\n<li><code>marathon</code>: path to the marathon app definition json file. Required. Variables can be used in the path and in the json file itsself.</li>\n<li><code>extra_vars</code>: key/value-pairs of of extra variables and values to be used when rendering the app definition file.</li>\n</ul>\n<p>If you want to start several apps from the same basic app definition, there is a meta option to allow this:</p>\n<pre><span class=\"nt\">mymultiapp</span><span class=\"p\">:</span>\n  <span class=\"nt\">type</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">app</span>\n  <span class=\"nt\">_template</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">marathon.json</span>\n  <span class=\"nt\">_vars</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">mymultiapp.yml</span>\n</pre>\n<p>This option and <code>loop</code> can not be used at the same time.</p>\n<p>The <code>marathon.json</code> file is your normal app definition json file, paramerised with mustache variables. The <code>mymultiapp.yml</code> file has the following structure:</p>\n<pre><span class=\"nt\">defaults</span><span class=\"p\">:</span> <span class=\"c1\"># optional, key/value-pairs of variables, can get overwritten in the instances section</span>\n  <span class=\"nt\">var1</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">foo</span>\n  <span class=\"nt\">var2</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">bar</span>\n<span class=\"nt\">instances</span><span class=\"p\">:</span>\n  <span class=\"nt\">instance1</span><span class=\"p\">:</span>\n    <span class=\"nt\">var1</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">baz</span>\n    <span class=\"nt\">var3</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">hello</span>\n  <span class=\"nt\">instance2</span><span class=\"p\">:</span>\n    <span class=\"nt\">var3</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">world</span>\n</pre>\n<p>For each key under <code>instances</code> dcos-deploy will create a marathon app from the template file specialized with the variables provided.</p>\n<p>In both the <code>extra_vars</code> and the variables in the <code>instances</code> you can define dependent variables. The variable name must be in the form <code>&lt;existing variable name&gt;:&lt;value to compare&gt;</code>, and the value must be key-value pairs of variables. If <code>&lt;existing variable name&gt;</code> has value <code>&lt;value to compare&gt;</code> the variables defined under that key will be added to the variables for that entity, otherwise they will be discarded. A simple example:</p>\n<pre><span class=\"nt\">variables</span><span class=\"p\">:</span>\n  <span class=\"nt\">env</span><span class=\"p\">:</span>\n    <span class=\"nt\">required</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">True</span>\n  <span class=\"nt\">foo</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">something</span>\n<span class=\"nt\">myapp</span><span class=\"p\">:</span>\n  <span class=\"nt\">type</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">app</span>\n  <span class=\"nt\">marathon</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">myapp.json</span>\n  <span class=\"nt\">extra_vars</span><span class=\"p\">:</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">env:test</span><span class=\"p p-Indicator\">:</span>\n      <span class=\"nt\">foo</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">bar</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">env:int</span><span class=\"p p-Indicator\">:</span>\n      <span class=\"nt\">foo</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">baz</span>\n</pre>\n<p>If <code>env</code> has value <code>test</code>, the variable <code>foo</code> will have value <code>bar</code>, if <code>env</code> has value <code>int</code>, the variable <code>foo</code> will have value <code>baz</code>, in neither of these cases <code>foo</code> will have value <code>something</code>.</p>\n<p>If not defined marathon will add a number of default fields to an app definition. dcos-deploy tries to detect these defaults and exclude them when checking for changes between the local definition and the one known to marathon. This is rather complex as these default values partly depend on several modes (network, container type, etc.). If you find a case where dcos-deploy falesly reports a change please open an issue at the github project and attach your app definition and the definition reported by marathon (via <code>dcos marathon app show &lt;app-id&gt;</code>).</p>\n<h3>Framework</h3>\n<p><code>type: framework</code> defines a DC/OS framework. It has the following specific options:</p>\n<ul>\n<li><code>path</code>: name of the framework. If not specified the <code>service.name</code> field of the package options are used. Variables can be used.</li>\n<li><code>package</code>:\n<ul>\n<li><code>name</code>: name of the package in the DC/OS universe. This is the same as used when installing a package via the cli (<code>dcos package install &lt;packagename&gt;</code>). Required. Variables can be used.</li>\n<li><code>version</code>: version of the package. This is the same as used when installing a package via the cli (<code>dcos package install &lt;packagename&gt; --package-version=&lt;version&gt;</code>). Required. Variables can be used.</li>\n<li><code>options</code>: Path to the json options file to configure the package. Required. Variables can be used in the path and in the json file itsself.</li>\n</ul>\n</li>\n</ul>\n<p>These options correspond to the parameters provided when installing a package via the dcos-cli: <code>dcos package install &lt;packagename&gt; --package-version=&lt;version&gt; --options=&lt;options.json&gt;</code>.</p>\n<p>During installation of a package dcos-deploy will wait until the framework is installed. If the framework exposes the standard plan API (like all frameworks based on the <a href=\"https://github.com/mesosphere/dcos-commons/\" rel=\"nofollow\">Mesosphere SDK</a>, e.g. elastic, hdfs or kafka) dcos-deploy will also wait (with a timeout of 10 minutes) until the deploy-plan is complete.\nAny change in the options file or in the package version will trigger an update (the same as doing <code>dcos &lt;framework&gt; update start --package-version=&lt;version&gt; --options=&lt;options.json&gt;</code>). dcos-deploy will not wait for the completion of the update as it assumes that any updates are done in a rolling-restart fashion.</p>\n<h3>Metronome job</h3>\n<p><code>type: job</code> defines a metronome job. It has the following specific options:</p>\n<ul>\n<li><code>path</code>: id of the job. If not specified the <code>id</code> field of the job definition is used. Variables can be used.</li>\n<li><code>definition</code>: path to the metronome job definition json file. Required. Variables can be used in the path and in the json file itsself.</li>\n</ul>\n<p>The job definition is expected to be in the format used starting with DC/S 1.13. It is described in the <a href=\"https://docs.d2iq.com/mesosphere/dcos/1.13/release-notes/1.13.0/#using-separate-json-files-for-job-scheduling\" rel=\"nofollow\">DC/OS 1.13 release notes</a>.</p>\n<h3>Secret</h3>\n<p><code>type: secret</code> defines a secret. This can only be used on EE clusters. It has the following specific options:</p>\n<ul>\n<li><code>path</code>: path for the secret. Required. Variables can be used.</li>\n<li><code>value</code>: Value for the secret. Variables can be used. Either this or <code>file</code> is required.</li>\n<li><code>file</code>: Path to a file. The content of the file will be used as value for the secret. Either this or <code>value</code> is required.</li>\n<li><code>render</code>: Wether to render the file content with mustache. Use if your file contains variables. Boolean. Defaults to False.</li>\n</ul>\n<h3>Serviceaccount</h3>\n<p><code>type: serviceaccount</code> defines a serviceaccount. This can only be used on EE clusters. It has the following specific options:</p>\n<ul>\n<li><code>name</code>: Name of the serviceaccount. Required. Variables can be used.</li>\n<li><code>secret</code>: Path to use for the secret that contains the private key associated with the account. Required. Variables can be used.</li>\n<li><code>groups</code>: List of groups the account should be added to. Optional. Variables can be used.</li>\n<li><code>permissions</code>: Permissions to give to the account. Dictionary. Key is the name of the permission (rid), value is a list of actions to allow. If a permission does not yet exist, it will be created. Variables are not supported.</li>\n</ul>\n<p>This entity equates to the steps required to create a serviceaccount for a framework as described in the <a href=\"https://docs.mesosphere.com/1.12/security/ent/service-auth/custom-service-auth/\" rel=\"nofollow\">DC/OS documentation</a>.\nAny groups or permissions not specified in the config are removed from the account during the update process.</p>\n<p>Example:</p>\n<pre><code>type: serviceaccount\nname: hdfs-service-account\nsecret: hdfs/serviceaccount-secret\npermissions:\n  \"dcos:mesos:master:framework:role:hdfs-role\":\n    - create\n  \"dcos:mesos:master:reservation:role:hdfs-role\":\n    - create\n    - delete\n</code></pre>\n<h3>X.509 certificate</h3>\n<p><code>type: cert</code> is a special type that uses the <a href=\"https://docs.mesosphere.com/1.11/security/ent/tls-ssl/ca-api/\" rel=\"nofollow\">DC/OS CA API</a> to create a public-private keypair, sign it with the internal DC/OS CA and provide the key and certificate as secrets. This can only be used on EE clusters. It has the following specific options:</p>\n<ul>\n<li><code>cert_secret</code>: secret path to store the certificate. Required. Variables can be used.</li>\n<li><code>key_secret</code>: secret path to store the key. Required. Variables can be used.</li>\n<li><code>dn</code>: distinguished name to be used for the certificate. Requied. Variables can be used.</li>\n<li><code>hostnames</code>: List of hostnames to put in the certificate. Optional.</li>\n<li><code>encoding</code>: Encoding to use for the private key (PEM, DER), default is PEM. Optional.</li>\n<li><code>format</code>: Format to use for the private key (PKCS1, PKCS8), default is PKCS1. Optional.</li>\n<li><code>algorithm</code>: Algorithm to use for the private key (RSA, ECDSA), default is RSA. Optional.</li>\n<li><code>key_size</code>: Key size in bits to use for the private key. Default is 2048 for RSA and 256 for ECDSA. Optional.</li>\n</ul>\n<p>One example usecase for this is a service secured with TLS client certificate authentication (e.g. elasticsearch with x-pack or searchguard). You configure the service to accept client certificates signed by the cluster-internal DC/OS CA. Then using the <code>cert</code> entity provide appropriate certificates as secrets to your client services. The keys and certificates are securely kept inside the cluster and using the <a href=\"https://docs.mesosphere.com/1.11/security/ent/#spaces-for-secrets\" rel=\"nofollow\">path restrictions</a> for secrets can only be accessed by authorized services.</p>\n<h3>Package repository</h3>\n<p><code>type: repository</code> defines a package repository. It has the following specific options:</p>\n<ul>\n<li><code>name</code>: name of the repository. Required. Variables can be used.</li>\n<li><code>uri</code>: uri for the repositroy. Required. Variables can be used.</li>\n<li><code>index</code>: at what index to place the repository in the repository list. 0 for beginning. Do not set for end of list.</li>\n</ul>\n<p>With this type you can add additional package repositories to DC/OS. You can for example use it to add the Edge-LB repositories to your EE cluster. Set the repository as a dependency for any frameworks/packages installed from it.</p>\n<h3>Edge-LB pool</h3>\n<p><code>type: edgelb</code> defines an edgelb pool. This can only be used on EE clusters. It has the following specific options:</p>\n<ul>\n<li><code>api_server</code>: path to the edgelb-api server if not installed at default location. Variables can be used. Optional. E.g. if you installed Edge-LB at <code>infra/edgelb</code> then use this for <code>api_server</code>.</li>\n<li><code>name</code>: name of the pool. Taken from pool config if not present. Variables can be used.</li>\n<li><code>pool</code>: filename to the yaml file for configuring the pool. Required. The filename itsself and the yaml file can contain variables.</li>\n</ul>\n<p>For configuring a pool see the <a href=\"https://docs.mesosphere.com/services/edge-lb/1.2/pool-configuration/\" rel=\"nofollow\">Edge-LB configuration</a>.</p>\n<p>For this to work you must have the edgelb package installed via the repository URLs provided by Mesosphere (use the <code>repository</code> and <code>package</code> types, there is a <a href=\"examples/edgelb\" rel=\"nofollow\">complete example</a> available).\nAt the moment dcos-deploy can not safely detect if the pool config was changed so it will always apply it. Be aware that changing certain options in the pool config (like ports or secrets) will result in a restart of the haproxy instances. Make sure you have a HA setup so that there is no downtime.</p>\n<h3>S3 File</h3>\n<p><code>type: s3file</code> defines a file to be uploaded to S3-compatible storage. If you are not running on AWS you can use <a href=\"https://minio.io/\" rel=\"nofollow\">Minio</a> or any other storage service with an s3-compatible interface. This entitiy is designed to provide files for apps / services (via the fetch mechanism) that are either too big or otherwise not suited for storage as secrets (for example plugins for a service). It has the following specific options:</p>\n<ul>\n<li><code>server</code>:\n<ul>\n<li><code>endpoint</code>: endpoint of your s3-compatible storage. If you use AWS S3 set this to your region endpoint (e.g. <code>s3.eu-central-1.amazonaws.com</code>). Required. Variables can be used.</li>\n<li><code>access_key</code>: your access key. Required. Variables can be used. For security reasons you should provide the value via environment variable. Make sure the iam user associated with these credentials has all rights for retrieving and uploading objects (<code>s3:Get*</code> and <code>s3:Put*</code>).</li>\n<li><code>secret_key</code>: your secret access key. Required. Variables can be used. For security reasons you should provide the value via environment variable.</li>\n<li><code>ssl_verify</code>: Set to false to disable ssl verifcation for s3 connection. Defaults to true. Optional.</li>\n<li><code>secure</code>: Set to false to use insecure http connections for s3 connection. Defaults to true. Optional.</li>\n</ul>\n</li>\n<li><code>source</code>: path to your file or folder to be uploaded to s3. Should be relative to the <code>dcos.yml</code> file. Required. Variables can be used.</li>\n<li><code>destination</code>:\n<ul>\n<li><code>bucket</code>: name of the bucket to use. Required. Variables can be used.</li>\n<li><code>key</code>: key to store the file(s) under. Required. Variables can be used.</li>\n<li><code>create_bucket</code>: Set to true to create bucket if it does not exist. Defaults to false. Optional.</li>\n<li><code>bucket_policy</code>: S3 bucket policy content. When bucket will be created because of <code>create_bucket: true</code> the policy will be applied. Optional.</li>\n</ul>\n</li>\n<li><code>compress</code>: type of compressed archive to combine the files in. Currently only supported is <code>zip</code>. Optional. Variables can be used.</li>\n</ul>\n<p>This entity has several modes of operation:</p>\n<ul>\n<li>Upload a single file: <code>source</code> points to a file, <code>compress</code> is not set. The file is uploaded and <code>destination.key</code> is used as name.</li>\n<li>Upload a folder: <code>source</code> points to a folder, <code>compress</code> is not set. All files and folders under <code>source</code> are recursively uploaded. <code>destination.key</code> is used as prefix.</li>\n<li>Upload a folder as a zip file: <code>source</code> points to a folder, <code>compress: zip</code>. The files and folders under <code>source</code> are compressed into a zip file and uploaded. <code>destiation.key</code> is used as name of the zip file.</li>\n</ul>\n<p>Changes to the uploaded files are detected by comparing md5 hashsums, which are stored as metadata with the object in S3. Only changed files will be uploaded. If you try to manage files that were already uploaded some other way, on the first run they will be detected as changed (due to the missing hash value metadata) and reuploaded. For comparing hashsums for files to be uploaded in compressed form the tool will temporarily create the zip file to calculate the hashsum.</p>\n<p>For multi-file upload: If <code>source</code> does not end in a slash, the last part of the name will be used as base folder: For <code>source: files/foo</code> and <code>destination.key: bar</code>, all files will under foo be uploaded as <code>bar/foo/&lt;filename&gt;</code>. In contrast if <code>source: files/foo/</code>, all files will be uploaded as <code>bar/&lt;filename&gt;</code>.</p>\n<p>To use your S3 bucket to serve files to apps and services you must configure it for anonymous read access. For security reasons you should restrict that access to the IP range of your cluster. See the <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html\" rel=\"nofollow\">AWS S3 documentation</a> for details.\nCreating and configuring the bucket is outside the scope of this tool.</p>\n<p>To make sure a marathon app that uses s3 files is made aware of changes to the uploaded files, you should set the <code>s3file</code> entity as an <code>update</code> dependency to the <code>app</code> entity. dcos-deploy will restart the app whenever the uploaded file changes.</p>\n<p>If you run <code>apply</code> with <code>--debug</code> dcosdeploy will download already existing files from s3 and print the differences between the local and remote version in the unified diff format. So only use <code>--debug</code> for textual files.</p>\n<h3>Task exec</h3>\n<p><code>type: taskexec</code> allows to execute commands inside tasks. This is primarily meant to trigger configuration reloads on services that can do some sort of hot-reload as to avoid restarting a service. It has the following specific options:</p>\n<ul>\n<li><code>task</code>: task identifier to uniquely identify the task. Can be part of a task name. Required. Variables can be used.</li>\n<li><code>command</code>: command to execute in the task. Required. Variables can be used.</li>\n<li><code>print</code>: boolean. Wether to print the output of the executed command. Optional. Defaults to false.</li>\n</ul>\n<p>This has the same effect as running <code>dcos task exec &lt;task&gt; &lt;command&gt;</code>.</p>\n<p>As dcos-deploy has no state it cannnot detect if this command has been run before. As such it will run this command every time <code>apply</code> is called. You must either make the command idempotent so that running it multiple times is possible without changing the result, or (recommended) add dependencies (with <code>:update</code>) to it and add <code>when: dependencies-changed</code>. That way the command will only be executed when one of its dependencies has been changed.</p>\n<p>Example for this is: Uploading configuration files to S3 and triggering a redownload of the files into the service by a command. See <a href=\"examples/demo\" rel=\"nofollow\">examples</a> for details.</p>\n<h3>HttpCall</h3>\n<p><code>type: httpcall</code> allows to make HTTP calls as part of deployments. This is similar to <code>taskexec</code> and can be used to upload configuration to services as part of deployments. It has the following specific options:</p>\n<ul>\n<li><code>url</code>: HTTP(s) URL to call. Required. Variables can be used.</li>\n<li><code>method</code>: HTTP method to use for the call, defaults to <code>GET</code>.</li>\n<li><code>ignore_errors</code>: If set to true a non-200 result status code of the call will not be treated as a failure. Defaults to false.</li>\n<li><code>body</code>: Allows to send a request body along. Optional. Has the following sub options:\n<ul>\n<li><code>content</code>: Specifies the content of the request body.</li>\n<li><code>file</code>: Contents of this file will be used as request body. Is mutually exclusive with <code>content</code>. Variables can be used in the filename.</li>\n<li><code>render</code>: If set to true the request body from <code>file</code> or <code>content</code> will be rendered using mustache.</li>\n</ul>\n</li>\n</ul>\n<p>If the URL is neither a http nor a https url it wil be treated as a path behind the DC/OS adminrouter. When executing the call dcos-deploy will use its DC/OS credentials to authenticate against the adminrouter. So for example if a DC/OS cluster is reachable under <code>https://dcos.mycluster</code> and <code>url</code> is defined as <code>/service/myservice/reload</code> then dcos-deploy will call the URL <code>https://dcos.mycluster/service/myservice/reload</code>.</p>\n<p>The same restrictions from <code>taskexec</code> in regards to state apply. As such this entity will run every time <code>apply</code> is called unless <code>when</code> and dependencies are used (see description for <code>taskexec</code> above for details).</p>\n<h2>Deployment process</h2>\n<p>When running the <code>apply</code> command dcos-deploy will first check all entities if they have changed. To do this it will first render all options and files using the provided variables, retrieve the currently running configurations from the DC/OS cluster using the specific APIs (e.g. get the app definition from marathon) and compare them. It will print a list of changes and ask for confirmation (unless <code>--yes</code> is used). If an entity needs to be created it will first recursively create any dependencies.\nThere is no guaranteed order of execution. Only that any defined dependencies will be created before the entity itsself is created.</p>\n<p>The deployment process has some specific restrictions:</p>\n<ul>\n<li>Names/paths/ids may not be changed.</li>\n<li>Options files for frameworks, apps and jobs are not verified for structural correctness. If the new file is not accepted by the API an error will be thrown.</li>\n<li>For frameworks dcos-deploy does not verify beforehand if a version update is supported by the framework. If the framework does not accept the new version an error will be thrown.</li>\n<li>An already created <code>cert</code> entity will not be changed if the <code>dn</code> or <code>hostnames</code> fields change.</li>\n<li>Dependencies must be explicitly defined in the <code>dcos.yml</code>. Implicit dependencies (like a secret referenced in a marathon app) that are not explicitly stated are not honored by dcos-deploy.</li>\n</ul>\n<h3>Deleting entities</h3>\n<p>dcos-deploy has support for deleting entities. You can use it to delete one or all entities defined (for example to clean up after tests). Do so use the command <code>dcos-deploy delete</code>. It will delete all entities defined in your configuration, honoring the dependencies (e.g. deleting a service before deleting the secret associated with it). If you only want to delete a specific entity use <code>--only &lt;entity-name&gt;</code>. All entities that have this entity as a dependency will also be deleted (e.g. if you delete a secret a marathon app depending on it will also be deleted). Check the dry-run output to make sure you don't unintentionally delete the wrong entity. The command is idempotent, so deleting an already deleted entity has no effect.\nThe delete command will not modify your configuration files. So to make sure that the deleted entity will not be recreated during the next <code>apply</code>-run, remove the entity definition from your yaml files.</p>\n<h2>Roadmap</h2>\n<ul>\n<li>Support for creating and configuring kubernetes clusters</li>\n<li>Add as package to the Mesosphere universe for easy installation as a module for the dcos-cli</li>\n<li>Better and documented plugin support</li>\n<li>Provide some sort of verification for configurations</li>\n</ul>\n<h2>Contributing</h2>\n<p>If you found a bug or have a feature request, please open an issue in Github.</p>\n\n          </div>"}, "last_serial": 6563484, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "21a7fd9726e96867ce48fdd2a0feabe0", "sha256": "6fa0338c3cad5d3b4224711a89f4e970882c2b6cd617f59c42a937315b351ff8"}, "downloads": -1, "filename": "dcos-deploy-0.1.0.tar.gz", "has_sig": false, "md5_digest": "21a7fd9726e96867ce48fdd2a0feabe0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 36628, "upload_time": "2018-12-19T12:06:03", "upload_time_iso_8601": "2018-12-19T12:06:03.873122Z", "url": "https://files.pythonhosted.org/packages/57/75/0617867c639a27fe6e53a44cb3d53f551f3a3cd27cfdd46e9a04639bd0bb/dcos-deploy-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "40a1f278024c99b712de3ec4f3ff7cdb", "sha256": "e948c8d719421a6a1bf2cbe79b18263674074a66e4fcaea21a103f80b899ff3b"}, "downloads": -1, "filename": "dcos-deploy-0.2.0.tar.gz", "has_sig": false, "md5_digest": "40a1f278024c99b712de3ec4f3ff7cdb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 53528, "upload_time": "2019-05-31T13:11:06", "upload_time_iso_8601": "2019-05-31T13:11:06.608660Z", "url": "https://files.pythonhosted.org/packages/f0/28/0f46f5c74ebbe23fa8e6f635f280f4848c948f2f2a43851533356bdc644f/dcos-deploy-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "2f6bc7bbe696631ee43f81d3b3c1e13a", "sha256": "a42c73d5e829a0f74f8b39d5eaa3ca1f441fd823df3667bf372cda2741ce96e5"}, "downloads": -1, "filename": "dcos-deploy-0.3.0.tar.gz", "has_sig": false, "md5_digest": "2f6bc7bbe696631ee43f81d3b3c1e13a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 60275, "upload_time": "2020-02-03T16:03:48", "upload_time_iso_8601": "2020-02-03T16:03:48.343991Z", "url": "https://files.pythonhosted.org/packages/a1/71/1969748b6a52477d6b416dbb5ff0b738fdb777cfe086e9c6fe99ceacc8c4/dcos-deploy-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2f6bc7bbe696631ee43f81d3b3c1e13a", "sha256": "a42c73d5e829a0f74f8b39d5eaa3ca1f441fd823df3667bf372cda2741ce96e5"}, "downloads": -1, "filename": "dcos-deploy-0.3.0.tar.gz", "has_sig": false, "md5_digest": "2f6bc7bbe696631ee43f81d3b3c1e13a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 60275, "upload_time": "2020-02-03T16:03:48", "upload_time_iso_8601": "2020-02-03T16:03:48.343991Z", "url": "https://files.pythonhosted.org/packages/a1/71/1969748b6a52477d6b416dbb5ff0b738fdb777cfe086e9c6fe99ceacc8c4/dcos-deploy-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:39:47 2020"}