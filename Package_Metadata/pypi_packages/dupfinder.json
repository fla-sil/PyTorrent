{"info": {"author": "Andriy Mylenkyy", "author_email": "mylanium@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "This package designed to find and manage duplications,\nand contains two utilities:\n\n * dupfind - to find duplications\n * dupmanage - to manage found duplications\n\n\nDUPFIND UTILITY:\n================\n\n*dupfind* utility allows you to find duplicated files and directories\nin your file system.\n\nShow how utility find duplicated files:\n=======================================\n\nBy default utility identifies *duplication files* by\nfile content.\n\nFirst of all - create several different files\nin the current directory.\n\n    >>> createFile('tfile1.txt', \"A\"*10)\n    >>> createFile('tfile2.txt', \"A\"*1025)\n    >>> createFile('tfile3.txt', \"A\"*2048)\n\nThen create other files in another directory, one\nof them to be the same as already created ones.\n\n    >>> mkd(\"dir1\")\n    >>> createFile('tfile1.txt', \"A\"*20, \"dir1\")\n    >>> createFile('tfile2.txt', \"A\"*1025, \"dir1\")\n    >>> createFile('tfile13.txt', \"A\"*48, \"dir1\")\n\nLook into the directories contents:\n\n    >>> ls()\n    === list directory ===\n    D :: dir1 :: ...\n    F :: tfile1.txt :: 10\n    F :: tfile2.txt :: 1025\n    F :: tfile3.txt :: 2048\n\n    >>> ls(\"dir1\")\n    === list dir1 directory ===\n    F :: tfile1.txt :: 20\n    F :: tfile13.txt :: 48\n    F :: tfile2.txt :: 1025\n\n\nWe see, that \"tfile2.txt\" is same in both directories,\nwhile \"tfile1.txt\" - has the same name, but differs in size.\nSo utility must identify only \"tfile2.txt\" as a duplication file.\n\nWe force output results with \"-o <output file name>\"\nargument to *outputf* file, and pass *testdir* as directory\nthat is looking for duplications.\n\n    >>> dupfind(\"-o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n\n\nNow check the results file for duplications.\n \n    >>> cat(outputf)\n    hash,size,type,ext,name,directory,modification,operation,operation_data\n    ...,1025,F,txt,tfile2.txt,.../tmp.../dir1,...\n    ...,1025,F,txt,tfile2.txt,.../tmp...,...\n\n\n\nShow quick/slow utility mode:\n=============================\n\nAs mentioned above - utility identifies *duplication files*\nby file contents. This mode slows down the system and consumes\na lot of system resources.\n\nHowever, in most cases the file name and size is enough to identify the\nduplication. So in that case you can use *quick* mode\n--quick (-q) option.\n\nSo test the previous files in the quick mode:\n\n    >>> dupfind(\"-q -o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n\n\nNow check the result file for duplications.\n\n    >>> cat(outputf)\n    hash,size,type,ext,name,directory,modification,operation,operation_data\n    ...,1025,F,txt,tfile2.txt,.../tmp.../dir1,...\n    ...,1025,F,txt,tfile2.txt,.../tmp...,...\n\n\nAs we can see the quick mode identifies duplications correctly.\n\n\nLet's show that there are cases when this mode can lead to mistakes.\nTo do that let's add a file with the same name and size but different content\nand apply utility in both modes:\n\n\n    >>> createFile('tfile000.txt', \"First  \"*20,)\n    >>> createFile('tfile000.txt', \"Second \"*20, \"dir1\")\n\n\nNow check the duplication results using default (not quick mode) ...\n\n\n    >>> dupfind(\" -o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n    >>> cat(outputf)\n    hash,size,type,ext,name,directory,modification,operation,operation_data\n    ...,1025,F,txt,tfile2.txt,.../tmp.../dir1,...\n    ...,1025,F,txt,tfile2.txt,.../tmp...,...\n\n\nAs we can see *not-quick* mode identifies duplications correctly.\n\nLet's check duplications using the quick mode...\n\n\n    >>> dupfind(\" -q -o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n    >>> cat(outputf)\n    hash,size,type,ext,name,directory,modification,operation,operation_data\n    ...,140,F,txt,tfile000.txt,.../tmp.../dir1,...\n    ...,140,F,txt,tfile000.txt,.../tmp...,...\n    ...,1025,F,txt,tfile2.txt,.../tmp.../dir1,...\n    ...,1025,F,txt,tfile2.txt,.../tmp...,...\n\n\nAs we can see wrong duplications are found using the quick-mode.\n\n\nCleanup the test\n\n    >>> cleanTestDir()\n\n\nShow how utility finds duplicated directories:\n==============================================\n\nUtility identifies *duplicated directories* as\ndirectories, all files of which are *duplicated*\nand all inner directories are also *duplicated\ndirectories*.\n\n\nFirst compare 2 directories with the same files.\n------------------------------------------------\n\nCreate directories with the same content.\n\n    >>> def mkDir(dpath):\n    ...     mkd(dpath)\n    ...     createFile('tfile1.txt', \"A\"*10, dpath)\n    ...     createFile('tfile2.txt', \"A\"*1025, dpath)\n    ...     createFile('tfile3.txt', \"A\"*2048, dpath)\n    ... \n    >>> mkDir(\"dir1\")\n    >>> mkDir(\"dir2\")\n\nConfirm that the directories' contents are really identical\n\n    >>> ls(\"dir1\")\n    === list dir1 directory ===\n    F :: tfile1.txt :: 10\n    F :: tfile2.txt :: 1025\n    F :: tfile3.txt :: 2048\n\n    >>> ls(\"dir2\")\n    === list dir2 directory ===\n    F :: tfile1.txt :: 10\n    F :: tfile2.txt :: 1025\n    F :: tfile3.txt :: 2048\n\n\nNow run the utility and check the result file:\n\n    >>> dupfind(\"-o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n    >>> cat(outputf)\n    hash,size,type,ext,name,directory,modification,operation,operation_data\n    ...,D,,dir1,...\n    ...,D,,dir2,...\n\n    \nCompare 2 directories with the same files and dirs.\n---------------------------------------------------\n\nCreate new directories with the same content, but different names in previously\ncreated directories. \n\nSo for directories to be interpreted as duplications - they don't need to have\nthe same name, but the identical content.\n\nAdd 2 identical directories to the previous ones.\n\n    >>> def mkDir1(dpath):\n    ...     mkd(dpath)\n    ...     createFile('tfile11.txt', \"B\"*4000, dpath)\n    ...     createFile('tfile12.txt', \"B\"*222, dpath)\n    ... \n    >>> mkDir1(\"dir1/dir11\")\n    >>> mkDir1(\"dir2/dir21\")\n\nNote that we added two directories with same contents,\nbut different names. This should not break duplications.\n\n    >>> def mkDir2(dpath):\n    ...     mkd(dpath)\n    ...     createFile('tfile21.txt', \"C\"*4096, dpath)\n    ...     createFile('tfile22.txt', \"C\"*123, dpath)\n    ...     createFile('tfile23.txt', \"C\"*444, dpath)\n    ...     createFile('tfile24.txt', \"C\"*555, dpath)\n    ... \n    >>> mkDir2(\"dir1/dir22\")\n    >>> mkDir2(\"dir2/dir22\")\n\n\nConfirm that directories' contents are really identical\n\n    >>> ls(\"dir1\")\n    === list dir1 directory ===\n    D :: dir11 :: -1\n    D :: dir22 :: -1\n    F :: tfile1.txt :: 10\n    F :: tfile2.txt :: 1025\n    F :: tfile3.txt :: 2048\n    >>> ls(\"dir2\")\n    === list dir2 directory ===\n    D :: dir21 :: -1\n    D :: dir22 :: -1\n    F :: tfile1.txt :: 10\n    F :: tfile2.txt :: 1025\n    F :: tfile3.txt :: 2048\n\nAnd contents for inner directories\n\nFirst subdirectory:\n\n    >>> ls(\"dir1/dir11\")\n    === list dir1/dir11 directory ===\n    F :: tfile11.txt :: 4000\n    F :: tfile12.txt :: 222\n    >>> ls(\"dir2/dir21\")\n    === list dir2/dir21 directory ===\n    F :: tfile11.txt :: 4000\n    F :: tfile12.txt :: 222\n\nSecond subdirectory:\n\n    >>> ls(\"dir1/dir22\")\n    === list dir1/dir22 directory ===\n    F :: tfile21.txt :: 4096\n    F :: tfile22.txt :: 123\n    F :: tfile23.txt :: 444\n    F :: tfile24.txt :: 555\n    >>> ls(\"dir2/dir22\")\n    === list dir2/dir22 directory ===\n    F :: tfile21.txt :: 4096\n    F :: tfile22.txt :: 123\n    F :: tfile23.txt :: 444\n    F :: tfile24.txt :: 555\n\n\nNow test the utility.\n\n    >>> dupfind(\"-o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n\n\nChecks the results file for duplications.\n\n    >>> cat(outputf)\n    hash,size,type,ext,name,directory,modification,operation,operation_data\n    ...,D,,dir1,...\n    ...,D,,dir2,...\n\n\nNOTE:\n-----\nInner duplication directories are excluded from the results:\n\n    >>> outputres = file(outputf).read()\n    >>> \"dir1/dir11\" in outputres\n    False\n    >>> \"dir1/dir22\" in outputres\n    False\n    >>> \"dir2/dir21\" in outputres\n    False\n    >>> \"dir2/dir22\" in outputres\n    False\n\n\nUtility accepts more than one argument as directories list:\n===========================================================\n\nUse previous directory structure to prove this:\n\nNow pass to utility \"dir1/dir11\" and \"dir2\" directories:\n\n    >>> dupfind(\"-o %(o)s %(dir1-11)s %(dir2)s\" % {\n    ...     'o':outputf,\n    ...     'dir1-11': os.path.join(testdir,\"dir1/dir11\"),\n    ...     'dir2': os.path.join(testdir,\"dir2\"),})\n\n\nNow check the result file for duplications.\n\n    >>> cat(outputf)\n    hash,size,type,ext,name,directory,modification,operation,operation_data\n    ...,D,,dir11,.../tmp.../dir1,...\n    ...,D,,dir21,.../tmp.../dir2,...\n\n\n\nDUPMANAGE UTILITY:\n==================\n\n\n*dupmanage* utility allows you to manage duplication files and directories\nof your file system with csv data file.\n\nUtility use csv-formatted data-file to process duplication items.\nData file must contain the following columns:\n\n  * type\n  * name\n  * directory\n  * operation\n  * operation_data\n\n\nUtility supports 2 types of operations with duplication items:\n\n  * deleting (\"D\")\n  * symlinking (\"L\") only for UNIX-like systems\n\n*operation_data* is only used for *symlinking* operation and \nmust contain the path to symlinking sorce item.\n\n\n\nShow how utility manages duplications:\n======================================\n\nTo show - use previous directory structure and \nalso add several duplications:\n\nCreate a file in the root directory and the same file\nin another catalog.\n\n    >>> createFile('tfile03.txt', \"D\"*100)\n    >>> mkd(\"dir3\")\n    >>> createFile('tfile03.txt', \"D\"*100, \"dir3\")\n\nLook into directories contents:\n\n    >>> ls()\n    === list directory ===\n    D :: dir1 :: ...\n    D :: dir2 :: ...\n    D :: dir3 :: ...\n    F :: tfile03.txt :: 100\n\n    >>> ls(\"dir3\")\n    === list dir3 directory ===\n    F :: tfile03.txt :: 100\n\n\nWe already know the previous duplications, so now we create\ncsv-formatted data file to manage duplications.\n\n    >>> manage_data = \"\"\"type,name,directory,operation,operation_data\n    ... F,tfile03.txt,%(testdir)s/dir3,L,%(testdir)s/tfile03.txt\n    ... D,dir2,%(testdir)s,D,\n    ... \"\"\" % {'testdir': testdir}\n    >>> createFile('manage.csv', manage_data)\n\n\n\nNow call the utility and check result directory content:\n\n    >>> manage_path = os.path.join(testdir, 'manage.csv')\n    >>> dupmanage(\"%s -v\" % manage_path)\n    [...                                                                           \n    [...]: Symlink .../tfile03.txt item to .../dir3/tfile03.txt\n    [...]: Remove .../dir2 directory                                            \n    [...]: Processed 2 items                                                                                                                               \n\n\nReview directory content:\n\n    >>> ls()\n    === list directory ===\n    D :: dir1 :: ...\n    D :: dir3 :: ...\n    F :: tfile03.txt :: 100\n\n    >>> ls(\"dir3\")\n    === list dir3 directory ===\n    L :: tfile03.txt :: ...\n\n\n\n\nHISTORY:\n========\n\n1.4.3\n-----\n\n * Comment useless for now output_format option\n   for dupfinder utility.\n\n\n1.4.2\n-----\n\n * Refactoring content comparison to use zlib.crc32\n   function to calculate file content diges -\n   speedup algorythm.\n * Fixed some bugs\n\n\n1.4\n---\n\n * Updated file duplication finding: added file\n   comparison by content oportunity. Made this\n   variant - default one.\n * Added -q (--quick) option to use quick file\n   comparison (by name and size)\n * Added tests for quick/not-quick duplication\n   finding\n\n\n1.2\n---\n\n * Added *dupmanage* utility for manage duplications\n * Added tests for *dupmanage* utility\n\n\n1.0\n---\n\n * Tests for *dupfinder* utility added\n\n\n0.8\n---\n\n * Refactoring classes: remove DupFilter,\n   move filtering into DupOut class.\n * Force implicitly hiding inner content of\n   a duplication directories.\n\n\n0.7\n---\n\n * Refactoring utility into classes\n * Fix bugs with bad files processing\n * Fix bug with size calculation\n\n\n0.5\n---\n\n * Refactoring inner finding algorithm\n * Implemented opportunity to remove from\n   the result report inner content from \n   duplication directories\n\n\n0.3\n---\n\n * Files finder implemented\n * Output in csv format\n * added filters by size\n\n\n0.1\n---\n\n * Initial release", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://python.org/pypi/dupfinder", "keywords": "file duplication finder manager", "license": "GPL", "maintainer": null, "maintainer_email": null, "name": "dupfinder", "package_url": "https://pypi.org/project/dupfinder/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/dupfinder/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://python.org/pypi/dupfinder"}, "release_url": "https://pypi.org/project/dupfinder/1.4.3/", "requires_dist": null, "requires_python": null, "summary": "Find and manage duplication files on the file system", "version": "1.4.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This package designed to find and manage duplications,\nand contains two utilities:</p>\n<blockquote>\n<ul>\n<li>dupfind - to find duplications</li>\n<li>dupmanage - to manage found duplications</li>\n</ul>\n</blockquote>\n<div id=\"dupfind-utility\">\n<h2>DUPFIND UTILITY:</h2>\n<p><em>dupfind</em> utility allows you to find duplicated files and directories\nin your file system.</p>\n</div>\n<div id=\"show-how-utility-find-duplicated-files\">\n<h2>Show how utility find duplicated files:</h2>\n<p>By default utility identifies <em>duplication files</em> by\nfile content.</p>\n<p>First of all - create several different files\nin the current directory.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; createFile('tfile1.txt', \"A\"*10)\n&gt;&gt;&gt; createFile('tfile2.txt', \"A\"*1025)\n&gt;&gt;&gt; createFile('tfile3.txt', \"A\"*2048)\n</pre>\n</blockquote>\n<p>Then create other files in another directory, one\nof them to be the same as already created ones.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; mkd(\"dir1\")\n&gt;&gt;&gt; createFile('tfile1.txt', \"A\"*20, \"dir1\")\n&gt;&gt;&gt; createFile('tfile2.txt', \"A\"*1025, \"dir1\")\n&gt;&gt;&gt; createFile('tfile13.txt', \"A\"*48, \"dir1\")\n</pre>\n</blockquote>\n<p>Look into the directories contents:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; ls()\n=== list directory ===\nD :: dir1 :: ...\nF :: tfile1.txt :: 10\nF :: tfile2.txt :: 1025\nF :: tfile3.txt :: 2048\n</pre>\n<pre>&gt;&gt;&gt; ls(\"dir1\")\n=== list dir1 directory ===\nF :: tfile1.txt :: 20\nF :: tfile13.txt :: 48\nF :: tfile2.txt :: 1025\n</pre>\n</blockquote>\n<p>We see, that \u201ctfile2.txt\u201d is same in both directories,\nwhile \u201ctfile1.txt\u201d - has the same name, but differs in size.\nSo utility must identify only \u201ctfile2.txt\u201d as a duplication file.</p>\n<p>We force output results with \u201c-o &lt;output file name&gt;\u201d\nargument to <em>outputf</em> file, and pass <em>testdir</em> as directory\nthat is looking for duplications.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; dupfind(\"-o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n</pre>\n</blockquote>\n<p>Now check the results file for duplications.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; cat(outputf)\nhash,size,type,ext,name,directory,modification,operation,operation_data\n...,1025,F,txt,tfile2.txt,.../tmp.../dir1,...\n...,1025,F,txt,tfile2.txt,.../tmp...,...\n</pre>\n</blockquote>\n</div>\n<div id=\"show-quick-slow-utility-mode\">\n<h2>Show quick/slow utility mode:</h2>\n<p>As mentioned above - utility identifies <em>duplication files</em>\nby file contents. This mode slows down the system and consumes\na lot of system resources.</p>\n<p>However, in most cases the file name and size is enough to identify the\nduplication. So in that case you can use <em>quick</em> mode\n\u2013quick (-q) option.</p>\n<p>So test the previous files in the quick mode:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; dupfind(\"-q -o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n</pre>\n</blockquote>\n<p>Now check the result file for duplications.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; cat(outputf)\nhash,size,type,ext,name,directory,modification,operation,operation_data\n...,1025,F,txt,tfile2.txt,.../tmp.../dir1,...\n...,1025,F,txt,tfile2.txt,.../tmp...,...\n</pre>\n</blockquote>\n<p>As we can see the quick mode identifies duplications correctly.</p>\n<p>Let\u2019s show that there are cases when this mode can lead to mistakes.\nTo do that let\u2019s add a file with the same name and size but different content\nand apply utility in both modes:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; createFile('tfile000.txt', \"First  \"*20,)\n&gt;&gt;&gt; createFile('tfile000.txt', \"Second \"*20, \"dir1\")\n</pre>\n</blockquote>\n<p>Now check the duplication results using default (not quick mode) \u2026</p>\n<blockquote>\n<pre>&gt;&gt;&gt; dupfind(\" -o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n&gt;&gt;&gt; cat(outputf)\nhash,size,type,ext,name,directory,modification,operation,operation_data\n...,1025,F,txt,tfile2.txt,.../tmp.../dir1,...\n...,1025,F,txt,tfile2.txt,.../tmp...,...\n</pre>\n</blockquote>\n<p>As we can see <em>not-quick</em> mode identifies duplications correctly.</p>\n<p>Let\u2019s check duplications using the quick mode\u2026</p>\n<blockquote>\n<pre>&gt;&gt;&gt; dupfind(\" -q -o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n&gt;&gt;&gt; cat(outputf)\nhash,size,type,ext,name,directory,modification,operation,operation_data\n...,140,F,txt,tfile000.txt,.../tmp.../dir1,...\n...,140,F,txt,tfile000.txt,.../tmp...,...\n...,1025,F,txt,tfile2.txt,.../tmp.../dir1,...\n...,1025,F,txt,tfile2.txt,.../tmp...,...\n</pre>\n</blockquote>\n<p>As we can see wrong duplications are found using the quick-mode.</p>\n<p>Cleanup the test</p>\n<blockquote>\n<pre>&gt;&gt;&gt; cleanTestDir()\n</pre>\n</blockquote>\n</div>\n<div id=\"show-how-utility-finds-duplicated-directories\">\n<h2>Show how utility finds duplicated directories:</h2>\n<p>Utility identifies <em>duplicated directories</em> as\ndirectories, all files of which are <em>duplicated</em>\nand all inner directories are also <em>duplicated\ndirectories</em>.</p>\n<div id=\"first-compare-2-directories-with-the-same-files\">\n<h3>First compare 2 directories with the same files.</h3>\n<p>Create directories with the same content.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; def mkDir(dpath):\n...     mkd(dpath)\n...     createFile('tfile1.txt', \"A\"*10, dpath)\n...     createFile('tfile2.txt', \"A\"*1025, dpath)\n...     createFile('tfile3.txt', \"A\"*2048, dpath)\n...\n&gt;&gt;&gt; mkDir(\"dir1\")\n&gt;&gt;&gt; mkDir(\"dir2\")\n</pre>\n</blockquote>\n<p>Confirm that the directories\u2019 contents are really identical</p>\n<blockquote>\n<pre>&gt;&gt;&gt; ls(\"dir1\")\n=== list dir1 directory ===\nF :: tfile1.txt :: 10\nF :: tfile2.txt :: 1025\nF :: tfile3.txt :: 2048\n</pre>\n<pre>&gt;&gt;&gt; ls(\"dir2\")\n=== list dir2 directory ===\nF :: tfile1.txt :: 10\nF :: tfile2.txt :: 1025\nF :: tfile3.txt :: 2048\n</pre>\n</blockquote>\n<p>Now run the utility and check the result file:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; dupfind(\"-o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n&gt;&gt;&gt; cat(outputf)\nhash,size,type,ext,name,directory,modification,operation,operation_data\n...,D,,dir1,...\n...,D,,dir2,...\n</pre>\n</blockquote>\n</div>\n<div id=\"compare-2-directories-with-the-same-files-and-dirs\">\n<h3>Compare 2 directories with the same files and dirs.</h3>\n<p>Create new directories with the same content, but different names in previously\ncreated directories.</p>\n<p>So for directories to be interpreted as duplications - they don\u2019t need to have\nthe same name, but the identical content.</p>\n<p>Add 2 identical directories to the previous ones.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; def mkDir1(dpath):\n...     mkd(dpath)\n...     createFile('tfile11.txt', \"B\"*4000, dpath)\n...     createFile('tfile12.txt', \"B\"*222, dpath)\n...\n&gt;&gt;&gt; mkDir1(\"dir1/dir11\")\n&gt;&gt;&gt; mkDir1(\"dir2/dir21\")\n</pre>\n</blockquote>\n<p>Note that we added two directories with same contents,\nbut different names. This should not break duplications.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; def mkDir2(dpath):\n...     mkd(dpath)\n...     createFile('tfile21.txt', \"C\"*4096, dpath)\n...     createFile('tfile22.txt', \"C\"*123, dpath)\n...     createFile('tfile23.txt', \"C\"*444, dpath)\n...     createFile('tfile24.txt', \"C\"*555, dpath)\n...\n&gt;&gt;&gt; mkDir2(\"dir1/dir22\")\n&gt;&gt;&gt; mkDir2(\"dir2/dir22\")\n</pre>\n</blockquote>\n<p>Confirm that directories\u2019 contents are really identical</p>\n<blockquote>\n<pre>&gt;&gt;&gt; ls(\"dir1\")\n=== list dir1 directory ===\nD :: dir11 :: -1\nD :: dir22 :: -1\nF :: tfile1.txt :: 10\nF :: tfile2.txt :: 1025\nF :: tfile3.txt :: 2048\n&gt;&gt;&gt; ls(\"dir2\")\n=== list dir2 directory ===\nD :: dir21 :: -1\nD :: dir22 :: -1\nF :: tfile1.txt :: 10\nF :: tfile2.txt :: 1025\nF :: tfile3.txt :: 2048\n</pre>\n</blockquote>\n<p>And contents for inner directories</p>\n<p>First subdirectory:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; ls(\"dir1/dir11\")\n=== list dir1/dir11 directory ===\nF :: tfile11.txt :: 4000\nF :: tfile12.txt :: 222\n&gt;&gt;&gt; ls(\"dir2/dir21\")\n=== list dir2/dir21 directory ===\nF :: tfile11.txt :: 4000\nF :: tfile12.txt :: 222\n</pre>\n</blockquote>\n<p>Second subdirectory:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; ls(\"dir1/dir22\")\n=== list dir1/dir22 directory ===\nF :: tfile21.txt :: 4096\nF :: tfile22.txt :: 123\nF :: tfile23.txt :: 444\nF :: tfile24.txt :: 555\n&gt;&gt;&gt; ls(\"dir2/dir22\")\n=== list dir2/dir22 directory ===\nF :: tfile21.txt :: 4096\nF :: tfile22.txt :: 123\nF :: tfile23.txt :: 444\nF :: tfile24.txt :: 555\n</pre>\n</blockquote>\n<p>Now test the utility.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; dupfind(\"-o %(o)s %(dir)s\" % {'o':outputf, 'dir': testdir})\n</pre>\n</blockquote>\n<p>Checks the results file for duplications.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; cat(outputf)\nhash,size,type,ext,name,directory,modification,operation,operation_data\n...,D,,dir1,...\n...,D,,dir2,...\n</pre>\n</blockquote>\n</div>\n<div id=\"note\">\n<h3>NOTE:</h3>\n<p>Inner duplication directories are excluded from the results:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; outputres = file(outputf).read()\n&gt;&gt;&gt; \"dir1/dir11\" in outputres\nFalse\n&gt;&gt;&gt; \"dir1/dir22\" in outputres\nFalse\n&gt;&gt;&gt; \"dir2/dir21\" in outputres\nFalse\n&gt;&gt;&gt; \"dir2/dir22\" in outputres\nFalse\n</pre>\n</blockquote>\n</div>\n</div>\n<div id=\"utility-accepts-more-than-one-argument-as-directories-list\">\n<h2>Utility accepts more than one argument as directories list:</h2>\n<p>Use previous directory structure to prove this:</p>\n<p>Now pass to utility \u201cdir1/dir11\u201d and \u201cdir2\u201d directories:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; dupfind(\"-o %(o)s %(dir1-11)s %(dir2)s\" % {\n...     'o':outputf,\n...     'dir1-11': os.path.join(testdir,\"dir1/dir11\"),\n...     'dir2': os.path.join(testdir,\"dir2\"),})\n</pre>\n</blockquote>\n<p>Now check the result file for duplications.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; cat(outputf)\nhash,size,type,ext,name,directory,modification,operation,operation_data\n...,D,,dir11,.../tmp.../dir1,...\n...,D,,dir21,.../tmp.../dir2,...\n</pre>\n</blockquote>\n</div>\n<div id=\"dupmanage-utility\">\n<h2>DUPMANAGE UTILITY:</h2>\n<p><em>dupmanage</em> utility allows you to manage duplication files and directories\nof your file system with csv data file.</p>\n<p>Utility use csv-formatted data-file to process duplication items.\nData file must contain the following columns:</p>\n<blockquote>\n<ul>\n<li>type</li>\n<li>name</li>\n<li>directory</li>\n<li>operation</li>\n<li>operation_data</li>\n</ul>\n</blockquote>\n<p>Utility supports 2 types of operations with duplication items:</p>\n<blockquote>\n<ul>\n<li>deleting (\u201cD\u201d)</li>\n<li>symlinking (\u201cL\u201d) only for UNIX-like systems</li>\n</ul>\n</blockquote>\n<p><em>operation_data</em> is only used for <em>symlinking</em> operation and\nmust contain the path to symlinking sorce item.</p>\n</div>\n<div id=\"show-how-utility-manages-duplications\">\n<h2>Show how utility manages duplications:</h2>\n<p>To show - use previous directory structure and\nalso add several duplications:</p>\n<p>Create a file in the root directory and the same file\nin another catalog.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; createFile('tfile03.txt', \"D\"*100)\n&gt;&gt;&gt; mkd(\"dir3\")\n&gt;&gt;&gt; createFile('tfile03.txt', \"D\"*100, \"dir3\")\n</pre>\n</blockquote>\n<p>Look into directories contents:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; ls()\n=== list directory ===\nD :: dir1 :: ...\nD :: dir2 :: ...\nD :: dir3 :: ...\nF :: tfile03.txt :: 100\n</pre>\n<pre>&gt;&gt;&gt; ls(\"dir3\")\n=== list dir3 directory ===\nF :: tfile03.txt :: 100\n</pre>\n</blockquote>\n<p>We already know the previous duplications, so now we create\ncsv-formatted data file to manage duplications.</p>\n<blockquote>\n<pre>&gt;&gt;&gt; manage_data = \"\"\"type,name,directory,operation,operation_data\n... F,tfile03.txt,%(testdir)s/dir3,L,%(testdir)s/tfile03.txt\n... D,dir2,%(testdir)s,D,\n... \"\"\" % {'testdir': testdir}\n&gt;&gt;&gt; createFile('manage.csv', manage_data)\n</pre>\n</blockquote>\n<p>Now call the utility and check result directory content:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; manage_path = os.path.join(testdir, 'manage.csv')\n&gt;&gt;&gt; dupmanage(\"%s -v\" % manage_path)\n[...\n[...]: Symlink .../tfile03.txt item to .../dir3/tfile03.txt\n[...]: Remove .../dir2 directory\n[...]: Processed 2 items\n</pre>\n</blockquote>\n<p>Review directory content:</p>\n<blockquote>\n<pre>&gt;&gt;&gt; ls()\n=== list directory ===\nD :: dir1 :: ...\nD :: dir3 :: ...\nF :: tfile03.txt :: 100\n</pre>\n<pre>&gt;&gt;&gt; ls(\"dir3\")\n=== list dir3 directory ===\nL :: tfile03.txt :: ...\n</pre>\n</blockquote>\n</div>\n<div id=\"history\">\n<h2>HISTORY:</h2>\n<div id=\"id1\">\n<h3>1.4.3</h3>\n<blockquote>\n<ul>\n<li>Comment useless for now output_format option\nfor dupfinder utility.</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"id2\">\n<h3>1.4.2</h3>\n<blockquote>\n<ul>\n<li>Refactoring content comparison to use zlib.crc32\nfunction to calculate file content diges -\nspeedup algorythm.</li>\n<li>Fixed some bugs</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"id3\">\n<h3>1.4</h3>\n<blockquote>\n<ul>\n<li>Updated file duplication finding: added file\ncomparison by content oportunity. Made this\nvariant - default one.</li>\n<li>Added -q (\u2013quick) option to use quick file\ncomparison (by name and size)</li>\n<li>Added tests for quick/not-quick duplication\nfinding</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"id4\">\n<h3>1.2</h3>\n<blockquote>\n<ul>\n<li>Added <em>dupmanage</em> utility for manage duplications</li>\n<li>Added tests for <em>dupmanage</em> utility</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"id5\">\n<h3>1.0</h3>\n<blockquote>\n<ul>\n<li>Tests for <em>dupfinder</em> utility added</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"id6\">\n<h3>0.8</h3>\n<blockquote>\n<ul>\n<li>Refactoring classes: remove DupFilter,\nmove filtering into DupOut class.</li>\n<li>Force implicitly hiding inner content of\na duplication directories.</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"id7\">\n<h3>0.7</h3>\n<blockquote>\n<ul>\n<li>Refactoring utility into classes</li>\n<li>Fix bugs with bad files processing</li>\n<li>Fix bug with size calculation</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"id8\">\n<h3>0.5</h3>\n<blockquote>\n<ul>\n<li>Refactoring inner finding algorithm</li>\n<li>Implemented opportunity to remove from\nthe result report inner content from\nduplication directories</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"id9\">\n<h3>0.3</h3>\n<blockquote>\n<ul>\n<li>Files finder implemented</li>\n<li>Output in csv format</li>\n<li>added filters by size</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"id10\">\n<h3>0.1</h3>\n<blockquote>\n<ul>\n<li>Initial release</li>\n</ul>\n</blockquote>\n</div>\n</div>\n\n          </div>"}, "last_serial": 147508, "releases": {"1.4.3": [{"comment_text": "", "digests": {"md5": "5b0697388a8a9913a0ffc56a5e38926a", "sha256": "507cdb501eb7161a91f4b6fdb4ffe72f0878016e71dfa10f975af6a9519e894f"}, "downloads": -1, "filename": "dupfinder-1.4.3-py2.4.egg", "has_sig": false, "md5_digest": "5b0697388a8a9913a0ffc56a5e38926a", "packagetype": "bdist_egg", "python_version": "2.4", "requires_python": null, "size": 23626, "upload_time": "2010-01-20T23:38:53", "upload_time_iso_8601": "2010-01-20T23:38:53.535071Z", "url": "https://files.pythonhosted.org/packages/5b/3a/a1c1f17d42048bd5a44555f158bd59a03da596b59248fb9e71f3a36b970b/dupfinder-1.4.3-py2.4.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "25b2b06a31e74ce1e59a23de682af1ff", "sha256": "3cab159008fa5a8b88da2bc8b1e0ed2761cecdcb9f983c0f3f8b7c771ab3f9af"}, "downloads": -1, "filename": "dupfinder-1.4.3.tar.gz", "has_sig": false, "md5_digest": "25b2b06a31e74ce1e59a23de682af1ff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10538, "upload_time": "2010-01-20T23:38:51", "upload_time_iso_8601": "2010-01-20T23:38:51.932279Z", "url": "https://files.pythonhosted.org/packages/54/c1/7c4fa491298a83f98318ef6e429c372797667a89f798fd7cf06748034c85/dupfinder-1.4.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5b0697388a8a9913a0ffc56a5e38926a", "sha256": "507cdb501eb7161a91f4b6fdb4ffe72f0878016e71dfa10f975af6a9519e894f"}, "downloads": -1, "filename": "dupfinder-1.4.3-py2.4.egg", "has_sig": false, "md5_digest": "5b0697388a8a9913a0ffc56a5e38926a", "packagetype": "bdist_egg", "python_version": "2.4", "requires_python": null, "size": 23626, "upload_time": "2010-01-20T23:38:53", "upload_time_iso_8601": "2010-01-20T23:38:53.535071Z", "url": "https://files.pythonhosted.org/packages/5b/3a/a1c1f17d42048bd5a44555f158bd59a03da596b59248fb9e71f3a36b970b/dupfinder-1.4.3-py2.4.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "25b2b06a31e74ce1e59a23de682af1ff", "sha256": "3cab159008fa5a8b88da2bc8b1e0ed2761cecdcb9f983c0f3f8b7c771ab3f9af"}, "downloads": -1, "filename": "dupfinder-1.4.3.tar.gz", "has_sig": false, "md5_digest": "25b2b06a31e74ce1e59a23de682af1ff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10538, "upload_time": "2010-01-20T23:38:51", "upload_time_iso_8601": "2010-01-20T23:38:51.932279Z", "url": "https://files.pythonhosted.org/packages/54/c1/7c4fa491298a83f98318ef6e429c372797667a89f798fd7cf06748034c85/dupfinder-1.4.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:49:00 2020"}