{"info": {"author": "Yassine Azzouz", "author_email": "yassine.azzouz@agmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Intended Audience :: System Administrators", "License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4"], "description": "ahdp\n==================================\n\nahdp is an ansible library of modules for integration with hadoop framework, it provides a way to interact with different hadoop services in a very simple and flexible way using ansible's easy syntax. The popse of this project is to provide DevOps and platform administrators a simple way to automate their operations on large scale hadoop clusters using ansible.\n\nFeatures\n---------------\n\nCurrently ahdp provides modules to interact with HDFS through WEBHDFS or HTTPFS and with hive server 2 or impala using thrift.\n\n* Ansible libraries and utilities functions for HDFS operations :\n    * create directories and files\n    * change directories and files attributes and ownership\n    * manage acls\n    * manage extra attributes\n    * fetch and copy files to HDFS\n    * advanced search functionalities.\n    * manage hdfs snapshots\n    * The HDFS modules are based on [ pywhdfs ](https://github.com/yassineazzouz/pywhdfs) project to establish WebHDFS and HTTPFS connections with hdfs service.\n        - Support both secure (Kerberos,Token) and insecure clusters\n        - Supports HA clusters and handle namenode failover\n        - Supports HDFS federation with multiple nameservices and mount points.\n    * Please refer to the [ hdfs modules documentation ](webdocs/web/hdfs-modules-docs.md) for more details about all the supported modules\n\n* Ansible libraries and utilities functions for HIVE operations :\n    * create and delete databases\n    * Manage privileges on hive database objects.\n    * The hive modules are based on [ impyla client ](https://github.com/cloudera/impyla) project to interact with HIVE Mmetastore:\n        - Support for multiple types of authentication (Kerberos, LDAP, PLAIN, NOSASL)\n        - Support for SSL\n        - Works with both hive server 2 and impala daemons connections.\n    * Please refer to the [ hive modules documentation ](webdocs/web/hive-modules-docs.md) for more details about all the supported modules\n\n\nInstallation & Configuration\n---------------\n\nThe ahdp module need to be installed on both the ansible server and on client machines (for instance the gateways that you will use as targets on the playbook). Normally simple ansible modules does not need to exist on target machines, however since the hdfs and hive modules uses a custom module_utils they need to be installed also on the target machine. \n\nTo install ahdp on the ansible host :\n\n```\npip install ahdp\n```\n\nor\n\n```\npip install ahdp[ansible]\n```\n\nTo install ahdp on the target hosts :\n\n```\npip install ahdp[client]\n```\n\nThe client extension will also install all dependencies that need to exist on target machines:\n* [ pywhdfs ](https://github.com/yassineazzouz/pywhdfs)\n* [ impyla client ](https://github.com/cloudera/impyla)\n* thrift_sasl\n* sasl \n\n\nMake sure the following packages are also installed on the target machines :\n    libffi-devel\n    gcc-c++\n    python-devel\n    krb5-devel\n\nNote:\nTo use ahdp modules you need to configure ansible to know where the modules are located, you need simply add the library configuration to your ~/.ansible.cfg or to /etc/ansible.cfg, for instance if you have python 2.7, the modules path will be  :\n\n```\nlibrary = /usr/lib/python2.7/site-packages/ahdp/modules/\n```\n\nYou can also place manually the modules in a path of your choise then set the library option to that path.\n\nDependencies\n\n\nUSAGE\n-------\n\nThe best way to use ahdp is to run ansible playbooks and see it as work, there are some testcases under \"test\" directory, which can give a high level idea of how an ansible project should be structured around hadoop.\n\nBelow a simple playbook that creates a User home directory in hdfs and a database in hive:\n\n```yml\n- hosts: localhost\n  vars:\n    nameservices:\n      - urls: \"http://localhost:50070\"\n        mounts: \"/\"\n    hs2_host: \"localhost\"\n  tasks:\n    - name: Create HDFS user home directory\n      hdfsfile:\n        authentication: \"none\"\n        state: \"directory\"\n        path: \"/user/ansible\"\n        owner: \"ansible\"\n        group: \"supergroup\"\n        mode: \"0700\"\n        nameservices: \"{{nameservices | to_json}}\"\n    - name: Create User hive database\n      hive_db:\n        authentication: \"NOSASL\"\n        user: \"hive\"\n        host: \"{{hs2_host}}\"\n        port: 10000\n        db: \"ansible\"\n        owner: \"ansible\"\n        state: \"present\"\n```\n\nTo run the playbook, simply run:\n\n```\n ansible-playbook simple_test.yml\n```\n\nSOME GOOD PRACTICES\n--------\n\nThe folowing project aim to make hadoop administration and operation easier using ansible, below some useful tips and gidelines on how to structure a hadoop project in ansible:\n\n* Create a separate inventory group for each hadoop cluster and create separate groups for different services and roles, If you are using a cloudera distribution you can also use dynamic inventory based the [ cloudera api ] ( tools/cloudera.py ).\n* Define a gateway or edge node for each cluster to use it as target in your ansible playbooks. Make sure the ahdp project and its dependencies are installed on all edge nodes, you can also configure [ pywhdfs ](https://github.com/yassineazzouz/pywhdfs) and use its cli to interact programatically with the HDFS service.\n* Create separate group variables for every cluster where you can define the connection parameters, for instance below a configuration example for a standalone hadoop installation :\n```\ncat group_vars/local/local.yml\n---\nnameservices:\n      - urls: \"http://localhost:50070\"\n        mounts: \"/\"\nhs2_host: \"localhost\"\nimpala_daemon_host: \"localhost\"\ncloudera_manager_host: \"localhost\"\n```\n* Use ansible vault for passwords, create a separate vault file for every cluster.\n```\nansible-vault view group_vars/local/local_encrypted.yml\n---\nhdfs_kerberos_password: \"password\"\nhdfs_principal: \"hdfs@LOCALDOMAIN\"\nhive_ldap_password: \"password\"\n```\n* Create ansible roles for different kind of operational tasks you perform on your platforms and use \"--limit=NAME_OF_CLUSTER\" to control the target cluster.\n\nQuestion or Ideas ?\n------------\n\nI'd love to hear what you think about ahdp and appreciate any idea or suggestion, Pull requests are also very welcome!", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/yassineazzouz/ahdp", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "ahdp", "package_url": "https://pypi.org/project/ahdp/", "platform": "", "project_url": "https://pypi.org/project/ahdp/", "project_urls": {"Homepage": "https://github.com/yassineazzouz/ahdp"}, "release_url": "https://pypi.org/project/ahdp/1.0.0/", "requires_dist": null, "requires_python": "", "summary": "pydistcp: python WebHDFS inter/intra-cluster data copy tool.", "version": "1.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            ahdp<br>==================================<br><br>ahdp is an ansible library of modules for integration with hadoop framework, it provides a way to interact with different hadoop services in a very simple and flexible way using ansible's easy syntax. The popse of this project is to provide DevOps and platform administrators a simple way to automate their operations on large scale hadoop clusters using ansible.<br><br>Features<br>---------------<br><br>Currently ahdp provides modules to interact with HDFS through WEBHDFS or HTTPFS and with hive server 2 or impala using thrift.<br><br>* Ansible libraries and utilities functions for HDFS operations :<br>    * create directories and files<br>    * change directories and files attributes and ownership<br>    * manage acls<br>    * manage extra attributes<br>    * fetch and copy files to HDFS<br>    * advanced search functionalities.<br>    * manage hdfs snapshots<br>    * The HDFS modules are based on [ pywhdfs ](https://github.com/yassineazzouz/pywhdfs) project to establish WebHDFS and HTTPFS connections with hdfs service.<br>        - Support both secure (Kerberos,Token) and insecure clusters<br>        - Supports HA clusters and handle namenode failover<br>        - Supports HDFS federation with multiple nameservices and mount points.<br>    * Please refer to the [ hdfs modules documentation ](webdocs/web/hdfs-modules-docs.md) for more details about all the supported modules<br><br>* Ansible libraries and utilities functions for HIVE operations :<br>    * create and delete databases<br>    * Manage privileges on hive database objects.<br>    * The hive modules are based on [ impyla client ](https://github.com/cloudera/impyla) project to interact with HIVE Mmetastore:<br>        - Support for multiple types of authentication (Kerberos, LDAP, PLAIN, NOSASL)<br>        - Support for SSL<br>        - Works with both hive server 2 and impala daemons connections.<br>    * Please refer to the [ hive modules documentation ](webdocs/web/hive-modules-docs.md) for more details about all the supported modules<br><br><br>Installation &amp; Configuration<br>---------------<br><br>The ahdp module need to be installed on both the ansible server and on client machines (for instance the gateways that you will use as targets on the playbook). Normally simple ansible modules does not need to exist on target machines, however since the hdfs and hive modules uses a custom module_utils they need to be installed also on the target machine. <br><br>To install ahdp on the ansible host :<br><br>```<br>pip install ahdp<br>```<br><br>or<br><br>```<br>pip install ahdp[ansible]<br>```<br><br>To install ahdp on the target hosts :<br><br>```<br>pip install ahdp[client]<br>```<br><br>The client extension will also install all dependencies that need to exist on target machines:<br>* [ pywhdfs ](https://github.com/yassineazzouz/pywhdfs)<br>* [ impyla client ](https://github.com/cloudera/impyla)<br>* thrift_sasl<br>* sasl <br><br><br>Make sure the following packages are also installed on the target machines :<br>    libffi-devel<br>    gcc-c++<br>    python-devel<br>    krb5-devel<br><br>Note:<br>To use ahdp modules you need to configure ansible to know where the modules are located, you need simply add the library configuration to your ~/.ansible.cfg or to /etc/ansible.cfg, for instance if you have python 2.7, the modules path will be  :<br><br>```<br>library = /usr/lib/python2.7/site-packages/ahdp/modules/<br>```<br><br>You can also place manually the modules in a path of your choise then set the library option to that path.<br><br>Dependencies<br><br><br>USAGE<br>-------<br><br>The best way to use ahdp is to run ansible playbooks and see it as work, there are some testcases under \"test\" directory, which can give a high level idea of how an ansible project should be structured around hadoop.<br><br>Below a simple playbook that creates a User home directory in hdfs and a database in hive:<br><br>```yml<br>- hosts: localhost<br>  vars:<br>    nameservices:<br>      - urls: \"http://localhost:50070\"<br>        mounts: \"/\"<br>    hs2_host: \"localhost\"<br>  tasks:<br>    - name: Create HDFS user home directory<br>      hdfsfile:<br>        authentication: \"none\"<br>        state: \"directory\"<br>        path: \"/user/ansible\"<br>        owner: \"ansible\"<br>        group: \"supergroup\"<br>        mode: \"0700\"<br>        nameservices: \"{{nameservices | to_json}}\"<br>    - name: Create User hive database<br>      hive_db:<br>        authentication: \"NOSASL\"<br>        user: \"hive\"<br>        host: \"{{hs2_host}}\"<br>        port: 10000<br>        db: \"ansible\"<br>        owner: \"ansible\"<br>        state: \"present\"<br>```<br><br>To run the playbook, simply run:<br><br>```<br> ansible-playbook simple_test.yml<br>```<br><br>SOME GOOD PRACTICES<br>--------<br><br>The folowing project aim to make hadoop administration and operation easier using ansible, below some useful tips and gidelines on how to structure a hadoop project in ansible:<br><br>* Create a separate inventory group for each hadoop cluster and create separate groups for different services and roles, If you are using a cloudera distribution you can also use dynamic inventory based the [ cloudera api ] ( tools/cloudera.py ).<br>* Define a gateway or edge node for each cluster to use it as target in your ansible playbooks. Make sure the ahdp project and its dependencies are installed on all edge nodes, you can also configure [ pywhdfs ](https://github.com/yassineazzouz/pywhdfs) and use its cli to interact programatically with the HDFS service.<br>* Create separate group variables for every cluster where you can define the connection parameters, for instance below a configuration example for a standalone hadoop installation :<br>```<br>cat group_vars/local/local.yml<br>---<br>nameservices:<br>      - urls: \"http://localhost:50070\"<br>        mounts: \"/\"<br>hs2_host: \"localhost\"<br>impala_daemon_host: \"localhost\"<br>cloudera_manager_host: \"localhost\"<br>```<br>* Use ansible vault for passwords, create a separate vault file for every cluster.<br>```<br>ansible-vault view group_vars/local/local_encrypted.yml<br>---<br>hdfs_kerberos_password: \"password\"<br>hdfs_principal: \"hdfs@LOCALDOMAIN\"<br>hive_ldap_password: \"password\"<br>```<br>* Create ansible roles for different kind of operational tasks you perform on your platforms and use \"--limit=NAME_OF_CLUSTER\" to control the target cluster.<br><br>Question or Ideas ?<br>------------<br><br>I'd love to hear what you think about ahdp and appreciate any idea or suggestion, Pull requests are also very welcome!\n          </div>"}, "last_serial": 2669013, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "1e5d5690b1c581bbff8b2ef82c335e71", "sha256": "c94650576d99bdac6cc497a4d0169e6264ce0ad227aeeb5dd35afe77352771d5"}, "downloads": -1, "filename": "ahdp-1.0.0.tar.gz", "has_sig": false, "md5_digest": "1e5d5690b1c581bbff8b2ef82c335e71", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 57419, "upload_time": "2017-02-26T14:37:18", "upload_time_iso_8601": "2017-02-26T14:37:18.588102Z", "url": "https://files.pythonhosted.org/packages/ef/d7/e5c7197ce633271d7e0d600b295573d8af06769955cbb8368e5effa73605/ahdp-1.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1e5d5690b1c581bbff8b2ef82c335e71", "sha256": "c94650576d99bdac6cc497a4d0169e6264ce0ad227aeeb5dd35afe77352771d5"}, "downloads": -1, "filename": "ahdp-1.0.0.tar.gz", "has_sig": false, "md5_digest": "1e5d5690b1c581bbff8b2ef82c335e71", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 57419, "upload_time": "2017-02-26T14:37:18", "upload_time_iso_8601": "2017-02-26T14:37:18.588102Z", "url": "https://files.pythonhosted.org/packages/ef/d7/e5c7197ce633271d7e0d600b295573d8af06769955cbb8368e5effa73605/ahdp-1.0.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 16:22:17 2020"}