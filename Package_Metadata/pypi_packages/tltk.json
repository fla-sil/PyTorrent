{"info": {"author": "Wirote Aroonmanakun", "author_email": "awirote@chula.ac.th", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Programming Language :: Python :: 3.4", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Text Processing :: Linguistic"], "description": "Thai Language Toolkit Project  version 1.3.5\n============================================\n\nTLTK is a Python package for Thai language processing: syllable, word, discouse unit segmentation, pos tagging, named entity recognition, grapheme2phoneme, ipa transcription, romanization, etc.  TLTK requires Python 3.4 or higher.\nThe project is a part of open source software developed at Chulalongkorn University.\nSince version 1.2.2 pacakge license is changed to New BSD License (BSD-3-Clause)\n\nInput : must be utf8 Thai texts.\n\nUpdates:\n--------\n\nMore compound words are added in the dictionary. Version 1.1.3-1.1.5 have many entries that are not a word and contain a few errors. Those entries are removed in later versions.\n\nNER tagger model was updated by using more NE data from AiforThai project. \n\ntltk.nlp  :  basic tools for Thai language processing.\n------------------------------------------------------\n\n>>tltk.nlp.chunk(Text) : chunk parsing. The output includes markups for word segments (|), elementary discourse units (<u/>), pos tags (/POS),and named entities (<NEx>...</NEx>), e.g. tltk.nlp.chunk(\"\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e40\u0e02\u0e15\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07\u0e27\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e19\u0e33\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e1b\u0e25\u0e34\u0e07\u0e44\u0e1b\u0e1b\u0e31\u0e01\u0e15\u0e32\u0e21\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33 \u0e43\u0e19\u0e40\u0e02\u0e15\u0e2d\u0e33\u0e40\u0e20\u0e2d\u0e40\u0e21\u0e37\u0e2d\u0e07 \u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07 \u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e19\u0e32\u0e22\u0e2a\u0e38\u0e01\u0e34\u0e08 \u0e2d\u0e32\u0e22\u0e38 65 \u0e1b\u0e35 \u0e16\u0e39\u0e01\u0e1b\u0e25\u0e34\u0e07\u0e01\u0e31\u0e14\u0e41\u0e25\u0e49\u0e27\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e1e\u0e1a\u0e41\u0e1e\u0e17\u0e22\u0e4c\")\n\n=> '<NEo>\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19/NOUN|\u0e40\u0e02\u0e15/NOUN|\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23/PROPN|</NEo>\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07/VERB|\u0e27\u0e48\u0e32/SCONJ|<s/>/PUNCT|\u0e44\u0e14\u0e49/AUX|\u0e19\u0e33/VERB|\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28/NOUN|\u0e40\u0e15\u0e37\u0e2d\u0e19/VERB|\u0e1b\u0e25\u0e34\u0e07/NOUN|\u0e44\u0e1b/VERB|\u0e1b\u0e31\u0e01/VERB|\u0e15\u0e32\u0e21/ADP|\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33/NOUN|<u/>\u0e43\u0e19/ADP|<NEl>\u0e40\u0e02\u0e15/NOUN|\u0e2d\u0e33\u0e40\u0e20\u0e2d/NOUN|\u0e40\u0e21\u0e37\u0e2d\u0e07/NOUN|<s/>/PUNCT|\u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14/NOUN|\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07/PROPN|</NEl><u/>\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01/SCONJ|<NEp>\u0e19\u0e32\u0e22/NOUN|\u0e2a\u0e38/PROPN|\u0e01\u0e34\u0e08/NOUN|</NEp><s/>/PUNCT|\u0e2d\u0e32\u0e22\u0e38/NOUN|<u/>65/NUM|<s/>/PUNCT|\u0e1b\u0e35/NOUN|<u/>\u0e16\u0e39\u0e01/AUX|\u0e1b\u0e25\u0e34\u0e07/VERB|\u0e01\u0e31\u0e14/VERB|\u0e41\u0e25\u0e49\u0e27/ADV|\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49/AUX|\u0e44\u0e1b/VERB|\u0e1e\u0e1a/VERB|\u0e41\u0e1e\u0e17\u0e22\u0e4c/NOUN|<u/>'\n\n>>tltk.nlp.ner_tag(Text) : The output includes markups for named entities (<NEx>...</NEx>), e.g. tltk.nlp.ner_tag(\"\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e40\u0e02\u0e15\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07\u0e27\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e19\u0e33\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e1b\u0e25\u0e34\u0e07\u0e44\u0e1b\u0e1b\u0e31\u0e01\u0e15\u0e32\u0e21\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33 \u0e43\u0e19\u0e40\u0e02\u0e15\u0e2d\u0e33\u0e40\u0e20\u0e2d\u0e40\u0e21\u0e37\u0e2d\u0e07 \u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07 \u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e19\u0e32\u0e22\u0e2a\u0e38\u0e01\u0e34\u0e08 \u0e2d\u0e32\u0e22\u0e38 65 \u0e1b\u0e35 \u0e16\u0e39\u0e01\u0e1b\u0e25\u0e34\u0e07\u0e01\u0e31\u0e14\u0e41\u0e25\u0e49\u0e27\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e1e\u0e1a\u0e41\u0e1e\u0e17\u0e22\u0e4c\")\n\n=> '<NEo>\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e40\u0e02\u0e15\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23</NEo>\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07\u0e27\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e19\u0e33\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e1b\u0e25\u0e34\u0e07\u0e44\u0e1b\u0e1b\u0e31\u0e01\u0e15\u0e32\u0e21\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33 \u0e43\u0e19<NEl>\u0e40\u0e02\u0e15\u0e2d\u0e33\u0e40\u0e20\u0e2d\u0e40\u0e21\u0e37\u0e2d\u0e07 \u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07</NEl> \u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01<NEp>\u0e19\u0e32\u0e22\u0e2a\u0e38\u0e01\u0e34\u0e08</NEp> \u0e2d\u0e32\u0e22\u0e38 65 \u0e1b\u0e35 \u0e16\u0e39\u0e01\u0e1b\u0e25\u0e34\u0e07\u0e01\u0e31\u0e14\u0e41\u0e25\u0e49\u0e27\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e1e\u0e1a\u0e41\u0e1e\u0e17\u0e22\u0e4c'\n\n>>tltk.nlp.ner([(w,pos),....]) : module for named entity recognition (person, organization, location), e.g. tltk.nlp.ner([('\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19', 'NOUN'), ('\u0e40\u0e02\u0e15', 'NOUN'), ('\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23', 'PROPN'), ('\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07', 'VERB'), ('\u0e27\u0e48\u0e32', 'SCONJ'), ('<s/>', 'PUNCT')])\n\n=> [('\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19', 'NOUN', 'B-O'), ('\u0e40\u0e02\u0e15', 'NOUN', 'I-O'), ('\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23', 'PROPN', 'I-O'), ('\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07', 'VERB', 'O'), ('\u0e27\u0e48\u0e32', 'SCONJ', 'O'), ('<s/>', 'PUNCT', 'O')]\nNamed entity recognition is based on crf model adapted from http://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html Model is trainned with a corpus containing 170,000 named entities. B-O, I-O are tags for organizations; B-P, I-P are tags for persons; and B-L, I-L are tags for locations.\n\n>>tltk.nlp.pos_tag(Text,WordSegmentOption) : word segmentation and POS tagging (using nltk.tag.perceptron), e.g. tltk.nlp.pos_tag('\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e43\u0e2a\u0e48\u0e41\u0e17\u0e47\u0e01\u0e2b\u0e21\u0e27\u0e14\u0e04\u0e33\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22 \u0e27\u0e31\u0e19\u0e19\u0e35\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e44\u0e14\u0e49\u0e1a\u0e49\u0e32\u0e07\u0e41\u0e25\u0e49\u0e27') or  \n\n=> [[('\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21', 'NOUN'), ('\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a', 'ADP'), ('\u0e43\u0e2a\u0e48', 'VERB'), ('\u0e41\u0e17\u0e47\u0e01', 'NOUN'), ('\u0e2b\u0e21\u0e27\u0e14\u0e04\u0e33', 'NOUN'), ('\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', 'PROPN'), ('<s/>', 'PUNCT')], [('\u0e27\u0e31\u0e19\u0e19\u0e35\u0e49', 'NOUN'), ('\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19', 'VERB'), ('\u0e44\u0e14\u0e49', 'ADV'), ('\u0e1a\u0e49\u0e32\u0e07', 'ADV'), ('\u0e41\u0e25\u0e49\u0e27', 'ADV'), ('<s/>', 'PUNCT')]]\n\nBy default word_segment(Text,\"colloc\") will be used, but if option = \"mm\", word_segment(Text,\"mm\") will be used; POS tag set is based on Universal POS tags.. http://universaldependencies.org/u/pos/index.html\nnltk.tag.perceptron model is used for POS tagging. It is trainned with POS-tagged subcorpus in TNC (148,000 words)\n\n\n>>tltk.nlp.pos_tag_wordlist(WordLst) : Same as \"tltk.nlp.pos_tag\", but the input is a word list, [w1,w2,...]\n\n>>tltk.nlp.segment(Text) : segment a paragraph into elementary discourse units (edu) marked with <u/> and segment words in each edu e.g. tltk.nlp.segment(\"\u0e41\u0e15\u0e48\u0e2d\u0e32\u0e08\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e19\u0e01\u0e01\u0e34\u0e19\u0e1b\u0e25\u0e35\u0e2d\u0e01\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e1e\u0e48\u0e2d\u0e41\u0e21\u0e48\u0e21\u0e37\u0e2d\u0e43\u0e2b\u0e21\u0e48\u2005\u0e23\u0e31\u0e07\u0e17\u0e35\u0e48\u0e17\u0e33\u0e08\u0e36\u0e07\u0e44\u0e21\u0e48\u0e04\u0e48\u0e2d\u0e22\u0e41\u0e02\u0e47\u0e07\u0e41\u0e23\u0e07 \u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36\u0e48\u0e07\u0e23\u0e31\u0e07\u0e01\u0e47\u0e09\u0e35\u0e01\u0e40\u0e01\u0e37\u0e2d\u0e1a\u0e02\u0e32\u0e14\u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e2d\u0e07\u0e17\u0e48\u0e2d\u0e19\u0e2b\u0e49\u0e2d\u0e22\u0e15\u0e48\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e07 \u0e1c\u0e21\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\u0e2b\u0e32\u0e2d\u0e38\u0e1b\u0e01\u0e23\u0e13\u0e4c\u0e21\u0e32\u0e22\u0e36\u0e14\u0e23\u0e31\u0e07\u0e01\u0e25\u0e31\u0e1a\u0e04\u0e37\u0e19\u0e23\u0e39\u0e1b\u0e17\u0e23\u0e07\u0e40\u0e14\u0e34\u0e21 \u0e02\u0e13\u0e30\u0e17\u0e35\u0e48\u0e41\u0e21\u0e48\u0e19\u0e01\u0e01\u0e34\u0e19\u0e1b\u0e25\u0e35\u0e2d\u0e01\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e07\u0e2a\u0e48\u0e07\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e42\u0e27\u0e22\u0e27\u0e32\u0e22\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e01\u0e25\u0e49 \u0e46 \u0e41\u0e15\u0e48\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e44\u0e21\u0e48\u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08\u2005\u0e2a\u0e2d\u0e07\u0e2a\u0e32\u0e21\u0e27\u0e31\u0e19\u0e15\u0e48\u0e2d\u0e21\u0e32\u0e23\u0e31\u0e07\u0e17\u0e35\u0e48\u0e0a\u0e48\u0e27\u0e22\u0e0b\u0e48\u0e2d\u0e21\u0e01\u0e47\u0e1e\u0e31\u0e07\u0e44\u0e1b \u0e44\u0e21\u0e48\u0e40\u0e2b\u0e47\u0e19\u0e41\u0e21\u0e48\u0e19\u0e01\u0e1a\u0e34\u0e19\u0e01\u0e25\u0e31\u0e1a\u0e21\u0e32\u0e2d\u0e35\u0e01\u0e40\u0e25\u0e22\") \n\n=> '\u0e41\u0e15\u0e48|\u0e2d\u0e32\u0e08|\u0e40\u0e1e\u0e23\u0e32\u0e30|\u0e19\u0e01\u0e01\u0e34\u0e19\u0e1b\u0e25\u0e35\u0e2d\u0e01\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e07|\u0e40\u0e1b\u0e47\u0e19|\u0e1e\u0e48\u0e2d\u0e41\u0e21\u0e48|\u0e21\u0e37\u0e2d\u0e43\u0e2b\u0e21\u0e48|<s/>|\u0e23\u0e31\u0e07|\u0e17\u0e35\u0e48|\u0e17\u0e33|\u0e08\u0e36\u0e07|\u0e44\u0e21\u0e48|\u0e04\u0e48\u0e2d\u0e22|\u0e41\u0e02\u0e47\u0e07\u0e41\u0e23\u0e07<u/>\u0e27\u0e31\u0e19|\u0e2b\u0e19\u0e36\u0e48\u0e07|\u0e23\u0e31\u0e07|\u0e01\u0e47|\u0e09\u0e35\u0e01|\u0e40\u0e01\u0e37\u0e2d\u0e1a|\u0e02\u0e32\u0e14|\u0e40\u0e1b\u0e47\u0e19|\u0e2a\u0e2d\u0e07|\u0e17\u0e48\u0e2d\u0e19|\u0e2b\u0e49\u0e2d\u0e22|\u0e15\u0e48\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e07<u/>\u0e1c\u0e21|\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21|\u0e2b\u0e32|\u0e2d\u0e38\u0e1b\u0e01\u0e23\u0e13\u0e4c|\u0e21\u0e32|\u0e22\u0e36\u0e14|\u0e23\u0e31\u0e07|\u0e01\u0e25\u0e31\u0e1a\u0e04\u0e37\u0e19|\u0e23\u0e39\u0e1b\u0e17\u0e23\u0e07|\u0e40\u0e14\u0e34\u0e21<u/>\u0e02\u0e13\u0e30|\u0e17\u0e35\u0e48|\u0e41\u0e21\u0e48|\u0e19\u0e01\u0e01\u0e34\u0e19\u0e1b\u0e25\u0e35\u0e2d\u0e01\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e07|\u0e2a\u0e48\u0e07\u0e40\u0e2a\u0e35\u0e22\u0e07|\u0e42\u0e27\u0e22\u0e27\u0e32\u0e22|\u0e2d\u0e22\u0e39\u0e48|\u0e43\u0e01\u0e25\u0e49|\u0e46<u/>\u0e41\u0e15\u0e48|\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22|\u0e44\u0e21\u0e48|\u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08|<s/>|\u0e2a\u0e2d\u0e07|\u0e2a\u0e32\u0e21|\u0e27\u0e31\u0e19|\u0e15\u0e48\u0e2d|\u0e21\u0e32|\u0e23\u0e31\u0e07|\u0e17\u0e35\u0e48|\u0e0a\u0e48\u0e27\u0e22|\u0e0b\u0e48\u0e2d\u0e21|\u0e01\u0e47|\u0e1e\u0e31\u0e07|\u0e44\u0e1b<u/>\u0e44\u0e21\u0e48|\u0e40\u0e2b\u0e47\u0e19|\u0e41\u0e21\u0e48|\u0e19\u0e01|\u0e1a\u0e34\u0e19|\u0e01\u0e25\u0e31\u0e1a|\u0e21\u0e32|\u0e2d\u0e35\u0e01|\u0e40\u0e25\u0e22<u/>'   edu segmentation is based on syllable input using RandomForestClassifier model, which is trained on an edu-segmented corpus (approx. 7,000 edus)  created and used in Nalinee\u2019s thesis \n\n>>tltk.nlp.word_segment(Text,method='mm|ngram|colloc') : word segmentation using either maximum matching or ngram or maximum collocation approach. 'colloc' is used by default. Please note that the first run of ngram method would take a long time because TNC.3g will be loaded for ngram calculation. e.g. \n\ntltk.nlp.word_segment('\u0e1c\u0e39\u0e49\u0e2a\u0e37\u0e48\u0e2d\u0e02\u0e48\u0e32\u0e27\u0e23\u0e32\u0e22\u0e07\u0e32\u0e19\u0e27\u0e48\u0e32\u0e19\u0e32\u0e22\u0e01\u0e23\u0e31\u0e10\u0e21\u0e19\u0e15\u0e23\u0e35\u0e44\u0e21\u0e48\u0e21\u0e32\u0e17\u0e33\u0e07\u0e32\u0e19\u0e17\u0e35\u0e48\u0e17\u0e33\u0e40\u0e19\u0e35\u0e22\u0e1a\u0e23\u0e31\u0e10\u0e1a\u0e32\u0e25')\n=> '\u0e1c\u0e39\u0e49\u0e2a\u0e37\u0e48\u0e2d\u0e02\u0e48\u0e32\u0e27|\u0e23\u0e32\u0e22\u0e07\u0e32\u0e19|\u0e27\u0e48\u0e32|\u0e19\u0e32\u0e22\u0e01\u0e23\u0e31\u0e10\u0e21\u0e19\u0e15\u0e23\u0e35|\u0e44\u0e21\u0e48|\u0e21\u0e32|\u0e17\u0e33\u0e07\u0e32\u0e19|\u0e17\u0e35\u0e48|\u0e17\u0e33\u0e40\u0e19\u0e35\u0e22\u0e1a\u0e23\u0e31\u0e10\u0e1a\u0e32\u0e25|<s/>'\n\n>>tltk.nlp.syl_segment(Text) : syllable segmentation using 3gram statistics e.g. tltk.nlp.syl_segment('\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e1c\u0e25\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22') \n\n=> '\u0e42\u0e1b\u0e23~\u0e41\u0e01\u0e23\u0e21~\u0e2a\u0e33~\u0e2b\u0e23\u0e31\u0e1a~\u0e1b\u0e23\u0e30~\u0e21\u0e27\u0e25~\u0e1c\u0e25~\u0e20\u0e32~\u0e29\u0e32~\u0e44\u0e17\u0e22<s/>'\n\n>>tltk.nlp.word_segment_nbest(Text, N) : return the best N segmentations based on the assumption of minimum word approach. e.g. tltk.nlp.word_segment_nbest('\u0e04\u0e19\u0e02\u0e31\u0e1a\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\"',10) \n\n=> [['\u0e04\u0e19\u0e02\u0e31\u0e1a|\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28', '\u0e04\u0e19\u0e02\u0e31\u0e1a\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28', '\u0e04\u0e19|\u0e02\u0e31\u0e1a|\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28', '\u0e04\u0e19|\u0e02\u0e31\u0e1a\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28', '\u0e04\u0e19\u0e02\u0e31\u0e1a|\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28', '\u0e04\u0e19\u0e02\u0e31\u0e1a\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33|\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28', '\u0e04\u0e19\u0e02\u0e31\u0e1a|\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a|\u0e2d\u0e32\u0e01\u0e32\u0e28', '\u0e04\u0e19\u0e02\u0e31\u0e1a\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a|\u0e2d\u0e32\u0e01\u0e32\u0e28', '\u0e04\u0e19|\u0e02\u0e31\u0e1a|\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28', '\u0e04\u0e19\u0e02\u0e31\u0e1a|\u0e23|\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28']]\n\n>>tltk.nlp.g2p(Text)  : return Word segments and pronunciations\ne.g. tltk.nlp.g2p(\"\u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e2d\u0e38\u0e14\u0e21\u0e28\u0e36\u0e01\u0e29\u0e32\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e01\u0e49\u0e32\u0e27\u0e43\u0e2b\u0e49\u0e17\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07\u0e02\u0e2d\u0e07\u0e15\u0e25\u0e32\u0e14\u0e41\u0e23\u0e07\u0e07\u0e32\u0e19\")  \n\n=> \"\u0e2a\u0e16\u0e32~\u0e1a\u0e31\u0e19~\u0e2d\u0e38~\u0e14\u0e21~\u0e28\u0e36\u0e01~\u0e29\u0e32|\u0e44\u0e21\u0e48|\u0e2a\u0e32~\u0e21\u0e32\u0e23\u0e16|\u0e01\u0e49\u0e32\u0e27|\u0e43\u0e2b\u0e49|\u0e17\u0e31\u0e19|\u0e01\u0e32\u0e23|\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19~\u0e41\u0e1b\u0e25\u0e07|\u0e02\u0e2d\u0e07|\u0e15\u0e25\u0e32\u0e14~\u0e41\u0e23\u0e07~\u0e07\u0e32\u0e19<tr/>sa1'thaa4~ban0~?u1~dom0~sUk1~saa4|maj2|saa4~maat2|kaaw2|haj2|than0|kaan0|pliian1~plxxN0|khOON4|ta1'laat1~rxxN0~Naan0|<s/>\"\n\n>>tltk.nlp.th2ipa(Text) : return Thai transcription in IPA forms\ne.g. tltk.nlp.th2ipa(\"\u0e25\u0e07\u0e41\u0e21\u0e48\u0e19\u0e49\u0e33\u0e23\u0e2d\u0e40\u0e14\u0e34\u0e19\u0e44\u0e1b\u0e2b\u0e32\u0e1b\u0e25\u0e32\") \n\n=> 'lo\u014b1 m\u025b\u02d03.na\u02d0m4 r\u1d10\u02d01 d\u0264\u02d0n1 paj1 ha\u02d05 pla\u02d01 <s/>'\n\n>>tltk.nlp.th2roman(Text) : return Thai romanization according to Royal Thai Institute guideline.\n.e.g. tltk.nlp.th2roman(\"\u0e04\u0e37\u0e2d\u0e40\u0e02\u0e32\u0e40\u0e14\u0e34\u0e19\u0e40\u0e25\u0e22\u0e25\u0e07\u0e44\u0e1b\u0e23\u0e2d\u0e43\u0e19\u0e41\u0e21\u0e48\u0e19\u0e49\u0e33\u0e2a\u0e30\u0e2d\u0e32\u0e14\u0e44\u0e1b\u0e2b\u0e32\u0e21\u0e30\u0e1b\u0e23\u0e32\u0e07\") \n\n=> 'khue khaw doen loei long pai ro nai maenam sa-at pai ha maprang <s/>'\n\n>>tltk.nlp.g2p_all(Text) : return all transcriptions (IPA) as a list of tuple (syllable_list, transcription). Transcription is based on syllable reading rules. It could be different from th2ipa.\ne.g. tltk.nlp.g2p_all(\"\u0e23\u0e2d\u0e22\u0e01\u0e23\u0e48\u0e32\u0e07\") \n\n=> [('\u0e23\u0e2d\u0e22~\u0e01\u0e23\u0e48\u0e32\u0e07', 'r\u1d10\u02d0j1.ka2.ra\u02d0\u014b2'), ('\u0e23\u0e2d\u0e22~\u0e01\u0e23\u0e48\u0e32\u0e07', 'r\u1d10\u02d0j1.kra\u02d0\u014b2'), ('\u0e23\u0e2d~\u0e22\u0e01~\u0e23\u0e48\u0e32\u0e07', 'r\u1d10\u02d01.jok4.ra\u02d0\u014b3')]\n\n>>tltk.nlp.spell_candidates(Word) : list of possible correct words using minimum edit distance, e.g. tltk.nlp.spell_candidates('\u0e23\u0e31\u0e01\u0e29')\n\n=> ['\u0e23\u0e31\u0e01', '\u0e17\u0e31\u0e01\u0e29', '\u0e23\u0e31\u0e01\u0e29\u0e32', '\u0e23\u0e31\u0e01\u0e29\u0e4c']\n\n\nOther defined functions in the package:\n>>tltk.nlp.reset_thaidict() : clear dictionary content\n>>tltk.nlp.read_thaidict(DictFile) : add a new dictionary  e.g. tltk.nlp.read_thaidict('BEST.dict')\n>>tltk.nlp.check_thaidict(Word) : check whether Word exists in the dictionary\n\ntltk.corpus  :   basic tools for corpus enquiry\n-----------------------------------------------\n\n>>tltk.corpus.TNC_load()  by default load TNC.3g. The file can be in the working directory or TLTK package directory\n\n>>tltk.corpus.trigram_load(TRIGRAM)  ###  load Trigram data from other sourse saved in tab delimited format \"W1\\tW2\\tW3\\tFreq\"  e.g.  tltk.corpus.load3gram('TNC.3g') 'TNC.3g' can be downloaded separately from Thai National Corpus Project.\n\n>>tltk.corpus.unigram(w1)   return normalized frequecy (frequency/million) of w1 from the corpus\n\n>>tltk.corpus.bigram(w1,w2)   return frequency/million of Bigram w1-w2 from the corpus e.g. tltk.corpus.bigram(\"\u0e2b\u0e32\u0e22\",\"\u0e14\u0e35\") => 2.331959592765809\n\n>>tltk.corpus.trigram(w1,w2,w3)  return frequency/million of Trigram w1-w2-w3 from the corpus\n\n>>tltk.corpus.collocates(w, stat=\"chi2\", direct=\"both\", span=2, limit=10, minfq=1)   ### return all collocates of w, STAT = {freq,mi,chi2} DIR={left,right,both}  SPAN={1,2}  The output is a list of tuples  ((w1,w2), stat). e.g. tltk.corpus.collocates(\"\u0e27\u0e34\u0e48\u0e07\",limit=5) \n\n=> [(('\u0e27\u0e34\u0e48\u0e07', '\u0e41\u0e08\u0e49\u0e19'), 86633.93952758134), (('\u0e27\u0e34\u0e48\u0e07', '\u0e15\u0e37\u0e4b\u0e2d'), 77175.29122642518), (('\u0e27\u0e34\u0e48\u0e07', '\u0e01\u0e23\u0e30\u0e2b\u0e37\u0e14\u0e01\u0e23\u0e30\u0e2b\u0e2d\u0e1a'), 48598.79465339733), (('\u0e27\u0e34\u0e48\u0e07', '\u0e1b\u0e23\u0e39\u0e4a\u0e14'), 41111.63720974819), (('\u0e25\u0e39\u0e48', '\u0e27\u0e34\u0e48\u0e07'), 33990.56839021914)]\n\n>>tltk.corpus.w2v_load()  by deafult load word2vec file \"TNCc5model.bin\". The file can be in the working directory or TLTK package directory\n\n>>tltk.corpus.w2v_exist(w) check whether w has a vector representation  e.g. tltk.corpus.w2v_exist(\"\u0e2d\u0e32\u0e2b\u0e32\u0e23\") => True\n\n>>tltk.corpus.w2v(w)  return vector representation of w\n\n>>tltk.corpus.similarity(w1,w2) e.g. tltk.corpus.similarity(\"\u0e2d\u0e32\u0e2b\u0e32\u0e23\",\"\u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e27\u0e48\u0e32\u0e07\") => 0.783551877546\n\n>>tltk.corpus.similar_words(w, n=10, cutoff=0., score=\"n\")  e.g. tltk.corpus.similar_words(\"\u0e2d\u0e32\u0e2b\u0e32\u0e23\",n=5, score=\"y\") \n\n=> [('\u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e27\u0e48\u0e32\u0e07', 0.7835519313812256), ('\u0e02\u0e2d\u0e07\u0e27\u0e48\u0e32\u0e07', 0.7366500496864319), ('\u0e02\u0e2d\u0e07\u0e2b\u0e27\u0e32\u0e19', 0.703102707862854), ('\u0e40\u0e19\u0e37\u0e49\u0e2d\u0e2a\u0e31\u0e15\u0e27\u0e4c', 0.6960341930389404), ('\u0e1c\u0e25\u0e44\u0e21\u0e49', 0.6641997694969177)]\n\n>>tltk.corpus.outofgroup([w1,w2,w3,...]) e.g. tltk.corpus.outofgroup([\"\u0e19\u0e49\u0e33\",\"\u0e2d\u0e32\u0e2b\u0e32\u0e23\",\"\u0e02\u0e49\u0e32\u0e27\",\"\u0e23\u0e16\u0e22\u0e19\u0e15\u0e4c\",\"\u0e1c\u0e31\u0e01\"]) => \"\u0e23\u0e16\u0e22\u0e19\u0e15\u0e4c\"\n\n>>tltk.corpus.analogy(w1,w2,w3,n=1) e.g. tltk.corpus.analogy('\u0e1c\u0e39\u0e49\u0e0a\u0e32\u0e22','\u0e1e\u0e48\u0e2d','\u0e41\u0e21\u0e48') => ['\u0e1c\u0e39\u0e49\u0e2b\u0e0d\u0e34\u0e07']  \u0e1c\u0e39\u0e49\u0e0a\u0e32\u0e22 - \u0e1e\u0e48\u0e2d + \u0e41\u0e21\u0e48 =  \u0e1c\u0e39\u0e49\u0e2b\u0e0d\u0e34\u0e07\n\n>>tltk.corpus.w2v_plot([w1,w2,w3,...])  => plot a scratter graph of w1-wn in two dimensions\n\n>>tltk.corpus.w2v_compare_color([w1,w2,w3,...])  => visualize the components of vectors w1-wn in color\n\n\nNotes\n-----\n\n- Word segmentation is based on a maximum collocation approach described in this publication: \"Aroonmanakun, W. 2002. Collocation and Thai Word Segmentation. In Thanaruk Theeramunkong and Virach Sornlertlamvanich, eds. Proceedings of the Fifth Symposium on Natural Language Processing & The Fifth Oriental COCOSDA Workshop. Pathumthani: Sirindhorn International Institute of Technology. 68-75.\" (http://pioneer.chula.ac.th/~awirote/ling/SNLP2002-0051c.pdf)\n\n- Use tltk.nlp.word_segment(Text) or tltk.nlp.syl_segment(Text) for segmenting Thai texts. Syllable segmentation now is based on a trigram model trainned on 3.1 million syllable corpus. Input text is a paragraph of Thai texts which can be mixed with English texts. Spaces in the paragraph will be marked as \"<s/>\". Word boundary is marked by \"|\". Syllable boundary is marked by \"~\". Syllables here are written syllables. One written syllable may be pronounced as two syllables, i.e. \"\u0e2a\u0e01\u0e31\u0e14\" is segemnted here as one written syllable, but it is pronounced as two syllables \"sa1-kat1\".\n\n- Determining words in a sentence is based on the dictionary and maximum collocation strength between syllables. Since many compounds and idioms, e.g. '\u0e40\u0e15\u0e32\u0e44\u0e21\u0e42\u0e04\u0e23\u0e40\u0e27\u0e1f', '\u0e44\u0e1f\u0e1f\u0e49\u0e32\u0e01\u0e23\u0e30\u0e41\u0e2a\u0e2a\u0e25\u0e31\u0e1a', '\u0e1b\u0e35\u0e07\u0e1a\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13', '\u0e2d\u0e38\u0e42\u0e21\u0e07\u0e04\u0e4c\u0e43\u0e15\u0e49\u0e14\u0e34\u0e19', '\u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e08\u0e32\u0e19\u0e14\u0e48\u0e27\u0e19', '\u0e1b\u0e39\u0e19\u0e02\u0e32\u0e27\u0e1c\u0e2a\u0e21\u0e1e\u0e34\u0e40\u0e28\u0e29', '\u0e40\u0e15\u0e49\u0e19\u0e41\u0e23\u0e49\u0e07\u0e40\u0e15\u0e49\u0e19\u0e01\u0e32' etc., are included in the standard dictionary, these will likely be segmented as one word. For applications that prefer shortest meaningful words (i.e. '\u0e23\u0e16|\u0e42\u0e14\u0e22\u0e2a\u0e32\u0e23', '\u0e04\u0e19|\u0e43\u0e0a\u0e49', '\u0e01\u0e25\u0e32\u0e07|\u0e04\u0e37\u0e19', '\u0e15\u0e49\u0e19|\u0e44\u0e21\u0e49' as segmented in BEST corpus), users should reset the default dictionary used in this package and reload a new dictionary containing only simple words or shortest meaningful words. Use \"reset_thaidict()\" to clear default dictionary content, and \"read_thaidict('DICT_FIILE')\" to load a new dictionary. A list of words compiled from BEST corpus is included in this package as a file 'BEST.dict' \n\n- The standard dictionary used in this package has more then 65,000 entries including abbreviations and transliterations compiled from various sources. A dictionary of 8,700 proper names e.g. country names, organization names, location names, animal names, plant names, food names, ..., such as '\u0e2d\u0e38\u0e0b\u0e40\u0e1a\u0e01\u0e34\u0e2a\u0e16\u0e32\u0e19', '\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e40\u0e25\u0e02\u0e32\u0e18\u0e34\u0e01\u0e32\u0e23\u0e19\u0e32\u0e22\u0e01\u0e23\u0e31\u0e10\u0e21\u0e19\u0e15\u0e23\u0e35', '\u0e27\u0e31\u0e14\u0e43\u0e2b\u0e0d\u0e48\u0e2a\u0e38\u0e27\u0e23\u0e23\u0e13\u0e32\u0e23\u0e32\u0e21', '\u0e2b\u0e19\u0e2d\u0e19\u0e40\u0e08\u0e32\u0e30\u0e25\u0e33\u0e15\u0e49\u0e19\u0e02\u0e49\u0e32\u0e27\u0e42\u0e1e\u0e14', '\u0e1b\u0e25\u0e32\u0e2b\u0e21\u0e36\u0e01\u0e01\u0e23\u0e30\u0e40\u0e17\u0e35\u0e22\u0e21\u0e1e\u0e23\u0e34\u0e01\u0e44\u0e17\u0e22', are also added as a list of words in the system.\n\n- For segmenting a specific domain text, a specialized dicionary can be used by adding more dictionary before segmenting texts. This can be done by calling read_thaidict(\"SPECIALIZED_DICT\"). Please note that the dictionary is a text file in \"iso-8859-11\" encoding. The format is one word per one line.\n\n- 'setence segment' or actually 'edu segment' is a process to break a paragraph into a chunk of discourse units, which usually are a clause. It is based on RandomForestClassifier model, which is trained on an edu-segmented corpus (approx. 7,000 edus) created and used in Nalinee's thesis (http://www.arts.chula.ac.th/~ling/thesis/2556MA-LING-Nalinee.pdf). Accuracy of the model is 97.8%. The reason behind using edu can be found in [Aroonmanakun, W. 2007. Thoughts on Word and Sentence Segmentation in Thai. In Proceedings of the Seventh Symposium on Natural Language Processing, Dec 13-15, 2007, Pattaya, Thailand. 85-90.] [Intasaw, N. and Aroonmanakun, W. 2013. Basic Principles for Segmenting Thai EDUs. in Proceedings of 27th Pacific Asia Conference on Language, Information, and Computation, pages 491-498, Nov 22-24, 2013, Taipei.]\n\n- 'grapheme to phoneme' (g2p), as well as IPA transcription (th2ipa) and Thai romanization (th2roman) is based on the hybrid approach presented in the paper \"A Unified Model of Thai Romanization and Word Segmentation\". The Thai Royal Institute guidline for Thai romanization can be downloaded from \"http://www.arts.chula.ac.th/~ling/tts/ThaiRoman.pdf\", or \"http://www.royin.go.th/?page_id=619\" [Aroonmanakun, W., and W. Rivepiboon. 2004. A Unified Model of Thai Word Segmentation and Romanization. In  Proceedings of The 18th Pacific Asia Conference on Language, Information and Computation, Dec 8-10, 2004, Tokyo, Japan. 205-214.] (http://www.aclweb.org/anthology/Y04-1021)\n\nRemarks\n-------\n\n- TNC Trigram data (TNC.3g)  and  TNC word2vec (TNCc5model.bin) can be downloaded from TNC website. http://www.arts.chula.ac.th/ling/tnc/searchtnc/\n- Module \"spell_candidates\" is modified from Peter Norvig's Python codes at http://norvig.com/spell-correct.html \n- Module \"w2v_compare_color\" is modified from http://chrisculy.net/lx/wordvectors/wvecs_visualization.html\n- BEST corpus is the corpus released by NECTEC  (https://www.nectec.or.th/corpus/) \n- Universal POS tags are used in this project. For more information, please see http://universaldependencies.org/u/pos/index.html and http://www.arts.chula.ac.th/~ling/contents/File/UD%20Annotation%20for%20Thai.pdf\n- pos_tag is based on PerceptronTagger in nltk.tag.perceptron. It is trained with TNC data manually pos-taged (approx. 148,000 words). Accuracy on pos tagging is 91.68%.  NLTK PerceptronTagger is a port of the Textblob Averaged Perceptron Tagger, which can be found at https://explosion.ai/blog/part-of-speech-pos-tagger-in-python \n- named entiy recognition module is a CRF model adapted from this tutorial (http://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html). The model is trained with NER data used in Sasimimon's and Nutcha's theses (altogether 7,354 names in a corpus of 183,300 words). (http://pioneer.chula.ac.th/~awirote/Data-Nutcha.zip, http://pioneer.chula.ac.th/~awirote/ Data-Sasiwimon.zip) and NER data from AIforThai  (https://aiforthai.in.th/) Only valid NE files from AIforThai are used. The total number of all NEs is 170,076. Accuracy of the model is reported below (88%).\n\n\n============  ===========  ======= =========  ========\n        tag    precision    recall  f1-score   support\n------------  -----------  ------- ---------  --------\n         B-L       0.56      0.48      0.52     27105\n         B-O       0.72      0.58      0.64     59613\n         B-P       0.82      0.83      0.83     83358\n         I-L       0.52      0.43      0.47     17859\n         I-O       0.67      0.59      0.63     67396\n         I-P       0.85      0.88      0.86    175069\n           O       0.92      0.94      0.93   1032377\n------------  -----------  ------- ---------  --------\n    accuracy                           0.88   1462777\n   macro avg       0.72      0.68      0.70   1462777\nweighted avg       0.87      0.88      0.88   1462777\n============  ===========  ======= =========  ========", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://pypi.python.org/pypi/tltk/", "keywords": "Thai language toolkit,Thai language processing,segmentation,pos tag,transcription,romanization", "license": "BSD-3-Clause", "maintainer": "", "maintainer_email": "", "name": "tltk", "package_url": "https://pypi.org/project/tltk/", "platform": "", "project_url": "https://pypi.org/project/tltk/", "project_urls": {"Homepage": "http://pypi.python.org/pypi/tltk/"}, "release_url": "https://pypi.org/project/tltk/1.3.5/", "requires_dist": null, "requires_python": "", "summary": "Thai Language Toolkit", "version": "1.3.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>TLTK is a Python package for Thai language processing: syllable, word, discouse unit segmentation, pos tagging, named entity recognition, grapheme2phoneme, ipa transcription, romanization, etc.  TLTK requires Python 3.4 or higher.\nThe project is a part of open source software developed at Chulalongkorn University.\nSince version 1.2.2 pacakge license is changed to New BSD License (BSD-3-Clause)</p>\n<p>Input : must be utf8 Thai texts.</p>\n<div id=\"updates\">\n<h2>Updates:</h2>\n<p>More compound words are added in the dictionary. Version 1.1.3-1.1.5 have many entries that are not a word and contain a few errors. Those entries are removed in later versions.</p>\n<p>NER tagger model was updated by using more NE data from AiforThai project.</p>\n</div>\n<div id=\"tltk-nlp-basic-tools-for-thai-language-processing\">\n<h2>tltk.nlp  :  basic tools for Thai language processing.</h2>\n<p>&gt;&gt;tltk.nlp.chunk(Text) : chunk parsing. The output includes markups for word segments (|), elementary discourse units (&lt;u/&gt;), pos tags (/POS),and named entities (&lt;NEx&gt;\u2026&lt;/NEx&gt;), e.g. tltk.nlp.chunk(\u201c\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e40\u0e02\u0e15\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07\u0e27\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e19\u0e33\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e1b\u0e25\u0e34\u0e07\u0e44\u0e1b\u0e1b\u0e31\u0e01\u0e15\u0e32\u0e21\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33 \u0e43\u0e19\u0e40\u0e02\u0e15\u0e2d\u0e33\u0e40\u0e20\u0e2d\u0e40\u0e21\u0e37\u0e2d\u0e07 \u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07 \u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e19\u0e32\u0e22\u0e2a\u0e38\u0e01\u0e34\u0e08 \u0e2d\u0e32\u0e22\u0e38 65 \u0e1b\u0e35 \u0e16\u0e39\u0e01\u0e1b\u0e25\u0e34\u0e07\u0e01\u0e31\u0e14\u0e41\u0e25\u0e49\u0e27\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e1e\u0e1a\u0e41\u0e1e\u0e17\u0e22\u0e4c\u201d)</p>\n<p>=&gt; \u2018&lt;NEo&gt;\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19/NOUN|\u0e40\u0e02\u0e15/NOUN|\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23/PROPN|&lt;/NEo&gt;\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07/VERB|\u0e27\u0e48\u0e32/SCONJ|&lt;s/&gt;/PUNCT|\u0e44\u0e14\u0e49/AUX|\u0e19\u0e33/VERB|\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28/NOUN|\u0e40\u0e15\u0e37\u0e2d\u0e19/VERB|\u0e1b\u0e25\u0e34\u0e07/NOUN|\u0e44\u0e1b/VERB|\u0e1b\u0e31\u0e01/VERB|\u0e15\u0e32\u0e21/ADP|\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33/NOUN|&lt;u/&gt;\u0e43\u0e19/ADP|&lt;NEl&gt;\u0e40\u0e02\u0e15/NOUN|\u0e2d\u0e33\u0e40\u0e20\u0e2d/NOUN|\u0e40\u0e21\u0e37\u0e2d\u0e07/NOUN|&lt;s/&gt;/PUNCT|\u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14/NOUN|\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07/PROPN|&lt;/NEl&gt;&lt;u/&gt;\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01/SCONJ|&lt;NEp&gt;\u0e19\u0e32\u0e22/NOUN|\u0e2a\u0e38/PROPN|\u0e01\u0e34\u0e08/NOUN|&lt;/NEp&gt;&lt;s/&gt;/PUNCT|\u0e2d\u0e32\u0e22\u0e38/NOUN|&lt;u/&gt;65/NUM|&lt;s/&gt;/PUNCT|\u0e1b\u0e35/NOUN|&lt;u/&gt;\u0e16\u0e39\u0e01/AUX|\u0e1b\u0e25\u0e34\u0e07/VERB|\u0e01\u0e31\u0e14/VERB|\u0e41\u0e25\u0e49\u0e27/ADV|\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49/AUX|\u0e44\u0e1b/VERB|\u0e1e\u0e1a/VERB|\u0e41\u0e1e\u0e17\u0e22\u0e4c/NOUN|&lt;u/&gt;\u2019</p>\n<p>&gt;&gt;tltk.nlp.ner_tag(Text) : The output includes markups for named entities (&lt;NEx&gt;\u2026&lt;/NEx&gt;), e.g. tltk.nlp.ner_tag(\u201c\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e40\u0e02\u0e15\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07\u0e27\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e19\u0e33\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e1b\u0e25\u0e34\u0e07\u0e44\u0e1b\u0e1b\u0e31\u0e01\u0e15\u0e32\u0e21\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33 \u0e43\u0e19\u0e40\u0e02\u0e15\u0e2d\u0e33\u0e40\u0e20\u0e2d\u0e40\u0e21\u0e37\u0e2d\u0e07 \u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07 \u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e19\u0e32\u0e22\u0e2a\u0e38\u0e01\u0e34\u0e08 \u0e2d\u0e32\u0e22\u0e38 65 \u0e1b\u0e35 \u0e16\u0e39\u0e01\u0e1b\u0e25\u0e34\u0e07\u0e01\u0e31\u0e14\u0e41\u0e25\u0e49\u0e27\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e1e\u0e1a\u0e41\u0e1e\u0e17\u0e22\u0e4c\u201d)</p>\n<p>=&gt; \u2018&lt;NEo&gt;\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e40\u0e02\u0e15\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23&lt;/NEo&gt;\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07\u0e27\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e19\u0e33\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e1b\u0e25\u0e34\u0e07\u0e44\u0e1b\u0e1b\u0e31\u0e01\u0e15\u0e32\u0e21\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33 \u0e43\u0e19&lt;NEl&gt;\u0e40\u0e02\u0e15\u0e2d\u0e33\u0e40\u0e20\u0e2d\u0e40\u0e21\u0e37\u0e2d\u0e07 \u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07&lt;/NEl&gt; \u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01&lt;NEp&gt;\u0e19\u0e32\u0e22\u0e2a\u0e38\u0e01\u0e34\u0e08&lt;/NEp&gt; \u0e2d\u0e32\u0e22\u0e38 65 \u0e1b\u0e35 \u0e16\u0e39\u0e01\u0e1b\u0e25\u0e34\u0e07\u0e01\u0e31\u0e14\u0e41\u0e25\u0e49\u0e27\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e1e\u0e1a\u0e41\u0e1e\u0e17\u0e22\u0e4c\u2019</p>\n<p>&gt;&gt;tltk.nlp.ner([(w,pos),\u2026.]) : module for named entity recognition (person, organization, location), e.g. tltk.nlp.ner([(\u2018\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u2019, \u2018NOUN\u2019), (\u2018\u0e40\u0e02\u0e15\u2019, \u2018NOUN\u2019), (\u2018\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23\u2019, \u2018PROPN\u2019), (\u2018\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07\u2019, \u2018VERB\u2019), (\u2018\u0e27\u0e48\u0e32\u2019, \u2018SCONJ\u2019), (\u2018&lt;s/&gt;\u2019, \u2018PUNCT\u2019)])</p>\n<p>=&gt; [(\u2018\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u2019, \u2018NOUN\u2019, \u2018B-O\u2019), (\u2018\u0e40\u0e02\u0e15\u2019, \u2018NOUN\u2019, \u2018I-O\u2019), (\u2018\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23\u2019, \u2018PROPN\u2019, \u2018I-O\u2019), (\u2018\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07\u2019, \u2018VERB\u2019, \u2018O\u2019), (\u2018\u0e27\u0e48\u0e32\u2019, \u2018SCONJ\u2019, \u2018O\u2019), (\u2018&lt;s/&gt;\u2019, \u2018PUNCT\u2019, \u2018O\u2019)]\nNamed entity recognition is based on crf model adapted from <a href=\"http://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html\" rel=\"nofollow\">http://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html</a> Model is trainned with a corpus containing 170,000 named entities. B-O, I-O are tags for organizations; B-P, I-P are tags for persons; and B-L, I-L are tags for locations.</p>\n<p>&gt;&gt;tltk.nlp.pos_tag(Text,WordSegmentOption) : word segmentation and POS tagging (using nltk.tag.perceptron), e.g. tltk.nlp.pos_tag(\u2018\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e43\u0e2a\u0e48\u0e41\u0e17\u0e47\u0e01\u0e2b\u0e21\u0e27\u0e14\u0e04\u0e33\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22 \u0e27\u0e31\u0e19\u0e19\u0e35\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e44\u0e14\u0e49\u0e1a\u0e49\u0e32\u0e07\u0e41\u0e25\u0e49\u0e27\u2019) or</p>\n<p>=&gt; [[(\u2018\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u2019, \u2018NOUN\u2019), (\u2018\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u2019, \u2018ADP\u2019), (\u2018\u0e43\u0e2a\u0e48\u2019, \u2018VERB\u2019), (\u2018\u0e41\u0e17\u0e47\u0e01\u2019, \u2018NOUN\u2019), (\u2018\u0e2b\u0e21\u0e27\u0e14\u0e04\u0e33\u2019, \u2018NOUN\u2019), (\u2018\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u2019, \u2018PROPN\u2019), (\u2018&lt;s/&gt;\u2019, \u2018PUNCT\u2019)], [(\u2018\u0e27\u0e31\u0e19\u0e19\u0e35\u0e49\u2019, \u2018NOUN\u2019), (\u2018\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u2019, \u2018VERB\u2019), (\u2018\u0e44\u0e14\u0e49\u2019, \u2018ADV\u2019), (\u2018\u0e1a\u0e49\u0e32\u0e07\u2019, \u2018ADV\u2019), (\u2018\u0e41\u0e25\u0e49\u0e27\u2019, \u2018ADV\u2019), (\u2018&lt;s/&gt;\u2019, \u2018PUNCT\u2019)]]</p>\n<p>By default word_segment(Text,\u201dcolloc\u201d) will be used, but if option = \u201cmm\u201d, word_segment(Text,\u201dmm\u201d) will be used; POS tag set is based on Universal POS tags.. <a href=\"http://universaldependencies.org/u/pos/index.html\" rel=\"nofollow\">http://universaldependencies.org/u/pos/index.html</a>\nnltk.tag.perceptron model is used for POS tagging. It is trainned with POS-tagged subcorpus in TNC (148,000 words)</p>\n<p>&gt;&gt;tltk.nlp.pos_tag_wordlist(WordLst) : Same as \u201ctltk.nlp.pos_tag\u201d, but the input is a word list, [w1,w2,\u2026]</p>\n<p>&gt;&gt;tltk.nlp.segment(Text) : segment a paragraph into elementary discourse units (edu) marked with &lt;u/&gt; and segment words in each edu e.g. tltk.nlp.segment(\u201c\u0e41\u0e15\u0e48\u0e2d\u0e32\u0e08\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e19\u0e01\u0e01\u0e34\u0e19\u0e1b\u0e25\u0e35\u0e2d\u0e01\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e1e\u0e48\u0e2d\u0e41\u0e21\u0e48\u0e21\u0e37\u0e2d\u0e43\u0e2b\u0e21\u0e48\u2005\u0e23\u0e31\u0e07\u0e17\u0e35\u0e48\u0e17\u0e33\u0e08\u0e36\u0e07\u0e44\u0e21\u0e48\u0e04\u0e48\u0e2d\u0e22\u0e41\u0e02\u0e47\u0e07\u0e41\u0e23\u0e07 \u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36\u0e48\u0e07\u0e23\u0e31\u0e07\u0e01\u0e47\u0e09\u0e35\u0e01\u0e40\u0e01\u0e37\u0e2d\u0e1a\u0e02\u0e32\u0e14\u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e2d\u0e07\u0e17\u0e48\u0e2d\u0e19\u0e2b\u0e49\u0e2d\u0e22\u0e15\u0e48\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e07 \u0e1c\u0e21\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\u0e2b\u0e32\u0e2d\u0e38\u0e1b\u0e01\u0e23\u0e13\u0e4c\u0e21\u0e32\u0e22\u0e36\u0e14\u0e23\u0e31\u0e07\u0e01\u0e25\u0e31\u0e1a\u0e04\u0e37\u0e19\u0e23\u0e39\u0e1b\u0e17\u0e23\u0e07\u0e40\u0e14\u0e34\u0e21 \u0e02\u0e13\u0e30\u0e17\u0e35\u0e48\u0e41\u0e21\u0e48\u0e19\u0e01\u0e01\u0e34\u0e19\u0e1b\u0e25\u0e35\u0e2d\u0e01\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e07\u0e2a\u0e48\u0e07\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e42\u0e27\u0e22\u0e27\u0e32\u0e22\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e01\u0e25\u0e49 \u0e46 \u0e41\u0e15\u0e48\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e44\u0e21\u0e48\u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08\u2005\u0e2a\u0e2d\u0e07\u0e2a\u0e32\u0e21\u0e27\u0e31\u0e19\u0e15\u0e48\u0e2d\u0e21\u0e32\u0e23\u0e31\u0e07\u0e17\u0e35\u0e48\u0e0a\u0e48\u0e27\u0e22\u0e0b\u0e48\u0e2d\u0e21\u0e01\u0e47\u0e1e\u0e31\u0e07\u0e44\u0e1b \u0e44\u0e21\u0e48\u0e40\u0e2b\u0e47\u0e19\u0e41\u0e21\u0e48\u0e19\u0e01\u0e1a\u0e34\u0e19\u0e01\u0e25\u0e31\u0e1a\u0e21\u0e32\u0e2d\u0e35\u0e01\u0e40\u0e25\u0e22\u201d)</p>\n<p>=&gt; \u2018\u0e41\u0e15\u0e48|\u0e2d\u0e32\u0e08|\u0e40\u0e1e\u0e23\u0e32\u0e30|\u0e19\u0e01\u0e01\u0e34\u0e19\u0e1b\u0e25\u0e35\u0e2d\u0e01\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e07|\u0e40\u0e1b\u0e47\u0e19|\u0e1e\u0e48\u0e2d\u0e41\u0e21\u0e48|\u0e21\u0e37\u0e2d\u0e43\u0e2b\u0e21\u0e48|&lt;s/&gt;|\u0e23\u0e31\u0e07|\u0e17\u0e35\u0e48|\u0e17\u0e33|\u0e08\u0e36\u0e07|\u0e44\u0e21\u0e48|\u0e04\u0e48\u0e2d\u0e22|\u0e41\u0e02\u0e47\u0e07\u0e41\u0e23\u0e07&lt;u/&gt;\u0e27\u0e31\u0e19|\u0e2b\u0e19\u0e36\u0e48\u0e07|\u0e23\u0e31\u0e07|\u0e01\u0e47|\u0e09\u0e35\u0e01|\u0e40\u0e01\u0e37\u0e2d\u0e1a|\u0e02\u0e32\u0e14|\u0e40\u0e1b\u0e47\u0e19|\u0e2a\u0e2d\u0e07|\u0e17\u0e48\u0e2d\u0e19|\u0e2b\u0e49\u0e2d\u0e22|\u0e15\u0e48\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e07&lt;u/&gt;\u0e1c\u0e21|\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21|\u0e2b\u0e32|\u0e2d\u0e38\u0e1b\u0e01\u0e23\u0e13\u0e4c|\u0e21\u0e32|\u0e22\u0e36\u0e14|\u0e23\u0e31\u0e07|\u0e01\u0e25\u0e31\u0e1a\u0e04\u0e37\u0e19|\u0e23\u0e39\u0e1b\u0e17\u0e23\u0e07|\u0e40\u0e14\u0e34\u0e21&lt;u/&gt;\u0e02\u0e13\u0e30|\u0e17\u0e35\u0e48|\u0e41\u0e21\u0e48|\u0e19\u0e01\u0e01\u0e34\u0e19\u0e1b\u0e25\u0e35\u0e2d\u0e01\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e07|\u0e2a\u0e48\u0e07\u0e40\u0e2a\u0e35\u0e22\u0e07|\u0e42\u0e27\u0e22\u0e27\u0e32\u0e22|\u0e2d\u0e22\u0e39\u0e48|\u0e43\u0e01\u0e25\u0e49|\u0e46&lt;u/&gt;\u0e41\u0e15\u0e48|\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22|\u0e44\u0e21\u0e48|\u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08|&lt;s/&gt;|\u0e2a\u0e2d\u0e07|\u0e2a\u0e32\u0e21|\u0e27\u0e31\u0e19|\u0e15\u0e48\u0e2d|\u0e21\u0e32|\u0e23\u0e31\u0e07|\u0e17\u0e35\u0e48|\u0e0a\u0e48\u0e27\u0e22|\u0e0b\u0e48\u0e2d\u0e21|\u0e01\u0e47|\u0e1e\u0e31\u0e07|\u0e44\u0e1b&lt;u/&gt;\u0e44\u0e21\u0e48|\u0e40\u0e2b\u0e47\u0e19|\u0e41\u0e21\u0e48|\u0e19\u0e01|\u0e1a\u0e34\u0e19|\u0e01\u0e25\u0e31\u0e1a|\u0e21\u0e32|\u0e2d\u0e35\u0e01|\u0e40\u0e25\u0e22&lt;u/&gt;\u2019   edu segmentation is based on syllable input using RandomForestClassifier model, which is trained on an edu-segmented corpus (approx. 7,000 edus)  created and used in Nalinee\u2019s thesis</p>\n<p>&gt;&gt;tltk.nlp.word_segment(Text,method=\u2019mm|ngram|colloc\u2019) : word segmentation using either maximum matching or ngram or maximum collocation approach. \u2018colloc\u2019 is used by default. Please note that the first run of ngram method would take a long time because TNC.3g will be loaded for ngram calculation. e.g.</p>\n<p>tltk.nlp.word_segment(\u2018\u0e1c\u0e39\u0e49\u0e2a\u0e37\u0e48\u0e2d\u0e02\u0e48\u0e32\u0e27\u0e23\u0e32\u0e22\u0e07\u0e32\u0e19\u0e27\u0e48\u0e32\u0e19\u0e32\u0e22\u0e01\u0e23\u0e31\u0e10\u0e21\u0e19\u0e15\u0e23\u0e35\u0e44\u0e21\u0e48\u0e21\u0e32\u0e17\u0e33\u0e07\u0e32\u0e19\u0e17\u0e35\u0e48\u0e17\u0e33\u0e40\u0e19\u0e35\u0e22\u0e1a\u0e23\u0e31\u0e10\u0e1a\u0e32\u0e25\u2019)\n=&gt; \u2018\u0e1c\u0e39\u0e49\u0e2a\u0e37\u0e48\u0e2d\u0e02\u0e48\u0e32\u0e27|\u0e23\u0e32\u0e22\u0e07\u0e32\u0e19|\u0e27\u0e48\u0e32|\u0e19\u0e32\u0e22\u0e01\u0e23\u0e31\u0e10\u0e21\u0e19\u0e15\u0e23\u0e35|\u0e44\u0e21\u0e48|\u0e21\u0e32|\u0e17\u0e33\u0e07\u0e32\u0e19|\u0e17\u0e35\u0e48|\u0e17\u0e33\u0e40\u0e19\u0e35\u0e22\u0e1a\u0e23\u0e31\u0e10\u0e1a\u0e32\u0e25|&lt;s/&gt;\u2019</p>\n<p>&gt;&gt;tltk.nlp.syl_segment(Text) : syllable segmentation using 3gram statistics e.g. tltk.nlp.syl_segment(\u2018\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e1c\u0e25\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u2019)</p>\n<p>=&gt; \u2018\u0e42\u0e1b\u0e23~\u0e41\u0e01\u0e23\u0e21~\u0e2a\u0e33~\u0e2b\u0e23\u0e31\u0e1a~\u0e1b\u0e23\u0e30~\u0e21\u0e27\u0e25~\u0e1c\u0e25~\u0e20\u0e32~\u0e29\u0e32~\u0e44\u0e17\u0e22&lt;s/&gt;\u2019</p>\n<p>&gt;&gt;tltk.nlp.word_segment_nbest(Text, N) : return the best N segmentations based on the assumption of minimum word approach. e.g. tltk.nlp.word_segment_nbest(\u2018\u0e04\u0e19\u0e02\u0e31\u0e1a\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\u201d\u2019,10)</p>\n<p>=&gt; [[\u2018\u0e04\u0e19\u0e02\u0e31\u0e1a|\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019, \u2018\u0e04\u0e19\u0e02\u0e31\u0e1a\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019, \u2018\u0e04\u0e19|\u0e02\u0e31\u0e1a|\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019, \u2018\u0e04\u0e19|\u0e02\u0e31\u0e1a\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019, \u2018\u0e04\u0e19\u0e02\u0e31\u0e1a|\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019, \u2018\u0e04\u0e19\u0e02\u0e31\u0e1a\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33|\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019, \u2018\u0e04\u0e19\u0e02\u0e31\u0e1a|\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a|\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019, \u2018\u0e04\u0e19\u0e02\u0e31\u0e1a\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a|\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019, \u2018\u0e04\u0e19|\u0e02\u0e31\u0e1a|\u0e23\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019, \u2018\u0e04\u0e19\u0e02\u0e31\u0e1a|\u0e23|\u0e16|\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e17\u0e32\u0e07|\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e32\u0e01\u0e32\u0e28\u2019]]</p>\n<p>&gt;&gt;tltk.nlp.g2p(Text)  : return Word segments and pronunciations\ne.g. tltk.nlp.g2p(\u201c\u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e2d\u0e38\u0e14\u0e21\u0e28\u0e36\u0e01\u0e29\u0e32\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e01\u0e49\u0e32\u0e27\u0e43\u0e2b\u0e49\u0e17\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07\u0e02\u0e2d\u0e07\u0e15\u0e25\u0e32\u0e14\u0e41\u0e23\u0e07\u0e07\u0e32\u0e19\u201d)</p>\n<p>=&gt; \u201c\u0e2a\u0e16\u0e32~\u0e1a\u0e31\u0e19~\u0e2d\u0e38~\u0e14\u0e21~\u0e28\u0e36\u0e01~\u0e29\u0e32|\u0e44\u0e21\u0e48|\u0e2a\u0e32~\u0e21\u0e32\u0e23\u0e16|\u0e01\u0e49\u0e32\u0e27|\u0e43\u0e2b\u0e49|\u0e17\u0e31\u0e19|\u0e01\u0e32\u0e23|\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19~\u0e41\u0e1b\u0e25\u0e07|\u0e02\u0e2d\u0e07|\u0e15\u0e25\u0e32\u0e14~\u0e41\u0e23\u0e07~\u0e07\u0e32\u0e19&lt;tr/&gt;sa1\u2019thaa4~ban0~?u1~dom0~sUk1~saa4|maj2|saa4~maat2|kaaw2|haj2|than0|kaan0|pliian1~plxxN0|khOON4|ta1\u2019laat1~rxxN0~Naan0|&lt;s/&gt;\u201d</p>\n<p>&gt;&gt;tltk.nlp.th2ipa(Text) : return Thai transcription in IPA forms\ne.g. tltk.nlp.th2ipa(\u201c\u0e25\u0e07\u0e41\u0e21\u0e48\u0e19\u0e49\u0e33\u0e23\u0e2d\u0e40\u0e14\u0e34\u0e19\u0e44\u0e1b\u0e2b\u0e32\u0e1b\u0e25\u0e32\u201d)</p>\n<p>=&gt; \u2018lo\u014b1 m\u025b\u02d03.na\u02d0m4 r\u1d10\u02d01 d\u0264\u02d0n1 paj1 ha\u02d05 pla\u02d01 &lt;s/&gt;\u2019</p>\n<p>&gt;&gt;tltk.nlp.th2roman(Text) : return Thai romanization according to Royal Thai Institute guideline.\n.e.g. tltk.nlp.th2roman(\u201c\u0e04\u0e37\u0e2d\u0e40\u0e02\u0e32\u0e40\u0e14\u0e34\u0e19\u0e40\u0e25\u0e22\u0e25\u0e07\u0e44\u0e1b\u0e23\u0e2d\u0e43\u0e19\u0e41\u0e21\u0e48\u0e19\u0e49\u0e33\u0e2a\u0e30\u0e2d\u0e32\u0e14\u0e44\u0e1b\u0e2b\u0e32\u0e21\u0e30\u0e1b\u0e23\u0e32\u0e07\u201d)</p>\n<p>=&gt; \u2018khue khaw doen loei long pai ro nai maenam sa-at pai ha maprang &lt;s/&gt;\u2019</p>\n<p>&gt;&gt;tltk.nlp.g2p_all(Text) : return all transcriptions (IPA) as a list of tuple (syllable_list, transcription). Transcription is based on syllable reading rules. It could be different from th2ipa.\ne.g. tltk.nlp.g2p_all(\u201c\u0e23\u0e2d\u0e22\u0e01\u0e23\u0e48\u0e32\u0e07\u201d)</p>\n<p>=&gt; [(\u2018\u0e23\u0e2d\u0e22~\u0e01\u0e23\u0e48\u0e32\u0e07\u2019, \u2018r\u1d10\u02d0j1.ka2.ra\u02d0\u014b2\u2019), (\u2018\u0e23\u0e2d\u0e22~\u0e01\u0e23\u0e48\u0e32\u0e07\u2019, \u2018r\u1d10\u02d0j1.kra\u02d0\u014b2\u2019), (\u2018\u0e23\u0e2d~\u0e22\u0e01~\u0e23\u0e48\u0e32\u0e07\u2019, \u2018r\u1d10\u02d01.jok4.ra\u02d0\u014b3\u2019)]</p>\n<p>&gt;&gt;tltk.nlp.spell_candidates(Word) : list of possible correct words using minimum edit distance, e.g. tltk.nlp.spell_candidates(\u2018\u0e23\u0e31\u0e01\u0e29\u2019)</p>\n<p>=&gt; [\u2018\u0e23\u0e31\u0e01\u2019, \u2018\u0e17\u0e31\u0e01\u0e29\u2019, \u2018\u0e23\u0e31\u0e01\u0e29\u0e32\u2019, \u2018\u0e23\u0e31\u0e01\u0e29\u0e4c\u2019]</p>\n<p>Other defined functions in the package:\n&gt;&gt;tltk.nlp.reset_thaidict() : clear dictionary content\n&gt;&gt;tltk.nlp.read_thaidict(DictFile) : add a new dictionary  e.g. tltk.nlp.read_thaidict(\u2018BEST.dict\u2019)\n&gt;&gt;tltk.nlp.check_thaidict(Word) : check whether Word exists in the dictionary</p>\n</div>\n<div id=\"tltk-corpus-basic-tools-for-corpus-enquiry\">\n<h2>tltk.corpus  :   basic tools for corpus enquiry</h2>\n<p>&gt;&gt;tltk.corpus.TNC_load()  by default load TNC.3g. The file can be in the working directory or TLTK package directory</p>\n<p>&gt;&gt;tltk.corpus.trigram_load(TRIGRAM)  ###  load Trigram data from other sourse saved in tab delimited format \u201cW1tW2tW3tFreq\u201d  e.g.  tltk.corpus.load3gram(\u2018TNC.3g\u2019) \u2018TNC.3g\u2019 can be downloaded separately from Thai National Corpus Project.</p>\n<p>&gt;&gt;tltk.corpus.unigram(w1)   return normalized frequecy (frequency/million) of w1 from the corpus</p>\n<p>&gt;&gt;tltk.corpus.bigram(w1,w2)   return frequency/million of Bigram w1-w2 from the corpus e.g. tltk.corpus.bigram(\u201c\u0e2b\u0e32\u0e22\u201d,\u201d\u0e14\u0e35\u201d) =&gt; 2.331959592765809</p>\n<p>&gt;&gt;tltk.corpus.trigram(w1,w2,w3)  return frequency/million of Trigram w1-w2-w3 from the corpus</p>\n<p>&gt;&gt;tltk.corpus.collocates(w, stat=\u201dchi2\u201d, direct=\u201dboth\u201d, span=2, limit=10, minfq=1)   ### return all collocates of w, STAT = {freq,mi,chi2} DIR={left,right,both}  SPAN={1,2}  The output is a list of tuples  ((w1,w2), stat). e.g. tltk.corpus.collocates(\u201c\u0e27\u0e34\u0e48\u0e07\u201d,limit=5)</p>\n<p>=&gt; [((\u2018\u0e27\u0e34\u0e48\u0e07\u2019, \u2018\u0e41\u0e08\u0e49\u0e19\u2019), 86633.93952758134), ((\u2018\u0e27\u0e34\u0e48\u0e07\u2019, \u2018\u0e15\u0e37\u0e4b\u0e2d\u2019), 77175.29122642518), ((\u2018\u0e27\u0e34\u0e48\u0e07\u2019, \u2018\u0e01\u0e23\u0e30\u0e2b\u0e37\u0e14\u0e01\u0e23\u0e30\u0e2b\u0e2d\u0e1a\u2019), 48598.79465339733), ((\u2018\u0e27\u0e34\u0e48\u0e07\u2019, \u2018\u0e1b\u0e23\u0e39\u0e4a\u0e14\u2019), 41111.63720974819), ((\u2018\u0e25\u0e39\u0e48\u2019, \u2018\u0e27\u0e34\u0e48\u0e07\u2019), 33990.56839021914)]</p>\n<p>&gt;&gt;tltk.corpus.w2v_load()  by deafult load word2vec file \u201cTNCc5model.bin\u201d. The file can be in the working directory or TLTK package directory</p>\n<p>&gt;&gt;tltk.corpus.w2v_exist(w) check whether w has a vector representation  e.g. tltk.corpus.w2v_exist(\u201c\u0e2d\u0e32\u0e2b\u0e32\u0e23\u201d) =&gt; True</p>\n<p>&gt;&gt;tltk.corpus.w2v(w)  return vector representation of w</p>\n<p>&gt;&gt;tltk.corpus.similarity(w1,w2) e.g. tltk.corpus.similarity(\u201c\u0e2d\u0e32\u0e2b\u0e32\u0e23\u201d,\u201d\u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e27\u0e48\u0e32\u0e07\u201d) =&gt; 0.783551877546</p>\n<p>&gt;&gt;tltk.corpus.similar_words(w, n=10, cutoff=0., score=\u201dn\u201d)  e.g. tltk.corpus.similar_words(\u201c\u0e2d\u0e32\u0e2b\u0e32\u0e23\u201d,n=5, score=\u201dy\u201d)</p>\n<p>=&gt; [(\u2018\u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e27\u0e48\u0e32\u0e07\u2019, 0.7835519313812256), (\u2018\u0e02\u0e2d\u0e07\u0e27\u0e48\u0e32\u0e07\u2019, 0.7366500496864319), (\u2018\u0e02\u0e2d\u0e07\u0e2b\u0e27\u0e32\u0e19\u2019, 0.703102707862854), (\u2018\u0e40\u0e19\u0e37\u0e49\u0e2d\u0e2a\u0e31\u0e15\u0e27\u0e4c\u2019, 0.6960341930389404), (\u2018\u0e1c\u0e25\u0e44\u0e21\u0e49\u2019, 0.6641997694969177)]</p>\n<p>&gt;&gt;tltk.corpus.outofgroup([w1,w2,w3,\u2026]) e.g. tltk.corpus.outofgroup([\u201c\u0e19\u0e49\u0e33\u201d,\u201d\u0e2d\u0e32\u0e2b\u0e32\u0e23\u201d,\u201d\u0e02\u0e49\u0e32\u0e27\u201d,\u201d\u0e23\u0e16\u0e22\u0e19\u0e15\u0e4c\u201d,\u201d\u0e1c\u0e31\u0e01\u201d]) =&gt; \u201c\u0e23\u0e16\u0e22\u0e19\u0e15\u0e4c\u201d</p>\n<p>&gt;&gt;tltk.corpus.analogy(w1,w2,w3,n=1) e.g. tltk.corpus.analogy(\u2018\u0e1c\u0e39\u0e49\u0e0a\u0e32\u0e22\u2019,\u2019\u0e1e\u0e48\u0e2d\u2019,\u2019\u0e41\u0e21\u0e48\u2019) =&gt; [\u2018\u0e1c\u0e39\u0e49\u0e2b\u0e0d\u0e34\u0e07\u2019]  \u0e1c\u0e39\u0e49\u0e0a\u0e32\u0e22 - \u0e1e\u0e48\u0e2d + \u0e41\u0e21\u0e48 =  \u0e1c\u0e39\u0e49\u0e2b\u0e0d\u0e34\u0e07</p>\n<p>&gt;&gt;tltk.corpus.w2v_plot([w1,w2,w3,\u2026])  =&gt; plot a scratter graph of w1-wn in two dimensions</p>\n<p>&gt;&gt;tltk.corpus.w2v_compare_color([w1,w2,w3,\u2026])  =&gt; visualize the components of vectors w1-wn in color</p>\n</div>\n<div id=\"notes\">\n<h2>Notes</h2>\n<ul>\n<li>Word segmentation is based on a maximum collocation approach described in this publication: \u201cAroonmanakun, W. 2002. Collocation and Thai Word Segmentation. In Thanaruk Theeramunkong and Virach Sornlertlamvanich, eds. Proceedings of the Fifth Symposium on Natural Language Processing &amp; The Fifth Oriental COCOSDA Workshop. Pathumthani: Sirindhorn International Institute of Technology. 68-75.\u201d (<a href=\"http://pioneer.chula.ac.th/~awirote/ling/SNLP2002-0051c.pdf\" rel=\"nofollow\">http://pioneer.chula.ac.th/~awirote/ling/SNLP2002-0051c.pdf</a>)</li>\n<li>Use tltk.nlp.word_segment(Text) or tltk.nlp.syl_segment(Text) for segmenting Thai texts. Syllable segmentation now is based on a trigram model trainned on 3.1 million syllable corpus. Input text is a paragraph of Thai texts which can be mixed with English texts. Spaces in the paragraph will be marked as \u201c&lt;s/&gt;\u201d. Word boundary is marked by \u201c|\u201d. Syllable boundary is marked by \u201c~\u201d. Syllables here are written syllables. One written syllable may be pronounced as two syllables, i.e. \u201c\u0e2a\u0e01\u0e31\u0e14\u201d is segemnted here as one written syllable, but it is pronounced as two syllables \u201csa1-kat1\u201d.</li>\n<li>Determining words in a sentence is based on the dictionary and maximum collocation strength between syllables. Since many compounds and idioms, e.g. \u2018\u0e40\u0e15\u0e32\u0e44\u0e21\u0e42\u0e04\u0e23\u0e40\u0e27\u0e1f\u2019, \u2018\u0e44\u0e1f\u0e1f\u0e49\u0e32\u0e01\u0e23\u0e30\u0e41\u0e2a\u0e2a\u0e25\u0e31\u0e1a\u2019, \u2018\u0e1b\u0e35\u0e07\u0e1a\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13\u2019, \u2018\u0e2d\u0e38\u0e42\u0e21\u0e07\u0e04\u0e4c\u0e43\u0e15\u0e49\u0e14\u0e34\u0e19\u2019, \u2018\u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e08\u0e32\u0e19\u0e14\u0e48\u0e27\u0e19\u2019, \u2018\u0e1b\u0e39\u0e19\u0e02\u0e32\u0e27\u0e1c\u0e2a\u0e21\u0e1e\u0e34\u0e40\u0e28\u0e29\u2019, \u2018\u0e40\u0e15\u0e49\u0e19\u0e41\u0e23\u0e49\u0e07\u0e40\u0e15\u0e49\u0e19\u0e01\u0e32\u2019 etc., are included in the standard dictionary, these will likely be segmented as one word. For applications that prefer shortest meaningful words (i.e. \u2018\u0e23\u0e16|\u0e42\u0e14\u0e22\u0e2a\u0e32\u0e23\u2019, \u2018\u0e04\u0e19|\u0e43\u0e0a\u0e49\u2019, \u2018\u0e01\u0e25\u0e32\u0e07|\u0e04\u0e37\u0e19\u2019, \u2018\u0e15\u0e49\u0e19|\u0e44\u0e21\u0e49\u2019 as segmented in BEST corpus), users should reset the default dictionary used in this package and reload a new dictionary containing only simple words or shortest meaningful words. Use \u201creset_thaidict()\u201d to clear default dictionary content, and \u201cread_thaidict(\u2018DICT_FIILE\u2019)\u201d to load a new dictionary. A list of words compiled from BEST corpus is included in this package as a file \u2018BEST.dict\u2019</li>\n<li>The standard dictionary used in this package has more then 65,000 entries including abbreviations and transliterations compiled from various sources. A dictionary of 8,700 proper names e.g. country names, organization names, location names, animal names, plant names, food names, \u2026, such as \u2018\u0e2d\u0e38\u0e0b\u0e40\u0e1a\u0e01\u0e34\u0e2a\u0e16\u0e32\u0e19\u2019, \u2018\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e40\u0e25\u0e02\u0e32\u0e18\u0e34\u0e01\u0e32\u0e23\u0e19\u0e32\u0e22\u0e01\u0e23\u0e31\u0e10\u0e21\u0e19\u0e15\u0e23\u0e35\u2019, \u2018\u0e27\u0e31\u0e14\u0e43\u0e2b\u0e0d\u0e48\u0e2a\u0e38\u0e27\u0e23\u0e23\u0e13\u0e32\u0e23\u0e32\u0e21\u2019, \u2018\u0e2b\u0e19\u0e2d\u0e19\u0e40\u0e08\u0e32\u0e30\u0e25\u0e33\u0e15\u0e49\u0e19\u0e02\u0e49\u0e32\u0e27\u0e42\u0e1e\u0e14\u2019, \u2018\u0e1b\u0e25\u0e32\u0e2b\u0e21\u0e36\u0e01\u0e01\u0e23\u0e30\u0e40\u0e17\u0e35\u0e22\u0e21\u0e1e\u0e23\u0e34\u0e01\u0e44\u0e17\u0e22\u2019, are also added as a list of words in the system.</li>\n<li>For segmenting a specific domain text, a specialized dicionary can be used by adding more dictionary before segmenting texts. This can be done by calling read_thaidict(\u201cSPECIALIZED_DICT\u201d). Please note that the dictionary is a text file in \u201ciso-8859-11\u201d encoding. The format is one word per one line.</li>\n<li>\u2018setence segment\u2019 or actually \u2018edu segment\u2019 is a process to break a paragraph into a chunk of discourse units, which usually are a clause. It is based on RandomForestClassifier model, which is trained on an edu-segmented corpus (approx. 7,000 edus) created and used in Nalinee\u2019s thesis (<a href=\"http://www.arts.chula.ac.th/~ling/thesis/2556MA-LING-Nalinee.pdf\" rel=\"nofollow\">http://www.arts.chula.ac.th/~ling/thesis/2556MA-LING-Nalinee.pdf</a>). Accuracy of the model is 97.8%. The reason behind using edu can be found in [Aroonmanakun, W. 2007. Thoughts on Word and Sentence Segmentation in Thai. In Proceedings of the Seventh Symposium on Natural Language Processing, Dec 13-15, 2007, Pattaya, Thailand. 85-90.] [Intasaw, N. and Aroonmanakun, W. 2013. Basic Principles for Segmenting Thai EDUs. in Proceedings of 27th Pacific Asia Conference on Language, Information, and Computation, pages 491-498, Nov 22-24, 2013, Taipei.]</li>\n<li>\u2018grapheme to phoneme\u2019 (g2p), as well as IPA transcription (th2ipa) and Thai romanization (th2roman) is based on the hybrid approach presented in the paper \u201cA Unified Model of Thai Romanization and Word Segmentation\u201d. The Thai Royal Institute guidline for Thai romanization can be downloaded from \u201c<a href=\"http://www.arts.chula.ac.th/~ling/tts/ThaiRoman.pdf\" rel=\"nofollow\">http://www.arts.chula.ac.th/~ling/tts/ThaiRoman.pdf</a>\u201d, or \u201c<a href=\"http://www.royin.go.th/?page_id=619\" rel=\"nofollow\">http://www.royin.go.th/?page_id=619</a>\u201d [Aroonmanakun, W., and W. Rivepiboon. 2004. A Unified Model of Thai Word Segmentation and Romanization. In  Proceedings of The 18th Pacific Asia Conference on Language, Information and Computation, Dec 8-10, 2004, Tokyo, Japan. 205-214.] (<a href=\"http://www.aclweb.org/anthology/Y04-1021\" rel=\"nofollow\">http://www.aclweb.org/anthology/Y04-1021</a>)</li>\n</ul>\n</div>\n<div id=\"remarks\">\n<h2>Remarks</h2>\n<ul>\n<li>TNC Trigram data (TNC.3g)  and  TNC word2vec (TNCc5model.bin) can be downloaded from TNC website. <a href=\"http://www.arts.chula.ac.th/ling/tnc/searchtnc/\" rel=\"nofollow\">http://www.arts.chula.ac.th/ling/tnc/searchtnc/</a></li>\n<li>Module \u201cspell_candidates\u201d is modified from Peter Norvig\u2019s Python codes at <a href=\"http://norvig.com/spell-correct.html\" rel=\"nofollow\">http://norvig.com/spell-correct.html</a></li>\n<li>Module \u201cw2v_compare_color\u201d is modified from <a href=\"http://chrisculy.net/lx/wordvectors/wvecs_visualization.html\" rel=\"nofollow\">http://chrisculy.net/lx/wordvectors/wvecs_visualization.html</a></li>\n<li>BEST corpus is the corpus released by NECTEC  (<a href=\"https://www.nectec.or.th/corpus/\" rel=\"nofollow\">https://www.nectec.or.th/corpus/</a>)</li>\n<li>Universal POS tags are used in this project. For more information, please see <a href=\"http://universaldependencies.org/u/pos/index.html\" rel=\"nofollow\">http://universaldependencies.org/u/pos/index.html</a> and <a href=\"http://www.arts.chula.ac.th/~ling/contents/File/UD%20Annotation%20for%20Thai.pdf\" rel=\"nofollow\">http://www.arts.chula.ac.th/~ling/contents/File/UD%20Annotation%20for%20Thai.pdf</a></li>\n<li>pos_tag is based on PerceptronTagger in nltk.tag.perceptron. It is trained with TNC data manually pos-taged (approx. 148,000 words). Accuracy on pos tagging is 91.68%.  NLTK PerceptronTagger is a port of the Textblob Averaged Perceptron Tagger, which can be found at <a href=\"https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\" rel=\"nofollow\">https://explosion.ai/blog/part-of-speech-pos-tagger-in-python</a></li>\n<li>named entiy recognition module is a CRF model adapted from this tutorial (<a href=\"http://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html\" rel=\"nofollow\">http://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html</a>). The model is trained with NER data used in Sasimimon\u2019s and Nutcha\u2019s theses (altogether 7,354 names in a corpus of 183,300 words). (<a href=\"http://pioneer.chula.ac.th/~awirote/Data-Nutcha.zip\" rel=\"nofollow\">http://pioneer.chula.ac.th/~awirote/Data-Nutcha.zip</a>, <a href=\"http://pioneer.chula.ac.th/~awirote/\" rel=\"nofollow\">http://pioneer.chula.ac.th/~awirote/</a> Data-Sasiwimon.zip) and NER data from AIforThai  (<a href=\"https://aiforthai.in.th/\" rel=\"nofollow\">https://aiforthai.in.th/</a>) Only valid NE files from AIforThai are used. The total number of all NEs is 170,076. Accuracy of the model is reported below (88%).</li>\n</ul>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td>tag</td>\n<td>precision</td>\n<td>recall</td>\n<td>f1-score</td>\n<td>support</td>\n</tr>\n<tr><td>B-L</td>\n<td>0.56</td>\n<td>0.48</td>\n<td>0.52</td>\n<td>27105</td>\n</tr>\n<tr><td>B-O</td>\n<td>0.72</td>\n<td>0.58</td>\n<td>0.64</td>\n<td>59613</td>\n</tr>\n<tr><td>B-P</td>\n<td>0.82</td>\n<td>0.83</td>\n<td>0.83</td>\n<td>83358</td>\n</tr>\n<tr><td>I-L</td>\n<td>0.52</td>\n<td>0.43</td>\n<td>0.47</td>\n<td>17859</td>\n</tr>\n<tr><td>I-O</td>\n<td>0.67</td>\n<td>0.59</td>\n<td>0.63</td>\n<td>67396</td>\n</tr>\n<tr><td>I-P</td>\n<td>0.85</td>\n<td>0.88</td>\n<td>0.86</td>\n<td>175069</td>\n</tr>\n<tr><td>O</td>\n<td>0.92</td>\n<td>0.94</td>\n<td>0.93</td>\n<td>1032377</td>\n</tr>\n<tr><td>accuracy</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>0.88</td>\n<td>1462777</td>\n</tr>\n<tr><td>macro avg</td>\n<td>0.72</td>\n<td>0.68</td>\n<td>0.70</td>\n<td>1462777</td>\n</tr>\n<tr><td>weighted avg</td>\n<td>0.87</td>\n<td>0.88</td>\n<td>0.88</td>\n<td>1462777</td>\n</tr>\n</tbody>\n</table>\n</div>\n\n          </div>"}, "last_serial": 6506303, "releases": {"0.3.3": [{"comment_text": "", "digests": {"md5": "9898dc5e948fb1235a98760ebb600720", "sha256": "92e22608825703c35ebc25b1edff8607c1ce0c308de2f767f153d7a39b8b0696"}, "downloads": -1, "filename": "tltk-0.3.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "9898dc5e948fb1235a98760ebb600720", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 24280069, "upload_time": "2018-02-21T15:02:46", "upload_time_iso_8601": "2018-02-21T15:02:46.162965Z", "url": "https://files.pythonhosted.org/packages/51/ea/e3f1d8bd109a531c3cc7933f7d5e7e91742b6196e3aa480b2fb4d364e59e/tltk-0.3.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "342b6f1e5790141cab3246aa26243353", "sha256": "4ced58f6ab97db255028b6ebbc9c5bedba5d40a9abb4694e7ab2e3e5bd203e38"}, "downloads": -1, "filename": "tltk-0.3.3.tar.gz", "has_sig": false, "md5_digest": "342b6f1e5790141cab3246aa26243353", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23465453, "upload_time": "2018-02-21T15:11:00", "upload_time_iso_8601": "2018-02-21T15:11:00.223704Z", "url": "https://files.pythonhosted.org/packages/08/bf/f677049ccb0e1b43068829971d3c792ed4ee391ffb0a20a1dcde9a20f69d/tltk-0.3.3.tar.gz", "yanked": false}], "0.3.4": [{"comment_text": "", "digests": {"md5": "191eddd1e91890b8ead65cc3ab3d77db", "sha256": "f1313ba2898fd1a7eb05f6345c48e48a99ac7b2e437ccedcdc4736c93884c84e"}, "downloads": -1, "filename": "tltk-0.3.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "191eddd1e91890b8ead65cc3ab3d77db", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 24280020, "upload_time": "2018-02-21T16:01:13", "upload_time_iso_8601": "2018-02-21T16:01:13.936213Z", "url": "https://files.pythonhosted.org/packages/1c/a1/42929d1a1c74840a2a21dd2c07c223bf45992958399ab983e6095d94c0a1/tltk-0.3.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0a12822a86f68bbfebf2eba0cc21784b", "sha256": "da14b48089e9984ff97cab5ebfc4b9d53e40e21e39c56c05f3853cad74edf5fa"}, "downloads": -1, "filename": "tltk-0.3.4.tar.gz", "has_sig": false, "md5_digest": "0a12822a86f68bbfebf2eba0cc21784b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23465432, "upload_time": "2018-02-21T16:09:12", "upload_time_iso_8601": "2018-02-21T16:09:12.100729Z", "url": "https://files.pythonhosted.org/packages/e7/ca/9e84fc7fbc1821f38b27c4d9ac0cb8053460f07cb3181d6d107414c6c00c/tltk-0.3.4.tar.gz", "yanked": false}], "0.3.5": [{"comment_text": "", "digests": {"md5": "123134360510830c6918b4215e62821f", "sha256": "f5500a92059306350c5385e3184e175f6c5f9dd2d52d04b6724c7454643db9bc"}, "downloads": -1, "filename": "tltk-0.3.5-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "123134360510830c6918b4215e62821f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 24280268, "upload_time": "2018-02-22T15:36:49", "upload_time_iso_8601": "2018-02-22T15:36:49.792631Z", "url": "https://files.pythonhosted.org/packages/30/10/d385b9f1afb11aefc5e71509301a9ba4c4d00653a0b853cc17add75b925a/tltk-0.3.5-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0c392f32b7dd795884e31d14bbaf80fa", "sha256": "e3a883350d3706589c2e95081ca1e3d9665aa1ca4b236f7964d6579619ebcd21"}, "downloads": -1, "filename": "tltk-0.3.5.tar.gz", "has_sig": false, "md5_digest": "0c392f32b7dd795884e31d14bbaf80fa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23465911, "upload_time": "2018-02-22T15:53:15", "upload_time_iso_8601": "2018-02-22T15:53:15.568494Z", "url": "https://files.pythonhosted.org/packages/c3/0b/5a0bf088450bfb4c8ad5c70f097979df6f0f65d69d8a7a3ff17113fe284f/tltk-0.3.5.tar.gz", "yanked": false}], "0.3.6": [{"comment_text": "", "digests": {"md5": "18651044e6ddd8c31d0ff8929a3ba304", "sha256": "8a1dd68e4842068058dc96576069338c887592f1e3d9881f56bfe0cae8abd27c"}, "downloads": -1, "filename": "tltk-0.3.6.tar.gz", "has_sig": false, "md5_digest": "18651044e6ddd8c31d0ff8929a3ba304", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23463206, "upload_time": "2018-03-29T13:42:08", "upload_time_iso_8601": "2018-03-29T13:42:08.867994Z", "url": "https://files.pythonhosted.org/packages/ab/3c/087f330791542f10654fdf4100beb0cdc93c74fb164afab3d434ff98362e/tltk-0.3.6.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "0ebc583f6562cd5b9933aeaaf81249e7", "sha256": "e73191765d136b45d25961d7c45b96baf05f283d6857aab5e3385d6c86be1d9c"}, "downloads": -1, "filename": "tltk-0.4.0.tar.gz", "has_sig": false, "md5_digest": "0ebc583f6562cd5b9933aeaaf81249e7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25372582, "upload_time": "2018-04-03T05:50:12", "upload_time_iso_8601": "2018-04-03T05:50:12.818252Z", "url": "https://files.pythonhosted.org/packages/b8/2a/b7705ae5a4ac58cb28a3d7747ee1e1967d80b144f8a7ed8428970b7be604/tltk-0.4.0.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "f72a8197f1ae1c09846254961c42f3c1", "sha256": "63a375bd1c7039a1d61f7a285cb9addbae0b52e8dcfa5851805ddf6f07b31e27"}, "downloads": -1, "filename": "tltk-0.4.1.tar.gz", "has_sig": false, "md5_digest": "f72a8197f1ae1c09846254961c42f3c1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25372941, "upload_time": "2018-04-18T17:56:14", "upload_time_iso_8601": "2018-04-18T17:56:14.512370Z", "url": "https://files.pythonhosted.org/packages/86/64/73465b41704024b9d18f6a6e2b4e8c2042e5a130943e0529a260847a45db/tltk-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "30dbef002e53dabe2d380373b37ccb89", "sha256": "28b48a5b826c11f75d67399d7a8f33ecc9ce2b4b0f0fabbcd4176fb881971bf2"}, "downloads": -1, "filename": "tltk-0.4.2.tar.gz", "has_sig": false, "md5_digest": "30dbef002e53dabe2d380373b37ccb89", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25407465, "upload_time": "2018-04-21T11:38:03", "upload_time_iso_8601": "2018-04-21T11:38:03.334787Z", "url": "https://files.pythonhosted.org/packages/27/b4/9ba70eb7211f1764b6b8b14f1be713b7d4a0c2952dfab1cb889088e3b5f1/tltk-0.4.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "d0bd82b0459ecfa74b53caf385c31fe4", "sha256": "fd1cafe9b1a7b9226a3b4c3a5253155fa2f38a3d98e5a72c157f05c93415d1b9"}, "downloads": -1, "filename": "tltk-1.0.3.tar.gz", "has_sig": false, "md5_digest": "d0bd82b0459ecfa74b53caf385c31fe4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25997329, "upload_time": "2018-05-18T10:22:21", "upload_time_iso_8601": "2018-05-18T10:22:21.126959Z", "url": "https://files.pythonhosted.org/packages/d7/2c/56245dd68c079158ed0787456148d4538299fd6f7aa263615bd169bb21b0/tltk-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "56d905a7eef1e10f4f72af8da6974c3a", "sha256": "5a4a4e5ed19fdf8ab3b1c35d201764091470cb3bc601ace20c608493e906986e"}, "downloads": -1, "filename": "tltk-1.0.4.tar.gz", "has_sig": false, "md5_digest": "56d905a7eef1e10f4f72af8da6974c3a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25997363, "upload_time": "2018-05-24T16:52:04", "upload_time_iso_8601": "2018-05-24T16:52:04.576919Z", "url": "https://files.pythonhosted.org/packages/42/12/3606e60aa3f01634f3552b7d65040064e643413e9f84448ba868f70c006a/tltk-1.0.4.tar.gz", "yanked": false}], "1.0.5": [{"comment_text": "", "digests": {"md5": "c871061f7437962df71544a8d0ec28bf", "sha256": "236387ae302d30154700328916dc598d72c2f776117d5bfa88bbd272c7416c75"}, "downloads": -1, "filename": "tltk-1.0.5.tar.gz", "has_sig": false, "md5_digest": "c871061f7437962df71544a8d0ec28bf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25997376, "upload_time": "2018-05-25T17:22:46", "upload_time_iso_8601": "2018-05-25T17:22:46.455004Z", "url": "https://files.pythonhosted.org/packages/a5/93/3484c0523323e63494f11ef937d3e4563c7789b49a81b6bbaa5ef5fc519d/tltk-1.0.5.tar.gz", "yanked": false}], "1.0.9": [{"comment_text": "", "digests": {"md5": "0bd9d48bcd1618a1543ba97e2630455a", "sha256": "8dd14250dcd4bd2105cfff6d63d22d92b3c64d8d37f1a4d222a867a98c0fb6f5"}, "downloads": -1, "filename": "tltk-1.0.9.tar.gz", "has_sig": false, "md5_digest": "0bd9d48bcd1618a1543ba97e2630455a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26001104, "upload_time": "2018-06-21T07:07:27", "upload_time_iso_8601": "2018-06-21T07:07:27.306982Z", "url": "https://files.pythonhosted.org/packages/f1/0c/ca8fbc8b59cc7e3bfa1a6f439f9e09475af264851d54608fc34234da8621/tltk-1.0.9.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "d3066b9dbb5bd646d86d108431baf054", "sha256": "4d1c6ad8b421d4acd4a31ff194a8714e63625ef0cc8ef30df8b598aeaeacd99d"}, "downloads": -1, "filename": "tltk-1.1.0.tar.gz", "has_sig": false, "md5_digest": "d3066b9dbb5bd646d86d108431baf054", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26001853, "upload_time": "2018-06-30T05:51:13", "upload_time_iso_8601": "2018-06-30T05:51:13.446479Z", "url": "https://files.pythonhosted.org/packages/9b/b0/c8565c96902227f60d0c8c8227a162e906ca760157686ddacc788cc740d4/tltk-1.1.0.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "19c41b5a3915b11ff2e0facfb0bb2198", "sha256": "c450acd5b79f1ee3c05a991e14dfdc3d85bee08bb2aab5fde23c160dccefe6bf"}, "downloads": -1, "filename": "tltk-1.1.1.tar.gz", "has_sig": false, "md5_digest": "19c41b5a3915b11ff2e0facfb0bb2198", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26002403, "upload_time": "2018-07-12T12:07:50", "upload_time_iso_8601": "2018-07-12T12:07:50.158569Z", "url": "https://files.pythonhosted.org/packages/a6/82/7c1635c8b354f187d152cfb659dce0a96c17247bca7d835855ab1d83eb8f/tltk-1.1.1.tar.gz", "yanked": false}], "1.1.2": [{"comment_text": "", "digests": {"md5": "bc411a0e72a6bbb53dc94470fdb6d121", "sha256": "3c6fb63a01a3c8a4a9ebcff741314773e04e0afe00b9463e9cce9bc1e07a603a"}, "downloads": -1, "filename": "tltk-1.1.2.tar.gz", "has_sig": false, "md5_digest": "bc411a0e72a6bbb53dc94470fdb6d121", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10317932, "upload_time": "2018-07-27T09:30:04", "upload_time_iso_8601": "2018-07-27T09:30:04.392216Z", "url": "https://files.pythonhosted.org/packages/6d/74/994fb5a6a72d5b0e30ea15ba243afb08168d2e1e6f8c2bef8e7683300048/tltk-1.1.2.tar.gz", "yanked": false}], "1.1.6": [{"comment_text": "", "digests": {"md5": "246d6c2fd94fa6c84d0158630f18f700", "sha256": "3a361761c0b702e9493b8c4dfae4a305b45e6378434f9940214b6f5e67e78446"}, "downloads": -1, "filename": "tltk-1.1.6.tar.gz", "has_sig": false, "md5_digest": "246d6c2fd94fa6c84d0158630f18f700", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10516288, "upload_time": "2018-10-16T12:48:39", "upload_time_iso_8601": "2018-10-16T12:48:39.437492Z", "url": "https://files.pythonhosted.org/packages/df/87/59f7ca0d3ae4a11ef71721893db17f30bb5392910fe29a89ed9d1700ed1a/tltk-1.1.6.tar.gz", "yanked": false}], "1.1.7": [{"comment_text": "", "digests": {"md5": "5519e5d4bb49d69c692bbcc536670db0", "sha256": "be9327d6b9e2f778fb76a2d5549b9e8bac4572ce46cb86effaa0a83cb5471c21"}, "downloads": -1, "filename": "tltk-1.1.7.tar.gz", "has_sig": false, "md5_digest": "5519e5d4bb49d69c692bbcc536670db0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10516942, "upload_time": "2018-11-25T02:16:21", "upload_time_iso_8601": "2018-11-25T02:16:21.899151Z", "url": "https://files.pythonhosted.org/packages/ef/5e/aafe6922392ff02723747e12950e70e9b9f451590b82c697d00278c6462a/tltk-1.1.7.tar.gz", "yanked": false}], "1.1.8": [{"comment_text": "", "digests": {"md5": "6d21d61394434a04ad09466b44e05bc2", "sha256": "04749cb593fa55fac1c43f78ddc1fb91520acbbb97e2b6519e074adeff3c6a2e"}, "downloads": -1, "filename": "tltk-1.1.8.tar.gz", "has_sig": false, "md5_digest": "6d21d61394434a04ad09466b44e05bc2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10517312, "upload_time": "2018-12-21T04:03:52", "upload_time_iso_8601": "2018-12-21T04:03:52.450287Z", "url": "https://files.pythonhosted.org/packages/34/3a/6283a0beae2dec9766930749175e58129e53b5f3e462add39a918485df27/tltk-1.1.8.tar.gz", "yanked": false}], "1.1.9": [{"comment_text": "", "digests": {"md5": "ed499bab9ea95bc30f2fa00ff0387712", "sha256": "f004cb3d0f89afb60347da05be7bb7af990de7339afdb266558ea885ecc80f91"}, "downloads": -1, "filename": "tltk-1.1.9.tar.gz", "has_sig": false, "md5_digest": "ed499bab9ea95bc30f2fa00ff0387712", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10517328, "upload_time": "2018-12-21T04:56:34", "upload_time_iso_8601": "2018-12-21T04:56:34.703244Z", "url": "https://files.pythonhosted.org/packages/44/b2/c43050e2884232f33725fee6da0aaa2105978cfca60ca3f7d846b5307720/tltk-1.1.9.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "ccd423c1f96290d040c556182128ac8f", "sha256": "d425934d87ce18a501df1fe52be19a495cc32c4fcd93979dc8ee84420ada8dcd"}, "downloads": -1, "filename": "tltk-1.2.0.tar.gz", "has_sig": false, "md5_digest": "ccd423c1f96290d040c556182128ac8f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10517444, "upload_time": "2018-12-21T16:47:40", "upload_time_iso_8601": "2018-12-21T16:47:40.838058Z", "url": "https://files.pythonhosted.org/packages/35/4e/e6da67363cc1264eec65827702c7ab970d6713d877e97eb77e5711f1efcd/tltk-1.2.0.tar.gz", "yanked": false}], "1.2.1": [{"comment_text": "", "digests": {"md5": "760ed30fcc519324fda05c6a03a67fd7", "sha256": "761c19fc4c024cebe9ffede1593c3e07cd778a23794469825c254fd720f54afe"}, "downloads": -1, "filename": "tltk-1.2.1.tar.gz", "has_sig": false, "md5_digest": "760ed30fcc519324fda05c6a03a67fd7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10517319, "upload_time": "2018-12-27T05:41:02", "upload_time_iso_8601": "2018-12-27T05:41:02.675643Z", "url": "https://files.pythonhosted.org/packages/d6/a8/89c69547602f894f87df586ec42033b94ad2e55df7771a0096fa49487926/tltk-1.2.1.tar.gz", "yanked": false}], "1.2.2": [{"comment_text": "", "digests": {"md5": "7c093419658f90a0d9ec247d270cbc7e", "sha256": "a161f5564e9beda5dda92bde6fcf77eb7a59960ba30421e3b1b7a7056aeff02a"}, "downloads": -1, "filename": "tltk-1.2.2.tar.gz", "has_sig": false, "md5_digest": "7c093419658f90a0d9ec247d270cbc7e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10500065, "upload_time": "2019-09-13T04:28:20", "upload_time_iso_8601": "2019-09-13T04:28:20.144992Z", "url": "https://files.pythonhosted.org/packages/3d/f6/78acc155e751af4ede82cdd8e22812c9f0ad0924a605cd81183c14f07aea/tltk-1.2.2.tar.gz", "yanked": false}], "1.2.3": [{"comment_text": "", "digests": {"md5": "de86d68c29437b11895e43da61fb4f88", "sha256": "fcf9ec42f556ee4ec9d3420570b713b8adc7d70748a7c714c747f8d8f4d9a465"}, "downloads": -1, "filename": "tltk-1.2.3.tar.gz", "has_sig": false, "md5_digest": "de86d68c29437b11895e43da61fb4f88", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10500042, "upload_time": "2019-09-13T13:48:34", "upload_time_iso_8601": "2019-09-13T13:48:34.452789Z", "url": "https://files.pythonhosted.org/packages/fb/d3/b3ad4d7847be6a048ed8050ff40c342dad54f3dedf3b3dd1840b108ba53e/tltk-1.2.3.tar.gz", "yanked": false}], "1.2.4": [{"comment_text": "", "digests": {"md5": "fc8b9f3b585dd15ad8b3f4a6f7888a3d", "sha256": "aa7a53fdebf579907849e01c352dfcf0c30f1603904459da7d1551b581dd3529"}, "downloads": -1, "filename": "tltk-1.2.4.tar.gz", "has_sig": false, "md5_digest": "fc8b9f3b585dd15ad8b3f4a6f7888a3d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10500052, "upload_time": "2019-09-22T01:09:17", "upload_time_iso_8601": "2019-09-22T01:09:17.515177Z", "url": "https://files.pythonhosted.org/packages/d9/93/0278dbcdfbcaef4ecec0fa9dc23a7c0194ff7a26cf049002ddfa406291f9/tltk-1.2.4.tar.gz", "yanked": false}], "1.2.5": [{"comment_text": "", "digests": {"md5": "1d207088da112ea26399b08cb070a321", "sha256": "b637de21b4ff7872f4ea000995a4bf1489cd3cf3d01f3f1a39116bb5f1ffaa2c"}, "downloads": -1, "filename": "tltk-1.2.5.tar.gz", "has_sig": false, "md5_digest": "1d207088da112ea26399b08cb070a321", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11110930, "upload_time": "2019-10-17T03:41:49", "upload_time_iso_8601": "2019-10-17T03:41:49.499887Z", "url": "https://files.pythonhosted.org/packages/48/46/b511ff92ea3684be56c22e6c47b78f6cc345bbe37b6cbe2df4154ce04a41/tltk-1.2.5.tar.gz", "yanked": false}], "1.2.6": [{"comment_text": "", "digests": {"md5": "570640e5a17aab4a0744e1d21c53b418", "sha256": "112192d022d5b0b3448bbeae3108f4ad730b0a722efcceac6f062ec8443c1192"}, "downloads": -1, "filename": "tltk-1.2.6.tar.gz", "has_sig": false, "md5_digest": "570640e5a17aab4a0744e1d21c53b418", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11276520, "upload_time": "2019-10-23T09:30:26", "upload_time_iso_8601": "2019-10-23T09:30:26.585097Z", "url": "https://files.pythonhosted.org/packages/fd/6d/b0bd1052fcc9c48a8c2ce9b1b62fb87525cc55d90745f80db94fc0290aba/tltk-1.2.6.tar.gz", "yanked": false}], "1.2.7": [{"comment_text": "", "digests": {"md5": "28f8602988d10dfced6a5a5a5b6e4250", "sha256": "81011355c6e4f5214d832a96aacd6b512591261b784de185f9e1f2f53ab48d69"}, "downloads": -1, "filename": "tltk-1.2.7.tar.gz", "has_sig": false, "md5_digest": "28f8602988d10dfced6a5a5a5b6e4250", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11277033, "upload_time": "2019-10-27T08:44:41", "upload_time_iso_8601": "2019-10-27T08:44:41.857690Z", "url": "https://files.pythonhosted.org/packages/c8/33/efe61432a0eb219898ff8820576712420c695c660cadf52f0903a745c5dd/tltk-1.2.7.tar.gz", "yanked": false}], "1.2.8": [{"comment_text": "", "digests": {"md5": "99d85c5191c734cd466dcb36ed1d7311", "sha256": "322a60900cdfab6d7fcda10150c5f053cd691bf21313be7dbe243440eccf8164"}, "downloads": -1, "filename": "tltk-1.2.8.tar.gz", "has_sig": false, "md5_digest": "99d85c5191c734cd466dcb36ed1d7311", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11277441, "upload_time": "2019-11-14T13:56:03", "upload_time_iso_8601": "2019-11-14T13:56:03.723796Z", "url": "https://files.pythonhosted.org/packages/a6/d2/d35ba6b794c600bd425a69713aec14061b8bf883f2b19e346a216f4bdc08/tltk-1.2.8.tar.gz", "yanked": false}], "1.2.9": [{"comment_text": "", "digests": {"md5": "49503ff97eba450939946a3b26c94525", "sha256": "317cc5d9944f5e076b60a1377473ee1c70bbea82f1950ffa78a94940a702b86d"}, "downloads": -1, "filename": "tltk-1.2.9.tar.gz", "has_sig": false, "md5_digest": "49503ff97eba450939946a3b26c94525", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11277993, "upload_time": "2019-11-20T09:36:09", "upload_time_iso_8601": "2019-11-20T09:36:09.700814Z", "url": "https://files.pythonhosted.org/packages/e1/cb/950b854470b95968f8b48d744a448986824d6f60014b86f9a4ffeef97394/tltk-1.2.9.tar.gz", "yanked": false}], "1.3.0": [{"comment_text": "", "digests": {"md5": "0d3dbf9bf8a4d1ff870199ece518a6df", "sha256": "d477c6686ed62e10434034ca6e986e456c60a3fc4d3afe7183895fd6d75b2f67"}, "downloads": -1, "filename": "tltk-1.3.0.tar.gz", "has_sig": false, "md5_digest": "0d3dbf9bf8a4d1ff870199ece518a6df", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11278483, "upload_time": "2019-11-28T02:49:26", "upload_time_iso_8601": "2019-11-28T02:49:26.959321Z", "url": "https://files.pythonhosted.org/packages/54/4c/44099381d9372b6c3d8d33acfd6d6e3f86bf478c555fe79bcf800bf85f76/tltk-1.3.0.tar.gz", "yanked": false}], "1.3.1": [{"comment_text": "", "digests": {"md5": "89bdc65b5120ed45baea8dc95ec800ea", "sha256": "b47d7a39cfb4686de6fa26639eba994075360691f9cfac2f1415107f20d31bc5"}, "downloads": -1, "filename": "tltk-1.3.1.tar.gz", "has_sig": false, "md5_digest": "89bdc65b5120ed45baea8dc95ec800ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11278613, "upload_time": "2019-12-13T05:48:04", "upload_time_iso_8601": "2019-12-13T05:48:04.262659Z", "url": "https://files.pythonhosted.org/packages/78/10/7d6f9ab405933cea1dfb42048b2c8093c67e57a8d1854ad065a1c8dc8787/tltk-1.3.1.tar.gz", "yanked": false}], "1.3.2": [{"comment_text": "", "digests": {"md5": "26c418f83bf93aecda8db31375e7787c", "sha256": "bdd5204550ea754231cdda4937aa0c2bbabc00ae1d7ff721c6579b7826ea90bf"}, "downloads": -1, "filename": "tltk-1.3.2.tar.gz", "has_sig": false, "md5_digest": "26c418f83bf93aecda8db31375e7787c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11282112, "upload_time": "2020-01-22T06:58:54", "upload_time_iso_8601": "2020-01-22T06:58:54.096445Z", "url": "https://files.pythonhosted.org/packages/26/8f/68181fcb8157a16d93a9e7123bdf15ea5af340ab8b7c3f75dac62b937bf3/tltk-1.3.2.tar.gz", "yanked": false}], "1.3.3": [{"comment_text": "", "digests": {"md5": "0872dee98186a6ed211724cda63f8f9b", "sha256": "d8b5e0e4c0d434c5ef9560c23ded8049e7942c80b9d7c0a91ac4766b8b60ed37"}, "downloads": -1, "filename": "tltk-1.3.3.tar.gz", "has_sig": false, "md5_digest": "0872dee98186a6ed211724cda63f8f9b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11279015, "upload_time": "2020-01-22T14:28:13", "upload_time_iso_8601": "2020-01-22T14:28:13.259560Z", "url": "https://files.pythonhosted.org/packages/5d/9a/8747126c87d2d5dda91bac94db25019b245f488d548810cfaf32b25d228d/tltk-1.3.3.tar.gz", "yanked": false}], "1.3.4": [{"comment_text": "", "digests": {"md5": "7f3aaed361fe22c7d04c5a44efb35d5f", "sha256": "db84c33e4d19df77c03e2b7d8e69d4e5a87dd98bb8f9acb58a92ea180a0580ea"}, "downloads": -1, "filename": "tltk-1.3.4.tar.gz", "has_sig": false, "md5_digest": "7f3aaed361fe22c7d04c5a44efb35d5f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11279015, "upload_time": "2020-01-22T16:28:39", "upload_time_iso_8601": "2020-01-22T16:28:39.680275Z", "url": "https://files.pythonhosted.org/packages/de/0c/a981d2d55a1212d4581e721ba14182e8c7cc651415099235a869e8f46346/tltk-1.3.4.tar.gz", "yanked": false}], "1.3.5": [{"comment_text": "", "digests": {"md5": "c986e4e3c944c2aa9b40c269a8edb16f", "sha256": "70724d19d1f2475867e853d21c2355668bd14309096c0669fad1849e68db20a4"}, "downloads": -1, "filename": "tltk-1.3.5.tar.gz", "has_sig": false, "md5_digest": "c986e4e3c944c2aa9b40c269a8edb16f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11279023, "upload_time": "2020-01-23T12:50:37", "upload_time_iso_8601": "2020-01-23T12:50:37.882220Z", "url": "https://files.pythonhosted.org/packages/cd/87/fc57b0fb356222ae36fffaacf5f47ad9c38001d49695b42d4f21e03da844/tltk-1.3.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c986e4e3c944c2aa9b40c269a8edb16f", "sha256": "70724d19d1f2475867e853d21c2355668bd14309096c0669fad1849e68db20a4"}, "downloads": -1, "filename": "tltk-1.3.5.tar.gz", "has_sig": false, "md5_digest": "c986e4e3c944c2aa9b40c269a8edb16f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11279023, "upload_time": "2020-01-23T12:50:37", "upload_time_iso_8601": "2020-01-23T12:50:37.882220Z", "url": "https://files.pythonhosted.org/packages/cd/87/fc57b0fb356222ae36fffaacf5f47ad9c38001d49695b42d4f21e03da844/tltk-1.3.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:52:16 2020"}