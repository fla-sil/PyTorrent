{"info": {"author": "Angel Hernandez", "author_email": "ahernandez0691@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Databricks API Documentation\n*This package is a Python Implementation of the [Databricks API](https://docs.databricks.com/api/latest/index.html) for structured and programmatic use. This Python implementation requires that your Databricks API Token be saved as an environment variable in your system:* `export DATABRICKS_TOKEN=MY_DATABRICKS_TOKEN` in OSX / Linux. Or in Windows by searching for System Environment Variables in the Start Menu and adding it in the editor. For details see this [guide](https://superuser.com/questions/949560/how-do-i-set-system-environment-variables-in-windows-10).\n\n## UPDATE\n*Tokens can now be passed when a particular class is instantiated. E.g.:*\n\n```python\nfrom databricksapi import Workspace\nurl = 'https://my-databricks-instance.com'\ntoken = 'dapiXXXXXXXXXXXXX'\nws = Workspace(url, token)\n\n```\n\n## Installation\nYou can either use `pip install databricksapi` to install it globally, or you can clone the repository. Please note that only compatability with Python 3.7+ is guaranteed.\n\n## APIs Included  \n* Token\n* Secrets\n* Clusters\n* SCIM (Experimental)\n* Jobs\n* DBFS\n* Groups\n* Instance Profiles\n* Libraries\n* Workspace\n\n## Imports\nThe modules above can be imported as follows\n```python\nfrom databricksapi import Token, Jobs, DBFS\nurl = 'https://url.for.databricks.net'\n\ntoken_instance = Token(url)\njobs_instance = Jobs(url)\n```\n\n## Token API\nThe Token API allows any user to create, list, and revoke tokens that can be used to authenticate and access Databricks REST APIs. Initial authentication to this API is the same as for all of the Databricks API endpoints.\n\n### Methods\n1. createToken(*lifetime_seconds*, *comment*)\n2. listTokens()\n3. revokeToken(*token_id*)\n\n#### createToken(*lifetime_seconds*, *comment*)\nCreate and return a token.\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Token(url)\n\ndb_api.createToken(600, 'Test token')\n```\n\n#### listTokens()\nList all Token IDs in your Databricks Environment.\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Token(url)\n\ndb_api.listTokens()\n```\n\n#### revokeToken(*token_id*)\nRevoke an active Databricks token.\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Token(url)\n\n#token_id can be obtained from using the listTokens() method\ndb_api.revokeToken('5715498424f15ee0213be729257b53fc35a47d5953e3bdfd8ed22a0b93b339f4')\n```\n\n## Secrets API\nThe Secrets API allows you to manage secrets, secret scopes, and access permissions.\n\n### Methods\n1. createSecretScope(*scope*, *initial_manage_principal*)\n2. deleteSecretScope(*scope*)\n3. listSecretScopes()\n4. putSeceret(*value*, *value_type*, *scope*, *key*)\n5. deleteSecret(*scope*, *key*)\n6. listSecrets(*scope*)\n7. putSecretACL(*scope*, *principal*, *permission*)\n8. deleteSecretACL(*scope*, *principal*)\n9. getSecretACL(*scope, *principal*)\n10. listSecretACL(*scope*, *principal*)\n\n#### createSecretScope(*scope*, *initial_manage_princial*)\nCreates a new secret scope.\n\nThe scope name must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters. The maximum number of scopes in a workspace is 100.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\nscope = 'SomeSecretScope'\ninitial_manage_princial = 'users'\ndb_api.createSecretScope(scope, initial_manage_princial)\n```\n\n#### deleteSecretScope(*scope*)\n\nDelete a secret scope.\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\nscope = 'SomeSecretScope'\ndb_api.deleteSecretScope(scope)\n```\n\n#### listSecretScopes()\nList all secret scopes in the workspace\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\ndb_api.listSecretScopes()\n```\n\n#### putSecret(*value*, *value_type*, *scope*, *key*)  \nInserts a secret under the provided scope with the given name. If a secret already exists with the same name, this command overwrites the existing secret\u2019s value. The server encrypts the secret using the secret scope\u2019s encryption settings before storing it. You must have WRITE or MANAGE permission on the secret scope.\n\nThe `value_type` parameter can either be set to `string` or `bytes` depending on the type fo value the user passes in.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\n#set parameters\nvalue = 'BeepBoop'\nvalue_type = 'string'\nscope = 'SomeSecretScope'\nkey = 'uniqueScopekey'\n\ndb_api.putSecret(value, value_type, scope, key)\n```\n\n#### deleteSecret(*scope*, *key*)\nDeletes the secret stored in this secret scope. You must have WRITE or MANAGE permission on the secret scope.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\nscope = 'SomeSecretScope'\nkey = 'uniqueScopekey'\n\ndb_api.deleteSecret(scope, key)\n```\n\n#### listSecrets(*scope*)\nLists the secret keys that are stored at this scope. This is a metadata-only operation; secret data cannot be retrieved using this API. Users need READ permission to make this call.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\nscope = 'SomeSecretScope'\n\ndb_api.listSecrets(scope)\n```\n\n#### putSecretACL(*scope*, *principal*, *permission*)\nCreates or overwrites the ACL associated with the given principal (user or group) on the specified scope point. In general, a user or group will use the most powerful permission available to them\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\nscope = 'SomeSecretScope'\nprinicpal = 'users'\npermission = 'READ'\n\ndb_api.putSecretACL(scope, principal, permission)\n```\n\n#### deleteSecretACL(*scope*, *principal*)  \nDeletes the given ACL on the given scope.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\nscope = 'SomeSecretScope'\nprinicpal = 'users'\n\ndb_api.deleteSecretACL(scope, principal)\n```\n\n#### getSecretACL(*scope, *principal*)\nDescribes the details about the given ACL, such as the group and permission.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\nscope = 'SomeSecretScope'\nprinicpal = 'users'\n\ndb_api.getSecretACL(scope, principal)\n```\n\n#### listSecretACL(*scope*, *principal*)\nLists the ACLs set on the given scope.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Secrets(url)\n\nscope = 'SomeSecretScope'\nprinicpal = 'users'\n\ndb_api.listSecretACL(scope, principal)\n```\n\n## Clusters API\nThe Clusters API allows you to create, start, edit, list, terminate, and delete clusters via the API. The maximum allowed size of a request to the Clusters API is 10MB.\n\n### Methods\n1. createCluster(*worker*, *worker_type*, *cluster_name*, *spark_version*, *cluster_log_conf*, *node_type_id*, *driver_node_type_id=None*, *spark_conf=None*, *aws_attributes=None*, *ssh_public_keys=None*, *custom_tags=None*, *init_scripts=None*, *spark_env_vars=None*, *autotermination_minutes=None*, *enable_elastic_disk=None*)\n2. editCluster(*worker*, *worker_type*, *cluster_name*, *spark_version*, *cluster_log_conf*, *node_type_id*, *driver_node_type_id=None*, *spark_conf=None*, *aws_attributes=None*, *ssh_public_keys=None*, *custom_tags=None*, *init_scripts=None*, *spark_env_vars=None*, *autotermination_minutes=None*, *enable_elastic_disk=None*)\n3. startCluster(*cluster_id*)\n4. restartCluster(*cluster_id*)\n5. resizeCluster(*cluster_id*, *worker*, *worker_type*)\n6. terminateCluster(*cluster_id*)\n7. deleteCluster(*cluster_id*)\n8. getCluster(*cluster_id*)\n9. pinCluster(*cluster_id*)\n10. unpinCluster(*cluster_id*)\n11. listClusters()\n12. listNodeTypes()\n13. listZones()\n14. getSparkVersions()\n15. getClusterEvents(*cluster_id*, *order='DESC'*, *start_time=None*, *end_time=None*, *event_types=None*, *offset=None*, *limit=None*)\n\n#### createCluster(*worker*, *worker_type*, *cluster_name*, *spark_version*, *cluster_log_conf*, *node_type_id*, *driver_node_type_id=None*, *spark_conf=None*, *aws_attributes=None*, *ssh_public_keys=None*, *custom_tags=None*, *init_scripts=None*, *spark_env_vars=None*, *autotermination_minutes=None*, *enable_elastic_disk=None*)\n\nCreates a new Spark cluster. This method acquires new instances from the cloud provider if necessary. This method is asynchronous; the returned cluster_id can be used to poll the cluster state. When this method returns, the cluster is in a PENDING state. The cluster is usable once it enters a RUNNING state.\n\nThe `worker_type` can be either `workers` or `autoscale`. If a `autoscale` is set, then the `min_workers` and `max_workers` must be specified. \n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\nworker = 25\nworker_type = 'workers'\ncluster_name = 'TestCluster'\nspark_version = '4.0.x-scala2.11'\ncluster_log_conf = '/dbfs/log/path'\nnode_type_id = 'i3.xlarge'\n\ndb_api.createCluster(worker=worker, worker_type=worker_type, cluster_name=cluster_name, spark_version=spark_version, cluster_log_conf=cluster_log_conf, node_type_id=node_type_id)\n```\n\n#### editCluster(*worker*, *worker_type*, *cluster_name*, *spark_version*, *cluster_log_conf*, *node_type_id*, *driver_node_type_id=None*, *spark_conf=None*, *aws_attributes=None*, *ssh_public_keys=None*, *custom_tags=None*, *init_scripts=None*, *spark_env_vars=None*, *autotermination_minutes=None*, *enable_elastic_disk=None*)\n\nEdit an existings clusters configuration.\n\nThe `worker_type` can be either `workers` or `autoscale`. If a `autoscale` is set, then the `min_workers` and `max_workers` must be specified.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\nworker = 35\nworker_type = 'workers'\ncluster_name = 'TestCluster'\nspark_version = '4.0.x-scala2.11'\ncluster_log_conf = '/dbfs/new/log/path'\nnode_type_id = 'i5.xlarge'\n\ndb_api.editCluster(worker=worker, worker_type=worker_type, cluster_name=cluster_name, spark_version=spark_version, cluster_log_conf=cluster_log_conf, node_type_id=node_type_id)\n```\n\n#### startCluster(*cluster_id*)\u00a0\nStarts a terminated Spark cluster given its ID.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ncluster_id = '1202-211320-brick1'\ndb_api.startCluster(cluster_id)\n```\n\n#### restartCluster(*cluster_id*)\nRestarts a Spark cluster given its id. If the cluster is not in a RUNNING state, nothing will happen.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ncluster_id = '1202-211320-brick1'\ndb_api.restartCluster(cluster_id)\n```\n\n#### resizeCluster(*cluster_id*, *worker*, *worker_type*)\nResizes a cluster to have a desired number of workers. This will fail unless the cluster is in a RUNNING state.\n\nThe parameter `worker_type` can be one of `workers` or `autoscale`. \n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ncluster_id = '1202-211320-brick1'\nworkers = 30\n\ndb_api.resizeCluster(cluster_id, workers, worker_type='workers')\n```\n\n#### terminateCluster(*cluster_id*)\nTerminates a Spark cluster given its id. The cluster is removed asynchronously. Once the termination has completed, the cluster will be in a TERMINATED state. If the cluster is already in a TERMINATING or TERMINATED state, nothing will happen.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ncluster_id = '1202-211320-brick1'\n\ndb_api.terminateCluster(cluster_id)\n```\n\n#### deleteCluster(*cluster_id*)\nPermanently deletes a Spark cluster. If the cluster is running, it is terminated and its resources are asynchronously removed. If the cluster is terminated, then it is immediately removed.\n\nYou cannot perform any action on a permanently deleted cluster and a permanently deleted cluster is no longer returned in the cluster list.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ncluster_id = '1202-211320-brick1'\n\ndb_api.deleteCluster(cluster_id)\n```\n\n#### getCluster(*cluster_id*)\nReturns information about all pinned clusters, currently active clusters, up to 70 of the most recently terminated interactive clusters in the past 30 days, and up to 30 of the most recently terminated job clusters in the past 30 days.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ncluster_id = '1202-211320-brick1'\n\ndb_api.getCluster(cluster_id)\n```\n\n\n#### pinCluster(*cluster_id*)\nPinning a cluster ensures that the cluster is always returned by the List API. Pinning a cluster that is already pinned has no effect.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ncluster_id = '1202-211320-brick1'\n\ndb_api.pinCluster(cluster_id)\n```\n\n#### unpinCluster(*cluster_id*)\nUnpinning a cluster will allow the cluster to eventually be removed from the list returned by the List API. Unpinning a cluster that is not pinned has no effect.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ncluster_id = '1202-211320-brick1'\n\ndb_api.unpinCluster(cluster_id)\n```\n\n#### listClusters()\nRetrieves the information for a cluster given its identifier. Clusters can be described while they are running, or up to 30 days after they are terminated.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ndb_api.listClusters()\n```\n\n#### listNodeTypes()\nReturns a list of supported Spark node types. These node types can be used to launch a cluster.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ndb_api.listNodeTypes()\n```\n\n#### listZones()\nReturns a list of availability zones where clusters can be created in (ex: us-west-2a). These zones can be used to launch a cluster.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ndb_api.listZones()\n```\n\n#### getSparkVersions()\nReturns the list of available Spark versions. These versions can be used to launch a cluster.\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ndb_api.getSparkVersions()\n```\n\n#### getClusterEvents(*cluster_id*, *order='DESC'*, *start_time=None*, *end_time=None*, *event_types=None*, *offset=None*, *limit=None*)\nRetrieves a list of events about the activity of a cluster. This API is paginated. If there are more events to read, the response includes all the parameters necessary to request the next page of events.\n\n\n\n```python\nurl = 'https://url.for.databricks.net'\ndb_api = Clusters(url)\n\ncluster_id = '1202-211320-brick1'\n\ndb_api.getClusterEvents(cluster_id)\n```\n\n## Jobs API\nThe Jobs API allows you to create, edit, and delete jobs via the API. The maximum allowed size of a request to the Jobs API is 10MB.\n\n### Methods\n1. createJob(*cluster*, *cluster_type*, *task*, *task_type*, *name*, *libraries=None*, *email_notications=None*, *timeout_seconds=None*, *max_retries=None*, *min_retry_interval_millis=None*, *retry_on_timeout=None*,*schedule=None*, *max_concurrent_runs=None*)\n2. listJobs()\n3. deleteJob(*job_id*)\n4. batchDelete(*\\*args*)\n5. getJob(*job_id*)\n6. resetJob(*job_id*, *new_settings*)\n7. runJob(*job_id*, *job_type*, *params*)\n8. runsSubmit(*run_name*, *cluster*, *task*, *cluster_type*, *task_type*, *libraries=None*, *timeout_seconds=None*)\n9. runsList(*run*, *run_type*, *job_id*, *offset*, *limit*)\n10. runsGet(*run_id*)\n11. runsExport(*run_id*, *views_to_export*)\n12. runsCancel(*run_id*)\n13. runsGetOutput(*run_id*)\n14. runsDelete(*run_id*)\u00a0\n\n#### createJob(*cluster*, *cluster_type*, *task*, *task_type*, *name*, *libraries=None*, *email_notications=None*, *timeout_seconds=None*, *max_retries=None*, *min_retry_interval_millis=None*, *retry_on_timeout=None*,*schedule=None*, *max_concurrent_runs=None*)\nThe `cluster_type` parameter can be one of `existing` or `new`.\nThe `task_type` parameter must be one of `notebook`, `jar`, `submit`, or `python`.\n\nAll other parameters are documented in the Databricks Rest API.\n\n#### batchDelete(*\\*args*)\nTakes in a comma separated list of Job IDs to be deleted. This method is a wrapper around the `deleteJob` method.\n\n#### runJob(*job_id*, *job_type*, *params*)\nThe `job_type` parameter must be one of `notebook`, `jar`, `submit` or `python`.\n\n#### runsSubmit(*run_name*, *cluster*, *task*, *cluster_type*, *task_type*, *libraries=None*, *timeout_seconds=None*)\nThe `cluster_type` parameter can be one of `existing` or `new`.\n\n#### runsList(*run*, *run_type*, *job_id*, *offset*, *limit*)\nThe `run_type` parameter must be one of `completed` or `active`.\t\n\n## DBFS API\nThe DBFS API is a Databricks API that makes it simple to interact with various data sources without having to include your credentials every time you read a file. \n\n### Methods\n1. addBlock(*data*, *handle*)\n2. closeStream(*handle*)\n3. createFile(*path*, *overwrite*)\n4. deleteFile(*path*, *recursive*)\n5. getStatus(*path*)\n6. listFiles(*path*)\n7. makeDirs(*path*)\n8. moveFiles(*source_path*, *target_path*)\n9. putFiles(*path*, *overwrite*, *files*, *contents=None*)\n10. readFiles(*path*, *offset*, *length*)\n\n## Groups API\nThe Groups API allows you to manage groups of users via the API. You must be a Databricks administrator to invoke this API.\n\n### Methods\n1. addMember(*parent_name*, *name*, *name_type*)\n2. createGroup(*group_name*)\n3. listGroupMembers(*group_name*, *return_type='json'*)\n4. listGroups()\n5. listParents(*name*, *name_type*)\n6. removeMember(*name*, *parent_name*, *name_type*)\n7. deleteGroup(*group_name*)\n\n#### listGroupMembers(*group_name*, *return_type='json'*)\nThe default `return_type` can be one of `json` or `list`, by default the paramter is set to `json`. This is provide to simplify pulling usernames from the default return time which can be cumbersome.\n\n#### listParents(*name*, *name_type*)\nThe `name_type` parameter must be one of `user` or `group`.\n\n#### removeMember(*name*, *parent_name*, *name_type*)\nThe `name_type` parameter must be one of `user` or `group`.\n\n## Instance Profiles API\nThe Instance Profiles API allows admins to add, list, and remove instance profiles that users can launch clusters with. Regular users can list the instance profiles available to them.\n\n### Methods\n1. addProfile(*profile_arn*, *skip_validation=None*)\n2. listProfiles()\n3. removeProfile()\n\n## Libraries API\nThe Libraries API allows you to install and uninstall libraries and get the status of libraries on a cluster via the API.\n\n### Methods\n1. allClusterStatuses(*status*)\n2. clusterStatus(*cluster_id*)\n3. installLibrary(*cluster_id*, *libraries*)\n4. uninstallLibrary(*cluster_id*, *libraries*)\n\n## Worspace API\n1. deleteWorkspace(*path*, *recursive*)\n2. exportWorkspace(*path*, *export_format*, *direct_download*)\n3. getWorkspaceStatus(*path*)\n4. importWorkspace(*path*, *export_format*, *language*, *content*, *overwrite*)\n5. listWorkspace(*path*)\n6. mkdirsWorkspace(*path*)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/angel-hernandez91/databricks_api", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "databricksapi", "package_url": "https://pypi.org/project/databricksapi/", "platform": "", "project_url": "https://pypi.org/project/databricksapi/", "project_urls": {"Homepage": "https://github.com/angel-hernandez91/databricks_api"}, "release_url": "https://pypi.org/project/databricksapi/1.1.8/", "requires_dist": null, "requires_python": "", "summary": "Python Databricks API wrapper using requests module", "version": "1.1.8", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Databricks API Documentation</h1>\n<p><em>This package is a Python Implementation of the <a href=\"https://docs.databricks.com/api/latest/index.html\" rel=\"nofollow\">Databricks API</a> for structured and programmatic use. This Python implementation requires that your Databricks API Token be saved as an environment variable in your system:</em> <code>export DATABRICKS_TOKEN=MY_DATABRICKS_TOKEN</code> in OSX / Linux. Or in Windows by searching for System Environment Variables in the Start Menu and adding it in the editor. For details see this <a href=\"https://superuser.com/questions/949560/how-do-i-set-system-environment-variables-in-windows-10\" rel=\"nofollow\">guide</a>.</p>\n<h2>UPDATE</h2>\n<p><em>Tokens can now be passed when a particular class is instantiated. E.g.:</em></p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">databricksapi</span> <span class=\"kn\">import</span> <span class=\"n\">Workspace</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://my-databricks-instance.com'</span>\n<span class=\"n\">token</span> <span class=\"o\">=</span> <span class=\"s1\">'dapiXXXXXXXXXXXXX'</span>\n<span class=\"n\">ws</span> <span class=\"o\">=</span> <span class=\"n\">Workspace</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">token</span><span class=\"p\">)</span>\n</pre>\n<h2>Installation</h2>\n<p>You can either use <code>pip install databricksapi</code> to install it globally, or you can clone the repository. Please note that only compatability with Python 3.7+ is guaranteed.</p>\n<h2>APIs Included</h2>\n<ul>\n<li>Token</li>\n<li>Secrets</li>\n<li>Clusters</li>\n<li>SCIM (Experimental)</li>\n<li>Jobs</li>\n<li>DBFS</li>\n<li>Groups</li>\n<li>Instance Profiles</li>\n<li>Libraries</li>\n<li>Workspace</li>\n</ul>\n<h2>Imports</h2>\n<p>The modules above can be imported as follows</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">databricksapi</span> <span class=\"kn\">import</span> <span class=\"n\">Token</span><span class=\"p\">,</span> <span class=\"n\">Jobs</span><span class=\"p\">,</span> <span class=\"n\">DBFS</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n\n<span class=\"n\">token_instance</span> <span class=\"o\">=</span> <span class=\"n\">Token</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n<span class=\"n\">jobs_instance</span> <span class=\"o\">=</span> <span class=\"n\">Jobs</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n</pre>\n<h2>Token API</h2>\n<p>The Token API allows any user to create, list, and revoke tokens that can be used to authenticate and access Databricks REST APIs. Initial authentication to this API is the same as for all of the Databricks API endpoints.</p>\n<h3>Methods</h3>\n<ol>\n<li>createToken(<em>lifetime_seconds</em>, <em>comment</em>)</li>\n<li>listTokens()</li>\n<li>revokeToken(<em>token_id</em>)</li>\n</ol>\n<h4>createToken(<em>lifetime_seconds</em>, <em>comment</em>)</h4>\n<p>Create and return a token.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Token</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">createToken</span><span class=\"p\">(</span><span class=\"mi\">600</span><span class=\"p\">,</span> <span class=\"s1\">'Test token'</span><span class=\"p\">)</span>\n</pre>\n<h4>listTokens()</h4>\n<p>List all Token IDs in your Databricks Environment.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Token</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">listTokens</span><span class=\"p\">()</span>\n</pre>\n<h4>revokeToken(<em>token_id</em>)</h4>\n<p>Revoke an active Databricks token.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Token</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#token_id can be obtained from using the listTokens() method</span>\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">revokeToken</span><span class=\"p\">(</span><span class=\"s1\">'5715498424f15ee0213be729257b53fc35a47d5953e3bdfd8ed22a0b93b339f4'</span><span class=\"p\">)</span>\n</pre>\n<h2>Secrets API</h2>\n<p>The Secrets API allows you to manage secrets, secret scopes, and access permissions.</p>\n<h3>Methods</h3>\n<ol>\n<li>createSecretScope(<em>scope</em>, <em>initial_manage_principal</em>)</li>\n<li>deleteSecretScope(<em>scope</em>)</li>\n<li>listSecretScopes()</li>\n<li>putSeceret(<em>value</em>, <em>value_type</em>, <em>scope</em>, <em>key</em>)</li>\n<li>deleteSecret(<em>scope</em>, <em>key</em>)</li>\n<li>listSecrets(<em>scope</em>)</li>\n<li>putSecretACL(<em>scope</em>, <em>principal</em>, <em>permission</em>)</li>\n<li>deleteSecretACL(<em>scope</em>, <em>principal</em>)</li>\n<li>getSecretACL(*scope, <em>principal</em>)</li>\n<li>listSecretACL(<em>scope</em>, <em>principal</em>)</li>\n</ol>\n<h4>createSecretScope(<em>scope</em>, <em>initial_manage_princial</em>)</h4>\n<p>Creates a new secret scope.</p>\n<p>The scope name must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters. The maximum number of scopes in a workspace is 100.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"s1\">'SomeSecretScope'</span>\n<span class=\"n\">initial_manage_princial</span> <span class=\"o\">=</span> <span class=\"s1\">'users'</span>\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">createSecretScope</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">initial_manage_princial</span><span class=\"p\">)</span>\n</pre>\n<h4>deleteSecretScope(<em>scope</em>)</h4>\n<p>Delete a secret scope.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"s1\">'SomeSecretScope'</span>\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">deleteSecretScope</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"p\">)</span>\n</pre>\n<h4>listSecretScopes()</h4>\n<p>List all secret scopes in the workspace</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">listSecretScopes</span><span class=\"p\">()</span>\n</pre>\n<h4>putSecret(<em>value</em>, <em>value_type</em>, <em>scope</em>, <em>key</em>)</h4>\n<p>Inserts a secret under the provided scope with the given name. If a secret already exists with the same name, this command overwrites the existing secret\u2019s value. The server encrypts the secret using the secret scope\u2019s encryption settings before storing it. You must have WRITE or MANAGE permission on the secret scope.</p>\n<p>The <code>value_type</code> parameter can either be set to <code>string</code> or <code>bytes</code> depending on the type fo value the user passes in.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#set parameters</span>\n<span class=\"n\">value</span> <span class=\"o\">=</span> <span class=\"s1\">'BeepBoop'</span>\n<span class=\"n\">value_type</span> <span class=\"o\">=</span> <span class=\"s1\">'string'</span>\n<span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"s1\">'SomeSecretScope'</span>\n<span class=\"n\">key</span> <span class=\"o\">=</span> <span class=\"s1\">'uniqueScopekey'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">putSecret</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">value_type</span><span class=\"p\">,</span> <span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">)</span>\n</pre>\n<h4>deleteSecret(<em>scope</em>, <em>key</em>)</h4>\n<p>Deletes the secret stored in this secret scope. You must have WRITE or MANAGE permission on the secret scope.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"s1\">'SomeSecretScope'</span>\n<span class=\"n\">key</span> <span class=\"o\">=</span> <span class=\"s1\">'uniqueScopekey'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">deleteSecret</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">)</span>\n</pre>\n<h4>listSecrets(<em>scope</em>)</h4>\n<p>Lists the secret keys that are stored at this scope. This is a metadata-only operation; secret data cannot be retrieved using this API. Users need READ permission to make this call.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"s1\">'SomeSecretScope'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">listSecrets</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"p\">)</span>\n</pre>\n<h4>putSecretACL(<em>scope</em>, <em>principal</em>, <em>permission</em>)</h4>\n<p>Creates or overwrites the ACL associated with the given principal (user or group) on the specified scope point. In general, a user or group will use the most powerful permission available to them</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"s1\">'SomeSecretScope'</span>\n<span class=\"n\">prinicpal</span> <span class=\"o\">=</span> <span class=\"s1\">'users'</span>\n<span class=\"n\">permission</span> <span class=\"o\">=</span> <span class=\"s1\">'READ'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">putSecretACL</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">principal</span><span class=\"p\">,</span> <span class=\"n\">permission</span><span class=\"p\">)</span>\n</pre>\n<h4>deleteSecretACL(<em>scope</em>, <em>principal</em>)</h4>\n<p>Deletes the given ACL on the given scope.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"s1\">'SomeSecretScope'</span>\n<span class=\"n\">prinicpal</span> <span class=\"o\">=</span> <span class=\"s1\">'users'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">deleteSecretACL</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">principal</span><span class=\"p\">)</span>\n</pre>\n<h4>getSecretACL(*scope, <em>principal</em>)</h4>\n<p>Describes the details about the given ACL, such as the group and permission.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"s1\">'SomeSecretScope'</span>\n<span class=\"n\">prinicpal</span> <span class=\"o\">=</span> <span class=\"s1\">'users'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">getSecretACL</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">principal</span><span class=\"p\">)</span>\n</pre>\n<h4>listSecretACL(<em>scope</em>, <em>principal</em>)</h4>\n<p>Lists the ACLs set on the given scope.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Secrets</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">scope</span> <span class=\"o\">=</span> <span class=\"s1\">'SomeSecretScope'</span>\n<span class=\"n\">prinicpal</span> <span class=\"o\">=</span> <span class=\"s1\">'users'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">listSecretACL</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">principal</span><span class=\"p\">)</span>\n</pre>\n<h2>Clusters API</h2>\n<p>The Clusters API allows you to create, start, edit, list, terminate, and delete clusters via the API. The maximum allowed size of a request to the Clusters API is 10MB.</p>\n<h3>Methods</h3>\n<ol>\n<li>createCluster(<em>worker</em>, <em>worker_type</em>, <em>cluster_name</em>, <em>spark_version</em>, <em>cluster_log_conf</em>, <em>node_type_id</em>, <em>driver_node_type_id=None</em>, <em>spark_conf=None</em>, <em>aws_attributes=None</em>, <em>ssh_public_keys=None</em>, <em>custom_tags=None</em>, <em>init_scripts=None</em>, <em>spark_env_vars=None</em>, <em>autotermination_minutes=None</em>, <em>enable_elastic_disk=None</em>)</li>\n<li>editCluster(<em>worker</em>, <em>worker_type</em>, <em>cluster_name</em>, <em>spark_version</em>, <em>cluster_log_conf</em>, <em>node_type_id</em>, <em>driver_node_type_id=None</em>, <em>spark_conf=None</em>, <em>aws_attributes=None</em>, <em>ssh_public_keys=None</em>, <em>custom_tags=None</em>, <em>init_scripts=None</em>, <em>spark_env_vars=None</em>, <em>autotermination_minutes=None</em>, <em>enable_elastic_disk=None</em>)</li>\n<li>startCluster(<em>cluster_id</em>)</li>\n<li>restartCluster(<em>cluster_id</em>)</li>\n<li>resizeCluster(<em>cluster_id</em>, <em>worker</em>, <em>worker_type</em>)</li>\n<li>terminateCluster(<em>cluster_id</em>)</li>\n<li>deleteCluster(<em>cluster_id</em>)</li>\n<li>getCluster(<em>cluster_id</em>)</li>\n<li>pinCluster(<em>cluster_id</em>)</li>\n<li>unpinCluster(<em>cluster_id</em>)</li>\n<li>listClusters()</li>\n<li>listNodeTypes()</li>\n<li>listZones()</li>\n<li>getSparkVersions()</li>\n<li>getClusterEvents(<em>cluster_id</em>, <em>order='DESC'</em>, <em>start_time=None</em>, <em>end_time=None</em>, <em>event_types=None</em>, <em>offset=None</em>, <em>limit=None</em>)</li>\n</ol>\n<h4>createCluster(<em>worker</em>, <em>worker_type</em>, <em>cluster_name</em>, <em>spark_version</em>, <em>cluster_log_conf</em>, <em>node_type_id</em>, <em>driver_node_type_id=None</em>, <em>spark_conf=None</em>, <em>aws_attributes=None</em>, <em>ssh_public_keys=None</em>, <em>custom_tags=None</em>, <em>init_scripts=None</em>, <em>spark_env_vars=None</em>, <em>autotermination_minutes=None</em>, <em>enable_elastic_disk=None</em>)</h4>\n<p>Creates a new Spark cluster. This method acquires new instances from the cloud provider if necessary. This method is asynchronous; the returned cluster_id can be used to poll the cluster state. When this method returns, the cluster is in a PENDING state. The cluster is usable once it enters a RUNNING state.</p>\n<p>The <code>worker_type</code> can be either <code>workers</code> or <code>autoscale</code>. If a <code>autoscale</code> is set, then the <code>min_workers</code> and <code>max_workers</code> must be specified.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">worker</span> <span class=\"o\">=</span> <span class=\"mi\">25</span>\n<span class=\"n\">worker_type</span> <span class=\"o\">=</span> <span class=\"s1\">'workers'</span>\n<span class=\"n\">cluster_name</span> <span class=\"o\">=</span> <span class=\"s1\">'TestCluster'</span>\n<span class=\"n\">spark_version</span> <span class=\"o\">=</span> <span class=\"s1\">'4.0.x-scala2.11'</span>\n<span class=\"n\">cluster_log_conf</span> <span class=\"o\">=</span> <span class=\"s1\">'/dbfs/log/path'</span>\n<span class=\"n\">node_type_id</span> <span class=\"o\">=</span> <span class=\"s1\">'i3.xlarge'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">createCluster</span><span class=\"p\">(</span><span class=\"n\">worker</span><span class=\"o\">=</span><span class=\"n\">worker</span><span class=\"p\">,</span> <span class=\"n\">worker_type</span><span class=\"o\">=</span><span class=\"n\">worker_type</span><span class=\"p\">,</span> <span class=\"n\">cluster_name</span><span class=\"o\">=</span><span class=\"n\">cluster_name</span><span class=\"p\">,</span> <span class=\"n\">spark_version</span><span class=\"o\">=</span><span class=\"n\">spark_version</span><span class=\"p\">,</span> <span class=\"n\">cluster_log_conf</span><span class=\"o\">=</span><span class=\"n\">cluster_log_conf</span><span class=\"p\">,</span> <span class=\"n\">node_type_id</span><span class=\"o\">=</span><span class=\"n\">node_type_id</span><span class=\"p\">)</span>\n</pre>\n<h4>editCluster(<em>worker</em>, <em>worker_type</em>, <em>cluster_name</em>, <em>spark_version</em>, <em>cluster_log_conf</em>, <em>node_type_id</em>, <em>driver_node_type_id=None</em>, <em>spark_conf=None</em>, <em>aws_attributes=None</em>, <em>ssh_public_keys=None</em>, <em>custom_tags=None</em>, <em>init_scripts=None</em>, <em>spark_env_vars=None</em>, <em>autotermination_minutes=None</em>, <em>enable_elastic_disk=None</em>)</h4>\n<p>Edit an existings clusters configuration.</p>\n<p>The <code>worker_type</code> can be either <code>workers</code> or <code>autoscale</code>. If a <code>autoscale</code> is set, then the <code>min_workers</code> and <code>max_workers</code> must be specified.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">worker</span> <span class=\"o\">=</span> <span class=\"mi\">35</span>\n<span class=\"n\">worker_type</span> <span class=\"o\">=</span> <span class=\"s1\">'workers'</span>\n<span class=\"n\">cluster_name</span> <span class=\"o\">=</span> <span class=\"s1\">'TestCluster'</span>\n<span class=\"n\">spark_version</span> <span class=\"o\">=</span> <span class=\"s1\">'4.0.x-scala2.11'</span>\n<span class=\"n\">cluster_log_conf</span> <span class=\"o\">=</span> <span class=\"s1\">'/dbfs/new/log/path'</span>\n<span class=\"n\">node_type_id</span> <span class=\"o\">=</span> <span class=\"s1\">'i5.xlarge'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">editCluster</span><span class=\"p\">(</span><span class=\"n\">worker</span><span class=\"o\">=</span><span class=\"n\">worker</span><span class=\"p\">,</span> <span class=\"n\">worker_type</span><span class=\"o\">=</span><span class=\"n\">worker_type</span><span class=\"p\">,</span> <span class=\"n\">cluster_name</span><span class=\"o\">=</span><span class=\"n\">cluster_name</span><span class=\"p\">,</span> <span class=\"n\">spark_version</span><span class=\"o\">=</span><span class=\"n\">spark_version</span><span class=\"p\">,</span> <span class=\"n\">cluster_log_conf</span><span class=\"o\">=</span><span class=\"n\">cluster_log_conf</span><span class=\"p\">,</span> <span class=\"n\">node_type_id</span><span class=\"o\">=</span><span class=\"n\">node_type_id</span><span class=\"p\">)</span>\n</pre>\n<h4>startCluster(<em>cluster_id</em>)\u00a0</h4>\n<p>Starts a terminated Spark cluster given its ID.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster_id</span> <span class=\"o\">=</span> <span class=\"s1\">'1202-211320-brick1'</span>\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">startCluster</span><span class=\"p\">(</span><span class=\"n\">cluster_id</span><span class=\"p\">)</span>\n</pre>\n<h4>restartCluster(<em>cluster_id</em>)</h4>\n<p>Restarts a Spark cluster given its id. If the cluster is not in a RUNNING state, nothing will happen.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster_id</span> <span class=\"o\">=</span> <span class=\"s1\">'1202-211320-brick1'</span>\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">restartCluster</span><span class=\"p\">(</span><span class=\"n\">cluster_id</span><span class=\"p\">)</span>\n</pre>\n<h4>resizeCluster(<em>cluster_id</em>, <em>worker</em>, <em>worker_type</em>)</h4>\n<p>Resizes a cluster to have a desired number of workers. This will fail unless the cluster is in a RUNNING state.</p>\n<p>The parameter <code>worker_type</code> can be one of <code>workers</code> or <code>autoscale</code>.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster_id</span> <span class=\"o\">=</span> <span class=\"s1\">'1202-211320-brick1'</span>\n<span class=\"n\">workers</span> <span class=\"o\">=</span> <span class=\"mi\">30</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">resizeCluster</span><span class=\"p\">(</span><span class=\"n\">cluster_id</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"p\">,</span> <span class=\"n\">worker_type</span><span class=\"o\">=</span><span class=\"s1\">'workers'</span><span class=\"p\">)</span>\n</pre>\n<h4>terminateCluster(<em>cluster_id</em>)</h4>\n<p>Terminates a Spark cluster given its id. The cluster is removed asynchronously. Once the termination has completed, the cluster will be in a TERMINATED state. If the cluster is already in a TERMINATING or TERMINATED state, nothing will happen.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster_id</span> <span class=\"o\">=</span> <span class=\"s1\">'1202-211320-brick1'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">terminateCluster</span><span class=\"p\">(</span><span class=\"n\">cluster_id</span><span class=\"p\">)</span>\n</pre>\n<h4>deleteCluster(<em>cluster_id</em>)</h4>\n<p>Permanently deletes a Spark cluster. If the cluster is running, it is terminated and its resources are asynchronously removed. If the cluster is terminated, then it is immediately removed.</p>\n<p>You cannot perform any action on a permanently deleted cluster and a permanently deleted cluster is no longer returned in the cluster list.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster_id</span> <span class=\"o\">=</span> <span class=\"s1\">'1202-211320-brick1'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">deleteCluster</span><span class=\"p\">(</span><span class=\"n\">cluster_id</span><span class=\"p\">)</span>\n</pre>\n<h4>getCluster(<em>cluster_id</em>)</h4>\n<p>Returns information about all pinned clusters, currently active clusters, up to 70 of the most recently terminated interactive clusters in the past 30 days, and up to 30 of the most recently terminated job clusters in the past 30 days.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster_id</span> <span class=\"o\">=</span> <span class=\"s1\">'1202-211320-brick1'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">getCluster</span><span class=\"p\">(</span><span class=\"n\">cluster_id</span><span class=\"p\">)</span>\n</pre>\n<h4>pinCluster(<em>cluster_id</em>)</h4>\n<p>Pinning a cluster ensures that the cluster is always returned by the List API. Pinning a cluster that is already pinned has no effect.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster_id</span> <span class=\"o\">=</span> <span class=\"s1\">'1202-211320-brick1'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">pinCluster</span><span class=\"p\">(</span><span class=\"n\">cluster_id</span><span class=\"p\">)</span>\n</pre>\n<h4>unpinCluster(<em>cluster_id</em>)</h4>\n<p>Unpinning a cluster will allow the cluster to eventually be removed from the list returned by the List API. Unpinning a cluster that is not pinned has no effect.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster_id</span> <span class=\"o\">=</span> <span class=\"s1\">'1202-211320-brick1'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">unpinCluster</span><span class=\"p\">(</span><span class=\"n\">cluster_id</span><span class=\"p\">)</span>\n</pre>\n<h4>listClusters()</h4>\n<p>Retrieves the information for a cluster given its identifier. Clusters can be described while they are running, or up to 30 days after they are terminated.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">listClusters</span><span class=\"p\">()</span>\n</pre>\n<h4>listNodeTypes()</h4>\n<p>Returns a list of supported Spark node types. These node types can be used to launch a cluster.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">listNodeTypes</span><span class=\"p\">()</span>\n</pre>\n<h4>listZones()</h4>\n<p>Returns a list of availability zones where clusters can be created in (ex: us-west-2a). These zones can be used to launch a cluster.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">listZones</span><span class=\"p\">()</span>\n</pre>\n<h4>getSparkVersions()</h4>\n<p>Returns the list of available Spark versions. These versions can be used to launch a cluster.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">getSparkVersions</span><span class=\"p\">()</span>\n</pre>\n<h4>getClusterEvents(<em>cluster_id</em>, <em>order='DESC'</em>, <em>start_time=None</em>, <em>end_time=None</em>, <em>event_types=None</em>, <em>offset=None</em>, <em>limit=None</em>)</h4>\n<p>Retrieves a list of events about the activity of a cluster. This API is paginated. If there are more events to read, the response includes all the parameters necessary to request the next page of events.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'https://url.for.databricks.net'</span>\n<span class=\"n\">db_api</span> <span class=\"o\">=</span> <span class=\"n\">Clusters</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster_id</span> <span class=\"o\">=</span> <span class=\"s1\">'1202-211320-brick1'</span>\n\n<span class=\"n\">db_api</span><span class=\"o\">.</span><span class=\"n\">getClusterEvents</span><span class=\"p\">(</span><span class=\"n\">cluster_id</span><span class=\"p\">)</span>\n</pre>\n<h2>Jobs API</h2>\n<p>The Jobs API allows you to create, edit, and delete jobs via the API. The maximum allowed size of a request to the Jobs API is 10MB.</p>\n<h3>Methods</h3>\n<ol>\n<li>createJob(<em>cluster</em>, <em>cluster_type</em>, <em>task</em>, <em>task_type</em>, <em>name</em>, <em>libraries=None</em>, <em>email_notications=None</em>, <em>timeout_seconds=None</em>, <em>max_retries=None</em>, <em>min_retry_interval_millis=None</em>, <em>retry_on_timeout=None</em>,<em>schedule=None</em>, <em>max_concurrent_runs=None</em>)</li>\n<li>listJobs()</li>\n<li>deleteJob(<em>job_id</em>)</li>\n<li>batchDelete(<em>*args</em>)</li>\n<li>getJob(<em>job_id</em>)</li>\n<li>resetJob(<em>job_id</em>, <em>new_settings</em>)</li>\n<li>runJob(<em>job_id</em>, <em>job_type</em>, <em>params</em>)</li>\n<li>runsSubmit(<em>run_name</em>, <em>cluster</em>, <em>task</em>, <em>cluster_type</em>, <em>task_type</em>, <em>libraries=None</em>, <em>timeout_seconds=None</em>)</li>\n<li>runsList(<em>run</em>, <em>run_type</em>, <em>job_id</em>, <em>offset</em>, <em>limit</em>)</li>\n<li>runsGet(<em>run_id</em>)</li>\n<li>runsExport(<em>run_id</em>, <em>views_to_export</em>)</li>\n<li>runsCancel(<em>run_id</em>)</li>\n<li>runsGetOutput(<em>run_id</em>)</li>\n<li>runsDelete(<em>run_id</em>)\u00a0</li>\n</ol>\n<h4>createJob(<em>cluster</em>, <em>cluster_type</em>, <em>task</em>, <em>task_type</em>, <em>name</em>, <em>libraries=None</em>, <em>email_notications=None</em>, <em>timeout_seconds=None</em>, <em>max_retries=None</em>, <em>min_retry_interval_millis=None</em>, <em>retry_on_timeout=None</em>,<em>schedule=None</em>, <em>max_concurrent_runs=None</em>)</h4>\n<p>The <code>cluster_type</code> parameter can be one of <code>existing</code> or <code>new</code>.\nThe <code>task_type</code> parameter must be one of <code>notebook</code>, <code>jar</code>, <code>submit</code>, or <code>python</code>.</p>\n<p>All other parameters are documented in the Databricks Rest API.</p>\n<h4>batchDelete(<em>*args</em>)</h4>\n<p>Takes in a comma separated list of Job IDs to be deleted. This method is a wrapper around the <code>deleteJob</code> method.</p>\n<h4>runJob(<em>job_id</em>, <em>job_type</em>, <em>params</em>)</h4>\n<p>The <code>job_type</code> parameter must be one of <code>notebook</code>, <code>jar</code>, <code>submit</code> or <code>python</code>.</p>\n<h4>runsSubmit(<em>run_name</em>, <em>cluster</em>, <em>task</em>, <em>cluster_type</em>, <em>task_type</em>, <em>libraries=None</em>, <em>timeout_seconds=None</em>)</h4>\n<p>The <code>cluster_type</code> parameter can be one of <code>existing</code> or <code>new</code>.</p>\n<h4>runsList(<em>run</em>, <em>run_type</em>, <em>job_id</em>, <em>offset</em>, <em>limit</em>)</h4>\n<p>The <code>run_type</code> parameter must be one of <code>completed</code> or <code>active</code>.</p>\n<h2>DBFS API</h2>\n<p>The DBFS API is a Databricks API that makes it simple to interact with various data sources without having to include your credentials every time you read a file.</p>\n<h3>Methods</h3>\n<ol>\n<li>addBlock(<em>data</em>, <em>handle</em>)</li>\n<li>closeStream(<em>handle</em>)</li>\n<li>createFile(<em>path</em>, <em>overwrite</em>)</li>\n<li>deleteFile(<em>path</em>, <em>recursive</em>)</li>\n<li>getStatus(<em>path</em>)</li>\n<li>listFiles(<em>path</em>)</li>\n<li>makeDirs(<em>path</em>)</li>\n<li>moveFiles(<em>source_path</em>, <em>target_path</em>)</li>\n<li>putFiles(<em>path</em>, <em>overwrite</em>, <em>files</em>, <em>contents=None</em>)</li>\n<li>readFiles(<em>path</em>, <em>offset</em>, <em>length</em>)</li>\n</ol>\n<h2>Groups API</h2>\n<p>The Groups API allows you to manage groups of users via the API. You must be a Databricks administrator to invoke this API.</p>\n<h3>Methods</h3>\n<ol>\n<li>addMember(<em>parent_name</em>, <em>name</em>, <em>name_type</em>)</li>\n<li>createGroup(<em>group_name</em>)</li>\n<li>listGroupMembers(<em>group_name</em>, <em>return_type='json'</em>)</li>\n<li>listGroups()</li>\n<li>listParents(<em>name</em>, <em>name_type</em>)</li>\n<li>removeMember(<em>name</em>, <em>parent_name</em>, <em>name_type</em>)</li>\n<li>deleteGroup(<em>group_name</em>)</li>\n</ol>\n<h4>listGroupMembers(<em>group_name</em>, <em>return_type='json'</em>)</h4>\n<p>The default <code>return_type</code> can be one of <code>json</code> or <code>list</code>, by default the paramter is set to <code>json</code>. This is provide to simplify pulling usernames from the default return time which can be cumbersome.</p>\n<h4>listParents(<em>name</em>, <em>name_type</em>)</h4>\n<p>The <code>name_type</code> parameter must be one of <code>user</code> or <code>group</code>.</p>\n<h4>removeMember(<em>name</em>, <em>parent_name</em>, <em>name_type</em>)</h4>\n<p>The <code>name_type</code> parameter must be one of <code>user</code> or <code>group</code>.</p>\n<h2>Instance Profiles API</h2>\n<p>The Instance Profiles API allows admins to add, list, and remove instance profiles that users can launch clusters with. Regular users can list the instance profiles available to them.</p>\n<h3>Methods</h3>\n<ol>\n<li>addProfile(<em>profile_arn</em>, <em>skip_validation=None</em>)</li>\n<li>listProfiles()</li>\n<li>removeProfile()</li>\n</ol>\n<h2>Libraries API</h2>\n<p>The Libraries API allows you to install and uninstall libraries and get the status of libraries on a cluster via the API.</p>\n<h3>Methods</h3>\n<ol>\n<li>allClusterStatuses(<em>status</em>)</li>\n<li>clusterStatus(<em>cluster_id</em>)</li>\n<li>installLibrary(<em>cluster_id</em>, <em>libraries</em>)</li>\n<li>uninstallLibrary(<em>cluster_id</em>, <em>libraries</em>)</li>\n</ol>\n<h2>Worspace API</h2>\n<ol>\n<li>deleteWorkspace(<em>path</em>, <em>recursive</em>)</li>\n<li>exportWorkspace(<em>path</em>, <em>export_format</em>, <em>direct_download</em>)</li>\n<li>getWorkspaceStatus(<em>path</em>)</li>\n<li>importWorkspace(<em>path</em>, <em>export_format</em>, <em>language</em>, <em>content</em>, <em>overwrite</em>)</li>\n<li>listWorkspace(<em>path</em>)</li>\n<li>mkdirsWorkspace(<em>path</em>)</li>\n</ol>\n\n          </div>"}, "last_serial": 6945342, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "276d0a46ed524dbdc85f0117e65fdfa6", "sha256": "47d6fded8bc8a33c2b5cd36ff65fdf0565fcf68ca1aa0c192bfcff14b5f57641"}, "downloads": -1, "filename": "databricksapi-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "276d0a46ed524dbdc85f0117e65fdfa6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2281, "upload_time": "2018-11-06T07:09:54", "upload_time_iso_8601": "2018-11-06T07:09:54.208445Z", "url": "https://files.pythonhosted.org/packages/43/fc/9e99729f5356cdcd4971e58b8310a89ca0fc2fe2e04736059dfdcfd36e3c/databricksapi-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "27a64059bfd2d7c7a467d04fedfda2a8", "sha256": "36969062a7499081699d1dd9a7aa24ae9e8fe513d55616dc6d0388553c0c706f"}, "downloads": -1, "filename": "databricksapi-0.1.tar.gz", "has_sig": false, "md5_digest": "27a64059bfd2d7c7a467d04fedfda2a8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1141, "upload_time": "2018-11-06T07:09:57", "upload_time_iso_8601": "2018-11-06T07:09:57.212645Z", "url": "https://files.pythonhosted.org/packages/90/d5/413779935701e9d641ab70160dd3cd43d29d73da1842d742120c450d3194/databricksapi-0.1.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "36d2f5babd584bc59738072bf3f6efb4", "sha256": "ec266ebf0e4d4f216fb6c6c166a641d67428f5c296ad32f465564bb89b17421a"}, "downloads": -1, "filename": "databricksapi-1.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "36d2f5babd584bc59738072bf3f6efb4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10658, "upload_time": "2018-11-07T05:32:19", "upload_time_iso_8601": "2018-11-07T05:32:19.695121Z", "url": "https://files.pythonhosted.org/packages/4c/7a/3818d34727c56cbe00150d19a83f88a843a656da128c9df8fba326d37496/databricksapi-1.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e62bf005e32698cdb98e9ce9c69a2fe4", "sha256": "677c90cbec5ae1f63c958972dcac4aaf8a8bd3cd725fbf48ab413e50808ddb26"}, "downloads": -1, "filename": "databricksapi-1.0.4.tar.gz", "has_sig": false, "md5_digest": "e62bf005e32698cdb98e9ce9c69a2fe4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6369, "upload_time": "2018-11-07T05:32:20", "upload_time_iso_8601": "2018-11-07T05:32:20.973590Z", "url": "https://files.pythonhosted.org/packages/05/d0/4a2f844c947a8a7f0184f31e53eb128ee5bab42a559dedb40c2e9c227a15/databricksapi-1.0.4.tar.gz", "yanked": false}], "1.0.5": [{"comment_text": "", "digests": {"md5": "916622d5a9423a8323bb54d84e0f0b4d", "sha256": "89e2207ebdabcd2d3b68939dcc88d2958c6dee4a6f11b97492380c7ed68543b7"}, "downloads": -1, "filename": "databricksapi-1.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "916622d5a9423a8323bb54d84e0f0b4d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 11595, "upload_time": "2018-11-18T03:27:41", "upload_time_iso_8601": "2018-11-18T03:27:41.297227Z", "url": "https://files.pythonhosted.org/packages/64/14/396fc7ad48b3dbd4f21602d044ec1b5b228749da6269575aa0df18a4cfea/databricksapi-1.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a64ebca7e11875457a3a3e6c1cdac7c2", "sha256": "d1f0833f57427ebcb2c74e24080e5760bf1ac9f14e81430cb9e410edb11b25ef"}, "downloads": -1, "filename": "databricksapi-1.0.5.tar.gz", "has_sig": false, "md5_digest": "a64ebca7e11875457a3a3e6c1cdac7c2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8495, "upload_time": "2018-11-18T03:27:42", "upload_time_iso_8601": "2018-11-18T03:27:42.703465Z", "url": "https://files.pythonhosted.org/packages/e2/dc/0660d040b73644e354d16520a879563324a810e3eb4ed8e6ddee87b06c76/databricksapi-1.0.5.tar.gz", "yanked": false}], "1.0.6": [{"comment_text": "", "digests": {"md5": "e9907ba7759b77ba9cb62a95e4bbf354", "sha256": "7ff016a3a2b78df1de747ffa5efe28b1570acd17721951fbf50524f05b5e2abf"}, "downloads": -1, "filename": "databricksapi-1.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "e9907ba7759b77ba9cb62a95e4bbf354", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 13780, "upload_time": "2018-12-15T06:02:28", "upload_time_iso_8601": "2018-12-15T06:02:28.569712Z", "url": "https://files.pythonhosted.org/packages/a3/9e/c1311c1741459391fa56af9a8acaeb0492b7ff75119ec4cb8823b97e62df/databricksapi-1.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "15685e9e129fa2d636b1c4b9cebca588", "sha256": "6c5782f78cf58de0cd067e2bb3ae5031a2f0721f299712f34394ad97550adeeb"}, "downloads": -1, "filename": "databricksapi-1.0.6.tar.gz", "has_sig": false, "md5_digest": "15685e9e129fa2d636b1c4b9cebca588", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13224, "upload_time": "2018-12-15T06:02:30", "upload_time_iso_8601": "2018-12-15T06:02:30.008005Z", "url": "https://files.pythonhosted.org/packages/bf/0c/389b7f091ca92ea687f09fc6f0a260ee950a3ef4237a3b9f3397457e8cc7/databricksapi-1.0.6.tar.gz", "yanked": false}], "1.0.9": [{"comment_text": "", "digests": {"md5": "4e9012ce2dede4a39b30b5daaf6588e7", "sha256": "ca4c7b0c52107a0a5988f9543dbeb11d607c00583397357c790b1e6cb9d7e937"}, "downloads": -1, "filename": "databricksapi-1.0.9-py3-none-any.whl", "has_sig": false, "md5_digest": "4e9012ce2dede4a39b30b5daaf6588e7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 15133, "upload_time": "2019-02-16T22:14:33", "upload_time_iso_8601": "2019-02-16T22:14:33.422880Z", "url": "https://files.pythonhosted.org/packages/d3/45/230cccab0fb5abeba2d0474c4525cf0314c7bf21f82dd36823d03071443a/databricksapi-1.0.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a458252740e5205c7ab2ce12c2b3d0db", "sha256": "c6a388679e8718d84aafdfb8db779f03be3fe42685ce4ad38a571889ae4a95f2"}, "downloads": -1, "filename": "databricksapi-1.0.9.tar.gz", "has_sig": false, "md5_digest": "a458252740e5205c7ab2ce12c2b3d0db", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16415, "upload_time": "2019-02-16T22:14:35", "upload_time_iso_8601": "2019-02-16T22:14:35.253537Z", "url": "https://files.pythonhosted.org/packages/e1/b5/9d49f2d1b0061b76e028be1e276ab247db96b134d12a6c65730b2ef1afaf/databricksapi-1.0.9.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "80e6905eae0c39578b99adfe5ebb1e7e", "sha256": "dda0f693f220d59069643792850b8b6ed7073237a8f7e3ca92cd936b21e7b1e7"}, "downloads": -1, "filename": "databricksapi-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "80e6905eae0c39578b99adfe5ebb1e7e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 15141, "upload_time": "2019-02-19T06:47:19", "upload_time_iso_8601": "2019-02-19T06:47:19.514580Z", "url": "https://files.pythonhosted.org/packages/41/ab/d9a88926b898d177b2ce545c52fd99e556cfc55e5168e7c379e0928082f2/databricksapi-1.1.0-py3-none-any.whl", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "a4c7dacdef326805af49a6584cd6f324", "sha256": "8151748fd196ad3d1154d0e14b288414f24348bde2d25ab31cd7b0a8739b8ac1"}, "downloads": -1, "filename": "databricksapi-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "a4c7dacdef326805af49a6584cd6f324", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 15134, "upload_time": "2019-02-19T06:49:58", "upload_time_iso_8601": "2019-02-19T06:49:58.435488Z", "url": "https://files.pythonhosted.org/packages/10/c5/3c28bec2c4151e9892c9e9b9175a58ef3256fa353a82b81fb207378cac93/databricksapi-1.1.1-py3-none-any.whl", "yanked": false}], "1.1.2": [{"comment_text": "", "digests": {"md5": "dafa32599c1e8c4e6ec4b6794d96da89", "sha256": "12773cf020970f34f31cb73933e582d89e99e1b941e230d5f63993fb5d6df823"}, "downloads": -1, "filename": "databricksapi-1.1.2.tar.gz", "has_sig": false, "md5_digest": "dafa32599c1e8c4e6ec4b6794d96da89", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16541, "upload_time": "2019-09-24T18:27:07", "upload_time_iso_8601": "2019-09-24T18:27:07.947739Z", "url": "https://files.pythonhosted.org/packages/7d/87/1aaaca284b43f6513e4779cda918cf5baee6dd49c0a55d6a4bc1c9175aed/databricksapi-1.1.2.tar.gz", "yanked": false}], "1.1.3": [{"comment_text": "", "digests": {"md5": "1321e2233f670a43d39c15a469d0b3f2", "sha256": "d9bdc0807c32f2360922acae13237220243bba67649366df868661324dedb835"}, "downloads": -1, "filename": "databricksapi-1.1.3.tar.gz", "has_sig": false, "md5_digest": "1321e2233f670a43d39c15a469d0b3f2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16740, "upload_time": "2019-10-17T18:19:28", "upload_time_iso_8601": "2019-10-17T18:19:28.567429Z", "url": "https://files.pythonhosted.org/packages/3a/bc/19ed7e9b85aa84f8d481cd0e6695f992339a4e40cbd8a9495d419e974833/databricksapi-1.1.3.tar.gz", "yanked": false}], "1.1.4": [{"comment_text": "", "digests": {"md5": "4c7bea44a4835d6cb17e291e720a006a", "sha256": "c7a7ee3a2e1e38438a1142504b5977a88454da82ff12547a7cf60ddaa158884b"}, "downloads": -1, "filename": "databricksapi-1.1.4.tar.gz", "has_sig": false, "md5_digest": "4c7bea44a4835d6cb17e291e720a006a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16738, "upload_time": "2019-11-11T19:41:17", "upload_time_iso_8601": "2019-11-11T19:41:17.245759Z", "url": "https://files.pythonhosted.org/packages/5c/0b/e9c8c33768e3dfb949f0053717730ff4a2711def8c4146ca9c0b4f6cebb9/databricksapi-1.1.4.tar.gz", "yanked": false}], "1.1.5": [{"comment_text": "", "digests": {"md5": "ec8bffcab67f1170ac52e609e4d2c7a6", "sha256": "b7924e7aabe42b6651942fe589e312587db6857ca12e252d76a7fcc7fca308b3"}, "downloads": -1, "filename": "databricksapi-1.1.5.tar.gz", "has_sig": false, "md5_digest": "ec8bffcab67f1170ac52e609e4d2c7a6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16784, "upload_time": "2019-11-19T21:42:21", "upload_time_iso_8601": "2019-11-19T21:42:21.179849Z", "url": "https://files.pythonhosted.org/packages/05/aa/007b76d69241d3dac2819ae259feee786c64aad159230d2ffc251972d421/databricksapi-1.1.5.tar.gz", "yanked": false}], "1.1.6": [{"comment_text": "", "digests": {"md5": "4810f1c02dc17f7d442cd89733d8feb5", "sha256": "9fe762ed32eb4b0ab1b02c4130fdc6ede725ee742d345cbdbb0742f481cae8f2"}, "downloads": -1, "filename": "databricksapi-1.1.6.tar.gz", "has_sig": false, "md5_digest": "4810f1c02dc17f7d442cd89733d8feb5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16787, "upload_time": "2019-11-22T00:22:31", "upload_time_iso_8601": "2019-11-22T00:22:31.010127Z", "url": "https://files.pythonhosted.org/packages/70/cb/80675a09ec74c700c5bba30c6eeb517e65be2de5e0b6ffbbf1b3ea1cc17d/databricksapi-1.1.6.tar.gz", "yanked": false}], "1.1.7": [{"comment_text": "", "digests": {"md5": "6c98f064b023e4eedc1c44ee56d2d4c1", "sha256": "45776a349a412767a7a44922a76eb8c9162573dbdc7dd6eaf5af9216a8ca49ce"}, "downloads": -1, "filename": "databricksapi-1.1.7.tar.gz", "has_sig": false, "md5_digest": "6c98f064b023e4eedc1c44ee56d2d4c1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16820, "upload_time": "2019-12-03T03:08:27", "upload_time_iso_8601": "2019-12-03T03:08:27.432596Z", "url": "https://files.pythonhosted.org/packages/3d/83/6d8421c6c2af1ad5ec42a3d88062ae3b458a3fcdbfcb7e2b37d379cd5519/databricksapi-1.1.7.tar.gz", "yanked": false}], "1.1.8": [{"comment_text": "", "digests": {"md5": "e5e47d48c6ef1a601260d2e040420754", "sha256": "e5a4d732f8087a738203a4207b72548f61315918c2739fd95a93fbcfb157c1f1"}, "downloads": -1, "filename": "databricksapi-1.1.8.tar.gz", "has_sig": false, "md5_digest": "e5e47d48c6ef1a601260d2e040420754", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16832, "upload_time": "2020-04-03T18:48:56", "upload_time_iso_8601": "2020-04-03T18:48:56.410512Z", "url": "https://files.pythonhosted.org/packages/c9/ea/d3c9533c516bf076d2923931e10d7b417a75c92528a873ebe840538eeacc/databricksapi-1.1.8.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e5e47d48c6ef1a601260d2e040420754", "sha256": "e5a4d732f8087a738203a4207b72548f61315918c2739fd95a93fbcfb157c1f1"}, "downloads": -1, "filename": "databricksapi-1.1.8.tar.gz", "has_sig": false, "md5_digest": "e5e47d48c6ef1a601260d2e040420754", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16832, "upload_time": "2020-04-03T18:48:56", "upload_time_iso_8601": "2020-04-03T18:48:56.410512Z", "url": "https://files.pythonhosted.org/packages/c9/ea/d3c9533c516bf076d2923931e10d7b417a75c92528a873ebe840538eeacc/databricksapi-1.1.8.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:40:30 2020"}