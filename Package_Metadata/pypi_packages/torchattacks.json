{"info": {"author": "Harry Kim <24k.harry@gmail.com>", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Visualization"], "description": "# Adversarial-Attacks-Pytorch\n\nThis is a lightweight repository of adversarial attacks for Pytorch.\n\nThere are popular attack methods and some utils.\n\nHere is a [documentation](https://adversarial-attacks-pytorch.readthedocs.io/en/latest/index.html) for this package.\n\n## Table of Contents\n1. [Usage](#Usage)\n2. [Attacks and Papers](#Attacks-and-Papers)\n3. [Demos](#Demos)\n4. [Update Records](#Update-Records)\n\n## Usage\n\n### Dependencies\n\n- torch 1.2.0\n- python 3.6\n\n### Installation\n\n- `pip install torchattacks` or\n- `git clone https://github.com/Harry24k/adversairal-attacks-pytorch`\n\n```python\nimport torchattacks\npgd_attack = torchattacks.PGD(model, eps = 4/255, alpha = 8/255)\nadversarial_images = pgd_attack(images, labels)\n```\n\n### Precautions\n\n* **WARNING** :: All images should be scaled to [0, 1] with transform[to.Tensor()] before used in attacks.\n* **WARNING** :: All models should return ONLY ONE vector of `(N, C)` where `C = number of classes`.\n\n## Attacks and Papers\n\nThe papers and the methods with a brief summary and example.\nAll attacks in this repository are provided as *CLASS*.\nIf you want to get attacks built in *Function*, please refer below repositories.\n\n* **Explaining and harnessing adversarial examples** : [Paper](https://arxiv.org/abs/1412.6572), [Repo](https://github.com/Harry24k/FGSM-pytorch)\n  - FGSM\n\n* **DeepFool: a simple and accurate method to fool deep neural networks** : [Paper](https://arxiv.org/abs/1511.04599)\n  - DeepFool\n\n* **Adversarial Examples in the Physical World** : [Paper](https://arxiv.org/abs/1607.02533), [Repo](https://github.com/Harry24k/AEPW-pytorch)\n  - BIM or iterative-FSGM\n  - StepLL\n\n* **Towards Evaluating the Robustness of Neural Networks** : [Paper](https://arxiv.org/abs/1608.04644), [Repo](https://github.com/Harry24k/CW-pytorch)\n  - CW(L2)\n\n* **Ensemble Adversarial Traning : Attacks and Defences** : [Paper](https://arxiv.org/abs/1705.07204), [Repo](https://github.com/Harry24k/RFGSM-pytorch)\n  - RFGSM\n\n* **Towards Deep Learning Models Resistant to Adversarial Attacks** : [Paper](https://arxiv.org/abs/1706.06083), [Repo](https://github.com/Harry24k/PGD-pytorch)\n  - PGD(Linf)\n\n* **Comment on \"Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network\"** : [Paper](https://arxiv.org/abs/1907.00895)\n  - APGD(EOT + PGD)\n\nAttack | Clean | Adversarial\n:---: | :---: | :---:\nFGSM | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/clean.png\" width=\"300\" height=\"300\"> | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/fgsm.png\" width=\"300\" height=\"300\">\nBIM | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/clean.png\" width=\"300\" height=\"300\"> | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/bim.png\" width=\"300\" height=\"300\">\nStepLL | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/clean.png\" width=\"300\" height=\"300\"> | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/stepll.png\" width=\"300\" height=\"300\">\nRFGSM | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/clean.png\" width=\"300\" height=\"300\"> | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/rfgsm.png\" width=\"300\" height=\"300\">\nCW | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/clean.png\" width=\"300\" height=\"300\"> | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/cw.png\" width=\"300\" height=\"300\">\nPGD(w/o random starts) | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/clean.png\" width=\"300\" height=\"300\"> | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/pgd.png\" width=\"300\" height=\"300\">\nPGD(w/ random starts) | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/clean.png\" width=\"300\" height=\"300\"> | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/rpgd.png\" width=\"300\" height=\"300\">\nDeepFool | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/clean.png\" width=\"300\" height=\"300\"> | <img src=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/pic/deepfool.png\" width=\"300\" height=\"300\">\n\n## Demos\n\n* **White Box Attack with Imagenet** ([code](https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/demos/White%20Box%20Attack%20with%20Imagenet.ipynb)): \nTo make adversarial examples with the Imagenet dataset to fool [Inception v3](https://arxiv.org/abs/1512.00567). However, the Imagenet dataset is too large, so only '[Giant Panda](http://www.image-net.org/)' is used.\n\n* **Black Box Attack with CIFAR10** ([code](https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/demos/Adversairal%20Training%20with%20MNIST.ipynb)): \nThis demo provides an example of black box attack with two different models. First, make adversarial datasets from a holdout model with CIFAR10 and save it as torch dataset. Second, use the adversarial datasets to attack a target model.\n\n* **Adversairal Training with MNIST** ([code](https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/demos/Adversairal%20Training%20with%20MNIST.ipynb)): \nThis demo shows how to do adversarial training with this repository. The MNIST dataset and a custom model are used in this code. The adversarial training is performed with PGD, and then FGSM is applied to test the model.\n\n\n## Update Records\n\n### ~ Version 0.3\n* **New Attacks** : FGSM, IFGSM, IterLL, RFGSM, CW(L2), PGD are added.\n* **Demos** are uploaded.\n\n### Version 0.4\n* **DO NOT USE** : 'init.py' is omitted.\n\n### Version 0.5\n* **Package name changed** : 'attacks' is changed to 'torchattacks'.\n* **New Attack** : APGD is added.\n* **attack.py** : 'update_model' method is added.\n\n### Version 0.6\n* **Error Solved** : \n    * Before this version, even after getting an adversarial image, the model remains evaluation mode.\n    * To solve this, below methods are modified.\n        * '_switch_model' method is added into **attack.py**. It will automatically change model mode to the previous mode after getting adversarial images. When getting adversarial images, model is switched to evaluation mode.\n        * '__call__' methods in all attack changed to forward. Instead of this, '__call__' method is added into 'attack.py'\n* **attack.py** : To provide ease of changing images to uint8 from float, 'set_mode' and '_to_uint' is added.\n    * 'set_mode' determines returning all outputs as 'int' OR 'flaot' through '_to_uint'.\n    * '_to_uint' changes all outputs into uint8.\n\n### Version 0.7\n* **All attacks are modified**\n    * clone().detach() is used instead of .data\n    * torch.autograd.grad is used instead of .backward() and .grad :\n        * It showed 2% reduction of computation time.\n\n### Version 0.8\n* **New Attack** : RPGD is added.\n* **attack.py** : 'update_model' method is depreciated. Because torch models are passed by call-by-reference, we don't need to update models.\n    * **cw.py** : In the process of cw attack, now masked_select uses a mask with dtype torch.bool instead of a mask with dtype torch.uint8.\n\n### Version 0.9\n* **New Attack** : DeepFool is added.\n* **Some attacks are renamed** :\n    * I-FGSM -> BIM\n    * IterLL -> StepLL\n\n### Version 1.0\n* **attack.py** :\n    * **load** : Load is depreciated. Instead, use TensorDataset and DataLoader.\n    * **save** : The problem of calculating invalid accuracy when the mode of the attack set to 'int' is solved.\n\n### Version 1.1\n* **DeepFool** :\n    * [**Error solved**](https://github.com/Harry24k/adversairal-attacks-pytorch/issues/2).\n\n### Version 1.2\n* **Description has been added for each module.**\n* **Sphinx Document uploaded** \n* **attack.py** : 'device' will be decided by [next(model.parameters()).device](https://github.com/Harry24k/adversarial-attacks-pytorch/issues/3#issue-602571865).\n* **Two attacks are merged** :\n    * RPGD, PGD -> PGD\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/HarryK24/adversairal-attacks-pytorch", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "torchattacks", "package_url": "https://pypi.org/project/torchattacks/", "platform": "", "project_url": "https://pypi.org/project/torchattacks/", "project_urls": {"Homepage": "https://github.com/HarryK24/adversairal-attacks-pytorch"}, "release_url": "https://pypi.org/project/torchattacks/1.3/", "requires_dist": null, "requires_python": ">=3", "summary": "Adversarial Attacks for PyTorch", "version": "1.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Adversarial-Attacks-Pytorch</h1>\n<p>This is a lightweight repository of adversarial attacks for Pytorch.</p>\n<p>There are popular attack methods and some utils.</p>\n<p>Here is a <a href=\"https://adversarial-attacks-pytorch.readthedocs.io/en/latest/index.html\" rel=\"nofollow\">documentation</a> for this package.</p>\n<h2>Table of Contents</h2>\n<ol>\n<li><a href=\"#Usage\" rel=\"nofollow\">Usage</a></li>\n<li><a href=\"#Attacks-and-Papers\" rel=\"nofollow\">Attacks and Papers</a></li>\n<li><a href=\"#Demos\" rel=\"nofollow\">Demos</a></li>\n<li><a href=\"#Update-Records\" rel=\"nofollow\">Update Records</a></li>\n</ol>\n<h2>Usage</h2>\n<h3>Dependencies</h3>\n<ul>\n<li>torch 1.2.0</li>\n<li>python 3.6</li>\n</ul>\n<h3>Installation</h3>\n<ul>\n<li><code>pip install torchattacks</code> or</li>\n<li><code>git clone https://github.com/Harry24k/adversairal-attacks-pytorch</code></li>\n</ul>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torchattacks</span>\n<span class=\"n\">pgd_attack</span> <span class=\"o\">=</span> <span class=\"n\">torchattacks</span><span class=\"o\">.</span><span class=\"n\">PGD</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">eps</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"o\">/</span><span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mi\">8</span><span class=\"o\">/</span><span class=\"mi\">255</span><span class=\"p\">)</span>\n<span class=\"n\">adversarial_images</span> <span class=\"o\">=</span> <span class=\"n\">pgd_attack</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<h3>Precautions</h3>\n<ul>\n<li><strong>WARNING</strong> :: All images should be scaled to [0, 1] with transform[to.Tensor()] before used in attacks.</li>\n<li><strong>WARNING</strong> :: All models should return ONLY ONE vector of <code>(N, C)</code> where <code>C = number of classes</code>.</li>\n</ul>\n<h2>Attacks and Papers</h2>\n<p>The papers and the methods with a brief summary and example.\nAll attacks in this repository are provided as <em>CLASS</em>.\nIf you want to get attacks built in <em>Function</em>, please refer below repositories.</p>\n<ul>\n<li>\n<p><strong>Explaining and harnessing adversarial examples</strong> : <a href=\"https://arxiv.org/abs/1412.6572\" rel=\"nofollow\">Paper</a>, <a href=\"https://github.com/Harry24k/FGSM-pytorch\" rel=\"nofollow\">Repo</a></p>\n<ul>\n<li>FGSM</li>\n</ul>\n</li>\n<li>\n<p><strong>DeepFool: a simple and accurate method to fool deep neural networks</strong> : <a href=\"https://arxiv.org/abs/1511.04599\" rel=\"nofollow\">Paper</a></p>\n<ul>\n<li>DeepFool</li>\n</ul>\n</li>\n<li>\n<p><strong>Adversarial Examples in the Physical World</strong> : <a href=\"https://arxiv.org/abs/1607.02533\" rel=\"nofollow\">Paper</a>, <a href=\"https://github.com/Harry24k/AEPW-pytorch\" rel=\"nofollow\">Repo</a></p>\n<ul>\n<li>BIM or iterative-FSGM</li>\n<li>StepLL</li>\n</ul>\n</li>\n<li>\n<p><strong>Towards Evaluating the Robustness of Neural Networks</strong> : <a href=\"https://arxiv.org/abs/1608.04644\" rel=\"nofollow\">Paper</a>, <a href=\"https://github.com/Harry24k/CW-pytorch\" rel=\"nofollow\">Repo</a></p>\n<ul>\n<li>CW(L2)</li>\n</ul>\n</li>\n<li>\n<p><strong>Ensemble Adversarial Traning : Attacks and Defences</strong> : <a href=\"https://arxiv.org/abs/1705.07204\" rel=\"nofollow\">Paper</a>, <a href=\"https://github.com/Harry24k/RFGSM-pytorch\" rel=\"nofollow\">Repo</a></p>\n<ul>\n<li>RFGSM</li>\n</ul>\n</li>\n<li>\n<p><strong>Towards Deep Learning Models Resistant to Adversarial Attacks</strong> : <a href=\"https://arxiv.org/abs/1706.06083\" rel=\"nofollow\">Paper</a>, <a href=\"https://github.com/Harry24k/PGD-pytorch\" rel=\"nofollow\">Repo</a></p>\n<ul>\n<li>PGD(Linf)</li>\n</ul>\n</li>\n<li>\n<p><strong>Comment on \"Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network\"</strong> : <a href=\"https://arxiv.org/abs/1907.00895\" rel=\"nofollow\">Paper</a></p>\n<ul>\n<li>APGD(EOT + PGD)</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Attack</th>\n<th align=\"center\">Clean</th>\n<th align=\"center\">Adversarial</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">FGSM</td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3db5a93d5c704b7e96b4d151dc4aa84f5d1b5ae2/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f636c65616e2e706e67\" width=\"300\"></td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/501b543ef487ff862f843a7c7e2ba6e10cc98897/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f6667736d2e706e67\" width=\"300\"></td>\n</tr>\n<tr>\n<td align=\"center\">BIM</td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3db5a93d5c704b7e96b4d151dc4aa84f5d1b5ae2/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f636c65616e2e706e67\" width=\"300\"></td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e85f3065305acbee0615eb19edaa2091f385ac93/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f62696d2e706e67\" width=\"300\"></td>\n</tr>\n<tr>\n<td align=\"center\">StepLL</td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3db5a93d5c704b7e96b4d151dc4aa84f5d1b5ae2/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f636c65616e2e706e67\" width=\"300\"></td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/59b6ea66b1998f22ceb0a051dba06daa8cee0df8/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f737465706c6c2e706e67\" width=\"300\"></td>\n</tr>\n<tr>\n<td align=\"center\">RFGSM</td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3db5a93d5c704b7e96b4d151dc4aa84f5d1b5ae2/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f636c65616e2e706e67\" width=\"300\"></td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/681f02967294c9439e91acac60eee73f5cc1cbed/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f726667736d2e706e67\" width=\"300\"></td>\n</tr>\n<tr>\n<td align=\"center\">CW</td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3db5a93d5c704b7e96b4d151dc4aa84f5d1b5ae2/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f636c65616e2e706e67\" width=\"300\"></td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f3afb2cece30b95a76a2143dd012c8f3a9e31087/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f63772e706e67\" width=\"300\"></td>\n</tr>\n<tr>\n<td align=\"center\">PGD(w/o random starts)</td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3db5a93d5c704b7e96b4d151dc4aa84f5d1b5ae2/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f636c65616e2e706e67\" width=\"300\"></td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c155de1eab6080b9176a92f4ed6f43dac22f8f63/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f7067642e706e67\" width=\"300\"></td>\n</tr>\n<tr>\n<td align=\"center\">PGD(w/ random starts)</td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3db5a93d5c704b7e96b4d151dc4aa84f5d1b5ae2/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f636c65616e2e706e67\" width=\"300\"></td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/77012653179139745d768422b5c56f7ba441481f/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f727067642e706e67\" width=\"300\"></td>\n</tr>\n<tr>\n<td align=\"center\">DeepFool</td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3db5a93d5c704b7e96b4d151dc4aa84f5d1b5ae2/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f636c65616e2e706e67\" width=\"300\"></td>\n<td align=\"center\"><img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6ae44417b58ec4e90e53d411bcf78db8f9c06e5c/68747470733a2f2f6769746875622e636f6d2f486172727932346b2f616476657273616972616c2d61747461636b732d7079746f7263682f626c6f622f6d61737465722f7069632f64656570666f6f6c2e706e67\" width=\"300\"></td>\n</tr></tbody></table>\n<h2>Demos</h2>\n<ul>\n<li>\n<p><strong>White Box Attack with Imagenet</strong> (<a href=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/demos/White%20Box%20Attack%20with%20Imagenet.ipynb\" rel=\"nofollow\">code</a>):\nTo make adversarial examples with the Imagenet dataset to fool <a href=\"https://arxiv.org/abs/1512.00567\" rel=\"nofollow\">Inception v3</a>. However, the Imagenet dataset is too large, so only '<a href=\"http://www.image-net.org/\" rel=\"nofollow\">Giant Panda</a>' is used.</p>\n</li>\n<li>\n<p><strong>Black Box Attack with CIFAR10</strong> (<a href=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/demos/Adversairal%20Training%20with%20MNIST.ipynb\" rel=\"nofollow\">code</a>):\nThis demo provides an example of black box attack with two different models. First, make adversarial datasets from a holdout model with CIFAR10 and save it as torch dataset. Second, use the adversarial datasets to attack a target model.</p>\n</li>\n<li>\n<p><strong>Adversairal Training with MNIST</strong> (<a href=\"https://github.com/Harry24k/adversairal-attacks-pytorch/blob/master/demos/Adversairal%20Training%20with%20MNIST.ipynb\" rel=\"nofollow\">code</a>):\nThis demo shows how to do adversarial training with this repository. The MNIST dataset and a custom model are used in this code. The adversarial training is performed with PGD, and then FGSM is applied to test the model.</p>\n</li>\n</ul>\n<h2>Update Records</h2>\n<h3>~ Version 0.3</h3>\n<ul>\n<li><strong>New Attacks</strong> : FGSM, IFGSM, IterLL, RFGSM, CW(L2), PGD are added.</li>\n<li><strong>Demos</strong> are uploaded.</li>\n</ul>\n<h3>Version 0.4</h3>\n<ul>\n<li><strong>DO NOT USE</strong> : 'init.py' is omitted.</li>\n</ul>\n<h3>Version 0.5</h3>\n<ul>\n<li><strong>Package name changed</strong> : 'attacks' is changed to 'torchattacks'.</li>\n<li><strong>New Attack</strong> : APGD is added.</li>\n<li><strong>attack.py</strong> : 'update_model' method is added.</li>\n</ul>\n<h3>Version 0.6</h3>\n<ul>\n<li><strong>Error Solved</strong> :\n<ul>\n<li>Before this version, even after getting an adversarial image, the model remains evaluation mode.</li>\n<li>To solve this, below methods are modified.\n<ul>\n<li>'_switch_model' method is added into <strong>attack.py</strong>. It will automatically change model mode to the previous mode after getting adversarial images. When getting adversarial images, model is switched to evaluation mode.</li>\n<li>'<strong>call</strong>' methods in all attack changed to forward. Instead of this, '<strong>call</strong>' method is added into 'attack.py'</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>attack.py</strong> : To provide ease of changing images to uint8 from float, 'set_mode' and '_to_uint' is added.\n<ul>\n<li>'set_mode' determines returning all outputs as 'int' OR 'flaot' through '_to_uint'.</li>\n<li>'_to_uint' changes all outputs into uint8.</li>\n</ul>\n</li>\n</ul>\n<h3>Version 0.7</h3>\n<ul>\n<li><strong>All attacks are modified</strong>\n<ul>\n<li>clone().detach() is used instead of .data</li>\n<li>torch.autograd.grad is used instead of .backward() and .grad :\n<ul>\n<li>It showed 2% reduction of computation time.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Version 0.8</h3>\n<ul>\n<li><strong>New Attack</strong> : RPGD is added.</li>\n<li><strong>attack.py</strong> : 'update_model' method is depreciated. Because torch models are passed by call-by-reference, we don't need to update models.\n<ul>\n<li><strong>cw.py</strong> : In the process of cw attack, now masked_select uses a mask with dtype torch.bool instead of a mask with dtype torch.uint8.</li>\n</ul>\n</li>\n</ul>\n<h3>Version 0.9</h3>\n<ul>\n<li><strong>New Attack</strong> : DeepFool is added.</li>\n<li><strong>Some attacks are renamed</strong> :\n<ul>\n<li>I-FGSM -&gt; BIM</li>\n<li>IterLL -&gt; StepLL</li>\n</ul>\n</li>\n</ul>\n<h3>Version 1.0</h3>\n<ul>\n<li><strong>attack.py</strong> :\n<ul>\n<li><strong>load</strong> : Load is depreciated. Instead, use TensorDataset and DataLoader.</li>\n<li><strong>save</strong> : The problem of calculating invalid accuracy when the mode of the attack set to 'int' is solved.</li>\n</ul>\n</li>\n</ul>\n<h3>Version 1.1</h3>\n<ul>\n<li><strong>DeepFool</strong> :\n<ul>\n<li><a href=\"https://github.com/Harry24k/adversairal-attacks-pytorch/issues/2\" rel=\"nofollow\"><strong>Error solved</strong></a>.</li>\n</ul>\n</li>\n</ul>\n<h3>Version 1.2</h3>\n<ul>\n<li><strong>Description has been added for each module.</strong></li>\n<li><strong>Sphinx Document uploaded</strong></li>\n<li><strong>attack.py</strong> : 'device' will be decided by <a href=\"https://github.com/Harry24k/adversarial-attacks-pytorch/issues/3#issue-602571865\" rel=\"nofollow\">next(model.parameters()).device</a>.</li>\n<li><strong>Two attacks are merged</strong> :\n<ul>\n<li>RPGD, PGD -&gt; PGD</li>\n</ul>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 7056796, "releases": {"1.3": [{"comment_text": "", "digests": {"md5": "15ab126863786b26f2662c911c6cdd1f", "sha256": "4c11eb4a9849e18f2f8a111f980abf73b3299411c79e49109b770b1851a7aa36"}, "downloads": -1, "filename": "torchattacks-1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "15ab126863786b26f2662c911c6cdd1f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 15949, "upload_time": "2020-04-20T00:56:26", "upload_time_iso_8601": "2020-04-20T00:56:26.050423Z", "url": "https://files.pythonhosted.org/packages/3a/15/43d6010be0ef23ce21633749dae17cc264e8fbae1590c941c0b66ab6de50/torchattacks-1.3-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "15ab126863786b26f2662c911c6cdd1f", "sha256": "4c11eb4a9849e18f2f8a111f980abf73b3299411c79e49109b770b1851a7aa36"}, "downloads": -1, "filename": "torchattacks-1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "15ab126863786b26f2662c911c6cdd1f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 15949, "upload_time": "2020-04-20T00:56:26", "upload_time_iso_8601": "2020-04-20T00:56:26.050423Z", "url": "https://files.pythonhosted.org/packages/3a/15/43d6010be0ef23ce21633749dae17cc264e8fbae1590c941c0b66ab6de50/torchattacks-1.3-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:50:37 2020"}