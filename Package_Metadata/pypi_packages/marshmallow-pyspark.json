{"info": {"author": "Ketan Goyal", "author_email": "ketangoyal1988@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: PyPy", "Topic :: Software Development", "Topic :: System :: Systems Administration", "Topic :: Utilities"], "description": "# marshmallow-pyspark\n\n[![Build Status](https://travis-ci.com/ketgo/marshmallow-pyspark.svg?token=oCVxhfjJAa2zDdszGjoy&branch=master)](https://travis-ci.com/ketgo/marshmallow-pyspark)\n[![codecov.io](https://codecov.io/gh/ketgo/marshmallow-pyspark/coverage.svg?branch=master)](https://codecov.io/gh/ketgo/marshmallow-pyspark/coverage.svg?branch=master)\n[![Apache 2.0 licensed](https://img.shields.io/badge/License-Apache%202.0-yellow.svg)](https://raw.githubusercontent.com/ketgo/marshmallow-pyspark/master/LICENSE)\n\n[Marshmallow](https://marshmallow.readthedocs.io/en/stable/) is a popular package used for data serialization and validation. \nOne defines data schemas in marshmallow containing rules on how input data should be marshalled. Similar to marshmallow, \n[pyspark](https://spark.apache.org/docs/latest/api/python/index.html) also comes with its own schema definitions used to \nprocess data frames. This package enables users to utilize marshmallow schemas and its powerful data validation capabilities \nin pyspark applications. Such capabilities can be utilized in data-pipeline ETL jobs where data consistency and quality \nis of importance.\n\n## Install\n\nThe package can be install using `pip`:\n```bash\n$ pip install marshmallow-pyspark\n```\n\n## Usage\n\nData schemas can can define the same way as you would using marshmallow. A quick example is shown below:\n```python\nfrom marshmallow_pyspark import Schema\nfrom marshmallow import fields\n\n# Create data schema.\nclass AlbumSchema(Schema):\n    title = fields.Str()\n    release_date = fields.Date()\n\n# Input data frame to validate.\ndf = spark.createDataFrame([\n    {\"title\": \"valid_1\", \"release_date\": \"2020-1-10\"},\n    {\"title\": \"valid_2\", \"release_date\": \"2020-1-11\"},\n    {\"title\": \"invalid_1\", \"release_date\": \"2020-31-11\"},\n    {\"title\": \"invalid_2\", \"release_date\": \"2020-1-51\"},\n])\n\n# Get data frames with valid rows and error prone rows \n# from input data frame by validating using the schema.\nvalid_df, errors_df = AlbumSchema().validate_df(df)\n\n# Output of valid data frame\nvalid_df.show()\n#    +-------+------------+\n#    |  title|release_date|\n#    +-------+------------+\n#    |valid_1|  2020-01-10|\n#    |valid_2|  2020-01-11|\n#    +-------+------------+\n\n# Output of errors data frame\nerrors_df.show()\n#    +--------------------+\n#    |             _errors|\n#    +--------------------+\n#    |{\"row\": {\"release...|\n#    |{\"row\": {\"release...|\n#    +--------------------+\n```\n\n### More Options\n\nOn top of marshmallow supported options, the `Schema` class comes with two additional initialization arguments:\n\n- `error_column_name`: name of the column to store validation errors. Default value is `_errors`.\n\n- `split_errors`: split rows with validation errors as a separate data frame from valid rows. When set to `False` the \n   rows with errors are returned together with valid rows as a single data frame. The field values of all error rows are \n   set to `null`. For user convenience the original field values can be found in the `row` attribute of the error JSON. \n   Default value is `True`. \n\nAn example is shown below:\n```python\nfrom marshmallow import EXCLUDE\n\nschema = AlbumSchema(\n    error_column_name=\"custom_errors\",     # Use 'custom_errors' as name for errors column\n    split_errors=False,                     # Don't split the input data frame into valid and errors\n    unkown=EXCLUDE                          # Marshmallow option to exclude fields not present in schema\n)\n\n# Input data frame to validate.\ndf = spark.createDataFrame([\n    {\"title\": \"valid_1\", \"release_date\": \"2020-1-10\", \"garbage\": \"wdacfa\"},\n    {\"title\": \"valid_2\", \"release_date\": \"2020-1-11\", \"garbage\": \"5wacfa\"},\n    {\"title\": \"invalid_1\", \"release_date\": \"2020-31-11\", \"garbage\": \"3aqf\"},\n    {\"title\": \"invalid_2\", \"release_date\": \"2020-1-51\", \"garbage\": \"vda\"},\n])\n\nvalid_df, errors_df = schema.validate_df(df)\n\n# Output of valid data frame. Contains rows with errors as\n# the option 'split_errors' was set to False.\nvalid_df.show()\n#    +-------+------------+--------------------+\n#    |  title|release_date|             _errors|\n#    +-------+------------+--------------------+\n#    |valid_1|  2020-01-10|                    |\n#    |valid_2|  2020-01-11|                    |\n#    |       |            |{\"row\": {\"release...|\n#    |       |            |{\"row\": {\"release...|\n#    +-------+------------+--------------------+\n\n# The errors data frame will be set to None\nassert errors_df is None        # True\n```\n\nLastly, on top of passing marshmallow specific options in the schema, you can also pass them in the `validate_df` method.\nThese are options are passed to the marshmallow's `load` method:\n```python\nschema = AlbumSchema(\n    error_column_name=\"custom_errors\",     # Use 'custom_errors' as name for errors column\n    split_errors=False,                     # Don't split the input data frame into valid and errors\n)\n\nvalid_df, errors_df = schema.validate_df(df, unkown=EXCLUDE)\n```\n\n### Duplicates\n\nMarshmallow-pyspark comes with the ability to validate one or more schema fields for duplicate values. This is achieved\nby adding the field names to the `UNIQUE` attribute of the schema as shown:\n```python\nclass AlbumSchema(Schema):\n    # Unique valued field \"title\" in the schema\n    UNIQUE = [\"title\"]\n\n    title = fields.Str()\n    release_date = fields.Date()\n\n# Input data frame to validate.\ndf = spark.createDataFrame([\n        {\"title\": \"title_1\", \"release_date\": \"2020-1-10\"},\n        {\"title\": \"title_2\", \"release_date\": \"2020-1-11\"},\n        {\"title\": \"title_2\", \"release_date\": \"2020-3-11\"},  # duplicate title\n        {\"title\": \"title_3\", \"release_date\": \"2020-1-51\"},\n    ])\n\n# Validate data frame\nvalid_df, errors_df = AlbumSchema().validate_df(df)\n    \n# List of valid rows\nvalid_rows = [row.asDict(recursive=True) for row in valid_df.collect()]\n#\n#   [\n#        {'title': 'title_1', 'release_date': datetime.date(2020, 1, 10)},\n#        {'title': 'title_2', 'release_date': datetime.date(2020, 1, 11)}\n#   ]\n#\n\n# Rows with errors\nerror_rows = [row.asDict(recursive=True) for row in errors_df.collect()]\n# \n#   [\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-3-11\", \"title\": \"title_2\", \"__count__title\": 2}, '\n#                    '\"errors\": [\"duplicate row\"]}'},\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-1-51\", \"title\": \"title_3\", \"__count__title\": 1}, '\n#                    '\"errors\": {\"release_date\": [\"Not a valid date.\"]}}'}\n#    ]\n#\n``` \nThe technique to drop duplicates but keep first is discussed in this [link](https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first).\nIn case there are multiple unique fields in the schema just add them to the `UNIQUE`, e.g. `UNIQUE=[\"title\", \"release_date\"]`. \nYou can even specify uniqueness for combination of fields by grouping them in a list:\n```python\nclass AlbumSchema(Schema):\n    # Combined values of \"title\" and \"release_date\" should be unique\n    UNIQUE = [[\"title\", \"release_date\"]]\n\n    title = fields.Str()\n    release_date = fields.Date()\n\n# Input data frame to validate.\ndf = spark.createDataFrame([\n        {\"title\": \"title_1\", \"release_date\": \"2020-1-10\"},\n        {\"title\": \"title_2\", \"release_date\": \"2020-1-11\"},\n        {\"title\": \"title_2\", \"release_date\": \"2020-3-11\"},\n        {\"title\": \"title_3\", \"release_date\": \"2020-1-21\"},\n        {\"title\": \"title_3\", \"release_date\": \"2020-1-21\"},\n        {\"title\": \"title_4\", \"release_date\": \"2020-1-51\"},\n    ])\n\n# Validate data frame\nvalid_df, errors_df = AlbumSchema().validate_df(df)\n    \n# List of valid rows\nvalid_rows = [row.asDict(recursive=True) for row in valid_df.collect()]\n#\n#   [\n#        {'title': 'title_1', 'release_date': datetime.date(2020, 1, 10)},\n#        {'title': 'title_2', 'release_date': datetime.date(2020, 1, 11)},\n#        {'title': 'title_3', 'release_date': datetime.date(2020, 1, 21)}\n#   ]\n#\n\n# Rows with errors\nerror_rows = [row.asDict(recursive=True) for row in errors_df.collect()]\n# \n#   [\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-1-21\", \"title\": \"title_3\", '\n#                    '\"__count__title\": 2, \"__count__release_date\": 2}, '\n#                    '\"errors\": [\"duplicate row\"]}'},\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-1-51\", \"title\": \"title_4\", '\n#                    '\"__count__title\": 1, \"__count__release_date\": 1}, '\n#                    '\"errors\": {\"release_date\": [\"Not a valid date.\"]}}'},\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-3-11\", \"title\": \"title_2\", '\n#                    '\"__count__title\": 2, \"__count__release_date\": 1}, '\n#                    '\"errors\": [\"duplicate row\"]}'}\n#    ]\n#\n```\n**WARNING**: Duplicate check requires data shuffle per unique field. Having large number of unique fields will effect \nspark job performance. By default `UNIQUE` is set to an empty list preventing any duplicate checks. \n\n### Fields\n\nMarshmallow comes with a variety of different fields that can be used to define schemas. Internally marshmallow-pyspark \nconvert these fields into pyspark SQL data types. The following table lists the supported marshmallow fields and their \nequivalent spark SQL data types:\n\n\n| Marshmallow | PySpark |\n| --- | --- |\n| `String` | `StringType` |\n| `DateTime` | `TimestampType` |\n| `Date` | `DateType` |\n| `Boolean` | `BooleanType` |\n| `Integer` | `IntegerType` |\n| `Float` | `FloatType` |\n| `Number` | `DoubleType` |\n| `List` | `ArrayType` |\n| `Dict` | `MapType` |\n| `Nested` | `StructType` |\n\nBy default the `StringType` data type is used for marshmallow fields not in the above table. The `spark_schema` property\nof your defined schema can be used to check the converted spark SQL schema:\n```python\n# Gets the spark schema for the Album schema\nAlbumSchema().spark_schema\n# StructType(List(StructField(title,StringType,true),StructField(release_date,DateType,true),StructField(_errors,StringType,true)))\n```\n\n#### Custom Fields\n\nIt is also possible to add support for custom marshmallow fields, or those missing in the above table. In order to do so, \nyou would need to create a converter for the custom field. The converter can be built using the `ConverterABC` interface:\n```python\nfrom marshmallow_pyspark import ConverterABC\nfrom pyspark.sql.types import StringType\n\n\nclass EmailConverter(ConverterABC):\n    \"\"\"\n        Converter to convert marshmallow's Email field to a pyspark \n        SQL data type.\n    \"\"\"\n\n    def convert(self, ma_field):\n        return StringType()\n```  \nThe `ma_field` argument in the `convert` method is provided to handle nested fields. For an example you can checkout \n`NestedConverter`. Now the final step would be to add the converter to the `CONVERTER_MAP` attribute of your schema:\n```python\nfrom marshmallow_pyspark import Schema\nfrom marshmallow import fields\n\n\nclass User(Schema):\n    name = fields.String(required=True)\n    email = fields.Email(required=True)\n\n# Adding email converter to schema.\nUser.CONVERTER_MAP[fields.Email] = EmailConverter\n\n# You can now use your schema to validate the input data frame.\nvalid_df, errors_df = User().validate_df(input_df)\n```\n\n## Development\n\nTo hack marshmallow-pyspark locally run:\n\n```bash\n$ pip install -e .[dev]\t\t\t# to install all dependencies\n$ pytest --cov-config .coveragerc --cov=./\t\t\t# to get coverage report\n$ pylint marshmallow_pyspark\t\t\t# to check code quality with PyLint\n```\n\nOptionally you can use `make` to perform development tasks.\n\n## License\n\nThe source code is licensed under Apache License Version 2.\n\n## Contributions\n\nPull requests always welcomed! :)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ketgo/marshmallow-pyspark", "keywords": "pyspark serializer marshmallow data-pipeline data-quality", "license": "Apache 2.0 license", "maintainer": "", "maintainer_email": "", "name": "marshmallow-pyspark", "package_url": "https://pypi.org/project/marshmallow-pyspark/", "platform": "", "project_url": "https://pypi.org/project/marshmallow-pyspark/", "project_urls": {"Homepage": "https://github.com/ketgo/marshmallow-pyspark"}, "release_url": "https://pypi.org/project/marshmallow-pyspark/0.2.0/", "requires_dist": null, "requires_python": ">=3.4", "summary": "PySpark data serializer", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>marshmallow-pyspark</h1>\n<p><a href=\"https://travis-ci.com/ketgo/marshmallow-pyspark\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c83fa32911065009b676b183f592171c8e97b367/68747470733a2f2f7472617669732d63692e636f6d2f6b6574676f2f6d617273686d616c6c6f772d7079737061726b2e7376673f746f6b656e3d6f43567868666a4a4161327a4464737a476a6f79266272616e63683d6d6173746572\"></a>\n<a href=\"https://codecov.io/gh/ketgo/marshmallow-pyspark/coverage.svg?branch=master\" rel=\"nofollow\"><img alt=\"codecov.io\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e137dd950cbfc11997d1def9a78ee418824b08ac/68747470733a2f2f636f6465636f762e696f2f67682f6b6574676f2f6d617273686d616c6c6f772d7079737061726b2f636f7665726167652e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://raw.githubusercontent.com/ketgo/marshmallow-pyspark/master/LICENSE\" rel=\"nofollow\"><img alt=\"Apache 2.0 licensed\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f5158a87f705edccbe3d715323e53a9054c09813/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d79656c6c6f772e737667\"></a></p>\n<p><a href=\"https://marshmallow.readthedocs.io/en/stable/\" rel=\"nofollow\">Marshmallow</a> is a popular package used for data serialization and validation.\nOne defines data schemas in marshmallow containing rules on how input data should be marshalled. Similar to marshmallow,\n<a href=\"https://spark.apache.org/docs/latest/api/python/index.html\" rel=\"nofollow\">pyspark</a> also comes with its own schema definitions used to\nprocess data frames. This package enables users to utilize marshmallow schemas and its powerful data validation capabilities\nin pyspark applications. Such capabilities can be utilized in data-pipeline ETL jobs where data consistency and quality\nis of importance.</p>\n<h2>Install</h2>\n<p>The package can be install using <code>pip</code>:</p>\n<pre>$ pip install marshmallow-pyspark\n</pre>\n<h2>Usage</h2>\n<p>Data schemas can can define the same way as you would using marshmallow. A quick example is shown below:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">marshmallow_pyspark</span> <span class=\"kn\">import</span> <span class=\"n\">Schema</span>\n<span class=\"kn\">from</span> <span class=\"nn\">marshmallow</span> <span class=\"kn\">import</span> <span class=\"n\">fields</span>\n\n<span class=\"c1\"># Create data schema.</span>\n<span class=\"k\">class</span> <span class=\"nc\">AlbumSchema</span><span class=\"p\">(</span><span class=\"n\">Schema</span><span class=\"p\">):</span>\n    <span class=\"n\">title</span> <span class=\"o\">=</span> <span class=\"n\">fields</span><span class=\"o\">.</span><span class=\"n\">Str</span><span class=\"p\">()</span>\n    <span class=\"n\">release_date</span> <span class=\"o\">=</span> <span class=\"n\">fields</span><span class=\"o\">.</span><span class=\"n\">Date</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Input data frame to validate.</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([</span>\n    <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"valid_1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-10\"</span><span class=\"p\">},</span>\n    <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"valid_2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-11\"</span><span class=\"p\">},</span>\n    <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"invalid_1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-31-11\"</span><span class=\"p\">},</span>\n    <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"invalid_2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-51\"</span><span class=\"p\">},</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Get data frames with valid rows and error prone rows </span>\n<span class=\"c1\"># from input data frame by validating using the schema.</span>\n<span class=\"n\">valid_df</span><span class=\"p\">,</span> <span class=\"n\">errors_df</span> <span class=\"o\">=</span> <span class=\"n\">AlbumSchema</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">validate_df</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Output of valid data frame</span>\n<span class=\"n\">valid_df</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"c1\">#    +-------+------------+</span>\n<span class=\"c1\">#    |  title|release_date|</span>\n<span class=\"c1\">#    +-------+------------+</span>\n<span class=\"c1\">#    |valid_1|  2020-01-10|</span>\n<span class=\"c1\">#    |valid_2|  2020-01-11|</span>\n<span class=\"c1\">#    +-------+------------+</span>\n\n<span class=\"c1\"># Output of errors data frame</span>\n<span class=\"n\">errors_df</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"c1\">#    +--------------------+</span>\n<span class=\"c1\">#    |             _errors|</span>\n<span class=\"c1\">#    +--------------------+</span>\n<span class=\"c1\">#    |{\"row\": {\"release...|</span>\n<span class=\"c1\">#    |{\"row\": {\"release...|</span>\n<span class=\"c1\">#    +--------------------+</span>\n</pre>\n<h3>More Options</h3>\n<p>On top of marshmallow supported options, the <code>Schema</code> class comes with two additional initialization arguments:</p>\n<ul>\n<li>\n<p><code>error_column_name</code>: name of the column to store validation errors. Default value is <code>_errors</code>.</p>\n</li>\n<li>\n<p><code>split_errors</code>: split rows with validation errors as a separate data frame from valid rows. When set to <code>False</code> the\nrows with errors are returned together with valid rows as a single data frame. The field values of all error rows are\nset to <code>null</code>. For user convenience the original field values can be found in the <code>row</code> attribute of the error JSON.\nDefault value is <code>True</code>.</p>\n</li>\n</ul>\n<p>An example is shown below:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">marshmallow</span> <span class=\"kn\">import</span> <span class=\"n\">EXCLUDE</span>\n\n<span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">AlbumSchema</span><span class=\"p\">(</span>\n    <span class=\"n\">error_column_name</span><span class=\"o\">=</span><span class=\"s2\">\"custom_errors\"</span><span class=\"p\">,</span>     <span class=\"c1\"># Use 'custom_errors' as name for errors column</span>\n    <span class=\"n\">split_errors</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>                     <span class=\"c1\"># Don't split the input data frame into valid and errors</span>\n    <span class=\"n\">unkown</span><span class=\"o\">=</span><span class=\"n\">EXCLUDE</span>                          <span class=\"c1\"># Marshmallow option to exclude fields not present in schema</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Input data frame to validate.</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([</span>\n    <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"valid_1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-10\"</span><span class=\"p\">,</span> <span class=\"s2\">\"garbage\"</span><span class=\"p\">:</span> <span class=\"s2\">\"wdacfa\"</span><span class=\"p\">},</span>\n    <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"valid_2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-11\"</span><span class=\"p\">,</span> <span class=\"s2\">\"garbage\"</span><span class=\"p\">:</span> <span class=\"s2\">\"5wacfa\"</span><span class=\"p\">},</span>\n    <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"invalid_1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-31-11\"</span><span class=\"p\">,</span> <span class=\"s2\">\"garbage\"</span><span class=\"p\">:</span> <span class=\"s2\">\"3aqf\"</span><span class=\"p\">},</span>\n    <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"invalid_2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-51\"</span><span class=\"p\">,</span> <span class=\"s2\">\"garbage\"</span><span class=\"p\">:</span> <span class=\"s2\">\"vda\"</span><span class=\"p\">},</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">valid_df</span><span class=\"p\">,</span> <span class=\"n\">errors_df</span> <span class=\"o\">=</span> <span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">validate_df</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Output of valid data frame. Contains rows with errors as</span>\n<span class=\"c1\"># the option 'split_errors' was set to False.</span>\n<span class=\"n\">valid_df</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"c1\">#    +-------+------------+--------------------+</span>\n<span class=\"c1\">#    |  title|release_date|             _errors|</span>\n<span class=\"c1\">#    +-------+------------+--------------------+</span>\n<span class=\"c1\">#    |valid_1|  2020-01-10|                    |</span>\n<span class=\"c1\">#    |valid_2|  2020-01-11|                    |</span>\n<span class=\"c1\">#    |       |            |{\"row\": {\"release...|</span>\n<span class=\"c1\">#    |       |            |{\"row\": {\"release...|</span>\n<span class=\"c1\">#    +-------+------------+--------------------+</span>\n\n<span class=\"c1\"># The errors data frame will be set to None</span>\n<span class=\"k\">assert</span> <span class=\"n\">errors_df</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span>        <span class=\"c1\"># True</span>\n</pre>\n<p>Lastly, on top of passing marshmallow specific options in the schema, you can also pass them in the <code>validate_df</code> method.\nThese are options are passed to the marshmallow's <code>load</code> method:</p>\n<pre><span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">AlbumSchema</span><span class=\"p\">(</span>\n    <span class=\"n\">error_column_name</span><span class=\"o\">=</span><span class=\"s2\">\"custom_errors\"</span><span class=\"p\">,</span>     <span class=\"c1\"># Use 'custom_errors' as name for errors column</span>\n    <span class=\"n\">split_errors</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>                     <span class=\"c1\"># Don't split the input data frame into valid and errors</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">valid_df</span><span class=\"p\">,</span> <span class=\"n\">errors_df</span> <span class=\"o\">=</span> <span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">validate_df</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">unkown</span><span class=\"o\">=</span><span class=\"n\">EXCLUDE</span><span class=\"p\">)</span>\n</pre>\n<h3>Duplicates</h3>\n<p>Marshmallow-pyspark comes with the ability to validate one or more schema fields for duplicate values. This is achieved\nby adding the field names to the <code>UNIQUE</code> attribute of the schema as shown:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">AlbumSchema</span><span class=\"p\">(</span><span class=\"n\">Schema</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Unique valued field \"title\" in the schema</span>\n    <span class=\"n\">UNIQUE</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"title\"</span><span class=\"p\">]</span>\n\n    <span class=\"n\">title</span> <span class=\"o\">=</span> <span class=\"n\">fields</span><span class=\"o\">.</span><span class=\"n\">Str</span><span class=\"p\">()</span>\n    <span class=\"n\">release_date</span> <span class=\"o\">=</span> <span class=\"n\">fields</span><span class=\"o\">.</span><span class=\"n\">Date</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Input data frame to validate.</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-10\"</span><span class=\"p\">},</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-11\"</span><span class=\"p\">},</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-3-11\"</span><span class=\"p\">},</span>  <span class=\"c1\"># duplicate title</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_3\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-51\"</span><span class=\"p\">},</span>\n    <span class=\"p\">])</span>\n\n<span class=\"c1\"># Validate data frame</span>\n<span class=\"n\">valid_df</span><span class=\"p\">,</span> <span class=\"n\">errors_df</span> <span class=\"o\">=</span> <span class=\"n\">AlbumSchema</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">validate_df</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n    \n<span class=\"c1\"># List of valid rows</span>\n<span class=\"n\">valid_rows</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">asDict</span><span class=\"p\">(</span><span class=\"n\">recursive</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">valid_df</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()]</span>\n<span class=\"c1\">#</span>\n<span class=\"c1\">#   [</span>\n<span class=\"c1\">#        {'title': 'title_1', 'release_date': datetime.date(2020, 1, 10)},</span>\n<span class=\"c1\">#        {'title': 'title_2', 'release_date': datetime.date(2020, 1, 11)}</span>\n<span class=\"c1\">#   ]</span>\n<span class=\"c1\">#</span>\n\n<span class=\"c1\"># Rows with errors</span>\n<span class=\"n\">error_rows</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">asDict</span><span class=\"p\">(</span><span class=\"n\">recursive</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">errors_df</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()]</span>\n<span class=\"c1\"># </span>\n<span class=\"c1\">#   [</span>\n<span class=\"c1\">#        {'_errors': '{\"row\": {\"release_date\": \"2020-3-11\", \"title\": \"title_2\", \"__count__title\": 2}, '</span>\n<span class=\"c1\">#                    '\"errors\": [\"duplicate row\"]}'},</span>\n<span class=\"c1\">#        {'_errors': '{\"row\": {\"release_date\": \"2020-1-51\", \"title\": \"title_3\", \"__count__title\": 1}, '</span>\n<span class=\"c1\">#                    '\"errors\": {\"release_date\": [\"Not a valid date.\"]}}'}</span>\n<span class=\"c1\">#    ]</span>\n<span class=\"c1\">#</span>\n</pre>\n<p>The technique to drop duplicates but keep first is discussed in this <a href=\"https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\" rel=\"nofollow\">link</a>.\nIn case there are multiple unique fields in the schema just add them to the <code>UNIQUE</code>, e.g. <code>UNIQUE=[\"title\", \"release_date\"]</code>.\nYou can even specify uniqueness for combination of fields by grouping them in a list:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">AlbumSchema</span><span class=\"p\">(</span><span class=\"n\">Schema</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Combined values of \"title\" and \"release_date\" should be unique</span>\n    <span class=\"n\">UNIQUE</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"s2\">\"title\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">]]</span>\n\n    <span class=\"n\">title</span> <span class=\"o\">=</span> <span class=\"n\">fields</span><span class=\"o\">.</span><span class=\"n\">Str</span><span class=\"p\">()</span>\n    <span class=\"n\">release_date</span> <span class=\"o\">=</span> <span class=\"n\">fields</span><span class=\"o\">.</span><span class=\"n\">Date</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Input data frame to validate.</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-10\"</span><span class=\"p\">},</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-11\"</span><span class=\"p\">},</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-3-11\"</span><span class=\"p\">},</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_3\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-21\"</span><span class=\"p\">},</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_3\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-21\"</span><span class=\"p\">},</span>\n        <span class=\"p\">{</span><span class=\"s2\">\"title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"title_4\"</span><span class=\"p\">,</span> <span class=\"s2\">\"release_date\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2020-1-51\"</span><span class=\"p\">},</span>\n    <span class=\"p\">])</span>\n\n<span class=\"c1\"># Validate data frame</span>\n<span class=\"n\">valid_df</span><span class=\"p\">,</span> <span class=\"n\">errors_df</span> <span class=\"o\">=</span> <span class=\"n\">AlbumSchema</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">validate_df</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n    \n<span class=\"c1\"># List of valid rows</span>\n<span class=\"n\">valid_rows</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">asDict</span><span class=\"p\">(</span><span class=\"n\">recursive</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">valid_df</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()]</span>\n<span class=\"c1\">#</span>\n<span class=\"c1\">#   [</span>\n<span class=\"c1\">#        {'title': 'title_1', 'release_date': datetime.date(2020, 1, 10)},</span>\n<span class=\"c1\">#        {'title': 'title_2', 'release_date': datetime.date(2020, 1, 11)},</span>\n<span class=\"c1\">#        {'title': 'title_3', 'release_date': datetime.date(2020, 1, 21)}</span>\n<span class=\"c1\">#   ]</span>\n<span class=\"c1\">#</span>\n\n<span class=\"c1\"># Rows with errors</span>\n<span class=\"n\">error_rows</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">asDict</span><span class=\"p\">(</span><span class=\"n\">recursive</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">errors_df</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()]</span>\n<span class=\"c1\"># </span>\n<span class=\"c1\">#   [</span>\n<span class=\"c1\">#        {'_errors': '{\"row\": {\"release_date\": \"2020-1-21\", \"title\": \"title_3\", '</span>\n<span class=\"c1\">#                    '\"__count__title\": 2, \"__count__release_date\": 2}, '</span>\n<span class=\"c1\">#                    '\"errors\": [\"duplicate row\"]}'},</span>\n<span class=\"c1\">#        {'_errors': '{\"row\": {\"release_date\": \"2020-1-51\", \"title\": \"title_4\", '</span>\n<span class=\"c1\">#                    '\"__count__title\": 1, \"__count__release_date\": 1}, '</span>\n<span class=\"c1\">#                    '\"errors\": {\"release_date\": [\"Not a valid date.\"]}}'},</span>\n<span class=\"c1\">#        {'_errors': '{\"row\": {\"release_date\": \"2020-3-11\", \"title\": \"title_2\", '</span>\n<span class=\"c1\">#                    '\"__count__title\": 2, \"__count__release_date\": 1}, '</span>\n<span class=\"c1\">#                    '\"errors\": [\"duplicate row\"]}'}</span>\n<span class=\"c1\">#    ]</span>\n<span class=\"c1\">#</span>\n</pre>\n<p><strong>WARNING</strong>: Duplicate check requires data shuffle per unique field. Having large number of unique fields will effect\nspark job performance. By default <code>UNIQUE</code> is set to an empty list preventing any duplicate checks.</p>\n<h3>Fields</h3>\n<p>Marshmallow comes with a variety of different fields that can be used to define schemas. Internally marshmallow-pyspark\nconvert these fields into pyspark SQL data types. The following table lists the supported marshmallow fields and their\nequivalent spark SQL data types:</p>\n<table>\n<thead>\n<tr>\n<th>Marshmallow</th>\n<th>PySpark</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>String</code></td>\n<td><code>StringType</code></td>\n</tr>\n<tr>\n<td><code>DateTime</code></td>\n<td><code>TimestampType</code></td>\n</tr>\n<tr>\n<td><code>Date</code></td>\n<td><code>DateType</code></td>\n</tr>\n<tr>\n<td><code>Boolean</code></td>\n<td><code>BooleanType</code></td>\n</tr>\n<tr>\n<td><code>Integer</code></td>\n<td><code>IntegerType</code></td>\n</tr>\n<tr>\n<td><code>Float</code></td>\n<td><code>FloatType</code></td>\n</tr>\n<tr>\n<td><code>Number</code></td>\n<td><code>DoubleType</code></td>\n</tr>\n<tr>\n<td><code>List</code></td>\n<td><code>ArrayType</code></td>\n</tr>\n<tr>\n<td><code>Dict</code></td>\n<td><code>MapType</code></td>\n</tr>\n<tr>\n<td><code>Nested</code></td>\n<td><code>StructType</code></td>\n</tr></tbody></table>\n<p>By default the <code>StringType</code> data type is used for marshmallow fields not in the above table. The <code>spark_schema</code> property\nof your defined schema can be used to check the converted spark SQL schema:</p>\n<pre><span class=\"c1\"># Gets the spark schema for the Album schema</span>\n<span class=\"n\">AlbumSchema</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">spark_schema</span>\n<span class=\"c1\"># StructType(List(StructField(title,StringType,true),StructField(release_date,DateType,true),StructField(_errors,StringType,true)))</span>\n</pre>\n<h4>Custom Fields</h4>\n<p>It is also possible to add support for custom marshmallow fields, or those missing in the above table. In order to do so,\nyou would need to create a converter for the custom field. The converter can be built using the <code>ConverterABC</code> interface:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">marshmallow_pyspark</span> <span class=\"kn\">import</span> <span class=\"n\">ConverterABC</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.types</span> <span class=\"kn\">import</span> <span class=\"n\">StringType</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">EmailConverter</span><span class=\"p\">(</span><span class=\"n\">ConverterABC</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">        Converter to convert marshmallow's Email field to a pyspark </span>\n<span class=\"sd\">        SQL data type.</span>\n<span class=\"sd\">    \"\"\"</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">convert</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">ma_field</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">StringType</span><span class=\"p\">()</span>\n</pre>\n<p>The <code>ma_field</code> argument in the <code>convert</code> method is provided to handle nested fields. For an example you can checkout\n<code>NestedConverter</code>. Now the final step would be to add the converter to the <code>CONVERTER_MAP</code> attribute of your schema:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">marshmallow_pyspark</span> <span class=\"kn\">import</span> <span class=\"n\">Schema</span>\n<span class=\"kn\">from</span> <span class=\"nn\">marshmallow</span> <span class=\"kn\">import</span> <span class=\"n\">fields</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">User</span><span class=\"p\">(</span><span class=\"n\">Schema</span><span class=\"p\">):</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"n\">fields</span><span class=\"o\">.</span><span class=\"n\">String</span><span class=\"p\">(</span><span class=\"n\">required</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">email</span> <span class=\"o\">=</span> <span class=\"n\">fields</span><span class=\"o\">.</span><span class=\"n\">Email</span><span class=\"p\">(</span><span class=\"n\">required</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Adding email converter to schema.</span>\n<span class=\"n\">User</span><span class=\"o\">.</span><span class=\"n\">CONVERTER_MAP</span><span class=\"p\">[</span><span class=\"n\">fields</span><span class=\"o\">.</span><span class=\"n\">Email</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">EmailConverter</span>\n\n<span class=\"c1\"># You can now use your schema to validate the input data frame.</span>\n<span class=\"n\">valid_df</span><span class=\"p\">,</span> <span class=\"n\">errors_df</span> <span class=\"o\">=</span> <span class=\"n\">User</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">validate_df</span><span class=\"p\">(</span><span class=\"n\">input_df</span><span class=\"p\">)</span>\n</pre>\n<h2>Development</h2>\n<p>To hack marshmallow-pyspark locally run:</p>\n<pre>$ pip install -e .<span class=\"o\">[</span>dev<span class=\"o\">]</span>\t\t\t<span class=\"c1\"># to install all dependencies</span>\n$ pytest --cov-config .coveragerc --cov<span class=\"o\">=</span>./\t\t\t<span class=\"c1\"># to get coverage report</span>\n$ pylint marshmallow_pyspark\t\t\t<span class=\"c1\"># to check code quality with PyLint</span>\n</pre>\n<p>Optionally you can use <code>make</code> to perform development tasks.</p>\n<h2>License</h2>\n<p>The source code is licensed under Apache License Version 2.</p>\n<h2>Contributions</h2>\n<p>Pull requests always welcomed! :)</p>\n\n          </div>"}, "last_serial": 6625848, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "7d0a8b93ffb68e19af7781f32c91e51c", "sha256": "cc4fbaea3b9254532b71fa112cf4cda16e6930bd2b95b151cd99b2057e086a97"}, "downloads": -1, "filename": "marshmallow-pyspark-0.1.0.tar.gz", "has_sig": false, "md5_digest": "7d0a8b93ffb68e19af7781f32c91e51c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 7177, "upload_time": "2020-02-10T23:24:56", "upload_time_iso_8601": "2020-02-10T23:24:56.123820Z", "url": "https://files.pythonhosted.org/packages/17/57/07d6c9060c89d14ef0898bb94804fd354c69c15474fda4c586aa00e31b69/marshmallow-pyspark-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "d11a1ed852f04d4541eb9f3e42c21ac0", "sha256": "fa0682112fc78b694b0b69eb652cc853a42d35177760cba90572569ad4bd5672"}, "downloads": -1, "filename": "marshmallow-pyspark-0.2.0.tar.gz", "has_sig": false, "md5_digest": "d11a1ed852f04d4541eb9f3e42c21ac0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 11951, "upload_time": "2020-02-13T20:49:17", "upload_time_iso_8601": "2020-02-13T20:49:17.546851Z", "url": "https://files.pythonhosted.org/packages/a1/1d/01551aa3e41ea03e50aea5e1ec611bf0390138bbf66e48c71048650fcd39/marshmallow-pyspark-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d11a1ed852f04d4541eb9f3e42c21ac0", "sha256": "fa0682112fc78b694b0b69eb652cc853a42d35177760cba90572569ad4bd5672"}, "downloads": -1, "filename": "marshmallow-pyspark-0.2.0.tar.gz", "has_sig": false, "md5_digest": "d11a1ed852f04d4541eb9f3e42c21ac0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 11951, "upload_time": "2020-02-13T20:49:17", "upload_time_iso_8601": "2020-02-13T20:49:17.546851Z", "url": "https://files.pythonhosted.org/packages/a1/1d/01551aa3e41ea03e50aea5e1ec611bf0390138bbf66e48c71048650fcd39/marshmallow-pyspark-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:57:34 2020"}