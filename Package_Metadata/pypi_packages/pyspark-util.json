{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "# pyspark-util\n\nA set of pyspark utility functions.\n\n```python\nimport pyspark_util as psu\n\ndata = [(1, 2, 3)]\ncolumns = ['a', 'b', 'c']\ndf = spark.createDataFrame(data, columns)\nprefixed = psu.prefix_columns(df, 'x')\nprefixed.show()\n\n# output:\n+---+---+---+\n|x_a|x_b|x_c|\n+---+---+---+\n|  1|  2|  3|\n+---+---+---+\n```\n\n## Development\n\n### Setup\n\n```\ndocker-compose build\ndocker-compose up -d\n```\n\n### Lint\n\n```\ndocker exec psu-cnt ./tools/lint.sh\n```\n\n### Test\n\n```\ndocker exec psu-cnt ./tools/test.sh\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/harupy/pyspark-util", "keywords": "", "license": "", "maintainer": "harupy", "maintainer_email": "hkawamura0130@gmail.com", "name": "pyspark-util", "package_url": "https://pypi.org/project/pyspark-util/", "platform": "", "project_url": "https://pypi.org/project/pyspark-util/", "project_urls": {"Bug Tracker": "https://github.com/harupy/pyspark-util/issues", "Homepage": "https://github.com/harupy/pyspark-util", "Source Code": "https://github.com/harupy/pyspark-util"}, "release_url": "https://pypi.org/project/pyspark-util/0.1.2/", "requires_dist": ["pyspark (>=2.4.4)"], "requires_python": ">=3.5", "summary": "PySpark utility functions", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>pyspark-util</h1>\n<p>A set of pyspark utility functions.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pyspark_util</span> <span class=\"k\">as</span> <span class=\"nn\">psu</span>\n\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)]</span>\n<span class=\"n\">columns</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">,</span> <span class=\"s1\">'c'</span><span class=\"p\">]</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"p\">)</span>\n<span class=\"n\">prefixed</span> <span class=\"o\">=</span> <span class=\"n\">psu</span><span class=\"o\">.</span><span class=\"n\">prefix_columns</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"s1\">'x'</span><span class=\"p\">)</span>\n<span class=\"n\">prefixed</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># output:</span>\n<span class=\"o\">+---+---+---+</span>\n<span class=\"o\">|</span><span class=\"n\">x_a</span><span class=\"o\">|</span><span class=\"n\">x_b</span><span class=\"o\">|</span><span class=\"n\">x_c</span><span class=\"o\">|</span>\n<span class=\"o\">+---+---+---+</span>\n<span class=\"o\">|</span>  <span class=\"mi\">1</span><span class=\"o\">|</span>  <span class=\"mi\">2</span><span class=\"o\">|</span>  <span class=\"mi\">3</span><span class=\"o\">|</span>\n<span class=\"o\">+---+---+---+</span>\n</pre>\n<h2>Development</h2>\n<h3>Setup</h3>\n<pre><code>docker-compose build\ndocker-compose up -d\n</code></pre>\n<h3>Lint</h3>\n<pre><code>docker exec psu-cnt ./tools/lint.sh\n</code></pre>\n<h3>Test</h3>\n<pre><code>docker exec psu-cnt ./tools/test.sh\n</code></pre>\n\n          </div>"}, "last_serial": 6312318, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "051d3c13463abaa196e4ba0030eb478e", "sha256": "4b310d283bd18e4d3e20cf9cc2e2f0453e98e57040613ede1082cea1e6c7397b"}, "downloads": -1, "filename": "pyspark_util-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "051d3c13463abaa196e4ba0030eb478e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 3306, "upload_time": "2019-12-16T16:36:13", "upload_time_iso_8601": "2019-12-16T16:36:13.564718Z", "url": "https://files.pythonhosted.org/packages/83/0c/0acf6b0471dfee4a4c5969d87b462fce981bbc9c43a799df987c16b47cf8/pyspark_util-0.1.2-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "051d3c13463abaa196e4ba0030eb478e", "sha256": "4b310d283bd18e4d3e20cf9cc2e2f0453e98e57040613ede1082cea1e6c7397b"}, "downloads": -1, "filename": "pyspark_util-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "051d3c13463abaa196e4ba0030eb478e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 3306, "upload_time": "2019-12-16T16:36:13", "upload_time_iso_8601": "2019-12-16T16:36:13.564718Z", "url": "https://files.pythonhosted.org/packages/83/0c/0acf6b0471dfee4a4c5969d87b462fce981bbc9c43a799df987c16b47cf8/pyspark_util-0.1.2-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 02:56:17 2020"}