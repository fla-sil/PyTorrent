{"info": {"author": "Christian Watts", "author_email": "civrev@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "RLRisk\n==============================\n\nA reinforcement learning environment based off the board game Risk. Designed with reinforcement learning in mind, this project aims to streamline research for economy-based games.\n\nGetting Started\n------------------------------\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n\nDependencies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n- numpy <= 1.14\n- pygame <= 1.9\n\nUser installation\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe easiest way to install this package is through PyPi\n\n::\n\n    pip install rlrisk\n\nOtherwise you can manually install this package by cloning\nthe repository to your local computer\n\n::\n\n    git clone https://github.com/civrev/RLRisk\n\nThen navigate to that directory and run setup.py\n\n::\n\n    python setup.py install\n\nWorking with RLRisk\n------------------------------\n\nRLRisk was a project concieved to streamline research by coding an environment already friendly to reinforcement learning techniques. As such working with RLRisk is extremely easy.\n\nThe Environment\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe environment is the standard Wolrd Domination game rules for Risk by Hasbro. You can find these rules here: https://www.hasbro.com/common/instruct/risk.pdf\n\nThe environment is located in the file risk.py and is implemented using the class Risk(). The class is flexible enough to be initiallized with extremely custom rules, but for the most common game of Risk you can create a Risk object using only a list of players\n\n::\n\n    from rlrisk.agents import *\n    from rlrisk.environment import *\n\n    players = [BaseAgent(), AggressiveAgent()]\n    env = Risk(players)\n    results = env.play()\n\nAgents\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRLRisk comes with 3 agents, the BaseAgent, Human, and AggressiveAgent classes. All new agents must be subclasses of the BaseAgent class, but looking at the BaseAgent will show you that the framework of agents for RLRisk is very straight forward. It takes in the information it need to make a decision, and then it outputs a decision.\n\nFinal Remarks\n------------------------------\nThis is my Senoir Project for my B.S. in Computer Science at the University of North Georgia\n(Graduating May, 2018)\n\n\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/civrev/rlrisk", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "rlrisk", "package_url": "https://pypi.org/project/rlrisk/", "platform": "", "project_url": "https://pypi.org/project/rlrisk/", "project_urls": {"Homepage": "https://github.com/civrev/rlrisk"}, "release_url": "https://pypi.org/project/rlrisk/1.0.2.3/", "requires_dist": ["numpy (>=1.14.1)", "pygame (>=1.9.3)"], "requires_python": "~=3.3", "summary": "A reinforcement learning environment based off the board game Risk", "version": "1.0.2.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>A reinforcement learning environment based off the board game Risk. Designed with reinforcement learning in mind, this project aims to streamline research for economy-based games.</p>\n<div id=\"getting-started\">\n<h2>Getting Started</h2>\n<p>These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.</p>\n<div id=\"dependencies\">\n<h3>Dependencies</h3>\n<ul>\n<li>numpy &lt;= 1.14</li>\n<li>pygame &lt;= 1.9</li>\n</ul>\n</div>\n<div id=\"user-installation\">\n<h3>User installation</h3>\n<p>The easiest way to install this package is through PyPi</p>\n<pre>pip install rlrisk\n</pre>\n<p>Otherwise you can manually install this package by cloning\nthe repository to your local computer</p>\n<pre>git clone https://github.com/civrev/RLRisk\n</pre>\n<p>Then navigate to that directory and run setup.py</p>\n<pre>python setup.py install\n</pre>\n</div>\n</div>\n<div id=\"working-with-rlrisk\">\n<h2>Working with RLRisk</h2>\n<p>RLRisk was a project concieved to streamline research by coding an environment already friendly to reinforcement learning techniques. As such working with RLRisk is extremely easy.</p>\n<div id=\"the-environment\">\n<h3>The Environment</h3>\n<p>The environment is the standard Wolrd Domination game rules for Risk by Hasbro. You can find these rules here: <a href=\"https://www.hasbro.com/common/instruct/risk.pdf\" rel=\"nofollow\">https://www.hasbro.com/common/instruct/risk.pdf</a></p>\n<p>The environment is located in the file risk.py and is implemented using the class Risk(). The class is flexible enough to be initiallized with extremely custom rules, but for the most common game of Risk you can create a Risk object using only a list of players</p>\n<pre>from rlrisk.agents import *\nfrom rlrisk.environment import *\n\nplayers = [BaseAgent(), AggressiveAgent()]\nenv = Risk(players)\nresults = env.play()\n</pre>\n</div>\n<div id=\"agents\">\n<h3>Agents</h3>\n<p>RLRisk comes with 3 agents, the BaseAgent, Human, and AggressiveAgent classes. All new agents must be subclasses of the BaseAgent class, but looking at the BaseAgent will show you that the framework of agents for RLRisk is very straight forward. It takes in the information it need to make a decision, and then it outputs a decision.</p>\n</div>\n</div>\n<div id=\"final-remarks\">\n<h2>Final Remarks</h2>\n<p>This is my Senoir Project for my B.S. in Computer Science at the University of North Georgia\n(Graduating May, 2018)</p>\n</div>\n\n          </div>"}, "last_serial": 3805459, "releases": {"1.0.2.2": [{"comment_text": "", "digests": {"md5": "68a19d8360ce6740bb0d8276f1a923c8", "sha256": "0726bc79b6c0bee8376e76bb1aabc58f1846a5b5456275355cb16496008aadb2"}, "downloads": -1, "filename": "rlrisk-1.0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "68a19d8360ce6740bb0d8276f1a923c8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "~=3.3", "size": 984347, "upload_time": "2018-04-25T03:58:45", "upload_time_iso_8601": "2018-04-25T03:58:45.580878Z", "url": "https://files.pythonhosted.org/packages/01/c6/be7fc2ee8512f751b4b3db26042b21b5133fcb4d2ff80d0a09b6d22a480c/rlrisk-1.0.2.2-py3-none-any.whl", "yanked": false}], "1.0.2.3": [{"comment_text": "", "digests": {"md5": "f6ed014731a85b0d2e61310c836ec4f9", "sha256": "8078edeeb6e69b9fe4299b65f3d98510f9be20cadce9f94af266bfa0b8e85769"}, "downloads": -1, "filename": "rlrisk-1.0.2.3-py3-none-any.whl", "has_sig": false, "md5_digest": "f6ed014731a85b0d2e61310c836ec4f9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "~=3.3", "size": 984347, "upload_time": "2018-04-25T04:00:55", "upload_time_iso_8601": "2018-04-25T04:00:55.173134Z", "url": "https://files.pythonhosted.org/packages/78/e4/8c617db01e8ac3257c1d42b09aa9a730791d937960da2d5183baef348ffc/rlrisk-1.0.2.3-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f6ed014731a85b0d2e61310c836ec4f9", "sha256": "8078edeeb6e69b9fe4299b65f3d98510f9be20cadce9f94af266bfa0b8e85769"}, "downloads": -1, "filename": "rlrisk-1.0.2.3-py3-none-any.whl", "has_sig": false, "md5_digest": "f6ed014731a85b0d2e61310c836ec4f9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "~=3.3", "size": 984347, "upload_time": "2018-04-25T04:00:55", "upload_time_iso_8601": "2018-04-25T04:00:55.173134Z", "url": "https://files.pythonhosted.org/packages/78/e4/8c617db01e8ac3257c1d42b09aa9a730791d937960da2d5183baef348ffc/rlrisk-1.0.2.3-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:02:15 2020"}