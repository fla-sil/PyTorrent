{"info": {"author": "B.Bapusiri, Suresh Jagannathan, D.Venkata Vara Prasad", "author_email": "siribapu@gmail.com , drjs72@outlook.com, dvvprasad@ssn.edu.in", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Topic :: Software Development :: Build Tools"], "description": "Sentiment Analysis(SA) is the use of natural language processing, statistics and text analysis to extract and identify the sentiment of text into positive, negative or neutral categories. The main objective is to construct a model to perform sentiment analysis for postive, negative and sarcastic sentences using RNN technique. The dataset is cleaned (removal of stop words and HTML tags). Word Vectors are generated for this using GloVe and Word2Vec. \n\nSA using Recurrent Neural Network (RNN).\n======================================== \n\nRNN is a class of artificial neural network where connections between units form a directed cycle. This allows it to exhibit dynamic temporal behavior. The hidden layer in RNN acts as storage for the network. The main difference between the normal neural network and RNN is global parameters(such as weights and bias) used, the network is temporal and dynamic since the network vary in size according to the size of the input and same task executed at each timestamp with different inputs. RNN works on temporal data, at each timestamp, a word is taken as  input and the next word will be the output to the network. The process will repeat until the end of sentence i.e, at first timestamp, the first word is given, it will give the second word as output. At second timestamp, second word is given as input, third word will get retrieved as output. This is how the network gets trained. If a sentence contains n words, it needs (n-1) timestamps. At last timestamp, the hidden layer values get stored further given to MLP for classification. The labelling has been done manually. \n\nUsage:\n==========\n\n1) Generate GloVe and Word2vec vectors of your required dimensions(Eg: 100,200,300) or download pre-generated vectors of both.\n\n2) Change the parameter dimension according to the word vector dimensionality\n\n3) Give appropriate file paths.\n\n4) Run sa.py as shown below.\n\n\t\" python ./sa.py -word_embedding W2V/GloVe/Both 'File_path that contains train and test folders' \"\n\n\n\n\nCode Details:\n==============\nsa.py:\n\tMain program to run code.\n\nmain.py : \n\tLoads GloVe for each sentence, calls RNN for a word in sentence and writes the S_t values to CSV File.\n\ndemo.sh, eval and SRC:\n\tThe code to produce the GloVe vectors.\n\nMain_GloVe.py: \n\tCall GloVe code to generate the word vectors. GloVe is generated using the code from Github link \"https://github.com/stanfordnlp/GloVe\" . This Github code produces the word vector file.\n\nGloVe_Extraction.py :\n\tThis code will load all those vectors corresponding to the words in sentences.   By every time the function called, word vector for a sentence is returned.\n\nMain_W2V.py:\n\tGenerates the Word2Vec by the calling W2V code. And this task\t is done using NLTK tool. \n\nW2VGenerate.py:\n\tProduce word vectors.\n\nRNN.py : \n\tThis code will take one word at each timestamp in sequence outputs immediate word. The parameters U, V, W, b1, b2 are parameters that are shared through out the network. It returns hidden layer values (S_t).\n\nMLP.py:\n\tThis is mainly used to classified the sentiment of the text. The Features extracted from the RNN is given as a input to this Multi-layer Perceptron.\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/fordra/Sample", "keywords": "Sentiment Analysis,RNN,Glove,Word2Vec,Sarcasm", "license": "", "maintainer": "", "maintainer_email": "", "name": "SampleSa", "package_url": "https://pypi.org/project/SampleSa/", "platform": "", "project_url": "https://pypi.org/project/SampleSa/", "project_urls": {"Bug Reports": "https://github.com/fordra/Sample/issues", "Funding": "https://donate.pypi.org", "Homepage": "https://github.com/fordra/Sample", "Say Thanks!": "http://saythanks.io/to/example", "Source": "https://github.com/fordra/Sample/"}, "release_url": "https://pypi.org/project/SampleSa/1.1.1/", "requires_dist": null, "requires_python": "", "summary": "Sentiment analysis using RNN", "version": "1.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Sentiment Analysis(SA) is the use of natural language processing, statistics and text analysis to extract and identify the sentiment of text into positive, negative or neutral categories. The main objective is to construct a model to perform sentiment analysis for postive, negative and sarcastic sentences using RNN technique. The dataset is cleaned (removal of stop words and HTML tags). Word Vectors are generated for this using GloVe and Word2Vec.</p>\n<div id=\"sa-using-recurrent-neural-network-rnn\">\n<h2>SA using Recurrent Neural Network (RNN).</h2>\n<p>RNN is a class of artificial neural network where connections between units form a directed cycle. This allows it to exhibit dynamic temporal behavior. The hidden layer in RNN acts as storage for the network. The main difference between the normal neural network and RNN is global parameters(such as weights and bias) used, the network is temporal and dynamic since the network vary in size according to the size of the input and same task executed at each timestamp with different inputs. RNN works on temporal data, at each timestamp, a word is taken as  input and the next word will be the output to the network. The process will repeat until the end of sentence i.e, at first timestamp, the first word is given, it will give the second word as output. At second timestamp, second word is given as input, third word will get retrieved as output. This is how the network gets trained. If a sentence contains n words, it needs (n-1) timestamps. At last timestamp, the hidden layer values get stored further given to MLP for classification. The labelling has been done manually.</p>\n</div>\n<div id=\"usage\">\n<h2>Usage:</h2>\n<ol>\n<li><p>Generate GloVe and Word2vec vectors of your required dimensions(Eg: 100,200,300) or download pre-generated vectors of both.</p>\n</li>\n<li><p>Change the parameter dimension according to the word vector dimensionality</p>\n</li>\n<li><p>Give appropriate file paths.</p>\n</li>\n<li><p>Run sa.py as shown below.</p>\n<blockquote>\n<p>\u201d python ./sa.py -word_embedding W2V/GloVe/Both \u2018File_path that contains train and test folders\u2019 \u201c</p>\n</blockquote>\n</li>\n</ol>\n</div>\n<div id=\"code-details\">\n<h2>Code Details:</h2>\n<dl>\n<dt>sa.py:</dt>\n<dd>Main program to run code.</dd>\n<dt>main.py :</dt>\n<dd>Loads GloVe for each sentence, calls RNN for a word in sentence and writes the S_t values to CSV File.</dd>\n<dt>demo.sh, eval and SRC:</dt>\n<dd>The code to produce the GloVe vectors.</dd>\n<dt>Main_GloVe.py:</dt>\n<dd>Call GloVe code to generate the word vectors. GloVe is generated using the code from Github link \u201c<a href=\"https://github.com/stanfordnlp/GloVe\" rel=\"nofollow\">https://github.com/stanfordnlp/GloVe</a>\u201d . This Github code produces the word vector file.</dd>\n<dt>GloVe_Extraction.py :</dt>\n<dd>This code will load all those vectors corresponding to the words in sentences.   By every time the function called, word vector for a sentence is returned.</dd>\n<dt>Main_W2V.py:</dt>\n<dd>Generates the Word2Vec by the calling W2V code. And this task    is done using NLTK tool.</dd>\n<dt>W2VGenerate.py:</dt>\n<dd>Produce word vectors.</dd>\n<dt>RNN.py :</dt>\n<dd>This code will take one word at each timestamp in sequence outputs immediate word. The parameters U, V, W, b1, b2 are parameters that are shared through out the network. It returns hidden layer values (S_t).</dd>\n<dt>MLP.py:</dt>\n<dd>This is mainly used to classified the sentiment of the text. The Features extracted from the RNN is given as a input to this Multi-layer Perceptron.</dd>\n</dl>\n</div>\n\n          </div>"}, "last_serial": 4298901, "releases": {"1.1.1": [{"comment_text": "", "digests": {"md5": "b1e65f0a07c45ed8cb1d1eb3e1030f61", "sha256": "622f9fcb9de7cf684796e93dc2b294d5c5d2a5cf89bf9c281eae8a92e206d394"}, "downloads": -1, "filename": "SampleSa-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b1e65f0a07c45ed8cb1d1eb3e1030f61", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2817, "upload_time": "2018-09-22T05:32:18", "upload_time_iso_8601": "2018-09-22T05:32:18.536762Z", "url": "https://files.pythonhosted.org/packages/da/2d/2a2ce03cb09724ee49305c2097b145ded4d85282c025329d87a7aad8c785/SampleSa-1.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5ed55d4a3970f8c311d4500f248c2c9a", "sha256": "eccb14734cb0b3bb7ee2e581a007205a13b668d17dbabd0d033d883448e20681"}, "downloads": -1, "filename": "SampleSa-1.1.1.tar.gz", "has_sig": false, "md5_digest": "5ed55d4a3970f8c311d4500f248c2c9a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2485, "upload_time": "2018-09-22T05:32:19", "upload_time_iso_8601": "2018-09-22T05:32:19.994024Z", "url": "https://files.pythonhosted.org/packages/41/4f/dfa6b79e5ddfdb2f66d7550b5a8f5716ccf728e821dcee746048a5c7a66c/SampleSa-1.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b1e65f0a07c45ed8cb1d1eb3e1030f61", "sha256": "622f9fcb9de7cf684796e93dc2b294d5c5d2a5cf89bf9c281eae8a92e206d394"}, "downloads": -1, "filename": "SampleSa-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b1e65f0a07c45ed8cb1d1eb3e1030f61", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2817, "upload_time": "2018-09-22T05:32:18", "upload_time_iso_8601": "2018-09-22T05:32:18.536762Z", "url": "https://files.pythonhosted.org/packages/da/2d/2a2ce03cb09724ee49305c2097b145ded4d85282c025329d87a7aad8c785/SampleSa-1.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5ed55d4a3970f8c311d4500f248c2c9a", "sha256": "eccb14734cb0b3bb7ee2e581a007205a13b668d17dbabd0d033d883448e20681"}, "downloads": -1, "filename": "SampleSa-1.1.1.tar.gz", "has_sig": false, "md5_digest": "5ed55d4a3970f8c311d4500f248c2c9a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2485, "upload_time": "2018-09-22T05:32:19", "upload_time_iso_8601": "2018-09-22T05:32:19.994024Z", "url": "https://files.pythonhosted.org/packages/41/4f/dfa6b79e5ddfdb2f66d7550b5a8f5716ccf728e821dcee746048a5c7a66c/SampleSa-1.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:58:55 2020"}