{"info": {"author": "Cristi Constantin", "author_email": "cristi.constantin@speedpost.net", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3 :: Only", "Topic :: Software Development"], "description": "# Scrapy-link-filter\n\n  [![Python ver][python-image]][python-url]\n  [![Build Status][build-image]][build-url]\n  [![Code coverage][cover-image]][cover-url]\n  [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n\nSpider Middleware that allows a [Scrapy Spider](https://scrapy.readthedocs.io/en/latest/topics/spiders.html) to filter requests.\nThere is similar functionality in the [CrawlSpider](https://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider) already using Rules and in the [RobotsTxtMiddleware](https://scrapy.readthedocs.io/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.robotstxt), but there are twists.\nThis middleware allows defining rules dinamically per request, or as spider arguments instead of project settings.\n\n\n## Install\n\nThis project requires [Python 3.6+](https://www.python.org/) and [pip](https://pip.pypa.io/). Using a [virtual environment](https://virtualenv.pypa.io/) is strongly encouraged.\n\n```sh\n$ pip install git+https://github.com/croqaz/scrapy-link-filter\n```\n\n\n## Usage\n\nFor the middleware to be enabled as a Spider Middleware, it must be added in the project `settings.py`:\n\n```\nSPIDER_MIDDLEWARES = {\n    # maybe other Spider Middlewares ...\n    # can go after DepthMiddleware: 900\n    'scrapy_link_filter.middleware.LinkFilterMiddleware': 950,\n}\n```\n\nOr, it can be enabled as a Downloader Middleware, in the project `settings.py`:\n\n```\nDOWNLOADER_MIDDLEWARES = {\n    # maybe other Downloader Middlewares ...\n    # can go before RobotsTxtMiddleware: 100\n    'scrapy_link_filter.middleware.LinkFilterMiddleware': 50,\n}\n```\n\nThe rules must be defined either in the spider instance, in a `spider.extract_rules` dict, or per request, in `request.meta['extract_rules']`.\nInternally, the extract_rules dict is converted into a [LinkExtractor](https://docs.scrapy.org/en/latest/topics/link-extractors.html), which is used to match the requests.\n\n**Note** that the URL matching is case-sensitive by default, which works in most cases. To enable case-insensitive matching, you can specify a \"(?i)\" inline flag in the beggining of each \"allow\", or \"deny\" rule that needs to be case-insensitive.\n\n\nExample of a specific allow filter, on a spider instance:\n\n```py\nfrom scrapy.spiders import Spider\n\nclass MySpider(Spider):\n    extract_rules = {\"allow_domains\": \"example.com\", \"allow\": \"/en/items/\"}\n```\n\nOr a specific deny filter, inside a request meta:\n\n```py\nrequest.meta['extract_rules'] = {\n    \"deny_domains\": [\"whatever.com\", \"ignore.me\"],\n    \"deny\": [\"/privacy-policy/?$\", \"/about-?(us)?$\"]\n}\n```\n\nThe possible fields are:\n* `allow_domains` and `deny_domains` - one, or more domains to specifically limit to, or specifically reject\n* `allow` and `deny` - one, or more sub-strings, or patterns to specifically allow, or reject\n\nAll fields can be defined as string, list, set, or tuple.\n\n-----\n\n## License\n\n[BSD3](LICENSE) \u00a9 Cristi Constantin.\n\n\n[build-image]: https://github.com/croqaz/scrapy-link-filter/workflows/Python/badge.svg\n[build-url]: https://github.com/croqaz/scrapy-link-filter/actions\n[cover-image]: https://codecov.io/gh/croqaz/scrapy-link-filter/branch/master/graph/badge.svg\n[cover-url]: https://codecov.io/gh/croqaz/scrapy-link-filter\n[python-image]: https://img.shields.io/badge/Python-3.6-blue.svg\n[python-url]: https://python.org\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/croqaz/scrapy-link-filter", "keywords": "scrapy link filter", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "scrapy-link-filter", "package_url": "https://pypi.org/project/scrapy-link-filter/", "platform": "Any", "project_url": "https://pypi.org/project/scrapy-link-filter/", "project_urls": {"Homepage": "https://github.com/croqaz/scrapy-link-filter"}, "release_url": "https://pypi.org/project/scrapy-link-filter/0.2.0/", "requires_dist": null, "requires_python": ">=3.6.0", "summary": "Scrapy Middleware that allows a Scrapy Spider to filter requests.", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Scrapy-link-filter</h1>\n<p><a href=\"https://python.org\" rel=\"nofollow\"><img alt=\"Python ver\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/03d2248f0737a4e1007a90c83c65c7e25ad6db61/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e362d626c75652e737667\"></a>\n<a href=\"https://github.com/croqaz/scrapy-link-filter/actions\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/063ed8761143877bda5424b4c7aac10c9073e077/68747470733a2f2f6769746875622e636f6d2f63726f71617a2f7363726170792d6c696e6b2d66696c7465722f776f726b666c6f77732f507974686f6e2f62616467652e737667\"></a>\n<a href=\"https://codecov.io/gh/croqaz/scrapy-link-filter\" rel=\"nofollow\"><img alt=\"Code coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6f367f9c5126cc443869622a066b4fe31f88c719/68747470733a2f2f636f6465636f762e696f2f67682f63726f71617a2f7363726170792d6c696e6b2d66696c7465722f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<a href=\"https://github.com/ambv/black\" rel=\"nofollow\"><img alt=\"Code style: black\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fbfdc7754183ecf079bc71ddeabaf88f6cbc5c00/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\"></a></p>\n<p>Spider Middleware that allows a <a href=\"https://scrapy.readthedocs.io/en/latest/topics/spiders.html\" rel=\"nofollow\">Scrapy Spider</a> to filter requests.\nThere is similar functionality in the <a href=\"https://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider\" rel=\"nofollow\">CrawlSpider</a> already using Rules and in the <a href=\"https://scrapy.readthedocs.io/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.robotstxt\" rel=\"nofollow\">RobotsTxtMiddleware</a>, but there are twists.\nThis middleware allows defining rules dinamically per request, or as spider arguments instead of project settings.</p>\n<h2>Install</h2>\n<p>This project requires <a href=\"https://www.python.org/\" rel=\"nofollow\">Python 3.6+</a> and <a href=\"https://pip.pypa.io/\" rel=\"nofollow\">pip</a>. Using a <a href=\"https://virtualenv.pypa.io/\" rel=\"nofollow\">virtual environment</a> is strongly encouraged.</p>\n<pre>$ pip install git+https://github.com/croqaz/scrapy-link-filter\n</pre>\n<h2>Usage</h2>\n<p>For the middleware to be enabled as a Spider Middleware, it must be added in the project <code>settings.py</code>:</p>\n<pre><code>SPIDER_MIDDLEWARES = {\n    # maybe other Spider Middlewares ...\n    # can go after DepthMiddleware: 900\n    'scrapy_link_filter.middleware.LinkFilterMiddleware': 950,\n}\n</code></pre>\n<p>Or, it can be enabled as a Downloader Middleware, in the project <code>settings.py</code>:</p>\n<pre><code>DOWNLOADER_MIDDLEWARES = {\n    # maybe other Downloader Middlewares ...\n    # can go before RobotsTxtMiddleware: 100\n    'scrapy_link_filter.middleware.LinkFilterMiddleware': 50,\n}\n</code></pre>\n<p>The rules must be defined either in the spider instance, in a <code>spider.extract_rules</code> dict, or per request, in <code>request.meta['extract_rules']</code>.\nInternally, the extract_rules dict is converted into a <a href=\"https://docs.scrapy.org/en/latest/topics/link-extractors.html\" rel=\"nofollow\">LinkExtractor</a>, which is used to match the requests.</p>\n<p><strong>Note</strong> that the URL matching is case-sensitive by default, which works in most cases. To enable case-insensitive matching, you can specify a \"(?i)\" inline flag in the beggining of each \"allow\", or \"deny\" rule that needs to be case-insensitive.</p>\n<p>Example of a specific allow filter, on a spider instance:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">scrapy.spiders</span> <span class=\"kn\">import</span> <span class=\"n\">Spider</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">MySpider</span><span class=\"p\">(</span><span class=\"n\">Spider</span><span class=\"p\">):</span>\n    <span class=\"n\">extract_rules</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"allow_domains\"</span><span class=\"p\">:</span> <span class=\"s2\">\"example.com\"</span><span class=\"p\">,</span> <span class=\"s2\">\"allow\"</span><span class=\"p\">:</span> <span class=\"s2\">\"/en/items/\"</span><span class=\"p\">}</span>\n</pre>\n<p>Or a specific deny filter, inside a request meta:</p>\n<pre><span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">'extract_rules'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"deny_domains\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"whatever.com\"</span><span class=\"p\">,</span> <span class=\"s2\">\"ignore.me\"</span><span class=\"p\">],</span>\n    <span class=\"s2\">\"deny\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"/privacy-policy/?$\"</span><span class=\"p\">,</span> <span class=\"s2\">\"/about-?(us)?$\"</span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</pre>\n<p>The possible fields are:</p>\n<ul>\n<li><code>allow_domains</code> and <code>deny_domains</code> - one, or more domains to specifically limit to, or specifically reject</li>\n<li><code>allow</code> and <code>deny</code> - one, or more sub-strings, or patterns to specifically allow, or reject</li>\n</ul>\n<p>All fields can be defined as string, list, set, or tuple.</p>\n<hr>\n<h2>License</h2>\n<p><a href=\"LICENSE\" rel=\"nofollow\">BSD3</a> \u00a9 Cristi Constantin.</p>\n\n          </div>"}, "last_serial": 6285826, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "a9d250b08095b6e707f8dad5fa301719", "sha256": "25093db548efc86823b6eb27db973c67b9877e0892713ceb676723d636be0b63"}, "downloads": -1, "filename": "scrapy_link_filter-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "a9d250b08095b6e707f8dad5fa301719", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 4931, "upload_time": "2019-12-12T13:16:41", "upload_time_iso_8601": "2019-12-12T13:16:41.940176Z", "url": "https://files.pythonhosted.org/packages/3e/b5/34c59d25a104dd87c90fd3f22f53f1bbbba080a06f3b0a230370c4a7606a/scrapy_link_filter-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "40db0993a46ff47482395c967187600a", "sha256": "eb562565d5ce4241d751f594998183516524b7a1f34d4dd4c3bf78ac090e3ddd"}, "downloads": -1, "filename": "scrapy-link-filter-0.1.1.tar.gz", "has_sig": false, "md5_digest": "40db0993a46ff47482395c967187600a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 4464, "upload_time": "2019-12-12T13:16:44", "upload_time_iso_8601": "2019-12-12T13:16:44.734449Z", "url": "https://files.pythonhosted.org/packages/03/6c/6e45ddd2686db395babbde8daaa5518bd13d6ff7335c2b8657808b102cae/scrapy-link-filter-0.1.1.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "d8319c5b016dd2c6f0a4f3e81e233b9a", "sha256": "f1f6c25569a765945a331daac3929731cd1f4e07ad129705ee53d0b6e65f9d36"}, "downloads": -1, "filename": "scrapy_link_filter-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d8319c5b016dd2c6f0a4f3e81e233b9a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 6214, "upload_time": "2019-12-12T13:17:11", "upload_time_iso_8601": "2019-12-12T13:17:11.647523Z", "url": "https://files.pythonhosted.org/packages/a8/d5/49b7e0fcd23809a513a59d9224812355d2b8424b672b3963e7d8155e4ba9/scrapy_link_filter-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c46a8775512a159c7e5898e48c779229", "sha256": "64bf701cbbbc9f51dad47094c1effbf9ca47b6d6e9f54c0f64cedefcbbbc72e4"}, "downloads": -1, "filename": "scrapy-link-filter-0.2.0.tar.gz", "has_sig": false, "md5_digest": "c46a8775512a159c7e5898e48c779229", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 5449, "upload_time": "2019-12-12T13:17:14", "upload_time_iso_8601": "2019-12-12T13:17:14.034780Z", "url": "https://files.pythonhosted.org/packages/3d/1c/175ecef969380c969abbd90f1063b8375fe68131f8a2c0c5995beb0b0a84/scrapy-link-filter-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d8319c5b016dd2c6f0a4f3e81e233b9a", "sha256": "f1f6c25569a765945a331daac3929731cd1f4e07ad129705ee53d0b6e65f9d36"}, "downloads": -1, "filename": "scrapy_link_filter-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d8319c5b016dd2c6f0a4f3e81e233b9a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 6214, "upload_time": "2019-12-12T13:17:11", "upload_time_iso_8601": "2019-12-12T13:17:11.647523Z", "url": "https://files.pythonhosted.org/packages/a8/d5/49b7e0fcd23809a513a59d9224812355d2b8424b672b3963e7d8155e4ba9/scrapy_link_filter-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c46a8775512a159c7e5898e48c779229", "sha256": "64bf701cbbbc9f51dad47094c1effbf9ca47b6d6e9f54c0f64cedefcbbbc72e4"}, "downloads": -1, "filename": "scrapy-link-filter-0.2.0.tar.gz", "has_sig": false, "md5_digest": "c46a8775512a159c7e5898e48c779229", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 5449, "upload_time": "2019-12-12T13:17:14", "upload_time_iso_8601": "2019-12-12T13:17:14.034780Z", "url": "https://files.pythonhosted.org/packages/3d/1c/175ecef969380c969abbd90f1063b8375fe68131f8a2c0c5995beb0b0a84/scrapy-link-filter-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:45 2020"}