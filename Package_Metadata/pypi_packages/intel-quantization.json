{"info": {"author": "intel", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# Quantization Python Programming API Quick Start\n\nContent:\n- [Quantization Python Programming API Quick Start](#quantization-python-programming-api-quick-start)\n  - [Goal](#goal)\n  - [Prerequisites](#prerequisites)\n  - [Step-by-step Procedure for ResNet-50 Quantization](#step-by-step-procedure-for-resnet-50-quantization)\n  - [Integration with Model Zoo for Intel Architecture](#integration-with-model-zoo-for-intel-architecture)\n  - [Tools](#tools)\n    - [Summarize graph](#summarize-graph)\n  - [Docker support](#docker-support)\n  - [FAQ](#faq)\n\n\n## Goal\n\nThe Quantization Python programming API is to:\n* Unify the quantization tools calling entry, \n* Remove the Tensorflow source build dependency,\n* Transparent the model quantization process, \n* Reduce the quantization steps,\n* Seamlessly adapt to inference with Python script.\n\nThis feature is under active development, and more intelligent features will come in next release.\n\n\n## Prerequisites\n\n* The binary installed Intel\u00ae optimizations for TensorFlow 1.14 or 1.15 are preferred. The Intel\u00ae optimizations for \nTensorFlow 2.0 is also supported for evaluation. \n\n  ```bash\n  $ pip install intel-tensorflow==1.15.0\n  $ pip install intel-quantization\n  ```\n* The source release repository of Model Zoo for Intel\u00ae Architecture is required, if want to execute the quantization\nof specific models in Model Zoo as examples.\n\n  ```bash\n  $ cd ~\n  $ git clone https://github.com/IntelAI/models.git models && cd models\n  $ git checkout v1.5.0\n  ```\n\n* The source release repository of Intel\u00ae AI Quantization Tools for TensorFlow.\n\n  ```bash\n  $ cd ~\n  $ https://github.com/IntelAI/tools.git  quantization && cd quantization\n  ```\n\n## Step-by-step Procedure for ResNet-50 Quantization\n\nIn this section, the frozen pre-trained model and ImageNet dataset will be required for fully automatic quantization. \n\n```bash\n$ cd ~/quantization/api/models/resnet50\n$ wget https://storage.googleapis.com/intel-optimized-tensorflow/models/resnet50_fp32_pretrained_model.pb\n```\nThe TensorFlow models repository provides [scripts and instructions](https://github.com/tensorflow/models/tree/master/research/slim#an-automated-script-for-processing-imagenet-data) to download, process, and convert the ImageNet dataset to the TFRecord format.\n\n\n1. Run demo script\n\nThere are three methods to run the quantization for specific models under `api/examples/`, including bash command for model zoo, bash command for custom model,\nand python programming APIs direct call. \n\nTo quantize the models in Model Zoo for Intel\u00ae Architecture, the bash commands for model zoo is an easy method with few input parameters.  \n```bash\n$ cd ~/quantization\n$ python api/examples/quantize_model_zoo.py \\\n--model resnet50 \\\n--in_graph path/to/resnet50_fp32_pretrained_model.pb \\\n--out_graph path/to/output.pb \\\n--data_location path/to/imagenet \\\n--models_zoo_location ~/models\n```\n\nCheck the input parameters of pre-trained model, dataset path to match with your local environment. \nAnd then execute the python script, you will get the fully automatic quantization conversion from FP32 to INT8.\n\n\nFor any custom models that are not supported by Model Zoo for Intel\u00ae Architecture, the other bash command `api/examples/quantize_cmd.py` is provided.\nThe main difference with model zoo bash command is that user needs to prepare the inference command and pass the string as parameter of callback. And then\nthe callback function will execute the temporary INT8 .pb generated in the middle process to output the min and max log information. Therefore, remove \nthe --input_graph parameters and value from the command string for callback function.\n\n```bash\n$ python api/examples/quantize_cmd.py \\ \n                       --input_graph   path/to/resnet50_fp32_pretrained_model.pb \\\n                       --output_graph  path/to/output.pb \\\n                       --callback      inference_command_with_small_subset_for_ min_max_log\n                       --inputs 'input'\n                       --outputs 'predict'\n                       --per_channel False\n                       --excluded_ops ''\n                       --excluded_nodes ''\n```\n  `--callback`:The command is to execute the inference with small subset of the training dataset to get the min and max log output.\n  `--inputs`:The input op names of the graph.\n  `--outputs`:The output op names of the grap.\n  `--per_channel`:Enable per-channel or not. The per-channel quantization has a different scale and offset for each convolutional kernel.\n  `--excluded_ops`:The ops list that excluded from quantization.\n  `--excluded_nodes`:The nodes list that excluded from quantization.\n\nThe third alternative method to execute the quantization by Python Programming APIs is by Python script directly. \nA template is provided in api/examples/quantize_python.py. The key code is below. \n\n```python\nimport os\nimport intel_quantization.graph_converter as converter\n\ndef rn50_callback_cmds():\n    # This command is to execute the inference with small subset of the training dataset, and get the min and max log output.\n\nif __name__ == '__main__':\n    rn50 = converter.GraphConverter('path/to/resnet50_fp32_pretrained_model.pb', None, ['input'], ['predict'])\n    # pass an inference script to `gen_calib_data_cmds` to generate calibration data.\n    rn50.gen_calib_data_cmds = rn50_callback_cmds()\n    # use \"debug\" option to save temp graph files, default False.\n    rn50.debug = True\n    rn50.covert()\n```\nA [Summarize graph](#summarize-graph) python tool is provided to detect the possible inputs and outputs nodes list of the input .pb graph.\n\n2. Performance Evaluation\n\nFinally, verify the quantized model performance:\n * Run inference using the final quantized graph and calculate the accuracy.\n * Typically, the accuracy target is the optimized FP32 model accuracy values.\n * The quantized INT8 graph accuracy should not drop more than ~0.5-1%.\n\n\nCheck [Intelai/models](https://github.com/IntelAI/models) repository and [ResNet50 README](https://github.com/IntelAI/models/tree/master/benchmarks/image_recognition/tensorflow/resnet50) for TensorFlow models inference benchmarks with different precisions.\n\n\n## Integration with Model Zoo for Intel Architecture\n\nAn integration component with Model Zoo for Intel\u00ae  Architecture is provided, that allows users run following models as reference:\n\n- ResNet-50\n- ResNet-50 V1_5\n- Faster-RCNN\n- Inception_V3\n- MobileNet_V1\n- ResNet-101\n- R-FCN\n- SSD-MobileNet_V1\n- SSD-ResNet34\n\n\nThe model name, launch inference commands for min/max log generation, and specific model quantization parameters are well defined in JSON configuation file `api/config/models.json`.\n\nTake ResNet-50 as an example.\n\n```\n{\n  \"MODEL_NAME\": \"resnet50\",\n  \"LAUNCH_BENCHMARK_PARAMS\": {\n    \"LAUNCH_BENCHMARK_SCRIPT\": \"benchmarks/launch_benchmark.py\",\n    \"LAUNCH_BENCHMARK_CMD\": [\n      \" --model-name=resnet50\",\n      \" --framework=tensorflow\",\n      \" --precision=int8\",\n      \" --mode=inference\",\n      \" --batch-size=100\",\n      \" --accuracy-only\"\n    ],\n    \"IN_GRAPH\": \" --in-graph={}\",\n    \"DATA_LOCATION\": \" --data-location={}\"\n  },\n  \"QUANTIZE_GRAPH_CONVERTER_PARAMS\": {\n    \"INPUT_NODE_LIST\": [\n      \"input\"\n    ],\n    \"OUTPUT_NODE_LIST\": [\n      \"predict\"\n    ],\n    \"EXCLUDED_OPS_LIST\": [],\n    \"EXCLUDED_NODE_LIST\": [],\n    \"PER_CHANNEL_FLAG\": false\n  }\n}\n```\n\n- MODEL_NAME: The model name.\n\n- LAUNCH_BENCHMARK_PARAMS\n  - LAUNCH_BENCHMARK_SCRIPT: The relative path of running script in Model Zoo.\n  - LAUNCH_BENCHMARK_CMD: The parameters to launch int8 accuracy script in Model Zoo.\n  - IN_GRAPH: The path of input graph file.\n  - DATA_LOCATION: The path of dataset.\n  - MODEL_SOURCE_DIR: The path of tensorflow-models.(optional)\n  - DIRECT_PASS_PARAMS_TO_MODEL: The parameters directly passed to the model.(optional)\n\n- QUANTIZE_GRAPH_CONVERTER_PARAMS\n  - INPUT_NODE_LIST: The input nodes name list of the model. You can use [Summarize graph](#summarize-graph) to get the inputs and outputs of the graph.\n  - OUTPUT_NODE_LIST: The output nodes name list of the model.\n  - EXCLUDED_OPS_LIST: The list of operations to be excluded from quantization.\n  - EXCLUDED_NODE_LIST: The list of nodes to be excluded from quantization.\n  - PER_CHANNEL_FLAG: If set True, enables weight quantization channel-wise.\n\n\n\n## Tools\n\n### Summarize graph\n\nIn order to remove the TensorFlow source build dependency, the independent Summarize graph tool `api/tools/summarize_graph.py` is provided to dump the possible inputs nodes and outputs nodes of the graph. It could be taken as the reference list for INPUT_NODE_LIST and OUTPUT_NODE_LIST parameters\nof graph_converter class. \n\n- If use graph in binary,\n\n```bash\n$ python summarize_graph.py --in_graph=path/to/graph --input_binary\n```\n\n- Or use graph in text,\n\n```bash\n$ python summarize_graph.py --in_graph=path/to/graph\n```\n\n\n## Docker support \n\n* [Docker]( https://docs.docker.com/install/ ) - Latest version.\n\n* Build a docker layer which contains Inteli\u00ae Optimizations for TensorFlow and Intel\u00ae AI Quantization Tools for Tensorflow with the command below. \n\n  ```bash\n  $ cd ~/quantization/api/docker\n  $ docker build \\\n       --build-arg HTTP_PROXY=${HTTP_PROXY} \\\n       --build-arg HTTPS_PROXY=${HTTPS_PROXY} \\\n       --build-arg http_proxy=${http_proxy} \\\n       --build-arg https_proxy=${https_proxy} \\\n       -t quantization:latest -f Dockerfile .\n  ```\n\n* Launch quantization script `launch_quantization.py` by providing args as below, this will get user into container environment (`/workspace`) with quantization tools.\n\n  `--docker-image`: Docker image tag from above step (`quantization:latest`).  \n  `--in_graph`: Path to your pre-trained model file, which will be mounted inside the container at `/workspace/pretrained_model`.   \n  `--out_graph`: When working in the container, all outputs should be saved to `/workspace/output`, so that results are written back to the local machine.  \n  `--debug`:Mount the volume and lauch the docker environment to Bash shell environment for debug purpose.   \n  `--model_name` and `--models_zoo` are the specific parameters for Model Zoo for Intel\u00ae Architecture. If user only want to launch the quantization environment in docker and execute own defined models with `--debug` parameter, both can be skipped. \n\n* Take the ResNet50 of Model Zoo as an example. \n\n  ```bash\n  $ cd ~/quantization/api\n  $ python launch_quantization.py  \\\n  --docker-image quantization:latest \\\n  --in_graph=/path/to/in_graph.pb \\\n  --model_name=resnet50 \\\n  --models_zoo=/path/to/models_zoo \\\n  --out_graph=/path/to/output.pb \\\n  --data_location=/path/to/dataset\n  ```\n\n## FAQ\n\n* What's the difference with between Quantization Programming APIs and Tensorflow native quantization?\n\n  The Quantization Programming APIs are specified for Intel\u00ae Optimizations for TensorFlow based on the MKLDNN enabled build. This APIs call the Tensorflow Python models as extension,\n  and provide some special fusion rules, such as, fold_convolutionwithbias_mul, fold_subdivmul_batch_norms, fuse_quantized_conv_and_requantize, mkl_fuse_pad_and_conv,\n  rerange_quantized_concat etc. \n\n* How to build the development environment?\n\n  For any code contributers, the .whl is easy to be rebuilt to include the specific code for debugging purpose. Refer the build command below.  \n\n  ```bash\n  $ cd ~/quantization/api/\n  $ python setup.py bdist_wheel\n  $ pip install\n  $ pip install dist/*.whl\n  ```\n\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/IntelAI/tools", "keywords": "Intel\u00ae AI Quantization Tools for Tensorflow*", "license": "", "maintainer": "", "maintainer_email": "", "name": "intel-quantization", "package_url": "https://pypi.org/project/intel-quantization/", "platform": "", "project_url": "https://pypi.org/project/intel-quantization/", "project_urls": {"Homepage": "https://github.com/IntelAI/tools"}, "release_url": "https://pypi.org/project/intel-quantization/1.0/", "requires_dist": null, "requires_python": ">=3.4, !=3.1.*, !=3.2.*, !=3.3.*, <3.8", "summary": "The Python programming APIs packages for Intel\u00ae AI Quantization Tools for Tensorflow*.", "version": "1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Quantization Python Programming API Quick Start</h1>\n<p>Content:</p>\n<ul>\n<li><a href=\"#quantization-python-programming-api-quick-start\" rel=\"nofollow\">Quantization Python Programming API Quick Start</a>\n<ul>\n<li><a href=\"#goal\" rel=\"nofollow\">Goal</a></li>\n<li><a href=\"#prerequisites\" rel=\"nofollow\">Prerequisites</a></li>\n<li><a href=\"#step-by-step-procedure-for-resnet-50-quantization\" rel=\"nofollow\">Step-by-step Procedure for ResNet-50 Quantization</a></li>\n<li><a href=\"#integration-with-model-zoo-for-intel-architecture\" rel=\"nofollow\">Integration with Model Zoo for Intel Architecture</a></li>\n<li><a href=\"#tools\" rel=\"nofollow\">Tools</a>\n<ul>\n<li><a href=\"#summarize-graph\" rel=\"nofollow\">Summarize graph</a></li>\n</ul>\n</li>\n<li><a href=\"#docker-support\" rel=\"nofollow\">Docker support</a></li>\n<li><a href=\"#faq\" rel=\"nofollow\">FAQ</a></li>\n</ul>\n</li>\n</ul>\n<h2>Goal</h2>\n<p>The Quantization Python programming API is to:</p>\n<ul>\n<li>Unify the quantization tools calling entry,</li>\n<li>Remove the Tensorflow source build dependency,</li>\n<li>Transparent the model quantization process,</li>\n<li>Reduce the quantization steps,</li>\n<li>Seamlessly adapt to inference with Python script.</li>\n</ul>\n<p>This feature is under active development, and more intelligent features will come in next release.</p>\n<h2>Prerequisites</h2>\n<ul>\n<li>\n<p>The binary installed Intel\u00ae optimizations for TensorFlow 1.14 or 1.15 are preferred. The Intel\u00ae optimizations for\nTensorFlow 2.0 is also supported for evaluation.</p>\n<pre>$ pip install intel-tensorflow<span class=\"o\">==</span><span class=\"m\">1</span>.15.0\n$ pip install intel-quantization\n</pre>\n</li>\n<li>\n<p>The source release repository of Model Zoo for Intel\u00ae Architecture is required, if want to execute the quantization\nof specific models in Model Zoo as examples.</p>\n<pre>$ <span class=\"nb\">cd</span> ~\n$ git clone https://github.com/IntelAI/models.git models <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">cd</span> models\n$ git checkout v1.5.0\n</pre>\n</li>\n<li>\n<p>The source release repository of Intel\u00ae AI Quantization Tools for TensorFlow.</p>\n<pre>$ <span class=\"nb\">cd</span> ~\n$ https://github.com/IntelAI/tools.git  quantization <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">cd</span> quantization\n</pre>\n</li>\n</ul>\n<h2>Step-by-step Procedure for ResNet-50 Quantization</h2>\n<p>In this section, the frozen pre-trained model and ImageNet dataset will be required for fully automatic quantization.</p>\n<pre>$ <span class=\"nb\">cd</span> ~/quantization/api/models/resnet50\n$ wget https://storage.googleapis.com/intel-optimized-tensorflow/models/resnet50_fp32_pretrained_model.pb\n</pre>\n<p>The TensorFlow models repository provides <a href=\"https://github.com/tensorflow/models/tree/master/research/slim#an-automated-script-for-processing-imagenet-data\" rel=\"nofollow\">scripts and instructions</a> to download, process, and convert the ImageNet dataset to the TFRecord format.</p>\n<ol>\n<li>Run demo script</li>\n</ol>\n<p>There are three methods to run the quantization for specific models under <code>api/examples/</code>, including bash command for model zoo, bash command for custom model,\nand python programming APIs direct call.</p>\n<p>To quantize the models in Model Zoo for Intel\u00ae Architecture, the bash commands for model zoo is an easy method with few input parameters.</p>\n<pre>$ <span class=\"nb\">cd</span> ~/quantization\n$ python api/examples/quantize_model_zoo.py <span class=\"se\">\\</span>\n--model resnet50 <span class=\"se\">\\</span>\n--in_graph path/to/resnet50_fp32_pretrained_model.pb <span class=\"se\">\\</span>\n--out_graph path/to/output.pb <span class=\"se\">\\</span>\n--data_location path/to/imagenet <span class=\"se\">\\</span>\n--models_zoo_location ~/models\n</pre>\n<p>Check the input parameters of pre-trained model, dataset path to match with your local environment.\nAnd then execute the python script, you will get the fully automatic quantization conversion from FP32 to INT8.</p>\n<p>For any custom models that are not supported by Model Zoo for Intel\u00ae Architecture, the other bash command <code>api/examples/quantize_cmd.py</code> is provided.\nThe main difference with model zoo bash command is that user needs to prepare the inference command and pass the string as parameter of callback. And then\nthe callback function will execute the temporary INT8 .pb generated in the middle process to output the min and max log information. Therefore, remove\nthe --input_graph parameters and value from the command string for callback function.</p>\n<pre>$ python api/examples/quantize_cmd.py <span class=\"se\">\\ </span>\n                       --input_graph   path/to/resnet50_fp32_pretrained_model.pb <span class=\"se\">\\</span>\n                       --output_graph  path/to/output.pb <span class=\"se\">\\</span>\n                       --callback      inference_command_with_small_subset_for_ min_max_log\n                       --inputs <span class=\"s1\">'input'</span>\n                       --outputs <span class=\"s1\">'predict'</span>\n                       --per_channel False\n                       --excluded_ops <span class=\"s1\">''</span>\n                       --excluded_nodes <span class=\"s1\">''</span>\n</pre>\n<p><code>--callback</code>:The command is to execute the inference with small subset of the training dataset to get the min and max log output.\n<code>--inputs</code>:The input op names of the graph.\n<code>--outputs</code>:The output op names of the grap.\n<code>--per_channel</code>:Enable per-channel or not. The per-channel quantization has a different scale and offset for each convolutional kernel.\n<code>--excluded_ops</code>:The ops list that excluded from quantization.\n<code>--excluded_nodes</code>:The nodes list that excluded from quantization.</p>\n<p>The third alternative method to execute the quantization by Python Programming APIs is by Python script directly.\nA template is provided in api/examples/quantize_python.py. The key code is below.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">intel_quantization.graph_converter</span> <span class=\"k\">as</span> <span class=\"nn\">converter</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">rn50_callback_cmds</span><span class=\"p\">():</span>\n    <span class=\"c1\"># This command is to execute the inference with small subset of the training dataset, and get the min and max log output.</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span><span class=\"p\">:</span>\n    <span class=\"n\">rn50</span> <span class=\"o\">=</span> <span class=\"n\">converter</span><span class=\"o\">.</span><span class=\"n\">GraphConverter</span><span class=\"p\">(</span><span class=\"s1\">'path/to/resnet50_fp32_pretrained_model.pb'</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s1\">'input'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'predict'</span><span class=\"p\">])</span>\n    <span class=\"c1\"># pass an inference script to `gen_calib_data_cmds` to generate calibration data.</span>\n    <span class=\"n\">rn50</span><span class=\"o\">.</span><span class=\"n\">gen_calib_data_cmds</span> <span class=\"o\">=</span> <span class=\"n\">rn50_callback_cmds</span><span class=\"p\">()</span>\n    <span class=\"c1\"># use \"debug\" option to save temp graph files, default False.</span>\n    <span class=\"n\">rn50</span><span class=\"o\">.</span><span class=\"n\">debug</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n    <span class=\"n\">rn50</span><span class=\"o\">.</span><span class=\"n\">covert</span><span class=\"p\">()</span>\n</pre>\n<p>A <a href=\"#summarize-graph\" rel=\"nofollow\">Summarize graph</a> python tool is provided to detect the possible inputs and outputs nodes list of the input .pb graph.</p>\n<ol>\n<li>Performance Evaluation</li>\n</ol>\n<p>Finally, verify the quantized model performance:</p>\n<ul>\n<li>Run inference using the final quantized graph and calculate the accuracy.</li>\n<li>Typically, the accuracy target is the optimized FP32 model accuracy values.</li>\n<li>The quantized INT8 graph accuracy should not drop more than ~0.5-1%.</li>\n</ul>\n<p>Check <a href=\"https://github.com/IntelAI/models\" rel=\"nofollow\">Intelai/models</a> repository and <a href=\"https://github.com/IntelAI/models/tree/master/benchmarks/image_recognition/tensorflow/resnet50\" rel=\"nofollow\">ResNet50 README</a> for TensorFlow models inference benchmarks with different precisions.</p>\n<h2>Integration with Model Zoo for Intel Architecture</h2>\n<p>An integration component with Model Zoo for Intel\u00ae  Architecture is provided, that allows users run following models as reference:</p>\n<ul>\n<li>ResNet-50</li>\n<li>ResNet-50 V1_5</li>\n<li>Faster-RCNN</li>\n<li>Inception_V3</li>\n<li>MobileNet_V1</li>\n<li>ResNet-101</li>\n<li>R-FCN</li>\n<li>SSD-MobileNet_V1</li>\n<li>SSD-ResNet34</li>\n</ul>\n<p>The model name, launch inference commands for min/max log generation, and specific model quantization parameters are well defined in JSON configuation file <code>api/config/models.json</code>.</p>\n<p>Take ResNet-50 as an example.</p>\n<pre><code>{\n  \"MODEL_NAME\": \"resnet50\",\n  \"LAUNCH_BENCHMARK_PARAMS\": {\n    \"LAUNCH_BENCHMARK_SCRIPT\": \"benchmarks/launch_benchmark.py\",\n    \"LAUNCH_BENCHMARK_CMD\": [\n      \" --model-name=resnet50\",\n      \" --framework=tensorflow\",\n      \" --precision=int8\",\n      \" --mode=inference\",\n      \" --batch-size=100\",\n      \" --accuracy-only\"\n    ],\n    \"IN_GRAPH\": \" --in-graph={}\",\n    \"DATA_LOCATION\": \" --data-location={}\"\n  },\n  \"QUANTIZE_GRAPH_CONVERTER_PARAMS\": {\n    \"INPUT_NODE_LIST\": [\n      \"input\"\n    ],\n    \"OUTPUT_NODE_LIST\": [\n      \"predict\"\n    ],\n    \"EXCLUDED_OPS_LIST\": [],\n    \"EXCLUDED_NODE_LIST\": [],\n    \"PER_CHANNEL_FLAG\": false\n  }\n}\n</code></pre>\n<ul>\n<li>\n<p>MODEL_NAME: The model name.</p>\n</li>\n<li>\n<p>LAUNCH_BENCHMARK_PARAMS</p>\n<ul>\n<li>LAUNCH_BENCHMARK_SCRIPT: The relative path of running script in Model Zoo.</li>\n<li>LAUNCH_BENCHMARK_CMD: The parameters to launch int8 accuracy script in Model Zoo.</li>\n<li>IN_GRAPH: The path of input graph file.</li>\n<li>DATA_LOCATION: The path of dataset.</li>\n<li>MODEL_SOURCE_DIR: The path of tensorflow-models.(optional)</li>\n<li>DIRECT_PASS_PARAMS_TO_MODEL: The parameters directly passed to the model.(optional)</li>\n</ul>\n</li>\n<li>\n<p>QUANTIZE_GRAPH_CONVERTER_PARAMS</p>\n<ul>\n<li>INPUT_NODE_LIST: The input nodes name list of the model. You can use <a href=\"#summarize-graph\" rel=\"nofollow\">Summarize graph</a> to get the inputs and outputs of the graph.</li>\n<li>OUTPUT_NODE_LIST: The output nodes name list of the model.</li>\n<li>EXCLUDED_OPS_LIST: The list of operations to be excluded from quantization.</li>\n<li>EXCLUDED_NODE_LIST: The list of nodes to be excluded from quantization.</li>\n<li>PER_CHANNEL_FLAG: If set True, enables weight quantization channel-wise.</li>\n</ul>\n</li>\n</ul>\n<h2>Tools</h2>\n<h3>Summarize graph</h3>\n<p>In order to remove the TensorFlow source build dependency, the independent Summarize graph tool <code>api/tools/summarize_graph.py</code> is provided to dump the possible inputs nodes and outputs nodes of the graph. It could be taken as the reference list for INPUT_NODE_LIST and OUTPUT_NODE_LIST parameters\nof graph_converter class.</p>\n<ul>\n<li>If use graph in binary,</li>\n</ul>\n<pre>$ python summarize_graph.py --in_graph<span class=\"o\">=</span>path/to/graph --input_binary\n</pre>\n<ul>\n<li>Or use graph in text,</li>\n</ul>\n<pre>$ python summarize_graph.py --in_graph<span class=\"o\">=</span>path/to/graph\n</pre>\n<h2>Docker support</h2>\n<ul>\n<li>\n<p><a href=\"https://docs.docker.com/install/\" rel=\"nofollow\">Docker</a> - Latest version.</p>\n</li>\n<li>\n<p>Build a docker layer which contains Inteli\u00ae Optimizations for TensorFlow and Intel\u00ae AI Quantization Tools for Tensorflow with the command below.</p>\n<pre>$ <span class=\"nb\">cd</span> ~/quantization/api/docker\n$ docker build <span class=\"se\">\\</span>\n     --build-arg <span class=\"nv\">HTTP_PROXY</span><span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">HTTP_PROXY</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n     --build-arg <span class=\"nv\">HTTPS_PROXY</span><span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">HTTPS_PROXY</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n     --build-arg <span class=\"nv\">http_proxy</span><span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">http_proxy</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n     --build-arg <span class=\"nv\">https_proxy</span><span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">https_proxy</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n     -t quantization:latest -f Dockerfile .\n</pre>\n</li>\n<li>\n<p>Launch quantization script <code>launch_quantization.py</code> by providing args as below, this will get user into container environment (<code>/workspace</code>) with quantization tools.</p>\n<p><code>--docker-image</code>: Docker image tag from above step (<code>quantization:latest</code>).<br>\n<code>--in_graph</code>: Path to your pre-trained model file, which will be mounted inside the container at <code>/workspace/pretrained_model</code>.<br>\n<code>--out_graph</code>: When working in the container, all outputs should be saved to <code>/workspace/output</code>, so that results are written back to the local machine.<br>\n<code>--debug</code>:Mount the volume and lauch the docker environment to Bash shell environment for debug purpose.<br>\n<code>--model_name</code> and <code>--models_zoo</code> are the specific parameters for Model Zoo for Intel\u00ae Architecture. If user only want to launch the quantization environment in docker and execute own defined models with <code>--debug</code> parameter, both can be skipped.</p>\n</li>\n<li>\n<p>Take the ResNet50 of Model Zoo as an example.</p>\n<pre>$ <span class=\"nb\">cd</span> ~/quantization/api\n$ python launch_quantization.py  <span class=\"se\">\\</span>\n--docker-image quantization:latest <span class=\"se\">\\</span>\n--in_graph<span class=\"o\">=</span>/path/to/in_graph.pb <span class=\"se\">\\</span>\n--model_name<span class=\"o\">=</span>resnet50 <span class=\"se\">\\</span>\n--models_zoo<span class=\"o\">=</span>/path/to/models_zoo <span class=\"se\">\\</span>\n--out_graph<span class=\"o\">=</span>/path/to/output.pb <span class=\"se\">\\</span>\n--data_location<span class=\"o\">=</span>/path/to/dataset\n</pre>\n</li>\n</ul>\n<h2>FAQ</h2>\n<ul>\n<li>\n<p>What's the difference with between Quantization Programming APIs and Tensorflow native quantization?</p>\n<p>The Quantization Programming APIs are specified for Intel\u00ae Optimizations for TensorFlow based on the MKLDNN enabled build. This APIs call the Tensorflow Python models as extension,\nand provide some special fusion rules, such as, fold_convolutionwithbias_mul, fold_subdivmul_batch_norms, fuse_quantized_conv_and_requantize, mkl_fuse_pad_and_conv,\nrerange_quantized_concat etc.</p>\n</li>\n<li>\n<p>How to build the development environment?</p>\n<p>For any code contributers, the .whl is easy to be rebuilt to include the specific code for debugging purpose. Refer the build command below.</p>\n<pre>$ <span class=\"nb\">cd</span> ~/quantization/api/\n$ python setup.py bdist_wheel\n$ pip install\n$ pip install dist/*.whl\n</pre>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 7067146, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "28e4825232da6fe035804d76fed1c404", "sha256": "1e65586a54982dfef3ff4a86030a3c2fcab7b8e7e76187adc504762a5cdc0f29"}, "downloads": -1, "filename": "intel_quantization-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "28e4825232da6fe035804d76fed1c404", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4, !=3.1.*, !=3.2.*, !=3.3.*, <3.8", "size": 53900, "upload_time": "2020-04-21T10:41:58", "upload_time_iso_8601": "2020-04-21T10:41:58.315673Z", "url": "https://files.pythonhosted.org/packages/60/da/8b171aed79be7f0f94ef917869bbba17e2fa716691ec1c3c8d8d80c2d96f/intel_quantization-1.0-py3-none-any.whl", "yanked": false}], "1.0b0": [{"comment_text": "", "digests": {"md5": "36d8ef551817ce90d10d5b508fd01f11", "sha256": "3876d489043b1554de06f52057bc62735e284a1eb3528be109939b51ede4fead"}, "downloads": -1, "filename": "intel_quantization-1.0b0-py3-none-any.whl", "has_sig": false, "md5_digest": "36d8ef551817ce90d10d5b508fd01f11", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4, !=3.1.*, !=3.2.*, !=3.3.*, <3.8", "size": 44511, "upload_time": "2020-02-07T05:45:53", "upload_time_iso_8601": "2020-02-07T05:45:53.736646Z", "url": "https://files.pythonhosted.org/packages/df/c3/bc309e7691a25a269d763b78d158ad54bc903d114ff4ba95c44ce494e787/intel_quantization-1.0b0-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "28e4825232da6fe035804d76fed1c404", "sha256": "1e65586a54982dfef3ff4a86030a3c2fcab7b8e7e76187adc504762a5cdc0f29"}, "downloads": -1, "filename": "intel_quantization-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "28e4825232da6fe035804d76fed1c404", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4, !=3.1.*, !=3.2.*, !=3.3.*, <3.8", "size": 53900, "upload_time": "2020-04-21T10:41:58", "upload_time_iso_8601": "2020-04-21T10:41:58.315673Z", "url": "https://files.pythonhosted.org/packages/60/da/8b171aed79be7f0f94ef917869bbba17e2fa716691ec1c3c8d8d80c2d96f/intel_quantization-1.0-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:55:31 2020"}