{"info": {"author": "Yasuhiro Fujita", "author_email": "fujita@preferred.jp", "bugtrack_url": null, "classifiers": [], "description": "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/chainer/chainerrl/master/assets/ChainerRL.png\" width=\"400\"/></div>\n\n# ChainerRL\n[![Build Status](https://travis-ci.org/chainer/chainerrl.svg?branch=master)](https://travis-ci.org/chainer/chainerrl)\n[![Coverage Status](https://coveralls.io/repos/github/chainer/chainerrl/badge.svg?branch=master)](https://coveralls.io/github/chainer/chainerrl?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/chainerrl/badge/?version=latest)](http://chainerrl.readthedocs.io/en/latest/?badge=latest)\n[![PyPI](https://img.shields.io/pypi/v/chainerrl.svg)](https://pypi.python.org/pypi/chainerrl)\n\nChainerRL is a deep reinforcement learning library that implements various state-of-the-art deep reinforcement algorithms in Python using [Chainer](https://github.com/chainer/chainer), a flexible deep learning framework.\n\n![Breakout](assets/breakout.gif)\n![Humanoid](assets/humanoid.gif)\n![Grasping](assets/grasping.gif)\n![Atlas](examples/atlas/assets/atlas.gif)\n\n## Installation\n\nChainerRL is tested with 3.5.1+. For other requirements, see [requirements.txt](requirements.txt).\n\nChainerRL can be installed via PyPI:\n```\npip install chainerrl\n```\n\nIt can also be installed from the source code:\n```\npython setup.py install\n```\n\nRefer to [Installation](http://chainerrl.readthedocs.io/en/latest/install.html) for more information on installation. \n\n## Getting started\n\nYou can try [ChainerRL Quickstart Guide](examples/quickstart/quickstart.ipynb) first, or check the [examples](examples) ready for Atari 2600 and Open AI Gym.\n\nFor more information, you can refer to [ChainerRL's documentation](http://chainerrl.readthedocs.io/en/latest/index.html).\n\n## Algorithms\n\n| Algorithm | Discrete Action | Continous Action | Recurrent Model | Batch Training | CPU Async Training |\n|:----------|:---------------:|:----------------:|:---------------:|:--------------:|:------------------:|\n| DQN (including DoubleDQN etc.) | \u2713 | \u2713 (NAF) | \u2713 | \u2713 | x |\n| Categorical DQN | \u2713 | x | \u2713 | \u2713 | x |\n| Rainbow | \u2713 | x | \u2713 | \u2713 | x |\n| IQN | \u2713 | x | \u2713 | \u2713 | x |\n| DDPG | x | \u2713 | \u2713 | \u2713 | x |\n| A3C  | \u2713 | \u2713 | \u2713 | \u2713 (A2C) | \u2713 |\n| ACER | \u2713 | \u2713 | \u2713 | x | \u2713 |\n| NSQ (N-step Q-learning) | \u2713 | \u2713 (NAF) | \u2713 | x | \u2713 |\n| PCL (Path Consistency Learning) | \u2713 | \u2713 | \u2713 | x | \u2713 |\n| PPO  | \u2713 | \u2713 | \u2713 | \u2713 | x |\n| TRPO | \u2713 | \u2713 | \u2713 | \u2713 | x |\n| TD3 | x | \u2713 | x | \u2713 | x |\n| SAC | x | \u2713 | x | \u2713 | x |\n\nFollowing algorithms have been implemented in ChainerRL:\n- [A2C (Synchronous variant of A3C)](https://openai.com/blog/baselines-acktr-a2c/)\n  - examples: [[atari (batched)]](examples/atari/train_a2c_ale.py) [[general gym (batched)]](examples/gym/train_a2c_gym.py)\n- [A3C (Asynchronous Advantage Actor-Critic)](https://arxiv.org/abs/1602.01783)\n  - examples: [[atari reproduction]](examples/atari/reproduction/a3c) [[atari]](examples/atari/train_a3c_ale.py) [[general gym]](examples/gym/train_a3c_gym.py)\n- [ACER (Actor-Critic with Experience Replay)](https://arxiv.org/abs/1611.01224)\n  - examples: [[atari]](examples/atari/train_acer_ale.py) [[general gym]](examples/gym/train_acer_gym.py)\n- [Asynchronous N-step Q-learning](https://arxiv.org/abs/1602.01783)\n  - examples: [[atari]](examples/atari/train_nsq_ale.py)\n- [Categorical DQN](https://arxiv.org/abs/1707.06887)\n  - examples: [[atari]](examples/atari/train_categorical_dqn_ale.py) [[general gym]](examples/gym/train_categorical_dqn_gym.py)\n- [DQN (Deep Q-Network)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (including [Double DQN](https://arxiv.org/abs/1509.06461), [Persistent Advantage Learning (PAL)](https://arxiv.org/abs/1512.04860), Double PAL, [Dynamic Policy Programming (DPP)](http://www.jmlr.org/papers/volume13/azar12a/azar12a.pdf))\n  - examples: [[atari reproduction]](examples/atari/reproduction/dqn) [[atari]](examples/atari/train_dqn_ale.py) [[atari (batched)]](examples/atari/train_dqn_batch_ale.py) [[flickering atari]](examples/atari/train_drqn_ale.py) [[general gym]](examples/gym/train_dqn_gym.py)\n- [DDPG (Deep Deterministic Policy Gradients)](https://arxiv.org/abs/1509.02971) (including [SVG(0)](https://arxiv.org/abs/1510.09142))\n  - examples: [[mujoco reproduction]](examples/mujoco/reproduction/ddpg) [[mujoco]](examples/mujoco/train_ddpg_gym.py) [[mujoco (batched)]](examples/mujoco/train_ddpg_batch_gym.py)\n- [IQN (Implicit Quantile Networks)](https://arxiv.org/abs/1806.06923)\n  - examples: [[atari reproduction]](examples/atari/reproduction/iqn) [[general gym]](examples/gym/train_iqn_gym.py)\n- [PCL (Path Consistency Learning)](https://arxiv.org/abs/1702.08892)\n  - examples: [[general gym]](examples/gym/train_pcl_gym.py)\n- [PPO (Proximal Policy Optimization)](https://arxiv.org/abs/1707.06347)\n  - examples: [[mujoco reproduction]](examples/mujoco/reproduction/ppo) [[atari]](examples/atari/train_ppo_ale.py) [[mujoco]](examples/mujoco/train_ppo_gym.py) [[mujoco (batched)]](examples/mujoco/train_ppo_batch_gym.py)\n- [Rainbow](https://arxiv.org/abs/1710.02298)\n  - examples: [[atari reproduction]](examples/atari/reproduction/rainbow)\n- [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n  - examples: [[general gym]](examples/gym/train_reinforce_gym.py)\n- [SAC (Soft Actor-Critic)](https://arxiv.org/abs/1812.05905)\n  - examples: [[mujoco reproduction]](examples/mujoco/reproduction/soft_actor_critic)\n- [TRPO (Trust Region Policy Optimization)](https://arxiv.org/abs/1502.05477) with [GAE (Generalized Advantage Estimation)](https://arxiv.org/abs/1506.02438)\n  - examples: [[mujoco]](examples/mujoco/train_trpo_gym.py)\n- [TD3 (Twin Delayed Deep Deterministic policy gradient algorithm)](https://arxiv.org/abs/1802.09477)\n  - examples: [[mujoco reproduction]](examples/mujoco/reproduction/td3)\n\nFollowing useful techniques have been also implemented in ChainerRL:\n- [NoisyNet](https://arxiv.org/abs/1706.10295)\n  - examples: [[Rainbow]](examples/atari/reproduction/rainbow) [[DQN/DoubleDQN/PAL]](examples/atari/train_dqn_ale.py)\n- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)\n  - examples: [[Rainbow]](examples/atari/reproduction/rainbow) [[DQN/DoubleDQN/PAL]](examples/atari/train_dqn_ale.py)\n- [Dueling Network](https://arxiv.org/abs/1511.06581)\n  - examples: [[Rainbow]](examples/atari/reproduction/rainbow) [[DQN/DoubleDQN/PAL]](examples/atari/train_dqn_ale.py)\n- [Normalized Advantage Function](https://arxiv.org/abs/1603.00748)\n  - examples: [[DQN]](examples/gym/train_dqn_gym.py) (for continuous-action envs only)\n- [Deep Recurrent Q-Network](https://arxiv.org/abs/1507.06527)\n  - examples: [[DQN]](examples/atari/train_drqn_ale.py)\n\n\n## Visualization\n\nChainerRL has a set of accompanying [visualization tools](https://github.com/chainer/chainerrl-visualizer) in order to aid developers' ability to understand and debug their RL agents. With this visualization tool, the behavior of ChainerRL agents can be easily inspected from a browser UI.\n\n\n## Environments\n\nEnvironments that support the subset of OpenAI Gym's interface (`reset` and `step` methods) can be used.\n\n## Contributing\n\nAny kind of contribution to ChainerRL would be highly appreciated! If you are interested in contributing to ChainerRL, please read [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## License\n\n[MIT License](LICENSE).\n\n## Citations\n\nTo cite ChainerRL in publications:\n\n```\n@InProceedings{fujita2019chainerrl,\n  author = {Fujita, Yasuhiro and Kataoka, Toshiki and Nagarajan, Prabhat and Ishikawa, Takahiro},\n  title = {ChainerRL: A Deep Reinforcement Learning Library},\n  booktitle = {Workshop on Deep Reinforcement Learning at the 33rd Conference on Neural Information Processing Systems},\n  location = {Vancouver, Canada},\n  month = {December},\n  year = {2019}\n}\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "chainerrl", "package_url": "https://pypi.org/project/chainerrl/", "platform": "", "project_url": "https://pypi.org/project/chainerrl/", "project_urls": null, "release_url": "https://pypi.org/project/chainerrl/0.8.0/", "requires_dist": null, "requires_python": "", "summary": "ChainerRL, a deep reinforcement learning library", "version": "0.8.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/893066ed32010c0ab4fd0db454c7b95dfdca6958/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f636861696e65722f636861696e6572726c2f6d61737465722f6173736574732f436861696e6572524c2e706e67\" width=\"400\"></div>\n<h1>ChainerRL</h1>\n<p><a href=\"https://travis-ci.org/chainer/chainerrl\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/de3785f1203a57396ad5114441fa0dc340f735db/68747470733a2f2f7472617669732d63692e6f72672f636861696e65722f636861696e6572726c2e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/chainer/chainerrl?branch=master\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1e2504ac47e6c271bf10be481f86c9aae0e9e1d7/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f636861696e65722f636861696e6572726c2f62616467652e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"http://chainerrl.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c3403ec7c0943c604fcf4a304bbbdf784298e5f2/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f636861696e6572726c2f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<a href=\"https://pypi.python.org/pypi/chainerrl\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/94d71cc2b8966a521fe933698adddbaf569caf4c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f636861696e6572726c2e737667\"></a></p>\n<p>ChainerRL is a deep reinforcement learning library that implements various state-of-the-art deep reinforcement algorithms in Python using <a href=\"https://github.com/chainer/chainer\" rel=\"nofollow\">Chainer</a>, a flexible deep learning framework.</p>\n<p><img alt=\"Breakout\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d23b811d594f66c4989f4daa036c3c39a9d4aa17/6173736574732f627265616b6f75742e676966\">\n<img alt=\"Humanoid\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5b7f20e2c1be97100721a7da81386bd241bd9e4d/6173736574732f68756d616e6f69642e676966\">\n<img alt=\"Grasping\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1d1c0bcf14a90da5550252c682a55d2093165b32/6173736574732f6772617370696e672e676966\">\n<img alt=\"Atlas\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c1775f3b03df06174c4f9ef60d965048f511cdd1/6578616d706c65732f61746c61732f6173736574732f61746c61732e676966\"></p>\n<h2>Installation</h2>\n<p>ChainerRL is tested with 3.5.1+. For other requirements, see <a href=\"requirements.txt\" rel=\"nofollow\">requirements.txt</a>.</p>\n<p>ChainerRL can be installed via PyPI:</p>\n<pre><code>pip install chainerrl\n</code></pre>\n<p>It can also be installed from the source code:</p>\n<pre><code>python setup.py install\n</code></pre>\n<p>Refer to <a href=\"http://chainerrl.readthedocs.io/en/latest/install.html\" rel=\"nofollow\">Installation</a> for more information on installation.</p>\n<h2>Getting started</h2>\n<p>You can try <a href=\"examples/quickstart/quickstart.ipynb\" rel=\"nofollow\">ChainerRL Quickstart Guide</a> first, or check the <a href=\"examples\" rel=\"nofollow\">examples</a> ready for Atari 2600 and Open AI Gym.</p>\n<p>For more information, you can refer to <a href=\"http://chainerrl.readthedocs.io/en/latest/index.html\" rel=\"nofollow\">ChainerRL's documentation</a>.</p>\n<h2>Algorithms</h2>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Algorithm</th>\n<th align=\"center\">Discrete Action</th>\n<th align=\"center\">Continous Action</th>\n<th align=\"center\">Recurrent Model</th>\n<th align=\"center\">Batch Training</th>\n<th align=\"center\">CPU Async Training</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">DQN (including DoubleDQN etc.)</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713 (NAF)</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n</tr>\n<tr>\n<td align=\"left\">Categorical DQN</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n</tr>\n<tr>\n<td align=\"left\">Rainbow</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n</tr>\n<tr>\n<td align=\"left\">IQN</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n</tr>\n<tr>\n<td align=\"left\">DDPG</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n</tr>\n<tr>\n<td align=\"left\">A3C</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713 (A2C)</td>\n<td align=\"center\">\u2713</td>\n</tr>\n<tr>\n<td align=\"left\">ACER</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n</tr>\n<tr>\n<td align=\"left\">NSQ (N-step Q-learning)</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713 (NAF)</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n</tr>\n<tr>\n<td align=\"left\">PCL (Path Consistency Learning)</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n</tr>\n<tr>\n<td align=\"left\">PPO</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n</tr>\n<tr>\n<td align=\"left\">TRPO</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n</tr>\n<tr>\n<td align=\"left\">TD3</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n</tr>\n<tr>\n<td align=\"left\">SAC</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n<td align=\"center\">\u2713</td>\n<td align=\"center\">x</td>\n</tr></tbody></table>\n<p>Following algorithms have been implemented in ChainerRL:</p>\n<ul>\n<li><a href=\"https://openai.com/blog/baselines-acktr-a2c/\" rel=\"nofollow\">A2C (Synchronous variant of A3C)</a>\n<ul>\n<li>examples: <a href=\"examples/atari/train_a2c_ale.py\" rel=\"nofollow\">[atari (batched)]</a> <a href=\"examples/gym/train_a2c_gym.py\" rel=\"nofollow\">[general gym (batched)]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1602.01783\" rel=\"nofollow\">A3C (Asynchronous Advantage Actor-Critic)</a>\n<ul>\n<li>examples: <a href=\"examples/atari/reproduction/a3c\" rel=\"nofollow\">[atari reproduction]</a> <a href=\"examples/atari/train_a3c_ale.py\" rel=\"nofollow\">[atari]</a> <a href=\"examples/gym/train_a3c_gym.py\" rel=\"nofollow\">[general gym]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1611.01224\" rel=\"nofollow\">ACER (Actor-Critic with Experience Replay)</a>\n<ul>\n<li>examples: <a href=\"examples/atari/train_acer_ale.py\" rel=\"nofollow\">[atari]</a> <a href=\"examples/gym/train_acer_gym.py\" rel=\"nofollow\">[general gym]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1602.01783\" rel=\"nofollow\">Asynchronous N-step Q-learning</a>\n<ul>\n<li>examples: <a href=\"examples/atari/train_nsq_ale.py\" rel=\"nofollow\">[atari]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1707.06887\" rel=\"nofollow\">Categorical DQN</a>\n<ul>\n<li>examples: <a href=\"examples/atari/train_categorical_dqn_ale.py\" rel=\"nofollow\">[atari]</a> <a href=\"examples/gym/train_categorical_dqn_gym.py\" rel=\"nofollow\">[general gym]</a></li>\n</ul>\n</li>\n<li><a href=\"https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\" rel=\"nofollow\">DQN (Deep Q-Network)</a> (including <a href=\"https://arxiv.org/abs/1509.06461\" rel=\"nofollow\">Double DQN</a>, <a href=\"https://arxiv.org/abs/1512.04860\" rel=\"nofollow\">Persistent Advantage Learning (PAL)</a>, Double PAL, <a href=\"http://www.jmlr.org/papers/volume13/azar12a/azar12a.pdf\" rel=\"nofollow\">Dynamic Policy Programming (DPP)</a>)\n<ul>\n<li>examples: <a href=\"examples/atari/reproduction/dqn\" rel=\"nofollow\">[atari reproduction]</a> <a href=\"examples/atari/train_dqn_ale.py\" rel=\"nofollow\">[atari]</a> <a href=\"examples/atari/train_dqn_batch_ale.py\" rel=\"nofollow\">[atari (batched)]</a> <a href=\"examples/atari/train_drqn_ale.py\" rel=\"nofollow\">[flickering atari]</a> <a href=\"examples/gym/train_dqn_gym.py\" rel=\"nofollow\">[general gym]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1509.02971\" rel=\"nofollow\">DDPG (Deep Deterministic Policy Gradients)</a> (including <a href=\"https://arxiv.org/abs/1510.09142\" rel=\"nofollow\">SVG(0)</a>)\n<ul>\n<li>examples: <a href=\"examples/mujoco/reproduction/ddpg\" rel=\"nofollow\">[mujoco reproduction]</a> <a href=\"examples/mujoco/train_ddpg_gym.py\" rel=\"nofollow\">[mujoco]</a> <a href=\"examples/mujoco/train_ddpg_batch_gym.py\" rel=\"nofollow\">[mujoco (batched)]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1806.06923\" rel=\"nofollow\">IQN (Implicit Quantile Networks)</a>\n<ul>\n<li>examples: <a href=\"examples/atari/reproduction/iqn\" rel=\"nofollow\">[atari reproduction]</a> <a href=\"examples/gym/train_iqn_gym.py\" rel=\"nofollow\">[general gym]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1702.08892\" rel=\"nofollow\">PCL (Path Consistency Learning)</a>\n<ul>\n<li>examples: <a href=\"examples/gym/train_pcl_gym.py\" rel=\"nofollow\">[general gym]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1707.06347\" rel=\"nofollow\">PPO (Proximal Policy Optimization)</a>\n<ul>\n<li>examples: <a href=\"examples/mujoco/reproduction/ppo\" rel=\"nofollow\">[mujoco reproduction]</a> <a href=\"examples/atari/train_ppo_ale.py\" rel=\"nofollow\">[atari]</a> <a href=\"examples/mujoco/train_ppo_gym.py\" rel=\"nofollow\">[mujoco]</a> <a href=\"examples/mujoco/train_ppo_batch_gym.py\" rel=\"nofollow\">[mujoco (batched)]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1710.02298\" rel=\"nofollow\">Rainbow</a>\n<ul>\n<li>examples: <a href=\"examples/atari/reproduction/rainbow\" rel=\"nofollow\">[atari reproduction]</a></li>\n</ul>\n</li>\n<li><a href=\"http://www-anw.cs.umass.edu/%7Ebarto/courses/cs687/williams92simple.pdf\" rel=\"nofollow\">REINFORCE</a>\n<ul>\n<li>examples: <a href=\"examples/gym/train_reinforce_gym.py\" rel=\"nofollow\">[general gym]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1812.05905\" rel=\"nofollow\">SAC (Soft Actor-Critic)</a>\n<ul>\n<li>examples: <a href=\"examples/mujoco/reproduction/soft_actor_critic\" rel=\"nofollow\">[mujoco reproduction]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1502.05477\" rel=\"nofollow\">TRPO (Trust Region Policy Optimization)</a> with <a href=\"https://arxiv.org/abs/1506.02438\" rel=\"nofollow\">GAE (Generalized Advantage Estimation)</a>\n<ul>\n<li>examples: <a href=\"examples/mujoco/train_trpo_gym.py\" rel=\"nofollow\">[mujoco]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1802.09477\" rel=\"nofollow\">TD3 (Twin Delayed Deep Deterministic policy gradient algorithm)</a>\n<ul>\n<li>examples: <a href=\"examples/mujoco/reproduction/td3\" rel=\"nofollow\">[mujoco reproduction]</a></li>\n</ul>\n</li>\n</ul>\n<p>Following useful techniques have been also implemented in ChainerRL:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1706.10295\" rel=\"nofollow\">NoisyNet</a>\n<ul>\n<li>examples: <a href=\"examples/atari/reproduction/rainbow\" rel=\"nofollow\">[Rainbow]</a> <a href=\"examples/atari/train_dqn_ale.py\" rel=\"nofollow\">[DQN/DoubleDQN/PAL]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1511.05952\" rel=\"nofollow\">Prioritized Experience Replay</a>\n<ul>\n<li>examples: <a href=\"examples/atari/reproduction/rainbow\" rel=\"nofollow\">[Rainbow]</a> <a href=\"examples/atari/train_dqn_ale.py\" rel=\"nofollow\">[DQN/DoubleDQN/PAL]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1511.06581\" rel=\"nofollow\">Dueling Network</a>\n<ul>\n<li>examples: <a href=\"examples/atari/reproduction/rainbow\" rel=\"nofollow\">[Rainbow]</a> <a href=\"examples/atari/train_dqn_ale.py\" rel=\"nofollow\">[DQN/DoubleDQN/PAL]</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1603.00748\" rel=\"nofollow\">Normalized Advantage Function</a>\n<ul>\n<li>examples: <a href=\"examples/gym/train_dqn_gym.py\" rel=\"nofollow\">[DQN]</a> (for continuous-action envs only)</li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1507.06527\" rel=\"nofollow\">Deep Recurrent Q-Network</a>\n<ul>\n<li>examples: <a href=\"examples/atari/train_drqn_ale.py\" rel=\"nofollow\">[DQN]</a></li>\n</ul>\n</li>\n</ul>\n<h2>Visualization</h2>\n<p>ChainerRL has a set of accompanying <a href=\"https://github.com/chainer/chainerrl-visualizer\" rel=\"nofollow\">visualization tools</a> in order to aid developers' ability to understand and debug their RL agents. With this visualization tool, the behavior of ChainerRL agents can be easily inspected from a browser UI.</p>\n<h2>Environments</h2>\n<p>Environments that support the subset of OpenAI Gym's interface (<code>reset</code> and <code>step</code> methods) can be used.</p>\n<h2>Contributing</h2>\n<p>Any kind of contribution to ChainerRL would be highly appreciated! If you are interested in contributing to ChainerRL, please read <a href=\"CONTRIBUTING.md\" rel=\"nofollow\">CONTRIBUTING.md</a>.</p>\n<h2>License</h2>\n<p><a href=\"LICENSE\" rel=\"nofollow\">MIT License</a>.</p>\n<h2>Citations</h2>\n<p>To cite ChainerRL in publications:</p>\n<pre><code>@InProceedings{fujita2019chainerrl,\n  author = {Fujita, Yasuhiro and Kataoka, Toshiki and Nagarajan, Prabhat and Ishikawa, Takahiro},\n  title = {ChainerRL: A Deep Reinforcement Learning Library},\n  booktitle = {Workshop on Deep Reinforcement Learning at the 33rd Conference on Neural Information Processing Systems},\n  location = {Vancouver, Canada},\n  month = {December},\n  year = {2019}\n}\n</code></pre>\n\n          </div>"}, "last_serial": 6628361, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "e9a57221a77dcbcc951e7a6291eb458b", "sha256": "7b730810cae3e040d64df8e77f3859dda69967e533b06990163e3821ac9a1db0"}, "downloads": -1, "filename": "chainerrl-0.0.1.tar.gz", "has_sig": false, "md5_digest": "e9a57221a77dcbcc951e7a6291eb458b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 666, "upload_time": "2017-02-01T10:47:02", "upload_time_iso_8601": "2017-02-01T10:47:02.401001Z", "url": "https://files.pythonhosted.org/packages/a5/04/47c39325bc51ded4f8d1ecdbe96a9efbe2446c1b63010d1b669e81d2303b/chainerrl-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "058d8443840f5cd7e68e94b52d7b1ca2", "sha256": "2031317267e358578fdecdd43d00a608b004014ae7ad6aaea6d4a291d815d9ee"}, "downloads": -1, "filename": "chainerrl-0.0.2.tar.gz", "has_sig": false, "md5_digest": "058d8443840f5cd7e68e94b52d7b1ca2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 41312, "upload_time": "2017-02-20T09:19:44", "upload_time_iso_8601": "2017-02-20T09:19:44.430944Z", "url": "https://files.pythonhosted.org/packages/08/e5/0523ae5a0f1c44e86304530fa0e13299ca698fdc63129c24993803babefe/chainerrl-0.0.2.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "01ef7b16cf85b9e74b4fa79136155d97", "sha256": "7df6fdcf5cf366a35b6f15c46eab828595d936bea8777e0f29271e259da437a2"}, "downloads": -1, "filename": "chainerrl-0.1.0.tar.gz", "has_sig": false, "md5_digest": "01ef7b16cf85b9e74b4fa79136155d97", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51938, "upload_time": "2017-03-27T03:00:23", "upload_time_iso_8601": "2017-03-27T03:00:23.355492Z", "url": "https://files.pythonhosted.org/packages/6f/55/4352d2ef454934e26fcf15683cc62fa8205c9c6d145bc08090a80842d107/chainerrl-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "5f3ea712b04a9db1e2806ca27d64b75d", "sha256": "b1c470e6c8cf3a8d698592117cc4b80293038de98e46a416ca89da2ad69b730f"}, "downloads": -1, "filename": "chainerrl-0.2.0.tar.gz", "has_sig": false, "md5_digest": "5f3ea712b04a9db1e2806ca27d64b75d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 56434, "upload_time": "2017-06-08T06:56:27", "upload_time_iso_8601": "2017-06-08T06:56:27.125960Z", "url": "https://files.pythonhosted.org/packages/cf/f1/82979944600911dd941db66530a9ce9c81eddf4957bb9e7ae174821dc548/chainerrl-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "9f83adda9478a660fd0e46ffb248c6a4", "sha256": "5c698d5978fc827fcc94d904967c9f96504af50ba59e165ff66384010dd54c9d"}, "downloads": -1, "filename": "chainerrl-0.3.0.tar.gz", "has_sig": false, "md5_digest": "9f83adda9478a660fd0e46ffb248c6a4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 64452, "upload_time": "2017-12-08T09:39:06", "upload_time_iso_8601": "2017-12-08T09:39:06.453155Z", "url": "https://files.pythonhosted.org/packages/a7/d2/e71626685001985cf02b76f5942c1fd1634291187cd77d1a038d54436d94/chainerrl-0.3.0.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "a752daa4882ccb044dbb5f338bfc3c41", "sha256": "8d7eea218872c35b171e65dd76f2eef0ef0791ba5259793e4780692010f55105"}, "downloads": -1, "filename": "chainerrl-0.4.0.tar.gz", "has_sig": false, "md5_digest": "a752daa4882ccb044dbb5f338bfc3c41", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 75951, "upload_time": "2018-07-23T13:45:15", "upload_time_iso_8601": "2018-07-23T13:45:15.882877Z", "url": "https://files.pythonhosted.org/packages/e9/0a/ef1b1522c8d18da8e9cda5a59486d9b62864806454410da2e283295ed9fe/chainerrl-0.4.0.tar.gz", "yanked": false}], "0.5.0": [{"comment_text": "", "digests": {"md5": "2cf1952f316a73fe0824c55df8b31f80", "sha256": "cac2880bcd587d2e472d14d4a3772ce22aff188d8a109dced54536491014db1f"}, "downloads": -1, "filename": "chainerrl-0.5.0.tar.gz", "has_sig": false, "md5_digest": "2cf1952f316a73fe0824c55df8b31f80", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 87757, "upload_time": "2018-11-15T08:18:28", "upload_time_iso_8601": "2018-11-15T08:18:28.314456Z", "url": "https://files.pythonhosted.org/packages/e0/ce/d1d1a5a86a847407f3dad2315ca9e63de081e1072799fc9ea9771e0b4f85/chainerrl-0.5.0.tar.gz", "yanked": false}], "0.6.0": [{"comment_text": "", "digests": {"md5": "f1ecdb2a0b5a6f246d328687d748303c", "sha256": "0354e372021520f193928a3cc5034eb441ffa068c5875c01f0ce21aa56135642"}, "downloads": -1, "filename": "chainerrl-0.6.0.tar.gz", "has_sig": false, "md5_digest": "f1ecdb2a0b5a6f246d328687d748303c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 94794, "upload_time": "2019-02-28T08:55:13", "upload_time_iso_8601": "2019-02-28T08:55:13.157893Z", "url": "https://files.pythonhosted.org/packages/66/bd/d418838af29161a30e3600d232397d49518b559ff3eb5ed816ea1c0ddda0/chainerrl-0.6.0.tar.gz", "yanked": false}], "0.7.0": [{"comment_text": "", "digests": {"md5": "f9e3f24dcdc55738ec0e5ef474896dd7", "sha256": "a0bc67348ac5795aec86af60c62d94467011aae8c0c4de7f649e0b5698af222a"}, "downloads": -1, "filename": "chainerrl-0.7.0.tar.gz", "has_sig": false, "md5_digest": "f9e3f24dcdc55738ec0e5ef474896dd7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 109375, "upload_time": "2019-06-28T10:09:44", "upload_time_iso_8601": "2019-06-28T10:09:44.320737Z", "url": "https://files.pythonhosted.org/packages/2d/d8/d03b4ecc859c06c757fd0ee40c66414bd64872e8900d292fdebcb6449cd7/chainerrl-0.7.0.tar.gz", "yanked": false}], "0.8.0": [{"comment_text": "", "digests": {"md5": "3d39d5f94bd5b41006d1b182788f3207", "sha256": "ee153b891360f35f88ab19017b4b5cad66f2df1cbf77dfb3cd66e4fd63a677b0"}, "downloads": -1, "filename": "chainerrl-0.8.0.tar.gz", "has_sig": false, "md5_digest": "3d39d5f94bd5b41006d1b182788f3207", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 121041, "upload_time": "2020-02-14T05:35:56", "upload_time_iso_8601": "2020-02-14T05:35:56.876941Z", "url": "https://files.pythonhosted.org/packages/33/21/9a489b1fe73d074df76f2c4bde970e8c0762b675fd0819e4ae9039b9252a/chainerrl-0.8.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3d39d5f94bd5b41006d1b182788f3207", "sha256": "ee153b891360f35f88ab19017b4b5cad66f2df1cbf77dfb3cd66e4fd63a677b0"}, "downloads": -1, "filename": "chainerrl-0.8.0.tar.gz", "has_sig": false, "md5_digest": "3d39d5f94bd5b41006d1b182788f3207", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 121041, "upload_time": "2020-02-14T05:35:56", "upload_time_iso_8601": "2020-02-14T05:35:56.876941Z", "url": "https://files.pythonhosted.org/packages/33/21/9a489b1fe73d074df76f2c4bde970e8c0762b675fd0819e4ae9039b9252a/chainerrl-0.8.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:34:32 2020"}