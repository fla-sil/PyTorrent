{"info": {"author": "Thykof", "author_email": "thykof@protonmail.ch", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Internet :: WWW/HTTP :: Indexing/Search"], "description": "# Swiftea Crawler\n\n[![Build Status](https://travis-ci.org/Swiftea/Crawler.svg?branch=master)](https://travis-ci.org/Swiftea/Crawler)\n[![Coverage Status](https://coveralls.io/repos/github/Swiftea/Crawler/badge.svg?branch=master)](https://coveralls.io/github/Swiftea/Crawler?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/crawler/badge/?version=master)](http://crawler.readthedocs.io/en/master/?badge=master)\n[![Code Health](https://landscape.io/github/Swiftea/Crawler/master/landscape.svg?style=flat)](https://landscape.io/github/Swiftea/Crawler/master)\n[![Requirements Status](https://requires.io/github/Swiftea/Crawler/requirements.svg?branch=master)](https://requires.io/github/Swiftea/Crawler/requirements/?branch=master)\n\n## Description\n\nSwiftea-Crawler is an open source web crawler for Swiftea search engine.\n\nCurrently, it can:\n  - Visit websites\n    - check robots.txt\n    - search encoding\n  - Parse them\n    - extract data\n      - title\n      - description\n      - ...\n    - extract words\n      - filter stopwords\n  - Index them\n    - in database\n    - in inverted-index\n  - Archive log files in a zip file\n\t- avoid duplicates (http and https)\n\nThe domain crawler focus on the links that belong to the given domain name.\nThe level option of the domain crawler defines how deep the crawl goes.\nFor example, the level 2 means the crawler will crawl all the links of the domain plus the links that all pages in this domain lead to.\n\nThe domain crawler can use a MongoDB database to store the inverted index.\n\n## Install and usage\n\n\n### Run crawler\n\nCreate `crawler-config.json` file and fill it:\n\n    {\n      \"DIR_DATA\": \"data\",\n\n      \"DB_HOST\": \"\",\n      \"DB_USER\": \"\",\n      \"DB_PASSWORD\": \"\",\n      \"DB_NAME\": \"\",\n      \"TABLE_NAMES\": [\"website\", \"suggestion\"],\n      \"DIR_INDEX\": \"ii/\",\n      \"FTP_HOST\": \"\",\n      \"FTP_USER\": \"\",\n      \"FTP_PASSWORD\": \"\",\n      \"FTP_PORT\": 21,\n      \"FTP_DATA\": \"/www/data/\",\n      \"FTP_INDEX\": \"/www/data/inverted_index\",\n\n      \"HOST\": \"\",\n\n      \"MONGODB_PASSWORD\": \"\",\n      \"MONGODB_USER\": \"\",\n      \"MONGODB_CON_STRING\": \"\"\n    }\n\nThen:\n\n    from crawler import main\n\n    # infinite crawling:\n    crawler = main(l1=50, l2=10, dir_data='data1')\n\n    # domain crawling:\n    crawler = main(url='http://example.example', level=0, target_level=1, dir_data='data1')\n    crawler = main(url='http://some.thing', level=1, target_level=3, use_mongodb=True)\n\n    crawler.start()\n\n### Setup\n\n    virtualenv -p /usr/bin/python3 crawler-env\n    source crawler-env/bin/activate\n    pip install -r requirements.txt\n\n### Run tests\n\nUsing only pytest:\n\n    python setup.py test\n\nWith coverage:\n\n    coverage run setup.py test\n    coverage report\n    coverage html\n\n\n### Build documentation\n\nYou must install `python3-sphinx` package.\n\n    cd docs\n    make html\n\n### Run linter\n\nInstall `prospector`, then:\n\n    prospector > prospector_output.json\n\n### Deploy\n\nCreate directories in ftp server:\n\n - /www/data/badwords\n - /www/data/stopwords\n - /www/data/inverted_index\n\nUpload the list of words: `/www/[type]/[lang].[type].txt`.\n\nCreate database with `sql/swiftea_mysql_db.sql`.\n\n\n## How it works?\n\nIf the files below don't exist, the crawler will download them from our server:\n\n- data/stopwords/fr.stopwords.txt\n- data/stopwords/en.stopwords.txt\n- data/badwords/fr.badwords.txt\n- data/badwords/en.badwords.txt\n\nIn `crawler-config.json`, if FTP_INDEX is `\"\"`, then the inverted index will be save in `DIR_INDEX` but not send on the FTP server.\n\n### Database:\nThe DatabaseSwiftea object can:\n - send documents\n - get the id of a document by the url\n - delete a document\n - select the suggestions\n - check if a doc exists\n - check for http and https duplicate\n\n## Limits\n\nWhen stoping the crawler (ctrl+V), it will not restart with the interupted url.\n\n## Version\n\nCurrent version is 1.1.3\n\n## Tech\n\nSwiftea's Crawler uses a number of open source projects to work properly:\n\n- [Python 3](https://www.python.org/)\n  - [Reppy](https://github.com/seomoz/reppy)\n  - [PyMySQL](https://github.com/PyMySQL/PyMySQL/)\n  - [Requests](https://github.com/kennethreitz/requests)\n\n\n## Contributing\n\nWant to contribute? Great!\n\nFork the repository. Then, run:\n\n    git clone git@github.com:<username>/Crawler.git\n    cd Crawler\n\nThen, do your work and commit your changes. Finally, make a pull request.\n\n### Commit conventions:\n\n#### General\n  - Use the present tense\n  - Use the imperative mood\n\n#### Examples\n  - Add something: \"Add feature ...\"\n  - Update: \"Update ...\"\n  - Improve something: \"Improve ...\"\n  - Change something: \"Change ...\"\n  - Fix something: \"Fix ...\"\n  - Fix an issue: \"Fix #123456\" or \"Close #123456\"\n\nLicense\n----\n\nGNU GENERAL PUBLIC LICENSE (v3)\n\n**Free Software, Hell Yeah!**\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Swiftea/Crawler", "keywords": "crawler swiftea", "license": "GNU GPL v3", "maintainer": "", "maintainer_email": "", "name": "swiftea-crawler", "package_url": "https://pypi.org/project/swiftea-crawler/", "platform": "", "project_url": "https://pypi.org/project/swiftea-crawler/", "project_urls": {"Homepage": "https://github.com/Swiftea/Crawler"}, "release_url": "https://pypi.org/project/swiftea-crawler/1.1.3/", "requires_dist": ["PyMySQL (==0.9.3)", "coverage (==4.5.3)", "dnspython (==1.16.0)", "paramiko (==2.5.0)", "pymodm (==0.4.1)", "pymongo (==3.8.0)", "pytest (==5.0.1)", "reppy (==0.4.13)", "requests", "sphinx-rtd-theme (==0.4.3)", "coverage ; extra == 'testing'", "pytest ; extra == 'testing'"], "requires_python": "", "summary": "Swiftea's Open Source Web Crawler", "version": "1.1.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Swiftea Crawler</h1>\n<p><a href=\"https://travis-ci.org/Swiftea/Crawler\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c118911a370cd38df0b65cedf90af33f25f8e0d7/68747470733a2f2f7472617669732d63692e6f72672f537769667465612f437261776c65722e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/Swiftea/Crawler?branch=master\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cd30f60b51c94ea140d2e7503aa588ede564f067/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f537769667465612f437261776c65722f62616467652e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"http://crawler.readthedocs.io/en/master/?badge=master\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fbdc11de5b6b4901ef1f2750fcc06ecbf67e09e0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f637261776c65722f62616467652f3f76657273696f6e3d6d6173746572\"></a>\n<a href=\"https://landscape.io/github/Swiftea/Crawler/master\" rel=\"nofollow\"><img alt=\"Code Health\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/322f9d591f6395cb40f0a26c9e26c143eb416acf/68747470733a2f2f6c616e6473636170652e696f2f6769746875622f537769667465612f437261776c65722f6d61737465722f6c616e6473636170652e7376673f7374796c653d666c6174\"></a>\n<a href=\"https://requires.io/github/Swiftea/Crawler/requirements/?branch=master\" rel=\"nofollow\"><img alt=\"Requirements Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0ef757825d14e6bede777ad9530507fc84062243/68747470733a2f2f72657175697265732e696f2f6769746875622f537769667465612f437261776c65722f726571756972656d656e74732e7376673f6272616e63683d6d6173746572\"></a></p>\n<h2>Description</h2>\n<p>Swiftea-Crawler is an open source web crawler for Swiftea search engine.</p>\n<p>Currently, it can:</p>\n<ul>\n<li>Visit websites\n<ul>\n<li>check robots.txt</li>\n<li>search encoding</li>\n</ul>\n</li>\n<li>Parse them\n<ul>\n<li>extract data\n<ul>\n<li>title</li>\n<li>description</li>\n<li>...</li>\n</ul>\n</li>\n<li>extract words\n<ul>\n<li>filter stopwords</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Index them\n<ul>\n<li>in database</li>\n<li>in inverted-index</li>\n</ul>\n</li>\n<li>Archive log files in a zip file\n<ul>\n<li>avoid duplicates (http and https)</li>\n</ul>\n</li>\n</ul>\n<p>The domain crawler focus on the links that belong to the given domain name.\nThe level option of the domain crawler defines how deep the crawl goes.\nFor example, the level 2 means the crawler will crawl all the links of the domain plus the links that all pages in this domain lead to.</p>\n<p>The domain crawler can use a MongoDB database to store the inverted index.</p>\n<h2>Install and usage</h2>\n<h3>Run crawler</h3>\n<p>Create <code>crawler-config.json</code> file and fill it:</p>\n<pre><code>{\n  \"DIR_DATA\": \"data\",\n\n  \"DB_HOST\": \"\",\n  \"DB_USER\": \"\",\n  \"DB_PASSWORD\": \"\",\n  \"DB_NAME\": \"\",\n  \"TABLE_NAMES\": [\"website\", \"suggestion\"],\n  \"DIR_INDEX\": \"ii/\",\n  \"FTP_HOST\": \"\",\n  \"FTP_USER\": \"\",\n  \"FTP_PASSWORD\": \"\",\n  \"FTP_PORT\": 21,\n  \"FTP_DATA\": \"/www/data/\",\n  \"FTP_INDEX\": \"/www/data/inverted_index\",\n\n  \"HOST\": \"\",\n\n  \"MONGODB_PASSWORD\": \"\",\n  \"MONGODB_USER\": \"\",\n  \"MONGODB_CON_STRING\": \"\"\n}\n</code></pre>\n<p>Then:</p>\n<pre><code>from crawler import main\n\n# infinite crawling:\ncrawler = main(l1=50, l2=10, dir_data='data1')\n\n# domain crawling:\ncrawler = main(url='http://example.example', level=0, target_level=1, dir_data='data1')\ncrawler = main(url='http://some.thing', level=1, target_level=3, use_mongodb=True)\n\ncrawler.start()\n</code></pre>\n<h3>Setup</h3>\n<pre><code>virtualenv -p /usr/bin/python3 crawler-env\nsource crawler-env/bin/activate\npip install -r requirements.txt\n</code></pre>\n<h3>Run tests</h3>\n<p>Using only pytest:</p>\n<pre><code>python setup.py test\n</code></pre>\n<p>With coverage:</p>\n<pre><code>coverage run setup.py test\ncoverage report\ncoverage html\n</code></pre>\n<h3>Build documentation</h3>\n<p>You must install <code>python3-sphinx</code> package.</p>\n<pre><code>cd docs\nmake html\n</code></pre>\n<h3>Run linter</h3>\n<p>Install <code>prospector</code>, then:</p>\n<pre><code>prospector &gt; prospector_output.json\n</code></pre>\n<h3>Deploy</h3>\n<p>Create directories in ftp server:</p>\n<ul>\n<li>/www/data/badwords</li>\n<li>/www/data/stopwords</li>\n<li>/www/data/inverted_index</li>\n</ul>\n<p>Upload the list of words: <code>/www/[type]/[lang].[type].txt</code>.</p>\n<p>Create database with <code>sql/swiftea_mysql_db.sql</code>.</p>\n<h2>How it works?</h2>\n<p>If the files below don't exist, the crawler will download them from our server:</p>\n<ul>\n<li>data/stopwords/fr.stopwords.txt</li>\n<li>data/stopwords/en.stopwords.txt</li>\n<li>data/badwords/fr.badwords.txt</li>\n<li>data/badwords/en.badwords.txt</li>\n</ul>\n<p>In <code>crawler-config.json</code>, if FTP_INDEX is <code>\"\"</code>, then the inverted index will be save in <code>DIR_INDEX</code> but not send on the FTP server.</p>\n<h3>Database:</h3>\n<p>The DatabaseSwiftea object can:</p>\n<ul>\n<li>send documents</li>\n<li>get the id of a document by the url</li>\n<li>delete a document</li>\n<li>select the suggestions</li>\n<li>check if a doc exists</li>\n<li>check for http and https duplicate</li>\n</ul>\n<h2>Limits</h2>\n<p>When stoping the crawler (ctrl+V), it will not restart with the interupted url.</p>\n<h2>Version</h2>\n<p>Current version is 1.1.3</p>\n<h2>Tech</h2>\n<p>Swiftea's Crawler uses a number of open source projects to work properly:</p>\n<ul>\n<li><a href=\"https://www.python.org/\" rel=\"nofollow\">Python 3</a>\n<ul>\n<li><a href=\"https://github.com/seomoz/reppy\" rel=\"nofollow\">Reppy</a></li>\n<li><a href=\"https://github.com/PyMySQL/PyMySQL/\" rel=\"nofollow\">PyMySQL</a></li>\n<li><a href=\"https://github.com/kennethreitz/requests\" rel=\"nofollow\">Requests</a></li>\n</ul>\n</li>\n</ul>\n<h2>Contributing</h2>\n<p>Want to contribute? Great!</p>\n<p>Fork the repository. Then, run:</p>\n<pre><code>git clone git@github.com:&lt;username&gt;/Crawler.git\ncd Crawler\n</code></pre>\n<p>Then, do your work and commit your changes. Finally, make a pull request.</p>\n<h3>Commit conventions:</h3>\n<h4>General</h4>\n<ul>\n<li>Use the present tense</li>\n<li>Use the imperative mood</li>\n</ul>\n<h4>Examples</h4>\n<ul>\n<li>Add something: \"Add feature ...\"</li>\n<li>Update: \"Update ...\"</li>\n<li>Improve something: \"Improve ...\"</li>\n<li>Change something: \"Change ...\"</li>\n<li>Fix something: \"Fix ...\"</li>\n<li>Fix an issue: \"Fix #123456\" or \"Close #123456\"</li>\n</ul>\n<h2>License</h2>\n<p>GNU GENERAL PUBLIC LICENSE (v3)</p>\n<p><strong>Free Software, Hell Yeah!</strong></p>\n\n          </div>"}, "last_serial": 5673059, "releases": {"1.1.2": [{"comment_text": "", "digests": {"md5": "136c609d336ea16ea47296199dd4bdf0", "sha256": "b9e8f8a0b47b6c0c7833ec677725cc72635890d721b2f0f3fbefe4b06a5207d5"}, "downloads": -1, "filename": "swiftea_crawler-1.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "136c609d336ea16ea47296199dd4bdf0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 60379, "upload_time": "2019-08-03T14:47:46", "upload_time_iso_8601": "2019-08-03T14:47:46.292692Z", "url": "https://files.pythonhosted.org/packages/a8/6b/53bd78db1c17d678abcffaec6dfd7405b3b7d03d1c05593eb23e94c309bf/swiftea_crawler-1.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4c54c2be7e5723f830f3f92199fba08b", "sha256": "76a2d020dd03538af8fc012177ee0ca7e3e2a641968f8bf494e685508678c42c"}, "downloads": -1, "filename": "swiftea-crawler-1.1.2.tar.gz", "has_sig": false, "md5_digest": "4c54c2be7e5723f830f3f92199fba08b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 37223, "upload_time": "2019-08-03T14:47:48", "upload_time_iso_8601": "2019-08-03T14:47:48.189832Z", "url": "https://files.pythonhosted.org/packages/be/4b/d1bde1aa12e57f92dae7a5c82dedffec337d06433f0e213b6da7d10fcba5/swiftea-crawler-1.1.2.tar.gz", "yanked": false}], "1.1.3": [{"comment_text": "", "digests": {"md5": "625ece7f7bce62f30303633731dac8de", "sha256": "2b9d31685b562bab6d5c7480e99ed5e9bbc78dd8148909f78343e56f61066e78"}, "downloads": -1, "filename": "swiftea_crawler-1.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "625ece7f7bce62f30303633731dac8de", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 60815, "upload_time": "2019-08-13T18:28:37", "upload_time_iso_8601": "2019-08-13T18:28:37.969016Z", "url": "https://files.pythonhosted.org/packages/3e/7f/627132b46cd676be0c2a4738c2047c8231cc18c80baec6256ef4a5843eb4/swiftea_crawler-1.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c2107dbbedfe2d43e91ba4956e82ac6c", "sha256": "f3335c2ee466292f61a76888be8d741b7750fdc5d9d3605906db46c9a229a8bb"}, "downloads": -1, "filename": "swiftea-crawler-1.1.3.tar.gz", "has_sig": false, "md5_digest": "c2107dbbedfe2d43e91ba4956e82ac6c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 37837, "upload_time": "2019-08-13T18:28:39", "upload_time_iso_8601": "2019-08-13T18:28:39.800328Z", "url": "https://files.pythonhosted.org/packages/a4/e2/77b6d78a07a3b3a292f6989413eb17932e2a34922787217081a3878110cf/swiftea-crawler-1.1.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "625ece7f7bce62f30303633731dac8de", "sha256": "2b9d31685b562bab6d5c7480e99ed5e9bbc78dd8148909f78343e56f61066e78"}, "downloads": -1, "filename": "swiftea_crawler-1.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "625ece7f7bce62f30303633731dac8de", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 60815, "upload_time": "2019-08-13T18:28:37", "upload_time_iso_8601": "2019-08-13T18:28:37.969016Z", "url": "https://files.pythonhosted.org/packages/3e/7f/627132b46cd676be0c2a4738c2047c8231cc18c80baec6256ef4a5843eb4/swiftea_crawler-1.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c2107dbbedfe2d43e91ba4956e82ac6c", "sha256": "f3335c2ee466292f61a76888be8d741b7750fdc5d9d3605906db46c9a229a8bb"}, "downloads": -1, "filename": "swiftea-crawler-1.1.3.tar.gz", "has_sig": false, "md5_digest": "c2107dbbedfe2d43e91ba4956e82ac6c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 37837, "upload_time": "2019-08-13T18:28:39", "upload_time_iso_8601": "2019-08-13T18:28:39.800328Z", "url": "https://files.pythonhosted.org/packages/a4/e2/77b6d78a07a3b3a292f6989413eb17932e2a34922787217081a3878110cf/swiftea-crawler-1.1.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:59:30 2020"}