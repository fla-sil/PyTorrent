{"info": {"author": "Zac Winzurk", "author_email": "zwinzurk@asu.edu", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "=====================================\nTorch Traps :leopard: :camera_flash:\n=====================================\n\n\n.. image:: https://img.shields.io/pypi/v/torchtraps.svg\n        :target: https://pypi.python.org/pypi/torchtraps\n\n.. image:: https://img.shields.io/travis/winzurk/torchtraps.svg\n        :target: https://travis-ci.com/winzurk/torchtraps\n\n.. image:: https://readthedocs.org/projects/torchtraps/badge/?version=latest\n        :target: https://torchtraps.readthedocs.io/en/latest/?badge=latest\n        :alt: Documentation Status\n\n\nTorch Traps is python package for *lighting* :zap: *fast* wildlife camera trap image annotation based on PyTorch. :fire:\n\n* Documentation: https://torchtraps.readthedocs.io.\n* GitHub: https://github.com/winzurk/torchtraps\n* PyPI: https://pypi.python.org/pypi/torchtraps\n* MIT License\n\n.. torchtraps/sample_images/NJP-2.JPG\n\n.. image:: https://github.com/winzurk/torchtraps/blob/master/torchtraps/sample_images/NJP-2.JPG\n        :target: https://github.com/winzurk/torchtraps/blob/master/torchtraps/sample_images/NJP-2.JPG\n        :width: 300\n\nPhoto Credit: Northern Jaguar Project\n\nOver the past several decades, biologists all over the world have widely adopted camera traps as a standard tool for\nmonitoring biodiversity, resulting in backlogs often on the order of millions of images waiting to be manually reviewed\nby humans to assess wildlife population densities. The application of modern computer vision and deep learning methods\nto accelerate the processing of wildlife camera trap data has the potential to alleviate existing bottlenecks for large\nscale biodiversity monitoring, thus dramatically increasing the speed at which researchers can obtain data-driven\ninsights about ecosystems, and ultimately leading to more efficient resource allocation and better informed policy\nmaking by NGOs and government agencies.\n\nTorch Traps aims to provide a simple tool (as little as 1 line of code) to bring state-of-the-art computer vision models\ninto the hands of biologists to accelerate the speed at which they can review camera trap imagery.\n\n\nInstall\n--------\n.. code-block:: bash\n\n    $ pip install torchtraps\n\nFast Inference on Folder of Images\n-------------------------------------------------\n\nClassify an entire folder of camera trap images in one line of code by simply passing the relative path to the folder\ncontaining images. Outputs are automatically saved to a csv file which can be further processed opened in an application\nlike Excel.\n\n.. code-block:: python\n\n    import torchtraps.lightning import kachow\n\n    kachow('path/to/image/folder')\n\n\n.. csv-table:: Example Output File\n    :header: \"image\", \"prediction\", \"confidence\"\n\n        \"image1.jpg\", \"jaguar\", 0.99\n        \"image2.jpg\", \"empty\", 0.98\n        \"image3.jpg\", \"agouti\", 0.91\n        \"image4.jpg\", \"empty\", 0.95\n        \"image5.jpg\", \"ocelot\", 0.87\n\n\n\nFeatures\n--------\n\n* Module for fast computer vision on camera trap images.\n* Train and fine-tune classification models on your own dataset.\n* Based on PyTorch\n* MIT license\n\n\nComplete Installation Tutorial from Scratch\n----------------------------------------------\nThis is a full tutorial on how to install and get up and running with Torch Traps. Zero programming knowledge is\nassumed in the attempt to make Torch Traps as accessible as possible. If you do run into any problems, please email\nme at zwinzurk@asu.edu\n\n* Step 1: Install Anaconda\n\n    Go to https://www.anaconda.com/distribution/\n\n    Download Anaconda Python 3.7 version for the operating system you are using (Windows, MacOS, or Linux).\n\n    Click on 64-Bit Graphical Installer (442 MB) to download the version with a Graphical User Interface.\n\n    .. image:: tutorial/AnacondaDownload.jpg\n        :width: 300\n\n    Why do I need Anaconda?\n\n        Torch Traps is a module written in `Python <http://www.python.org/>`_ (a programming language), so we first need to have Python installed\n        on our computer. There are several ways to install python, but Anaconda allows us to install Python and it comes\n        pre-installed with many of the common modules used for Data Science, and optionally comes with a GUI which can\n        be used to open notebooks.\n\n    After download is complete, double-click to install and follow installation instructions.\n\n    .. image:: tutorial/InstallAnaconda.jpg\n        :width: 300\n\n\n* Step 2: Open Anaconda Navigator\n\n    After installing Anaconda, open the Anaconda Navigator application on your computer.\n\n    .. image:: tutorial/OpenNavigator.jpg\n        :width: 300\n\n* Step 3: Launch Jupyter Lab\n\n    We will then launch a Jupyter Lab. Your web browser will open but the Jupyter server is running locally as you can\n    see the url should be http://localhost:8889/lab\n\n    .. image:: tutorial/LaunchJupyter.jpg\n        :width: 300\n\n* Step 4: Navigate to Working Folder on Left\n\n    By clicking on the folder icon in the upper-left corner we can navigate the file system.\n\n    Navigate to the directory on your computer where your camera trap image folder is located.\n\n* Step 5: Open Python3 Notebook\n\n    Now that we are working in the right directory, we can launch a new Python notebook. This will create a new file in\n    our working directory called Untitled.ipynb. We can right-click on the file name to re-name it.\n\n    .. image:: tutorial/CreateNotebook.jpg\n        :width: 300\n\n* Step 6: Install Torch Traps\n\n    Jupyter notebooks allow us to run python code one 'cell' at a time. So the first thing we need to do is install\n    torch traps, if we have not before. Copy the code below into the first cell, and then run the cell by either\n    clicking the play button or hitting SHIFT+ENTER at the same.\n\n    .. code-block:: bash\n\n        !pip install torch traps\n\n    .. image:: tutorial/InstallTorchTraps.jpg\n        :width: 300\n\n * Step 7: Run Torch Traps on Folder of Images\n\n    Now that the Torch Traps is installed, you can copy the code below into a new code cell.\n\n    Change the 'path/to/image/folder' to the name of your folder containing camera trap images (ex. 'camera_trap_images')\n\n    Run the cell. (SHIFT + ENTER)\n\n    Note: If running for the first time, an internet connection will be required to download the model file.\n\n    When complete an output.csv file will appear in the directory you are working in. You can double-click csv files to\n    view in Jupyter Lab or open with another application like Excel.\n\n    .. code-block:: python\n\n        import torchtraps.lightning import kachow\n        kachow('path/to/image/folder')\n\n\n    .. image:: tutorial/RunTorchTraps.jpg\n        :width: 300\n\n* Step 8: Open CSV File To See Classification Results\n\n.. Future: Step 9: View Images of Particular Class\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=======\nHistory\n=======\n\n0.1.0 (2020-03-30)\n------------------\n\n* First release on PyPI.\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/winzurk/torchtraps", "keywords": "torchtraps", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "torchtraps", "package_url": "https://pypi.org/project/torchtraps/", "platform": "", "project_url": "https://pypi.org/project/torchtraps/", "project_urls": {"Homepage": "https://github.com/winzurk/torchtraps"}, "release_url": "https://pypi.org/project/torchtraps/0.1.4/", "requires_dist": ["Pillow (>=7.0.0)", "barbar (>=0.2.1)", "jpeg4py (>=0.1.4)", "matplotlib (>=3.0.0)", "numpy (>=1.18.1)", "opencv-python (>=4.2.0.32)", "pandas (>=0.25.0)", "scikit-learn (>=0.22.1)", "scikit-plot (>=0.3.7)", "torch (>=1.4.0)", "torchvision (>=0.5.0)"], "requires_python": ">=3.5", "summary": "Python package for computer vision on camera trap images.", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"torch-traps-leopard-camera-flash\">\n<h2>Torch Traps :leopard: :camera_flash:</h2>\n<a href=\"https://pypi.python.org/pypi/torchtraps\" rel=\"nofollow\"><img alt=\"https://img.shields.io/pypi/v/torchtraps.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d269f9b7134b6c4c959c4ce7694056971cae0953/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f746f72636874726170732e737667\"></a>\n<a href=\"https://travis-ci.com/winzurk/torchtraps\" rel=\"nofollow\"><img alt=\"https://img.shields.io/travis/winzurk/torchtraps.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a5a32de219417868701da8fa75cfe3e6d8440309/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f77696e7a75726b2f746f72636874726170732e737667\"></a>\n<a href=\"https://torchtraps.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4bfe217db235691e44378aa5e541eb74c00f7f5a/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f746f72636874726170732f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<p>Torch Traps is python package for <em>lighting</em> :zap: <em>fast</em> wildlife camera trap image annotation based on PyTorch. :fire:</p>\n<ul>\n<li>Documentation: <a href=\"https://torchtraps.readthedocs.io\" rel=\"nofollow\">https://torchtraps.readthedocs.io</a>.</li>\n<li>GitHub: <a href=\"https://github.com/winzurk/torchtraps\" rel=\"nofollow\">https://github.com/winzurk/torchtraps</a></li>\n<li>PyPI: <a href=\"https://pypi.python.org/pypi/torchtraps\" rel=\"nofollow\">https://pypi.python.org/pypi/torchtraps</a></li>\n<li>MIT License</li>\n</ul>\n<a href=\"https://github.com/winzurk/torchtraps/blob/master/torchtraps/sample_images/NJP-2.JPG\" rel=\"nofollow\"><img alt=\"https://github.com/winzurk/torchtraps/blob/master/torchtraps/sample_images/NJP-2.JPG\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b8fad81e4de212aca028bc0611c33e770c244806/68747470733a2f2f6769746875622e636f6d2f77696e7a75726b2f746f72636874726170732f626c6f622f6d61737465722f746f72636874726170732f73616d706c655f696d616765732f4e4a502d322e4a5047\" width=\"300\"></a>\n<p>Photo Credit: Northern Jaguar Project</p>\n<p>Over the past several decades, biologists all over the world have widely adopted camera traps as a standard tool for\nmonitoring biodiversity, resulting in backlogs often on the order of millions of images waiting to be manually reviewed\nby humans to assess wildlife population densities. The application of modern computer vision and deep learning methods\nto accelerate the processing of wildlife camera trap data has the potential to alleviate existing bottlenecks for large\nscale biodiversity monitoring, thus dramatically increasing the speed at which researchers can obtain data-driven\ninsights about ecosystems, and ultimately leading to more efficient resource allocation and better informed policy\nmaking by NGOs and government agencies.</p>\n<p>Torch Traps aims to provide a simple tool (as little as 1 line of code) to bring state-of-the-art computer vision models\ninto the hands of biologists to accelerate the speed at which they can review camera trap imagery.</p>\n<div id=\"install\">\n<h3>Install</h3>\n<pre>$ pip install torchtraps\n</pre>\n</div>\n<div id=\"fast-inference-on-folder-of-images\">\n<h3>Fast Inference on Folder of Images</h3>\n<p>Classify an entire folder of camera trap images in one line of code by simply passing the relative path to the folder\ncontaining images. Outputs are automatically saved to a csv file which can be further processed opened in an application\nlike Excel.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torchtraps.lightning</span> <span class=\"kn\">import</span> <span class=\"nn\">kachow</span>\n\n<span class=\"n\">kachow</span><span class=\"p\">(</span><span class=\"s1\">'path/to/image/folder'</span><span class=\"p\">)</span>\n</pre>\n<table>\n<caption>Example Output File</caption>\n<colgroup>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>image</th>\n<th>prediction</th>\n<th>confidence</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>image1.jpg</td>\n<td>jaguar</td>\n<td>0.99</td>\n</tr>\n<tr><td>image2.jpg</td>\n<td>empty</td>\n<td>0.98</td>\n</tr>\n<tr><td>image3.jpg</td>\n<td>agouti</td>\n<td>0.91</td>\n</tr>\n<tr><td>image4.jpg</td>\n<td>empty</td>\n<td>0.95</td>\n</tr>\n<tr><td>image5.jpg</td>\n<td>ocelot</td>\n<td>0.87</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div id=\"features\">\n<h3>Features</h3>\n<ul>\n<li>Module for fast computer vision on camera trap images.</li>\n<li>Train and fine-tune classification models on your own dataset.</li>\n<li>Based on PyTorch</li>\n<li>MIT license</li>\n</ul>\n</div>\n<div id=\"complete-installation-tutorial-from-scratch\">\n<h3>Complete Installation Tutorial from Scratch</h3>\n<p>This is a full tutorial on how to install and get up and running with Torch Traps. Zero programming knowledge is\nassumed in the attempt to make Torch Traps as accessible as possible. If you do run into any problems, please email\nme at <a href=\"mailto:zwinzurk%40asu.edu\">zwinzurk<span>@</span>asu<span>.</span>edu</a></p>\n<ul>\n<li><p>Step 1: Install Anaconda</p>\n<blockquote>\n<p>Go to <a href=\"https://www.anaconda.com/distribution/\" rel=\"nofollow\">https://www.anaconda.com/distribution/</a></p>\n<p>Download Anaconda Python 3.7 version for the operating system you are using (Windows, MacOS, or Linux).</p>\n<p>Click on 64-Bit Graphical Installer (442 MB) to download the version with a Graphical User Interface.</p>\n<img alt=\"tutorial/AnacondaDownload.jpg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5a28001ddfeca1f4931d8cfe09d3b1519d088385/7475746f7269616c2f416e61636f6e6461446f776e6c6f61642e6a7067\" width=\"300\">\n<p>Why do I need Anaconda?</p>\n<blockquote>\n<p>Torch Traps is a module written in <a href=\"http://www.python.org/\" rel=\"nofollow\">Python</a> (a programming language), so we first need to have Python installed\non our computer. There are several ways to install python, but Anaconda allows us to install Python and it comes\npre-installed with many of the common modules used for Data Science, and optionally comes with a GUI which can\nbe used to open notebooks.</p>\n</blockquote>\n<p>After download is complete, double-click to install and follow installation instructions.</p>\n<img alt=\"tutorial/InstallAnaconda.jpg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5da79eb2677e4eff9bcf63437f77dcf92c0e2d78/7475746f7269616c2f496e7374616c6c416e61636f6e64612e6a7067\" width=\"300\">\n</blockquote>\n</li>\n<li><p>Step 2: Open Anaconda Navigator</p>\n<blockquote>\n<p>After installing Anaconda, open the Anaconda Navigator application on your computer.</p>\n<img alt=\"tutorial/OpenNavigator.jpg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cd92c6a13c0ef2e0d6696463fbfe56b7084a1e8d/7475746f7269616c2f4f70656e4e6176696761746f722e6a7067\" width=\"300\">\n</blockquote>\n</li>\n<li><p>Step 3: Launch Jupyter Lab</p>\n<blockquote>\n<p>We will then launch a Jupyter Lab. Your web browser will open but the Jupyter server is running locally as you can\nsee the url should be <a href=\"http://localhost:8889/lab\" rel=\"nofollow\">http://localhost:8889/lab</a></p>\n<img alt=\"tutorial/LaunchJupyter.jpg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7f4a601752c707e0df889b38b5e4ba4858a6e29d/7475746f7269616c2f4c61756e63684a7570797465722e6a7067\" width=\"300\">\n</blockquote>\n</li>\n<li><p>Step 4: Navigate to Working Folder on Left</p>\n<blockquote>\n<p>By clicking on the folder icon in the upper-left corner we can navigate the file system.</p>\n<p>Navigate to the directory on your computer where your camera trap image folder is located.</p>\n</blockquote>\n</li>\n<li><p>Step 5: Open Python3 Notebook</p>\n<blockquote>\n<p>Now that we are working in the right directory, we can launch a new Python notebook. This will create a new file in\nour working directory called Untitled.ipynb. We can right-click on the file name to re-name it.</p>\n<img alt=\"tutorial/CreateNotebook.jpg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b07ddf530bffd67fed16fc5055b8371d88d84f13/7475746f7269616c2f4372656174654e6f7465626f6f6b2e6a7067\" width=\"300\">\n</blockquote>\n</li>\n<li><p>Step 6: Install Torch Traps</p>\n<blockquote>\n<p>Jupyter notebooks allow us to run python code one \u2018cell\u2019 at a time. So the first thing we need to do is install\ntorch traps, if we have not before. Copy the code below into the first cell, and then run the cell by either\nclicking the play button or hitting SHIFT+ENTER at the same.</p>\n<pre>!pip install torch traps\n</pre>\n<img alt=\"tutorial/InstallTorchTraps.jpg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ee6805cd34a093555c7d0d26f94a42e590518ded/7475746f7269616c2f496e7374616c6c546f72636854726170732e6a7067\" width=\"300\">\n</blockquote>\n</li>\n</ul>\n<blockquote>\n<ul>\n<li><p>Step 7: Run Torch Traps on Folder of Images</p>\n<blockquote>\n<p>Now that the Torch Traps is installed, you can copy the code below into a new code cell.</p>\n<p>Change the \u2018path/to/image/folder\u2019 to the name of your folder containing camera trap images (ex. \u2018camera_trap_images\u2019)</p>\n<p>Run the cell. (SHIFT + ENTER)</p>\n<p>Note: If running for the first time, an internet connection will be required to download the model file.</p>\n<p>When complete an output.csv file will appear in the directory you are working in. You can double-click csv files to\nview in Jupyter Lab or open with another application like Excel.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torchtraps.lightning</span> <span class=\"kn\">import</span> <span class=\"nn\">kachow</span>\n<span class=\"n\">kachow</span><span class=\"p\">(</span><span class=\"s1\">'path/to/image/folder'</span><span class=\"p\">)</span>\n</pre>\n<img alt=\"tutorial/RunTorchTraps.jpg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/eebcb35a54d0e6a15b6669f20b99a1ffc5ed2d53/7475746f7269616c2f52756e546f72636854726170732e6a7067\" width=\"300\">\n</blockquote>\n</li>\n</ul>\n</blockquote>\n<ul>\n<li>Step 8: Open CSV File To See Classification Results</li>\n</ul>\n</div>\n</div>\n<div id=\"history\">\n<h2>History</h2>\n<h2 id=\"id1\"><span class=\"section-subtitle\">0.1.0 (2020-03-30)</span></h2>\n<ul>\n<li>First release on PyPI.</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 6919157, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "7b7a35e7ac2d30533a11b43812450fcf", "sha256": "876f59c37fbc4b4dc6e7c53a57137544b27fe926482d3ec5a9d77712e9c5b094"}, "downloads": -1, "filename": "torchtraps-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "7b7a35e7ac2d30533a11b43812450fcf", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 14329, "upload_time": "2020-03-30T08:19:28", "upload_time_iso_8601": "2020-03-30T08:19:28.553399Z", "url": "https://files.pythonhosted.org/packages/3b/ea/f1475c41f3e60abae45513d5d8431aaa8bd4aee47d935ed9fc1ae8fb13fd/torchtraps-0.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "865276a19dd6eeae0c884ac5b324193e", "sha256": "441696b5ec8ef3d9fb7ec5aeffa1b3a9b7489c1684bfe0d2e9c46fda87d454a9"}, "downloads": -1, "filename": "torchtraps-0.1.1.tar.gz", "has_sig": false, "md5_digest": "865276a19dd6eeae0c884ac5b324193e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 15610, "upload_time": "2020-03-30T08:19:30", "upload_time_iso_8601": "2020-03-30T08:19:30.734432Z", "url": "https://files.pythonhosted.org/packages/6c/b4/9b704a795a17417b50f7175e085af80bfcb9fb67da016f6ff82affb7ac90/torchtraps-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "a472fa28368306814cac775bccfb4c0d", "sha256": "8ab5b6a5e90f77351d1b10980acac5316eff2c148319b8dc93acbd06eb539dfc"}, "downloads": -1, "filename": "torchtraps-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "a472fa28368306814cac775bccfb4c0d", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 15798, "upload_time": "2020-03-31T00:34:19", "upload_time_iso_8601": "2020-03-31T00:34:19.134884Z", "url": "https://files.pythonhosted.org/packages/00/34/9f75febbb04b1bd889247a3043adbfac9066d97f77f2c5900ee6b2fcba90/torchtraps-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3c416cb8e45d0de1cbf7621a106b532b", "sha256": "5db8acaf79f187afe9e82caf5494556527888264be3d9d2b045bdb7a05083581"}, "downloads": -1, "filename": "torchtraps-0.1.2.tar.gz", "has_sig": false, "md5_digest": "3c416cb8e45d0de1cbf7621a106b532b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 16947, "upload_time": "2020-03-31T00:34:20", "upload_time_iso_8601": "2020-03-31T00:34:20.933496Z", "url": "https://files.pythonhosted.org/packages/99/7e/d46ae260a949fcb9310bee45b8b259a547548e0aa942e1af7ebba10ec6c7/torchtraps-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "aedf17944988b858e91048a757e6e6a3", "sha256": "3567d67ea5796eaef2e25947221e736faf286b3bd2b9dbba3f533a04d60567b8"}, "downloads": -1, "filename": "torchtraps-0.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "aedf17944988b858e91048a757e6e6a3", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 15783, "upload_time": "2020-03-31T02:48:14", "upload_time_iso_8601": "2020-03-31T02:48:14.727063Z", "url": "https://files.pythonhosted.org/packages/bd/92/8cb958d355f979cafd70199ee5e2c4f0ca11c22b8174741ed61e21e2a636/torchtraps-0.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0920f5a6bcc6eec28e6d2f49fc39f0cc", "sha256": "42fab3a198a0f47160d51eaf569082226c8f37d83175cb3d47c9580a469a43c8"}, "downloads": -1, "filename": "torchtraps-0.1.3.tar.gz", "has_sig": false, "md5_digest": "0920f5a6bcc6eec28e6d2f49fc39f0cc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 16938, "upload_time": "2020-03-31T02:48:17", "upload_time_iso_8601": "2020-03-31T02:48:17.630994Z", "url": "https://files.pythonhosted.org/packages/05/22/6a544cae3861c089c61e34a2b4cca85f7921c2ae3a3e8bb77261043c3905/torchtraps-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "60cebfcb26e66e45fec0b49d18c1faaf", "sha256": "3563858c5a28cbf99ced9b042feadaa62f30b2625380385d73b2596dacd8c07d"}, "downloads": -1, "filename": "torchtraps-0.1.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "60cebfcb26e66e45fec0b49d18c1faaf", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 17874, "upload_time": "2020-03-31T10:18:58", "upload_time_iso_8601": "2020-03-31T10:18:58.085894Z", "url": "https://files.pythonhosted.org/packages/2e/ea/0e520b8d426f025c7f7599a0e3c18d423e675a9f5d1b015824b5db036984/torchtraps-0.1.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "213592d9e954df9ac59c488a2372b6cf", "sha256": "a0a49d424f6e50a33112fc06917f629dbef64fcca122af254f85ecb209adefff"}, "downloads": -1, "filename": "torchtraps-0.1.4.tar.gz", "has_sig": false, "md5_digest": "213592d9e954df9ac59c488a2372b6cf", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 21417, "upload_time": "2020-03-31T10:19:01", "upload_time_iso_8601": "2020-03-31T10:19:01.111312Z", "url": "https://files.pythonhosted.org/packages/59/fe/8aeef89ebf7c0f04345fca1cc245c7f9b2fa6e254f84a467bbec0f762b66/torchtraps-0.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "60cebfcb26e66e45fec0b49d18c1faaf", "sha256": "3563858c5a28cbf99ced9b042feadaa62f30b2625380385d73b2596dacd8c07d"}, "downloads": -1, "filename": "torchtraps-0.1.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "60cebfcb26e66e45fec0b49d18c1faaf", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 17874, "upload_time": "2020-03-31T10:18:58", "upload_time_iso_8601": "2020-03-31T10:18:58.085894Z", "url": "https://files.pythonhosted.org/packages/2e/ea/0e520b8d426f025c7f7599a0e3c18d423e675a9f5d1b015824b5db036984/torchtraps-0.1.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "213592d9e954df9ac59c488a2372b6cf", "sha256": "a0a49d424f6e50a33112fc06917f629dbef64fcca122af254f85ecb209adefff"}, "downloads": -1, "filename": "torchtraps-0.1.4.tar.gz", "has_sig": false, "md5_digest": "213592d9e954df9ac59c488a2372b6cf", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 21417, "upload_time": "2020-03-31T10:19:01", "upload_time_iso_8601": "2020-03-31T10:19:01.111312Z", "url": "https://files.pythonhosted.org/packages/59/fe/8aeef89ebf7c0f04345fca1cc245c7f9b2fa6e254f84a467bbec0f762b66/torchtraps-0.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:05 2020"}