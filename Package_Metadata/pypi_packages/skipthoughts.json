{"info": {"author": "Remi Cadene", "author_email": "remi.cadene@icloud.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.6", "Topic :: Software Development :: Build Tools"], "description": "# Skip-Thoughts.torch for Pytorcb\n\n*Skip-Thoughts.torch* is a lightweight porting of [skip-thought pretrained models from Theano](https://github.com/ryankiros/skip-thoughts) to Pytorch.\n\n## Installation\n\n1. [python3 with anaconda](https://www.continuum.io/downloads)\n2. [pytorch with/out CUDA](http://pytorch.org)\n\n### Install from pip\n\n3. `pip install skipthoughts`\n\n### Install from repo\n\n3. `git clone https://github.com/Cadene/skip-thoughts.torch.git`\n4. `cd skip-thoughts.torch/pytorch`\n5. `python setup.py install`\n\n\n### Available pretrained models\n\n#### UniSkip\n\nIt uses the `nn.GRU` layer from torch with the cudnn backend. It is the fastest implementation, but the dropout is sampled after each time-step in the cudnn implementation... (equals bad regularization)\n\n#### DropUniSkip\n\nIt uses the `nn.GRUCell` layer from torch with the cudnn backend. It is slightly slower than UniSkip, however the dropout is sampled once for all time-steps in a sequence (good regularization).\n\n#### BayesianUniSkip\n\nIt uses a custom GRU layer with a torch backend. It is at least two times slower than UniSkip, however the dropout is sampled once for all time-steps for each Linear (best regularization).\n\n#### BiSkip\n\nEquivalent to UniSkip, but with a bi-sequential GRU.\n\n### Quick example\n\n```python\nimport torch\nfrom torch.autograd import Variable\nimport sys\nsys.path.append('skip-thoughts.torch/pytorch')\nfrom skipthoughts import UniSkip\n\ndir_st = 'data/skip-thoughts'\nvocab = ['robots', 'are', 'very', 'cool', '<eos>', 'BiDiBu']\nuniskip = UniSkip(dir_st, vocab)\n\ninput = Variable(torch.LongTensor([\n    [1,2,3,4,0], # robots are very cool 0\n    [6,2,3,4,5]  # bidibu are very cool <eos>\n])) # <eos> token is optional\nprint(input.size()) # batch_size x seq_len\n\noutput_seq2vec = uniskip(input, lengths=[4,5])\nprint(output_seq2vec.size()) # batch_size x 2400\n\noutput_seq2seq = uniskip(input)\nprint(output_seq2seq.size()) # batch_size x seq_len x 2400\n```\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/cadene/skip-thoughts.torch", "keywords": "pytorch pretrained models skipthoughts deep learning", "license": "", "maintainer": "", "maintainer_email": "", "name": "skipthoughts", "package_url": "https://pypi.org/project/skipthoughts/", "platform": "", "project_url": "https://pypi.org/project/skipthoughts/", "project_urls": {"Homepage": "https://github.com/cadene/skip-thoughts.torch"}, "release_url": "https://pypi.org/project/skipthoughts/0.0.1/", "requires_dist": ["torch", "numpy"], "requires_python": "", "summary": "Skipthoughts pretrained models for Pytorch", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Skip-Thoughts.torch for Pytorcb</h1>\n<p><em>Skip-Thoughts.torch</em> is a lightweight porting of <a href=\"https://github.com/ryankiros/skip-thoughts\" rel=\"nofollow\">skip-thought pretrained models from Theano</a> to Pytorch.</p>\n<h2>Installation</h2>\n<ol>\n<li><a href=\"https://www.continuum.io/downloads\" rel=\"nofollow\">python3 with anaconda</a></li>\n<li><a href=\"http://pytorch.org\" rel=\"nofollow\">pytorch with/out CUDA</a></li>\n</ol>\n<h3>Install from pip</h3>\n<ol>\n<li><code>pip install skipthoughts</code></li>\n</ol>\n<h3>Install from repo</h3>\n<ol>\n<li><code>git clone https://github.com/Cadene/skip-thoughts.torch.git</code></li>\n<li><code>cd skip-thoughts.torch/pytorch</code></li>\n<li><code>python setup.py install</code></li>\n</ol>\n<h3>Available pretrained models</h3>\n<h4>UniSkip</h4>\n<p>It uses the <code>nn.GRU</code> layer from torch with the cudnn backend. It is the fastest implementation, but the dropout is sampled after each time-step in the cudnn implementation... (equals bad regularization)</p>\n<h4>DropUniSkip</h4>\n<p>It uses the <code>nn.GRUCell</code> layer from torch with the cudnn backend. It is slightly slower than UniSkip, however the dropout is sampled once for all time-steps in a sequence (good regularization).</p>\n<h4>BayesianUniSkip</h4>\n<p>It uses a custom GRU layer with a torch backend. It is at least two times slower than UniSkip, however the dropout is sampled once for all time-steps for each Linear (best regularization).</p>\n<h4>BiSkip</h4>\n<p>Equivalent to UniSkip, but with a bi-sequential GRU.</p>\n<h3>Quick example</h3>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.autograd</span> <span class=\"kn\">import</span> <span class=\"n\">Variable</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n<span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"s1\">'skip-thoughts.torch/pytorch'</span><span class=\"p\">)</span>\n<span class=\"kn\">from</span> <span class=\"nn\">skipthoughts</span> <span class=\"kn\">import</span> <span class=\"n\">UniSkip</span>\n\n<span class=\"n\">dir_st</span> <span class=\"o\">=</span> <span class=\"s1\">'data/skip-thoughts'</span>\n<span class=\"n\">vocab</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'robots'</span><span class=\"p\">,</span> <span class=\"s1\">'are'</span><span class=\"p\">,</span> <span class=\"s1\">'very'</span><span class=\"p\">,</span> <span class=\"s1\">'cool'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;eos&gt;'</span><span class=\"p\">,</span> <span class=\"s1\">'BiDiBu'</span><span class=\"p\">]</span>\n<span class=\"n\">uniskip</span> <span class=\"o\">=</span> <span class=\"n\">UniSkip</span><span class=\"p\">(</span><span class=\"n\">dir_st</span><span class=\"p\">,</span> <span class=\"n\">vocab</span><span class=\"p\">)</span>\n\n<span class=\"nb\">input</span> <span class=\"o\">=</span> <span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">LongTensor</span><span class=\"p\">([</span>\n    <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"c1\"># robots are very cool 0</span>\n    <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">]</span>  <span class=\"c1\"># bidibu are very cool &lt;eos&gt;</span>\n<span class=\"p\">]))</span> <span class=\"c1\"># &lt;eos&gt; token is optional</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span> <span class=\"c1\"># batch_size x seq_len</span>\n\n<span class=\"n\">output_seq2vec</span> <span class=\"o\">=</span> <span class=\"n\">uniskip</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">,</span> <span class=\"n\">lengths</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_seq2vec</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span> <span class=\"c1\"># batch_size x 2400</span>\n\n<span class=\"n\">output_seq2seq</span> <span class=\"o\">=</span> <span class=\"n\">uniskip</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_seq2seq</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span> <span class=\"c1\"># batch_size x seq_len x 2400</span>\n</pre>\n\n          </div>"}, "last_serial": 5559388, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "9d3de149545a7045888c5dc2fb0b4074", "sha256": "4787f1fbaf4796d0132305893141079deb9b21219391b87341881814954cffde"}, "downloads": -1, "filename": "skipthoughts-0.0.0.tar.gz", "has_sig": false, "md5_digest": "9d3de149545a7045888c5dc2fb0b4074", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9378, "upload_time": "2018-03-25T13:44:49", "upload_time_iso_8601": "2018-03-25T13:44:49.081991Z", "url": "https://files.pythonhosted.org/packages/7b/3c/3ff876623ceb3dfd97beaf70ae3afd6c868464a1ab1cee63fb3a61d6fd63/skipthoughts-0.0.0.tar.gz", "yanked": false}], "0.0.1": [{"comment_text": "", "digests": {"md5": "04187fec0e63b409f815b4097dac63ff", "sha256": "5c5664194d4d37a3d29698147bcd103c9cbb7573f698abad0248e813e6064a4e"}, "downloads": -1, "filename": "skipthoughts-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "04187fec0e63b409f815b4097dac63ff", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9074, "upload_time": "2019-07-20T00:51:07", "upload_time_iso_8601": "2019-07-20T00:51:07.435964Z", "url": "https://files.pythonhosted.org/packages/34/10/e0b1f148f19b2a792f3811822feeb7f5561de5ae2fffe9b9544642447cdc/skipthoughts-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "13b41cfb452a02809db0b700d92e9630", "sha256": "8e4f87a29acd978db8a8b5f2d8ee0e9e1eef7d20845ce30c2dd3cf2f11b4f626"}, "downloads": -1, "filename": "skipthoughts-0.0.1.tar.gz", "has_sig": false, "md5_digest": "13b41cfb452a02809db0b700d92e9630", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9777, "upload_time": "2019-07-20T00:51:08", "upload_time_iso_8601": "2019-07-20T00:51:08.526979Z", "url": "https://files.pythonhosted.org/packages/7b/01/b6c1126dc402e0d09d90f0fbaf1a6e95e4c4a77293d742d6300c68d164ca/skipthoughts-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "04187fec0e63b409f815b4097dac63ff", "sha256": "5c5664194d4d37a3d29698147bcd103c9cbb7573f698abad0248e813e6064a4e"}, "downloads": -1, "filename": "skipthoughts-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "04187fec0e63b409f815b4097dac63ff", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9074, "upload_time": "2019-07-20T00:51:07", "upload_time_iso_8601": "2019-07-20T00:51:07.435964Z", "url": "https://files.pythonhosted.org/packages/34/10/e0b1f148f19b2a792f3811822feeb7f5561de5ae2fffe9b9544642447cdc/skipthoughts-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "13b41cfb452a02809db0b700d92e9630", "sha256": "8e4f87a29acd978db8a8b5f2d8ee0e9e1eef7d20845ce30c2dd3cf2f11b4f626"}, "downloads": -1, "filename": "skipthoughts-0.0.1.tar.gz", "has_sig": false, "md5_digest": "13b41cfb452a02809db0b700d92e9630", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9777, "upload_time": "2019-07-20T00:51:08", "upload_time_iso_8601": "2019-07-20T00:51:08.526979Z", "url": "https://files.pythonhosted.org/packages/7b/01/b6c1126dc402e0d09d90f0fbaf1a6e95e4c4a77293d742d6300c68d164ca/skipthoughts-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:08:58 2020"}