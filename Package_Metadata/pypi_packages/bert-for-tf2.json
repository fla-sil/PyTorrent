{"info": {"author": "kpe", "author_email": "kpe.git@gmailbox.org", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy"], "description": "BERT for TensorFlow v2\n======================\n\n|Build Status| |Coverage Status| |Version Status| |Python Versions| |Downloads|\n\nThis repo contains a `TensorFlow 2.0`_ `Keras`_ implementation of `google-research/bert`_\nwith support for loading of the original `pre-trained weights`_,\nand producing activations **numerically identical** to the one calculated by the original model.\n\n`ALBERT`_ and `adapter-BERT`_ are also supported by setting the corresponding\nconfiguration parameters (``shared_layer=True``, ``embedding_size`` for `ALBERT`_\nand ``adapter_size`` for `adapter-BERT`_). Setting both will result in an adapter-ALBERT\nby sharing the BERT parameters across all layers while adapting every layer with layer specific adapter.\n\nThe implementation is build from scratch using only basic tensorflow operations,\nfollowing the code in `google-research/bert/modeling.py`_\n(but skipping dead code and applying some simplifications). It also utilizes `kpe/params-flow`_ to reduce\ncommon Keras boilerplate code (related to passing model and layer configuration arguments).\n\n`bert-for-tf2`_ should work with both `TensorFlow 2.0`_ and `TensorFlow 1.14`_ or newer.\n\nNEWS\n----\n - **06.Apr.2020** - using latest ``py-params`` introducing ``WithParams`` base for ``Layer``\n   and ``Model``. See news in `kpe/py-params`_ for how to update (``_construct()`` signature has change and\n   requires calling ``super().__construct()``).\n - **06.Jan.2020** - support for loading the tar format weights from `google-research/ALBERT`.\n - **18.Nov.2019** - ALBERT tokenization added (make sure to import as ``from bert import albert_tokenization`` or ``from bert import bert_tokenization``).\n\n - **08.Nov.2019** - using v2 per default when loading the `TFHub/albert`_ weights of `google-research/ALBERT`_.\n\n - **05.Nov.2019** - minor ALBERT word embeddings refactoring (``word_embeddings_2`` -> ``word_embeddings_projector``) and related parameter freezing fixes.\n\n - **04.Nov.2019** - support for extra (task specific) token embeddings using negative token ids.\n\n - **29.Oct.2019** - support for loading of the pre-trained ALBERT weights released by `google-research/ALBERT`_  at `TFHub/albert`_.\n\n - **11.Oct.2019** - support for loading of the pre-trained ALBERT weights released by `brightmart/albert_zh ALBERT for Chinese`_.\n\n - **10.Oct.2019** - support for `ALBERT`_ through the ``shared_layer=True``\n   and ``embedding_size=128`` params.\n\n - **03.Sep.2019** - walkthrough on fine tuning with adapter-BERT and storing the\n   fine tuned fraction of the weights in a separate checkpoint (see ``tests/test_adapter_finetune.py``).\n\n - **02.Sep.2019** - support for extending the token type embeddings of a pre-trained model\n   by returning the mismatched weights in ``load_stock_weights()`` (see ``tests/test_extend_segments.py``).\n\n - **25.Jul.2019** - there are now two colab notebooks under ``examples/`` showing how to\n   fine-tune an IMDB Movie Reviews sentiment classifier from pre-trained BERT weights\n   using an `adapter-BERT`_ model architecture on a GPU or TPU in Google Colab.\n\n - **28.Jun.2019** - v.0.3.0 supports `adapter-BERT`_ (`google-research/adapter-bert`_)\n   for \"Parameter-Efficient Transfer Learning for NLP\", i.e. fine-tuning small overlay adapter\n   layers over BERT's transformer encoders without changing the frozen BERT weights.\n\n\n\nLICENSE\n-------\n\nMIT. See `License File <https://github.com/kpe/bert-for-tf2/blob/master/LICENSE.txt>`_.\n\nInstall\n-------\n\n``bert-for-tf2`` is on the Python Package Index (PyPI):\n\n::\n\n    pip install bert-for-tf2\n\n\nUsage\n-----\n\nBERT in `bert-for-tf2` is implemented as a Keras layer. You could instantiate it like this:\n\n.. code:: python\n\n  from bert import BertModelLayer\n\n  l_bert = BertModelLayer(**BertModelLayer.Params(\n    vocab_size               = 16000,        # embedding params\n    use_token_type           = True,\n    use_position_embeddings  = True,\n    token_type_vocab_size    = 2,\n\n    num_layers               = 12,           # transformer encoder params\n    hidden_size              = 768,\n    hidden_dropout           = 0.1,\n    intermediate_size        = 4*768,\n    intermediate_activation  = \"gelu\",\n\n    adapter_size             = None,         # see arXiv:1902.00751 (adapter-BERT)\n\n    shared_layer             = False,        # True for ALBERT (arXiv:1909.11942)\n    embedding_size           = None,         # None for BERT, wordpiece embedding size for ALBERT\n\n    name                     = \"bert\"        # any other Keras layer params\n  ))\n\nor by using the ``bert_config.json`` from a `pre-trained google model`_:\n\n.. code:: python\n\n  import bert\n\n  model_dir = \".models/uncased_L-12_H-768_A-12\"\n\n  bert_params = bert.params_from_pretrained_ckpt(model_dir)\n  l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n\n\nnow you can use the BERT layer in your Keras model like this:\n\n.. code:: python\n\n  from tensorflow import keras\n\n  max_seq_len = 128\n  l_input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n  l_token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n\n  # using the default token_type/segment id 0\n  output = l_bert(l_input_ids)                              # output: [batch_size, max_seq_len, hidden_size]\n  model = keras.Model(inputs=l_input_ids, outputs=output)\n  model.build(input_shape=(None, max_seq_len))\n\n  # provide a custom token_type/segment id as a layer input\n  output = l_bert([l_input_ids, l_token_type_ids])          # [batch_size, max_seq_len, hidden_size]\n  model = keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\n  model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n\nif you choose to use `adapter-BERT`_ by setting the `adapter_size` parameter,\nyou would also like to freeze all the original BERT layers by calling:\n\n.. code:: python\n\n  l_bert.apply_adapter_freeze()\n\nand once the model has been build or compiled, the original pre-trained weights\ncan be loaded in the BERT layer:\n\n.. code:: python\n\n  import bert\n\n  bert_ckpt_file   = os.path.join(model_dir, \"bert_model.ckpt\")\n  bert.load_stock_weights(l_bert, bert_ckpt_file)\n\n**N.B.** see `tests/test_bert_activations.py`_ for a complete example.\n\nFAQ\n---\n1. How to use BERT with the `google-research/bert`_ pre-trained weights?\n\n.. code:: python\n\n  model_name = \"uncased_L-12_H-768_A-12\"\n  model_dir = bert.fetch_google_bert_model(model_name, \".models\")\n  model_ckpt = os.path.join(model_dir, \"bert_model.ckpt\")\n\n  bert_params = bert.params_from_pretrained_ckpt(model_dir)\n  l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n\n  # use in Keras Model here, and call model.build()\n\n  bert.load_bert_weights(l_bert, model_ckpt)      # should be called after model.build()\n\n2. How to use ALBERT with the `google-research/ALBERT`_ pre-trained weights (fetching from TFHub)?\n\nsee `tests/nonci/test_load_pretrained_weights.py <https://github.com/kpe/bert-for-tf2/blob/master/tests/nonci/test_load_pretrained_weights.py>`_:\n\n.. code:: python\n\n  model_name = \"albert_base\"\n  model_dir    = bert.fetch_tfhub_albert_model(model_name, \".models\")\n  model_params = bert.albert_params(model_name)\n  l_bert = bert.BertModelLayer.from_params(model_params, name=\"albert\")\n\n  # use in Keras Model here, and call model.build()\n\n  bert.load_albert_weights(l_bert, albert_dir)      # should be called after model.build()\n\n3. How to use ALBERT with the `google-research/ALBERT`_ pre-trained weights (non TFHub)?\n\nsee `tests/nonci/test_load_pretrained_weights.py <https://github.com/kpe/bert-for-tf2/blob/master/tests/nonci/test_load_pretrained_weights.py>`_:\n\n.. code:: python\n\n  model_name = \"albert_base_v2\"\n  model_dir    = bert.fetch_google_albert_model(model_name, \".models\")\n  model_ckpt   = os.path.join(albert_dir, \"model.ckpt-best\")\n\n  model_params = bert.albert_params(model_dir)\n  l_bert = bert.BertModelLayer.from_params(model_params, name=\"albert\")\n\n  # use in Keras Model here, and call model.build()\n\n  bert.load_albert_weights(l_bert, model_ckpt)      # should be called after model.build()\n\n4. How to use ALBERT with the `brightmart/albert_zh`_ pre-trained weights?\n\nsee `tests/nonci/test_albert.py <https://github.com/kpe/bert-for-tf2/blob/master/tests/nonci/test_albert.py>`_:\n\n.. code:: python\n\n  model_name = \"albert_base\"\n  model_dir = bert.fetch_brightmart_albert_model(model_name, \".models\")\n  model_ckpt = os.path.join(model_dir, \"albert_model.ckpt\")\n\n  bert_params = bert.params_from_pretrained_ckpt(model_dir)\n  l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n\n  # use in a Keras Model here, and call model.build()\n\n  bert.load_albert_weights(l_bert, model_ckpt)      # should be called after model.build()\n\n5. How to tokenize the input for the `google-research/bert`_ models?\n\n.. code:: python\n\n  do_lower_case = not (model_name.find(\"cased\") == 0 or model_name.find(\"multi_cased\") == 0)\n  bert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case, model_ckpt)\n  vocab_file = os.path.join(model_dir, \"vocab.txt\")\n  tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n  tokens = tokenizer.tokenize(\"Hello, BERT-World!\")\n  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n6. How to tokenize the input for `brightmart/albert_zh`?\n\n.. code:: python\n\n  import params_flow pf\n\n  # fetch the vocab file\n  albert_zh_vocab_url = \"https://raw.githubusercontent.com/brightmart/albert_zh/master/albert_config/vocab.txt\"\n  vocab_file = pf.utils.fetch_url(albert_zh_vocab_url, model_dir)\n\n  tokenizer = bert.albert_tokenization.FullTokenizer(vocab_file)\n  tokens = tokenizer.tokenize(\"\u4f60\u597d\u4e16\u754c\")\n  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n7. How to tokenize the input for the `google-research/ALBERT`_ models?\n\n.. code:: python\n\n  import sentencepiece as spm\n\n  spm_model = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n  sp = spm.SentencePieceProcessor()\n  sp.load(spm_model)\n  do_lower_case = True\n\n  processed_text = bert.albert_tokenization.preprocess_text(\"Hello, World!\", lower=do_lower_case)\n  token_ids = bert.albert_tokenization.encode_ids(sp, processed_text)\n\n8. How to tokenize the input for the Chinese `google-research/ALBERT`_ models?\n\n.. code:: python\n\n  import bert\n\n  vocab_file = os.path.join(model_dir, \"vocab.txt\")\n  tokenizer = bert.albert_tokenization.FullTokenizer(vocab_file=vocab_file)\n  tokens = tokenizer.tokenize(u\"\u4f60\u597d\u4e16\u754c\")\n  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nResources\n---------\n\n- `BERT`_ - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n- `adapter-BERT`_ - adapter-BERT: Parameter-Efficient Transfer Learning for NLP\n- `ALBERT`_ - ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations\n- `google-research/bert`_ - the original `BERT`_ implementation\n- `google-research/ALBERT`_ - the original `ALBERT`_ implementation by Google\n- `google-research/albert(old)`_ - the old location of the original `ALBERT`_ implementation by Google\n- `brightmart/albert_zh`_ - pre-trained `ALBERT`_ weights for Chinese\n- `kpe/params-flow`_ - A Keras coding style for reducing `Keras`_ boilerplate code in custom layers by utilizing `kpe/py-params`_\n\n.. _`kpe/params-flow`: https://github.com/kpe/params-flow\n.. _`kpe/py-params`: https://github.com/kpe/py-params\n.. _`bert-for-tf2`: https://github.com/kpe/bert-for-tf2\n\n.. _`Keras`: https://keras.io\n.. _`pre-trained weights`: https://github.com/google-research/bert#pre-trained-models\n.. _`google-research/bert`: https://github.com/google-research/bert\n.. _`google-research/bert/modeling.py`: https://github.com/google-research/bert/blob/master/modeling.py\n.. _`BERT`: https://arxiv.org/abs/1810.04805\n.. _`pre-trained google model`: https://github.com/google-research/bert\n.. _`tests/test_bert_activations.py`: https://github.com/kpe/bert-for-tf2/blob/master/tests/test_compare_activations.py\n.. _`TensorFlow 2.0`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf\n.. _`TensorFlow 1.14`: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf\n\n.. _`google-research/adapter-bert`: https://github.com/google-research/adapter-bert/\n.. _`adapter-BERT`: https://arxiv.org/abs/1902.00751\n.. _`ALBERT`: https://arxiv.org/abs/1909.11942\n.. _`brightmart/albert_zh ALBERT for Chinese`: https://github.com/brightmart/albert_zh\n.. _`brightmart/albert_zh`: https://github.com/brightmart/albert_zh\n.. _`google ALBERT weights`: https://github.com/google-research/google-research/tree/master/albert\n.. _`google-research/albert(old)`: https://github.com/google-research/google-research/tree/master/albert\n.. _`google-research/ALBERT`: https://github.com/google-research/ALBERT\n.. _`TFHub/albert`: https://tfhub.dev/google/albert_base/2\n\n.. |Build Status| image:: https://travis-ci.org/kpe/bert-for-tf2.svg?branch=master\n   :target: https://travis-ci.org/kpe/bert-for-tf2\n.. |Coverage Status| image:: https://coveralls.io/repos/kpe/bert-for-tf2/badge.svg?branch=master\n   :target: https://coveralls.io/r/kpe/bert-for-tf2?branch=master\n.. |Version Status| image:: https://badge.fury.io/py/bert-for-tf2.svg\n   :target: https://badge.fury.io/py/bert-for-tf2\n.. |Python Versions| image:: https://img.shields.io/pypi/pyversions/bert-for-tf2.svg\n.. |Downloads| image:: https://img.shields.io/pypi/dm/bert-for-tf2.svg\n.. |Twitter| image:: https://img.shields.io/twitter/follow/siddhadev?logo=twitter&label=&style=\n   :target: https://twitter.com/intent/user?screen_name=siddhadev", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/kpe/bert-for-tf2/", "keywords": "tensorflow keras bert", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "bert-for-tf2", "package_url": "https://pypi.org/project/bert-for-tf2/", "platform": "", "project_url": "https://pypi.org/project/bert-for-tf2/", "project_urls": {"Homepage": "https://github.com/kpe/bert-for-tf2/"}, "release_url": "https://pypi.org/project/bert-for-tf2/0.14.4/", "requires_dist": null, "requires_python": ">=3.5", "summary": "A TensorFlow 2.0 Keras implementation of BERT.", "version": "0.14.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://travis-ci.org/kpe/bert-for-tf2\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3987b600a5e55942afa431e3833c435e0cb8c35e/68747470733a2f2f7472617669732d63692e6f72672f6b70652f626572742d666f722d7466322e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://coveralls.io/r/kpe/bert-for-tf2?branch=master\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/14b294d0e8532643f50d433550985480e9f2b555/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6b70652f626572742d666f722d7466322f62616467652e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://badge.fury.io/py/bert-for-tf2\" rel=\"nofollow\"><img alt=\"Version Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6c905cf1add222e4da9e6e318e4916858a239c66/68747470733a2f2f62616467652e667572792e696f2f70792f626572742d666f722d7466322e737667\"></a> <img alt=\"Python Versions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bcd8837afd1b824adabc1267353e5a3462c0f545/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f626572742d666f722d7466322e737667\"> <img alt=\"Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/43daa21c6ba81eab5150086a03326635ec8c459c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f626572742d666f722d7466322e737667\"></p>\n<p>This repo contains a <a href=\"https://www.tensorflow.org/versions/r2.0/api_docs/python/tf\" rel=\"nofollow\">TensorFlow 2.0</a> <a href=\"https://keras.io\" rel=\"nofollow\">Keras</a> implementation of <a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">google-research/bert</a>\nwith support for loading of the original <a href=\"https://github.com/google-research/bert#pre-trained-models\" rel=\"nofollow\">pre-trained weights</a>,\nand producing activations <strong>numerically identical</strong> to the one calculated by the original model.</p>\n<p><a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a> and <a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a> are also supported by setting the corresponding\nconfiguration parameters (<tt>shared_layer=True</tt>, <tt>embedding_size</tt> for <a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a>\nand <tt>adapter_size</tt> for <a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a>). Setting both will result in an adapter-ALBERT\nby sharing the BERT parameters across all layers while adapting every layer with layer specific adapter.</p>\n<p>The implementation is build from scratch using only basic tensorflow operations,\nfollowing the code in <a href=\"https://github.com/google-research/bert/blob/master/modeling.py\" rel=\"nofollow\">google-research/bert/modeling.py</a>\n(but skipping dead code and applying some simplifications). It also utilizes <a href=\"https://github.com/kpe/params-flow\" rel=\"nofollow\">kpe/params-flow</a> to reduce\ncommon Keras boilerplate code (related to passing model and layer configuration arguments).</p>\n<p><a href=\"https://github.com/kpe/bert-for-tf2\" rel=\"nofollow\">bert-for-tf2</a> should work with both <a href=\"https://www.tensorflow.org/versions/r2.0/api_docs/python/tf\" rel=\"nofollow\">TensorFlow 2.0</a> and <a href=\"https://www.tensorflow.org/versions/r1.14/api_docs/python/tf\" rel=\"nofollow\">TensorFlow 1.14</a> or newer.</p>\n<div id=\"news\">\n<h2>NEWS</h2>\n<blockquote>\n<ul>\n<li><strong>06.Apr.2020</strong> - using latest <tt><span class=\"pre\">py-params</span></tt> introducing <tt>WithParams</tt> base for <tt>Layer</tt>\nand <tt>Model</tt>. See news in <a href=\"https://github.com/kpe/py-params\" rel=\"nofollow\">kpe/py-params</a> for how to update (<tt>_construct()</tt> signature has change and\nrequires calling <tt><span class=\"pre\">super().__construct()</span></tt>).</li>\n<li><strong>06.Jan.2020</strong> - support for loading the tar format weights from <cite>google-research/ALBERT</cite>.</li>\n<li><strong>18.Nov.2019</strong> - ALBERT tokenization added (make sure to import as <tt>from bert import albert_tokenization</tt> or <tt>from bert import bert_tokenization</tt>).</li>\n<li><strong>08.Nov.2019</strong> - using v2 per default when loading the <a href=\"https://tfhub.dev/google/albert_base/2\" rel=\"nofollow\">TFHub/albert</a> weights of <a href=\"https://github.com/google-research/ALBERT\" rel=\"nofollow\">google-research/ALBERT</a>.</li>\n<li><strong>05.Nov.2019</strong> - minor ALBERT word embeddings refactoring (<tt>word_embeddings_2</tt> -&gt; <tt>word_embeddings_projector</tt>) and related parameter freezing fixes.</li>\n<li><strong>04.Nov.2019</strong> - support for extra (task specific) token embeddings using negative token ids.</li>\n<li><strong>29.Oct.2019</strong> - support for loading of the pre-trained ALBERT weights released by <a href=\"https://github.com/google-research/ALBERT\" rel=\"nofollow\">google-research/ALBERT</a>  at <a href=\"https://tfhub.dev/google/albert_base/2\" rel=\"nofollow\">TFHub/albert</a>.</li>\n<li><strong>11.Oct.2019</strong> - support for loading of the pre-trained ALBERT weights released by <a href=\"https://github.com/brightmart/albert_zh\" rel=\"nofollow\">brightmart/albert_zh ALBERT for Chinese</a>.</li>\n<li><strong>10.Oct.2019</strong> - support for <a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a> through the <tt>shared_layer=True</tt>\nand <tt>embedding_size=128</tt> params.</li>\n<li><strong>03.Sep.2019</strong> - walkthrough on fine tuning with adapter-BERT and storing the\nfine tuned fraction of the weights in a separate checkpoint (see <tt>tests/test_adapter_finetune.py</tt>).</li>\n<li><strong>02.Sep.2019</strong> - support for extending the token type embeddings of a pre-trained model\nby returning the mismatched weights in <tt>load_stock_weights()</tt> (see <tt>tests/test_extend_segments.py</tt>).</li>\n<li><strong>25.Jul.2019</strong> - there are now two colab notebooks under <tt>examples/</tt> showing how to\nfine-tune an IMDB Movie Reviews sentiment classifier from pre-trained BERT weights\nusing an <a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a> model architecture on a GPU or TPU in Google Colab.</li>\n<li><strong>28.Jun.2019</strong> - v.0.3.0 supports <a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a> (<a href=\"https://github.com/google-research/adapter-bert/\" rel=\"nofollow\">google-research/adapter-bert</a>)\nfor \u201cParameter-Efficient Transfer Learning for NLP\u201d, i.e. fine-tuning small overlay adapter\nlayers over BERT\u2019s transformer encoders without changing the frozen BERT weights.</li>\n</ul>\n</blockquote>\n</div>\n<div id=\"license\">\n<h2>LICENSE</h2>\n<p>MIT. See <a href=\"https://github.com/kpe/bert-for-tf2/blob/master/LICENSE.txt\" rel=\"nofollow\">License File</a>.</p>\n</div>\n<div id=\"install\">\n<h2>Install</h2>\n<p><tt><span class=\"pre\">bert-for-tf2</span></tt> is on the Python Package Index (PyPI):</p>\n<pre>pip install bert-for-tf2\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>BERT in <cite>bert-for-tf2</cite> is implemented as a Keras layer. You could instantiate it like this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">bert</span> <span class=\"kn\">import</span> <span class=\"n\">BertModelLayer</span>\n\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">BertModelLayer</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">Params</span><span class=\"p\">(</span>\n  <span class=\"n\">vocab_size</span>               <span class=\"o\">=</span> <span class=\"mi\">16000</span><span class=\"p\">,</span>        <span class=\"c1\"># embedding params</span>\n  <span class=\"n\">use_token_type</span>           <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n  <span class=\"n\">use_position_embeddings</span>  <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n  <span class=\"n\">token_type_vocab_size</span>    <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n\n  <span class=\"n\">num_layers</span>               <span class=\"o\">=</span> <span class=\"mi\">12</span><span class=\"p\">,</span>           <span class=\"c1\"># transformer encoder params</span>\n  <span class=\"n\">hidden_size</span>              <span class=\"o\">=</span> <span class=\"mi\">768</span><span class=\"p\">,</span>\n  <span class=\"n\">hidden_dropout</span>           <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n  <span class=\"n\">intermediate_size</span>        <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"o\">*</span><span class=\"mi\">768</span><span class=\"p\">,</span>\n  <span class=\"n\">intermediate_activation</span>  <span class=\"o\">=</span> <span class=\"s2\">\"gelu\"</span><span class=\"p\">,</span>\n\n  <span class=\"n\">adapter_size</span>             <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>         <span class=\"c1\"># see arXiv:1902.00751 (adapter-BERT)</span>\n\n  <span class=\"n\">shared_layer</span>             <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span>        <span class=\"c1\"># True for ALBERT (arXiv:1909.11942)</span>\n  <span class=\"n\">embedding_size</span>           <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>         <span class=\"c1\"># None for BERT, wordpiece embedding size for ALBERT</span>\n\n  <span class=\"n\">name</span>                     <span class=\"o\">=</span> <span class=\"s2\">\"bert\"</span>        <span class=\"c1\"># any other Keras layer params</span>\n<span class=\"p\">))</span>\n</pre>\n<p>or by using the <tt>bert_config.json</tt> from a <a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">pre-trained google model</a>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">bert</span>\n\n<span class=\"n\">model_dir</span> <span class=\"o\">=</span> <span class=\"s2\">\".models/uncased_L-12_H-768_A-12\"</span>\n\n<span class=\"n\">bert_params</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">params_from_pretrained_ckpt</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">)</span>\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">bert_params</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"bert\"</span><span class=\"p\">)</span>\n</pre>\n<p>now you can use the BERT layer in your Keras model like this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tensorflow</span> <span class=\"kn\">import</span> <span class=\"n\">keras</span>\n\n<span class=\"n\">max_seq_len</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n<span class=\"n\">l_input_ids</span>      <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">max_seq_len</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s1\">'int32'</span><span class=\"p\">)</span>\n<span class=\"n\">l_token_type_ids</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">max_seq_len</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s1\">'int32'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># using the default token_type/segment id 0</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">l_bert</span><span class=\"p\">(</span><span class=\"n\">l_input_ids</span><span class=\"p\">)</span>                              <span class=\"c1\"># output: [batch_size, max_seq_len, hidden_size]</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"n\">l_input_ids</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">output</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_seq_len</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># provide a custom token_type/segment id as a layer input</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">l_bert</span><span class=\"p\">([</span><span class=\"n\">l_input_ids</span><span class=\"p\">,</span> <span class=\"n\">l_token_type_ids</span><span class=\"p\">])</span>          <span class=\"c1\"># [batch_size, max_seq_len, hidden_size]</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">l_input_ids</span><span class=\"p\">,</span> <span class=\"n\">l_token_type_ids</span><span class=\"p\">],</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">output</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_seq_len</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_seq_len</span><span class=\"p\">)])</span>\n</pre>\n<p>if you choose to use <a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a> by setting the <cite>adapter_size</cite> parameter,\nyou would also like to freeze all the original BERT layers by calling:</p>\n<pre><span class=\"n\">l_bert</span><span class=\"o\">.</span><span class=\"n\">apply_adapter_freeze</span><span class=\"p\">()</span>\n</pre>\n<p>and once the model has been build or compiled, the original pre-trained weights\ncan be loaded in the BERT layer:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">bert</span>\n\n<span class=\"n\">bert_ckpt_file</span>   <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"bert_model.ckpt\"</span><span class=\"p\">)</span>\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">load_stock_weights</span><span class=\"p\">(</span><span class=\"n\">l_bert</span><span class=\"p\">,</span> <span class=\"n\">bert_ckpt_file</span><span class=\"p\">)</span>\n</pre>\n<p><strong>N.B.</strong> see <a href=\"https://github.com/kpe/bert-for-tf2/blob/master/tests/test_compare_activations.py\" rel=\"nofollow\">tests/test_bert_activations.py</a> for a complete example.</p>\n</div>\n<div id=\"faq\">\n<h2>FAQ</h2>\n<ol>\n<li>How to use BERT with the <a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">google-research/bert</a> pre-trained weights?</li>\n</ol>\n<pre><span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"uncased_L-12_H-768_A-12\"</span>\n<span class=\"n\">model_dir</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">fetch_google_bert_model</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"s2\">\".models\"</span><span class=\"p\">)</span>\n<span class=\"n\">model_ckpt</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"bert_model.ckpt\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">bert_params</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">params_from_pretrained_ckpt</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">)</span>\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">bert_params</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"bert\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># use in Keras Model here, and call model.build()</span>\n\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">load_bert_weights</span><span class=\"p\">(</span><span class=\"n\">l_bert</span><span class=\"p\">,</span> <span class=\"n\">model_ckpt</span><span class=\"p\">)</span>      <span class=\"c1\"># should be called after model.build()</span>\n</pre>\n<ol>\n<li>How to use ALBERT with the <a href=\"https://github.com/google-research/ALBERT\" rel=\"nofollow\">google-research/ALBERT</a> pre-trained weights (fetching from TFHub)?</li>\n</ol>\n<p>see <a href=\"https://github.com/kpe/bert-for-tf2/blob/master/tests/nonci/test_load_pretrained_weights.py\" rel=\"nofollow\">tests/nonci/test_load_pretrained_weights.py</a>:</p>\n<pre><span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"albert_base\"</span>\n<span class=\"n\">model_dir</span>    <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">fetch_tfhub_albert_model</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"s2\">\".models\"</span><span class=\"p\">)</span>\n<span class=\"n\">model_params</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">albert_params</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">)</span>\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">model_params</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"albert\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># use in Keras Model here, and call model.build()</span>\n\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">load_albert_weights</span><span class=\"p\">(</span><span class=\"n\">l_bert</span><span class=\"p\">,</span> <span class=\"n\">albert_dir</span><span class=\"p\">)</span>      <span class=\"c1\"># should be called after model.build()</span>\n</pre>\n<ol>\n<li>How to use ALBERT with the <a href=\"https://github.com/google-research/ALBERT\" rel=\"nofollow\">google-research/ALBERT</a> pre-trained weights (non TFHub)?</li>\n</ol>\n<p>see <a href=\"https://github.com/kpe/bert-for-tf2/blob/master/tests/nonci/test_load_pretrained_weights.py\" rel=\"nofollow\">tests/nonci/test_load_pretrained_weights.py</a>:</p>\n<pre><span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"albert_base_v2\"</span>\n<span class=\"n\">model_dir</span>    <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">fetch_google_albert_model</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"s2\">\".models\"</span><span class=\"p\">)</span>\n<span class=\"n\">model_ckpt</span>   <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">albert_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"model.ckpt-best\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">model_params</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">albert_params</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">)</span>\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">model_params</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"albert\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># use in Keras Model here, and call model.build()</span>\n\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">load_albert_weights</span><span class=\"p\">(</span><span class=\"n\">l_bert</span><span class=\"p\">,</span> <span class=\"n\">model_ckpt</span><span class=\"p\">)</span>      <span class=\"c1\"># should be called after model.build()</span>\n</pre>\n<ol>\n<li>How to use ALBERT with the <a href=\"https://github.com/brightmart/albert_zh\" rel=\"nofollow\">brightmart/albert_zh</a> pre-trained weights?</li>\n</ol>\n<p>see <a href=\"https://github.com/kpe/bert-for-tf2/blob/master/tests/nonci/test_albert.py\" rel=\"nofollow\">tests/nonci/test_albert.py</a>:</p>\n<pre><span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"albert_base\"</span>\n<span class=\"n\">model_dir</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">fetch_brightmart_albert_model</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"s2\">\".models\"</span><span class=\"p\">)</span>\n<span class=\"n\">model_ckpt</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"albert_model.ckpt\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">bert_params</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">params_from_pretrained_ckpt</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">)</span>\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">bert_params</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"bert\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># use in a Keras Model here, and call model.build()</span>\n\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">load_albert_weights</span><span class=\"p\">(</span><span class=\"n\">l_bert</span><span class=\"p\">,</span> <span class=\"n\">model_ckpt</span><span class=\"p\">)</span>      <span class=\"c1\"># should be called after model.build()</span>\n</pre>\n<ol>\n<li>How to tokenize the input for the <a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">google-research/bert</a> models?</li>\n</ol>\n<pre><span class=\"n\">do_lower_case</span> <span class=\"o\">=</span> <span class=\"ow\">not</span> <span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s2\">\"cased\"</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">or</span> <span class=\"n\">model_name</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s2\">\"multi_cased\"</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">bert_tokenization</span><span class=\"o\">.</span><span class=\"n\">validate_case_matches_checkpoint</span><span class=\"p\">(</span><span class=\"n\">do_lower_case</span><span class=\"p\">,</span> <span class=\"n\">model_ckpt</span><span class=\"p\">)</span>\n<span class=\"n\">vocab_file</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"vocab.txt\"</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">bert_tokenization</span><span class=\"o\">.</span><span class=\"n\">FullTokenizer</span><span class=\"p\">(</span><span class=\"n\">vocab_file</span><span class=\"p\">,</span> <span class=\"n\">do_lower_case</span><span class=\"p\">)</span>\n<span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"s2\">\"Hello, BERT-World!\"</span><span class=\"p\">)</span>\n<span class=\"n\">token_ids</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">convert_tokens_to_ids</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">)</span>\n</pre>\n<ol>\n<li>How to tokenize the input for <cite>brightmart/albert_zh</cite>?</li>\n</ol>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">params_flow</span> <span class=\"n\">pf</span>\n\n<span class=\"c1\"># fetch the vocab file</span>\n<span class=\"n\">albert_zh_vocab_url</span> <span class=\"o\">=</span> <span class=\"s2\">\"https://raw.githubusercontent.com/brightmart/albert_zh/master/albert_config/vocab.txt\"</span>\n<span class=\"n\">vocab_file</span> <span class=\"o\">=</span> <span class=\"n\">pf</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">fetch_url</span><span class=\"p\">(</span><span class=\"n\">albert_zh_vocab_url</span><span class=\"p\">,</span> <span class=\"n\">model_dir</span><span class=\"p\">)</span>\n\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">albert_tokenization</span><span class=\"o\">.</span><span class=\"n\">FullTokenizer</span><span class=\"p\">(</span><span class=\"n\">vocab_file</span><span class=\"p\">)</span>\n<span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"s2\">\"\u4f60\u597d\u4e16\u754c\"</span><span class=\"p\">)</span>\n<span class=\"n\">token_ids</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">convert_tokens_to_ids</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">)</span>\n</pre>\n<ol>\n<li>How to tokenize the input for the <a href=\"https://github.com/google-research/ALBERT\" rel=\"nofollow\">google-research/ALBERT</a> models?</li>\n</ol>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">sentencepiece</span> <span class=\"k\">as</span> <span class=\"nn\">spm</span>\n\n<span class=\"n\">spm_model</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"assets\"</span><span class=\"p\">,</span> <span class=\"s2\">\"30k-clean.model\"</span><span class=\"p\">)</span>\n<span class=\"n\">sp</span> <span class=\"o\">=</span> <span class=\"n\">spm</span><span class=\"o\">.</span><span class=\"n\">SentencePieceProcessor</span><span class=\"p\">()</span>\n<span class=\"n\">sp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">spm_model</span><span class=\"p\">)</span>\n<span class=\"n\">do_lower_case</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n\n<span class=\"n\">processed_text</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">albert_tokenization</span><span class=\"o\">.</span><span class=\"n\">preprocess_text</span><span class=\"p\">(</span><span class=\"s2\">\"Hello, World!\"</span><span class=\"p\">,</span> <span class=\"n\">lower</span><span class=\"o\">=</span><span class=\"n\">do_lower_case</span><span class=\"p\">)</span>\n<span class=\"n\">token_ids</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">albert_tokenization</span><span class=\"o\">.</span><span class=\"n\">encode_ids</span><span class=\"p\">(</span><span class=\"n\">sp</span><span class=\"p\">,</span> <span class=\"n\">processed_text</span><span class=\"p\">)</span>\n</pre>\n<ol>\n<li>How to tokenize the input for the Chinese <a href=\"https://github.com/google-research/ALBERT\" rel=\"nofollow\">google-research/ALBERT</a> models?</li>\n</ol>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">bert</span>\n\n<span class=\"n\">vocab_file</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"vocab.txt\"</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">albert_tokenization</span><span class=\"o\">.</span><span class=\"n\">FullTokenizer</span><span class=\"p\">(</span><span class=\"n\">vocab_file</span><span class=\"o\">=</span><span class=\"n\">vocab_file</span><span class=\"p\">)</span>\n<span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"sa\">u</span><span class=\"s2\">\"\u4f60\u597d\u4e16\u754c\"</span><span class=\"p\">)</span>\n<span class=\"n\">token_ids</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">convert_tokens_to_ids</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"resources\">\n<h2>Resources</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li>\n<li><a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a> - adapter-BERT: Parameter-Efficient Transfer Learning for NLP</li>\n<li><a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a> - ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations</li>\n<li><a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">google-research/bert</a> - the original <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a> implementation</li>\n<li><a href=\"https://github.com/google-research/ALBERT\" rel=\"nofollow\">google-research/ALBERT</a> - the original <a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a> implementation by Google</li>\n<li><a href=\"https://github.com/google-research/google-research/tree/master/albert\" rel=\"nofollow\">google-research/albert(old)</a> - the old location of the original <a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a> implementation by Google</li>\n<li><a href=\"https://github.com/brightmart/albert_zh\" rel=\"nofollow\">brightmart/albert_zh</a> - pre-trained <a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a> weights for Chinese</li>\n<li><a href=\"https://github.com/kpe/params-flow\" rel=\"nofollow\">kpe/params-flow</a> - A Keras coding style for reducing <a href=\"https://keras.io\" rel=\"nofollow\">Keras</a> boilerplate code in custom layers by utilizing <a href=\"https://github.com/kpe/py-params\" rel=\"nofollow\">kpe/py-params</a></li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 7041441, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "72c1dd97615bf1635918c8e434d77937", "sha256": "ec58aaa4d323ffd132a571c7278f9f3bf3df67e2814c03a8a5e4098f99b737b7"}, "downloads": -1, "filename": "bert-for-tf2-0.0.1.tar.gz", "has_sig": false, "md5_digest": "72c1dd97615bf1635918c8e434d77937", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 7566, "upload_time": "2019-05-22T13:40:41", "upload_time_iso_8601": "2019-05-22T13:40:41.154836Z", "url": "https://files.pythonhosted.org/packages/6f/6b/d835967fb9d7e10dd8bf0d786206634dc269c9c554526aad69dc2369a271/bert-for-tf2-0.0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "d5ddbe4f48454269d0806467b2ae4f45", "sha256": "6d85674dbf1082cdc813f779b40ac61f3d7cf0aaf1bcd0e329753ef8acd3e504"}, "downloads": -1, "filename": "bert_for_tf2-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d5ddbe4f48454269d0806467b2ae4f45", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 31738, "upload_time": "2019-05-23T16:08:49", "upload_time_iso_8601": "2019-05-23T16:08:49.015865Z", "url": "https://files.pythonhosted.org/packages/cd/f6/a6c9612201de1e5600c4903bb700f2e83f869b06c722344ac4314b415d52/bert_for_tf2-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2e0ee6b10d7d4454331a21043fd764e1", "sha256": "db385dea3e80d9a64e3956dc88592ed02b1884a4fd624a0a91f9a541c2ffb03e"}, "downloads": -1, "filename": "bert-for-tf2-0.1.1.tar.gz", "has_sig": false, "md5_digest": "2e0ee6b10d7d4454331a21043fd764e1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 23476, "upload_time": "2019-05-23T16:08:50", "upload_time_iso_8601": "2019-05-23T16:08:50.707588Z", "url": "https://files.pythonhosted.org/packages/18/03/7bdb2050a7dc1cd4dd0f3f05d005f6daef499ec1b11e67e5feb78a69dbba/bert-for-tf2-0.1.1.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "47c9225f1a04fbef867c12100fed2b0b", "sha256": "36b0dc8f3a2c5a3e73dea35db88e5eecc165a173e5101ee950754968c8ec1079"}, "downloads": -1, "filename": "bert-for-tf2-0.1.3.tar.gz", "has_sig": false, "md5_digest": "47c9225f1a04fbef867c12100fed2b0b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 24235, "upload_time": "2019-05-23T16:17:45", "upload_time_iso_8601": "2019-05-23T16:17:45.596760Z", "url": "https://files.pythonhosted.org/packages/21/fc/baa0b8ff1cd0c10aa0a78bdf54c4bae1cd73edc2c010392166aef21c4ff1/bert-for-tf2-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "02663a73e8266722a6ef6b93e6425de5", "sha256": "315613061597019ddccca11914296011c3a76d8af0f510d78426eb553488926e"}, "downloads": -1, "filename": "bert-for-tf2-0.1.4.tar.gz", "has_sig": false, "md5_digest": "02663a73e8266722a6ef6b93e6425de5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 25758, "upload_time": "2019-05-24T16:22:48", "upload_time_iso_8601": "2019-05-24T16:22:48.599721Z", "url": "https://files.pythonhosted.org/packages/d8/b6/b0c2e969b35c8692f9ea12aaa7881158be37c3820b4ef9ba6d6eed27bf73/bert-for-tf2-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "cf423ba2f384668766276988df7a11da", "sha256": "2c78dbfa72e25b39855af05a2b9dad2dffd0f0ef6b3c94acd17aa6e5270ee68f"}, "downloads": -1, "filename": "bert-for-tf2-0.1.5.tar.gz", "has_sig": false, "md5_digest": "cf423ba2f384668766276988df7a11da", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27226, "upload_time": "2019-06-05T11:16:45", "upload_time_iso_8601": "2019-06-05T11:16:45.646492Z", "url": "https://files.pythonhosted.org/packages/29/71/0ed46e4c3f8791d0b86e7283474bb20549e16d1445d5fba5205c984145cf/bert-for-tf2-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "0c70aca1d78c2afa2dab875111f49041", "sha256": "a6d2f4d340bc53ebaa85b66eeec74b34f1f245646326a97fca45e51612a22000"}, "downloads": -1, "filename": "bert-for-tf2-0.1.6.tar.gz", "has_sig": false, "md5_digest": "0c70aca1d78c2afa2dab875111f49041", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27205, "upload_time": "2019-06-05T15:18:36", "upload_time_iso_8601": "2019-06-05T15:18:36.150797Z", "url": "https://files.pythonhosted.org/packages/1f/47/d1e8d0f1c4a6cbf03569b04ffe6ffe407827692ab801d8b6811d428ecc6b/bert-for-tf2-0.1.6.tar.gz", "yanked": false}], "0.10.0": [{"comment_text": "", "digests": {"md5": "0cabff33a6799d18e2bd767cab65eb8e", "sha256": "fc417019a9e67e479b5f1e312c92559cedfd138de8a3c15b830199cef0b9ab8c"}, "downloads": -1, "filename": "bert-for-tf2-0.10.0.tar.gz", "has_sig": false, "md5_digest": "0cabff33a6799d18e2bd767cab65eb8e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 36363, "upload_time": "2019-11-05T17:42:58", "upload_time_iso_8601": "2019-11-05T17:42:58.361669Z", "url": "https://files.pythonhosted.org/packages/29/b8/05151f45d455c44911fc2dc36913e85f2191a62f94d5fa51be56d3a673b0/bert-for-tf2-0.10.0.tar.gz", "yanked": false}], "0.11.0": [{"comment_text": "", "digests": {"md5": "55e70270dffd5f620be840fd29e966b2", "sha256": "c214e488f863fae8a1714d3e94146b8c3878c7db344ad4c7846445f03c72c698"}, "downloads": -1, "filename": "bert-for-tf2-0.11.0.tar.gz", "has_sig": false, "md5_digest": "55e70270dffd5f620be840fd29e966b2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 36469, "upload_time": "2019-11-08T21:29:28", "upload_time_iso_8601": "2019-11-08T21:29:28.162789Z", "url": "https://files.pythonhosted.org/packages/fc/b7/97f73291c9361f977d45356000316aeffaff8662e502c196f150a2932e19/bert-for-tf2-0.11.0.tar.gz", "yanked": false}], "0.11.1": [{"comment_text": "", "digests": {"md5": "aca66627be8ff1add32a6790242fb7f9", "sha256": "4f61c278086924d7f30893c3c3e0e58c8bfc2d45cfb2049b153bd50cdbe08f79"}, "downloads": -1, "filename": "bert-for-tf2-0.11.1.tar.gz", "has_sig": false, "md5_digest": "aca66627be8ff1add32a6790242fb7f9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 36476, "upload_time": "2019-11-10T13:42:32", "upload_time_iso_8601": "2019-11-10T13:42:32.802985Z", "url": "https://files.pythonhosted.org/packages/39/43/ce98809e6c98d01bbb718f8004fd6f0de6a9bf26670ab9e538e93e5a2065/bert-for-tf2-0.11.1.tar.gz", "yanked": false}], "0.11.2": [{"comment_text": "", "digests": {"md5": "c93b66a3abef8dc1cca33720129d7172", "sha256": "6efd2c50bfb9b825873f00604e22bf453984e9d6e96e1d2e74c65ce1c606f1bd"}, "downloads": -1, "filename": "bert-for-tf2-0.11.2.tar.gz", "has_sig": false, "md5_digest": "c93b66a3abef8dc1cca33720129d7172", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 36474, "upload_time": "2019-11-10T14:01:53", "upload_time_iso_8601": "2019-11-10T14:01:53.444746Z", "url": "https://files.pythonhosted.org/packages/1d/09/66e094b1ef19d6737e03b821d14716ccf4f3f65a99488f4e4865bcf9c3d2/bert-for-tf2-0.11.2.tar.gz", "yanked": false}], "0.11.3": [{"comment_text": "", "digests": {"md5": "7211edc81d695cb85a9bcaeace1a6a3f", "sha256": "ef7a250049b8ae6e0e12ed5f9e6aa7dc5ab57afbcb3a36cd8033da68e2e4b218"}, "downloads": -1, "filename": "bert-for-tf2-0.11.3.tar.gz", "has_sig": false, "md5_digest": "7211edc81d695cb85a9bcaeace1a6a3f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 36497, "upload_time": "2019-11-13T23:54:32", "upload_time_iso_8601": "2019-11-13T23:54:32.212292Z", "url": "https://files.pythonhosted.org/packages/c1/b0/117f137f2376fdad8b27cdc790ec2c69200ff2c3176f44321e7013f8a37d/bert-for-tf2-0.11.3.tar.gz", "yanked": false}], "0.11.4": [{"comment_text": "", "digests": {"md5": "f246edb40e708f8d1e9df72bea4ac574", "sha256": "45cb3c3ef5a14fa97263fc5571e81a2faefc4986ff3aaf3423b2ea62f79453b0"}, "downloads": -1, "filename": "bert-for-tf2-0.11.4.tar.gz", "has_sig": false, "md5_digest": "f246edb40e708f8d1e9df72bea4ac574", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 36508, "upload_time": "2019-11-16T16:35:44", "upload_time_iso_8601": "2019-11-16T16:35:44.045477Z", "url": "https://files.pythonhosted.org/packages/2c/1d/4fc166b121bd03ef74431c5585a1474b1db75a5513f736e2c788953a25ea/bert-for-tf2-0.11.4.tar.gz", "yanked": false}], "0.11.5": [{"comment_text": "", "digests": {"md5": "16469aed12d78e23ae80d92fea076040", "sha256": "82ba56eaae6bb1535502f084d9decd635bce716be67e2ed681e567ec31e69f25"}, "downloads": -1, "filename": "bert-for-tf2-0.11.5.tar.gz", "has_sig": false, "md5_digest": "16469aed12d78e23ae80d92fea076040", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 36549, "upload_time": "2019-11-17T00:29:35", "upload_time_iso_8601": "2019-11-17T00:29:35.432463Z", "url": "https://files.pythonhosted.org/packages/2c/07/211bacfb4a116139d567e55b29c3d35c34afbd343fa6537601d16aab3310/bert-for-tf2-0.11.5.tar.gz", "yanked": false}], "0.11.6": [{"comment_text": "", "digests": {"md5": "0d4ccea4a5dc56a565d5a613f97a484f", "sha256": "53f8e151a47aae65e1880773f6278bcaa39f09e71e322df9d726d38969621396"}, "downloads": -1, "filename": "bert-for-tf2-0.11.6.tar.gz", "has_sig": false, "md5_digest": "0d4ccea4a5dc56a565d5a613f97a484f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 36560, "upload_time": "2019-11-17T01:10:27", "upload_time_iso_8601": "2019-11-17T01:10:27.286301Z", "url": "https://files.pythonhosted.org/packages/cb/45/1952205571c30b5bda14ad20d61f492110fdbb671b4c43b6e92c6ada290d/bert-for-tf2-0.11.6.tar.gz", "yanked": false}], "0.12.1": [{"comment_text": "", "digests": {"md5": "b7fa647d8f258b1ce999c8afdd3b9b83", "sha256": "3ceaadc1e805da6797b477d767157acb485a329c69dd1bd96a0c901613449118"}, "downloads": -1, "filename": "bert-for-tf2-0.12.1.tar.gz", "has_sig": false, "md5_digest": "b7fa647d8f258b1ce999c8afdd3b9b83", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 38293, "upload_time": "2019-11-18T16:52:21", "upload_time_iso_8601": "2019-11-18T16:52:21.441199Z", "url": "https://files.pythonhosted.org/packages/30/02/efb8c1f904b4a15bc4fc5692a2989de9fe765a214151d773f25753464687/bert-for-tf2-0.12.1.tar.gz", "yanked": false}], "0.12.2": [{"comment_text": "", "digests": {"md5": "c8cf0f75e6d88182542b5433646ee807", "sha256": "f549384bab19d7008294ae0bd0e41087cf7c0db13081135d285860bc3962ae35"}, "downloads": -1, "filename": "bert-for-tf2-0.12.2.tar.gz", "has_sig": false, "md5_digest": "c8cf0f75e6d88182542b5433646ee807", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 38289, "upload_time": "2019-11-19T09:48:55", "upload_time_iso_8601": "2019-11-19T09:48:55.595969Z", "url": "https://files.pythonhosted.org/packages/ae/de/bdb4d88c37b0ab045232bba41d110e81c1a9aac8e2794f798a52212f0841/bert-for-tf2-0.12.2.tar.gz", "yanked": false}], "0.12.3": [{"comment_text": "", "digests": {"md5": "75d29718762b0edd07717a7da1f65429", "sha256": "e2e7d8db33826fca3dbf67af479fc01786b9479b36d72ea29195d900b789173e"}, "downloads": -1, "filename": "bert-for-tf2-0.12.3.tar.gz", "has_sig": false, "md5_digest": "75d29718762b0edd07717a7da1f65429", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 39123, "upload_time": "2019-11-19T11:18:06", "upload_time_iso_8601": "2019-11-19T11:18:06.918790Z", "url": "https://files.pythonhosted.org/packages/a1/08/dec90e9f705957984f678ce3b6a243fb5d64a18adade52dc46826f884479/bert-for-tf2-0.12.3.tar.gz", "yanked": false}], "0.12.4": [{"comment_text": "", "digests": {"md5": "f7b10a39bd89226b56ec9f2ff80d554c", "sha256": "73b162064d75e68bc404f18ed3558e9e3825d01cb79d661bdc98f6e1ad1500e6"}, "downloads": -1, "filename": "bert-for-tf2-0.12.4.tar.gz", "has_sig": false, "md5_digest": "f7b10a39bd89226b56ec9f2ff80d554c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 39244, "upload_time": "2019-11-19T16:45:44", "upload_time_iso_8601": "2019-11-19T16:45:44.572398Z", "url": "https://files.pythonhosted.org/packages/13/09/cbc05c97949f860610d8a3033b7b84d1726919a1fe3eb43219b3d9d0fadf/bert-for-tf2-0.12.4.tar.gz", "yanked": false}], "0.12.5": [{"comment_text": "", "digests": {"md5": "a6345fda647a65ba4428073cf08c8666", "sha256": "ec44e37bd85128d2d58b4faf2aabf13a61fd83951c01b6f0d913da2ff5c58c10"}, "downloads": -1, "filename": "bert-for-tf2-0.12.5.tar.gz", "has_sig": false, "md5_digest": "a6345fda647a65ba4428073cf08c8666", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 39201, "upload_time": "2019-11-19T17:32:17", "upload_time_iso_8601": "2019-11-19T17:32:17.304778Z", "url": "https://files.pythonhosted.org/packages/2e/8a/26d383e822d650ef64b3dbccb7e3a97bfc86fb3af219556f9c6ed9b1839c/bert-for-tf2-0.12.5.tar.gz", "yanked": false}], "0.12.6": [{"comment_text": "", "digests": {"md5": "a03219498f7e013e854e3de097a91e64", "sha256": "46f8773097a41179353a3e4443f235f4c629a5bb0943b67bd082103a695101cb"}, "downloads": -1, "filename": "bert-for-tf2-0.12.6.tar.gz", "has_sig": false, "md5_digest": "a03219498f7e013e854e3de097a91e64", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 39192, "upload_time": "2019-11-23T19:35:26", "upload_time_iso_8601": "2019-11-23T19:35:26.815124Z", "url": "https://files.pythonhosted.org/packages/c2/d8/14e0cfa03bbeb72c314f0648267c490bcceec5e8fb25081ec31307b5509c/bert-for-tf2-0.12.6.tar.gz", "yanked": false}], "0.12.7": [{"comment_text": "", "digests": {"md5": "6fe2619bb83749740082599d36faf622", "sha256": "14a78f09e7de90ba9d1f7a384a6f8766b413abc42341d0595df10c75f8fa7e0f"}, "downloads": -1, "filename": "bert-for-tf2-0.12.7.tar.gz", "has_sig": false, "md5_digest": "6fe2619bb83749740082599d36faf622", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 39248, "upload_time": "2019-12-31T15:08:52", "upload_time_iso_8601": "2019-12-31T15:08:52.432294Z", "url": "https://files.pythonhosted.org/packages/93/31/1f9d1d5ccafb5b8bb621b02c4c5bd9e9f6599ec9b305f7307f1b6c5ae0b5/bert-for-tf2-0.12.7.tar.gz", "yanked": false}], "0.12.9": [{"comment_text": "", "digests": {"md5": "d6e60c9b712ba2494bfda5af1eb15e40", "sha256": "f8e5f912196512ff9bcb161573f4f5ea6237c2c5723b79a1c3f34bd2d62dc69b"}, "downloads": -1, "filename": "bert-for-tf2-0.12.9.tar.gz", "has_sig": false, "md5_digest": "d6e60c9b712ba2494bfda5af1eb15e40", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 39522, "upload_time": "2020-01-06T10:44:37", "upload_time_iso_8601": "2020-01-06T10:44:37.787419Z", "url": "https://files.pythonhosted.org/packages/8a/64/d399eae2a75c42d07d02a8303bb9e1cfae4edc03480aa53e853f84ca157f/bert-for-tf2-0.12.9.tar.gz", "yanked": false}], "0.13.2": [{"comment_text": "", "digests": {"md5": "4b5775a54df64191d46c98a706357dbd", "sha256": "a5c8a78ce2ce122a1ec75c11071c88ceb31680dd6927f4df2e219a9fa218bfe4"}, "downloads": -1, "filename": "bert-for-tf2-0.13.2.tar.gz", "has_sig": false, "md5_digest": "4b5775a54df64191d46c98a706357dbd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 40287, "upload_time": "2020-01-06T13:17:20", "upload_time_iso_8601": "2020-01-06T13:17:20.549804Z", "url": "https://files.pythonhosted.org/packages/60/b4/1a3da73498960866ad0510ead86b133569ff012bf1c77d82ce95203779fc/bert-for-tf2-0.13.2.tar.gz", "yanked": false}], "0.13.4": [{"comment_text": "", "digests": {"md5": "c5bbc8136a057c7d7fcc847bd9acdbf2", "sha256": "6278c478d04d2ca5a6fac5e788d220a88979675d88e59b9cac9d7b5ad82c4281"}, "downloads": -1, "filename": "bert-for-tf2-0.13.4.tar.gz", "has_sig": false, "md5_digest": "c5bbc8136a057c7d7fcc847bd9acdbf2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 40298, "upload_time": "2020-01-23T13:21:52", "upload_time_iso_8601": "2020-01-23T13:21:52.490239Z", "url": "https://files.pythonhosted.org/packages/75/15/03314e558f4c34642a90144fc8ec8bdbb0ef3c2ca80345007f9c7b007a07/bert-for-tf2-0.13.4.tar.gz", "yanked": false}], "0.13.5": [{"comment_text": "", "digests": {"md5": "1e3e2d39c815a784d05ef3f083489726", "sha256": "f7f7c9aab656bf4724d00b1ffdbc78e4a84879044fc27c9ea93bde481cf67c6d"}, "downloads": -1, "filename": "bert-for-tf2-0.13.5.tar.gz", "has_sig": false, "md5_digest": "1e3e2d39c815a784d05ef3f083489726", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 40323, "upload_time": "2020-02-26T18:03:40", "upload_time_iso_8601": "2020-02-26T18:03:40.983222Z", "url": "https://files.pythonhosted.org/packages/4c/2a/79f44178ac6f5b6995bc7804898fce2654da70e0c5c7f76332748420d6fd/bert-for-tf2-0.13.5.tar.gz", "yanked": false}], "0.14.0": [{"comment_text": "", "digests": {"md5": "5ffca7da9f0f134555af7dff9d3892dd", "sha256": "d56eb7d3b1b888de4f9a46485938dce7bbe9f7108c43ebfe7192e7a92654af04"}, "downloads": -1, "filename": "bert-for-tf2-0.14.0.tar.gz", "has_sig": false, "md5_digest": "5ffca7da9f0f134555af7dff9d3892dd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 40530, "upload_time": "2020-04-06T13:21:22", "upload_time_iso_8601": "2020-04-06T13:21:22.148633Z", "url": "https://files.pythonhosted.org/packages/a3/1d/d677bef062c34e2a7c59475e600b7d1b17de41c693ed53c8316f767c1ad8/bert-for-tf2-0.14.0.tar.gz", "yanked": false}], "0.14.1": [{"comment_text": "", "digests": {"md5": "4acfca036633869ffe8171327560aeba", "sha256": "2b84de14ec47074303b3c2b544b75e82ab7898f5ca5f2e54846092ab587d82a1"}, "downloads": -1, "filename": "bert-for-tf2-0.14.1.tar.gz", "has_sig": false, "md5_digest": "4acfca036633869ffe8171327560aeba", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 40532, "upload_time": "2020-04-06T14:26:58", "upload_time_iso_8601": "2020-04-06T14:26:58.919569Z", "url": "https://files.pythonhosted.org/packages/ff/84/1bea6c34d38f3e726830d3adeca76e6e901b98cf5babd635883dbedd7ecc/bert-for-tf2-0.14.1.tar.gz", "yanked": false}], "0.14.3": [{"comment_text": "", "digests": {"md5": "756ad63300eff92430957c9bbc60ff77", "sha256": "cde1bc05818d7bb8e8ca68384b8d384b1b66bfcbcb227c779c80f94b4f639799"}, "downloads": -1, "filename": "bert-for-tf2-0.14.3.tar.gz", "has_sig": false, "md5_digest": "756ad63300eff92430957c9bbc60ff77", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 40559, "upload_time": "2020-04-17T15:21:15", "upload_time_iso_8601": "2020-04-17T15:21:15.665454Z", "url": "https://files.pythonhosted.org/packages/fc/02/ca494086e4332abf66eb7de5ea077c6d19ed664f36b2358f4e632090f203/bert-for-tf2-0.14.3.tar.gz", "yanked": false}], "0.14.4": [{"comment_text": "", "digests": {"md5": "1e0996b85b1e1ffe8a68c72e71d96c66", "sha256": "7aa11fe13c6600e083e38508a7c7250025b845008d12a41ff10a2d4b2940f3e4"}, "downloads": -1, "filename": "bert-for-tf2-0.14.4.tar.gz", "has_sig": false, "md5_digest": "1e0996b85b1e1ffe8a68c72e71d96c66", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 40553, "upload_time": "2020-04-17T16:15:58", "upload_time_iso_8601": "2020-04-17T16:15:58.877340Z", "url": "https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "cb8e59e881472dbc2fdaa1adccb1e8c1", "sha256": "971bbec674d02aebf105ac94da6c07d5a8f1fbd7bb01fc77146e16877b94aae2"}, "downloads": -1, "filename": "bert-for-tf2-0.2.0.tar.gz", "has_sig": false, "md5_digest": "cb8e59e881472dbc2fdaa1adccb1e8c1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27563, "upload_time": "2019-06-24T10:24:56", "upload_time_iso_8601": "2019-06-24T10:24:56.727322Z", "url": "https://files.pythonhosted.org/packages/0e/94/f55cbdb7efde6dbf8c410142f6555288a7061688ea4df2a619bbd07b0aa3/bert-for-tf2-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "18a6b4f80d8ad2d307aa3ad62eca5559", "sha256": "8894a103bf5351bc63a02760bb084dedd727c1372c898fe100e29c32d0c8c776"}, "downloads": -1, "filename": "bert-for-tf2-0.2.1.tar.gz", "has_sig": false, "md5_digest": "18a6b4f80d8ad2d307aa3ad62eca5559", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27622, "upload_time": "2019-06-27T14:11:55", "upload_time_iso_8601": "2019-06-27T14:11:55.604288Z", "url": "https://files.pythonhosted.org/packages/2b/50/add1bdc5e9160ec82580f1f7c3eeb6b6a70fd6b7992d00d23c3a6e55b463/bert-for-tf2-0.2.1.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "2b12ed361fb3fcaccd7f9a9e131947e3", "sha256": "3232857e00b6dbd3b522246871e468ead775227d88795ac9a9b38f8778267d68"}, "downloads": -1, "filename": "bert-for-tf2-0.3.0.tar.gz", "has_sig": false, "md5_digest": "2b12ed361fb3fcaccd7f9a9e131947e3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28171, "upload_time": "2019-06-28T14:54:14", "upload_time_iso_8601": "2019-06-28T14:54:14.305412Z", "url": "https://files.pythonhosted.org/packages/ab/b6/9b36f2f1f006b2155fc7c60140c013fae92aeed11396b15368bee600f73f/bert-for-tf2-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "05377715f131418716b7d23adee38dcc", "sha256": "8d56b0540ac6137ca9cc19b486ef8678462cd112e272cb52fccf884d8a44b48e"}, "downloads": -1, "filename": "bert-for-tf2-0.3.1.tar.gz", "has_sig": false, "md5_digest": "05377715f131418716b7d23adee38dcc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28181, "upload_time": "2019-07-01T07:50:13", "upload_time_iso_8601": "2019-07-01T07:50:13.991793Z", "url": "https://files.pythonhosted.org/packages/f2/ff/dd2489e84f7026537c693a0722950453e121c365dc22f33ffea5236ea99b/bert-for-tf2-0.3.1.tar.gz", "yanked": false}], "0.3.10": [{"comment_text": "", "digests": {"md5": "8f6c26931e3f42c29f08344cb3f2f2d9", "sha256": "2db4bde539bba4a36c46ad1116cf78f862868001c3bf730afdc518a76fe51414"}, "downloads": -1, "filename": "bert-for-tf2-0.3.10.tar.gz", "has_sig": false, "md5_digest": "8f6c26931e3f42c29f08344cb3f2f2d9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 30193, "upload_time": "2019-08-10T16:14:41", "upload_time_iso_8601": "2019-08-10T16:14:41.610813Z", "url": "https://files.pythonhosted.org/packages/c4/ce/cebbf07a72652e4a617ba3df4f72460656dd1822114f765485babc4a7e9d/bert-for-tf2-0.3.10.tar.gz", "yanked": false}], "0.3.11": [{"comment_text": "", "digests": {"md5": "26a3a0234b4f3ad0668cf40a5938bc09", "sha256": "e0aa98dc0b989fe0634990aa5fd9bde87c7630efb76775efb0fa6bc08706eabd"}, "downloads": -1, "filename": "bert-for-tf2-0.3.11.tar.gz", "has_sig": false, "md5_digest": "26a3a0234b4f3ad0668cf40a5938bc09", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 30185, "upload_time": "2019-08-10T18:35:34", "upload_time_iso_8601": "2019-08-10T18:35:34.560526Z", "url": "https://files.pythonhosted.org/packages/a4/ec/a77c16d3ea880534ade7a548da1bd7ee15673896e082af1b5aea7be27572/bert-for-tf2-0.3.11.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "7cd911f7b65a82d127cb0ff7574bd4c5", "sha256": "9d523f83a784b909dea4665d0e9b9f53e549d4608e4fb2bf1d77a6b4cd0dbc35"}, "downloads": -1, "filename": "bert-for-tf2-0.3.2.tar.gz", "has_sig": false, "md5_digest": "7cd911f7b65a82d127cb0ff7574bd4c5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28617, "upload_time": "2019-07-25T12:05:29", "upload_time_iso_8601": "2019-07-25T12:05:29.972843Z", "url": "https://files.pythonhosted.org/packages/f5/b0/3bd848b68bd7aff2256af930d9913a7b328a80c8682318f027fe990f4648/bert-for-tf2-0.3.2.tar.gz", "yanked": false}], "0.3.3": [{"comment_text": "", "digests": {"md5": "dd937911b4f62e70cf91b4f1448a57b6", "sha256": "086b5db95df5ada82830b5662bea64269cd7e82759f9d181dfa220ecfd9ca4a3"}, "downloads": -1, "filename": "bert-for-tf2-0.3.3.tar.gz", "has_sig": false, "md5_digest": "dd937911b4f62e70cf91b4f1448a57b6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 29089, "upload_time": "2019-07-25T15:29:19", "upload_time_iso_8601": "2019-07-25T15:29:19.508160Z", "url": "https://files.pythonhosted.org/packages/e6/aa/c61e9e69a4e7dadce283b088fa909599a4e51b4ea7955eda804c44423f16/bert-for-tf2-0.3.3.tar.gz", "yanked": false}], "0.3.4": [{"comment_text": "", "digests": {"md5": "ac633861be7817be7aba05d33bbaa8c5", "sha256": "28b9bd9558a68b190ff67951edd7955ccda3d8aa34a360738db2f0e76571c6ef"}, "downloads": -1, "filename": "bert-for-tf2-0.3.4.tar.gz", "has_sig": false, "md5_digest": "ac633861be7817be7aba05d33bbaa8c5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 29453, "upload_time": "2019-07-30T12:48:37", "upload_time_iso_8601": "2019-07-30T12:48:37.218863Z", "url": "https://files.pythonhosted.org/packages/67/05/6ff3463595a75b5466f257ad3651ae7b0260e635f83d5dfe9609372dfdfe/bert-for-tf2-0.3.4.tar.gz", "yanked": false}], "0.3.5": [{"comment_text": "", "digests": {"md5": "cef108a47e2864155823f95b8f5bcc79", "sha256": "d1727284d9a258d5dff31aab61adc3bd1b0106599ebf030e65493553407097c5"}, "downloads": -1, "filename": "bert-for-tf2-0.3.5.tar.gz", "has_sig": false, "md5_digest": "cef108a47e2864155823f95b8f5bcc79", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 29568, "upload_time": "2019-07-30T14:58:55", "upload_time_iso_8601": "2019-07-30T14:58:55.856418Z", "url": "https://files.pythonhosted.org/packages/ae/fe/484988a6abaff2602d509e659546a17d877ea2de08218e2a05c2b115352c/bert-for-tf2-0.3.5.tar.gz", "yanked": false}], "0.3.6": [{"comment_text": "", "digests": {"md5": "9fa7bea9181f3706255a459e71e1ecb3", "sha256": "ba7feb1ce1f017fc16fb734482c56207cbd0fc5ae0578085d4e87e47be42dc54"}, "downloads": -1, "filename": "bert-for-tf2-0.3.6.tar.gz", "has_sig": false, "md5_digest": "9fa7bea9181f3706255a459e71e1ecb3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 29662, "upload_time": "2019-07-31T11:44:04", "upload_time_iso_8601": "2019-07-31T11:44:04.089989Z", "url": "https://files.pythonhosted.org/packages/4f/8f/d4f13895e8320252265928c13764dc11325b9d97a09f6f2257348d59c0a2/bert-for-tf2-0.3.6.tar.gz", "yanked": false}], "0.3.7": [{"comment_text": "", "digests": {"md5": "14699a525d6d2986328ca0e1e2ee3be3", "sha256": "8640ab8f274895a3fdfbca0bd4ab582e0a9ca743613b731be40fb4078f9c8687"}, "downloads": -1, "filename": "bert-for-tf2-0.3.7.tar.gz", "has_sig": false, "md5_digest": "14699a525d6d2986328ca0e1e2ee3be3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 29872, "upload_time": "2019-08-06T10:35:47", "upload_time_iso_8601": "2019-08-06T10:35:47.626615Z", "url": "https://files.pythonhosted.org/packages/06/60/6c28bf004c8b6706d3434d253d7e455ba63a63d0307ab0abe500f23af934/bert-for-tf2-0.3.7.tar.gz", "yanked": false}], "0.3.8": [{"comment_text": "", "digests": {"md5": "e08d6d77853a15fb9a77314b1dadf265", "sha256": "7e3074c2ef0b29c071e980c31fd3dfd34709fc0d85b1b46c8364d0ad13b5e9b5"}, "downloads": -1, "filename": "bert-for-tf2-0.3.8.tar.gz", "has_sig": false, "md5_digest": "e08d6d77853a15fb9a77314b1dadf265", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 30126, "upload_time": "2019-08-09T22:37:36", "upload_time_iso_8601": "2019-08-09T22:37:36.719823Z", "url": "https://files.pythonhosted.org/packages/db/88/b81ff15664564ba58d67f12077ea0343b739039c45ade96f8562020fe423/bert-for-tf2-0.3.8.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "81b0f41e4e270d09d7353467e4f301be", "sha256": "0e7ba03f1db9a093d14d23bd9aca163ec5b71418cdb09a735d113a81a7dd60f4"}, "downloads": -1, "filename": "bert-for-tf2-0.4.1.tar.gz", "has_sig": false, "md5_digest": "81b0f41e4e270d09d7353467e4f301be", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 30501, "upload_time": "2019-09-02T14:07:58", "upload_time_iso_8601": "2019-09-02T14:07:58.885918Z", "url": "https://files.pythonhosted.org/packages/2e/39/306749f8b4478452b3e8eadf6b7d2ec0d6fc5b49404b65d3f2e97dc231ac/bert-for-tf2-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "dcfec33cc9d0008c6d5c509b111fce2d", "sha256": "0f33c30c83631bcc9db15a20a16ae7eb8977aef5de29e61f1f8acc9b4079c986"}, "downloads": -1, "filename": "bert-for-tf2-0.4.2.tar.gz", "has_sig": false, "md5_digest": "dcfec33cc9d0008c6d5c509b111fce2d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 30653, "upload_time": "2019-09-03T14:50:02", "upload_time_iso_8601": "2019-09-03T14:50:02.201705Z", "url": "https://files.pythonhosted.org/packages/d3/89/ee123cc61809e7400b52554b8620e1f4a69dde83aede8349fff5c5ae1539/bert-for-tf2-0.4.2.tar.gz", "yanked": false}], "0.5.0": [{"comment_text": "", "digests": {"md5": "11dd9efd040bf9e88b7dd57d6cd7a16e", "sha256": "12aad7acabc274a4d5ef0ebe227414608c0ce992043be08a9554751e351395d6"}, "downloads": -1, "filename": "bert-for-tf2-0.5.0.tar.gz", "has_sig": false, "md5_digest": "11dd9efd040bf9e88b7dd57d6cd7a16e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 30664, "upload_time": "2019-09-09T11:59:36", "upload_time_iso_8601": "2019-09-09T11:59:36.619001Z", "url": "https://files.pythonhosted.org/packages/5d/0a/6f2ece967a2bd9abe8c30eef8865bfcf20632562473a4183d26c67d62f2e/bert-for-tf2-0.5.0.tar.gz", "yanked": false}], "0.6.0": [{"comment_text": "", "digests": {"md5": "63229aa56519f3d3469716ff9c20257f", "sha256": "f1d57430d143b6589faa78b4baa720a2423cb1948ddc27bad2cdeae0f0895aa5"}, "downloads": -1, "filename": "bert-for-tf2-0.6.0.tar.gz", "has_sig": false, "md5_digest": "63229aa56519f3d3469716ff9c20257f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 30667, "upload_time": "2019-10-02T10:25:10", "upload_time_iso_8601": "2019-10-02T10:25:10.898819Z", "url": "https://files.pythonhosted.org/packages/04/ee/cb3a3957c2ab2378c5fba64bdc92dc698ac2c81ddab3b85f7faf1f6d0698/bert-for-tf2-0.6.0.tar.gz", "yanked": false}], "0.7.0": [{"comment_text": "", "digests": {"md5": "5e66b473a965fe5dcecc5a8ae5317af8", "sha256": "ccd45e9958c5617396944b6cfe76ca67387ae92039b30913f88319643e2fe863"}, "downloads": -1, "filename": "bert-for-tf2-0.7.0.tar.gz", "has_sig": false, "md5_digest": "5e66b473a965fe5dcecc5a8ae5317af8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 31348, "upload_time": "2019-10-10T14:17:57", "upload_time_iso_8601": "2019-10-10T14:17:57.158783Z", "url": "https://files.pythonhosted.org/packages/5a/c8/200ae2383f8e05f8a83efde6fa976dc445853cd0cd216339eabee66d94ba/bert-for-tf2-0.7.0.tar.gz", "yanked": false}], "0.7.2": [{"comment_text": "", "digests": {"md5": "6307f6905ee1637e453b32ac77bb4b65", "sha256": "25c4d4d9053f0fc7802111f8fc35c2207c599a91f45c95303ba7f7d0f353ba0f"}, "downloads": -1, "filename": "bert-for-tf2-0.7.2.tar.gz", "has_sig": false, "md5_digest": "6307f6905ee1637e453b32ac77bb4b65", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32148, "upload_time": "2019-10-11T14:43:53", "upload_time_iso_8601": "2019-10-11T14:43:53.094782Z", "url": "https://files.pythonhosted.org/packages/51/5e/ba7e408eee9071bd340b64b5126282ec3a17f2cf09cd009364b69a773e77/bert-for-tf2-0.7.2.tar.gz", "yanked": false}], "0.8.0": [{"comment_text": "", "digests": {"md5": "201d4071cac04f3b68be1a443add2d49", "sha256": "3c4e8b60f8a98cafbd133c6b12b202e2aedc3f94fae0cdcafcb4439fa30316c2"}, "downloads": -1, "filename": "bert-for-tf2-0.8.0.tar.gz", "has_sig": false, "md5_digest": "201d4071cac04f3b68be1a443add2d49", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 35803, "upload_time": "2019-10-29T02:22:12", "upload_time_iso_8601": "2019-10-29T02:22:12.788387Z", "url": "https://files.pythonhosted.org/packages/fc/68/2d82bd2ba977c6c9d62a40c69d3937856116c94f2224a46cf40e0f6a76a9/bert-for-tf2-0.8.0.tar.gz", "yanked": false}], "0.9.0": [{"comment_text": "", "digests": {"md5": "107f1e3a075adc67d67e2d238fe296a1", "sha256": "1aafa9ac4960e9f88317cc71310a7b04a976f99a30dda1b51d58cc93473b6294"}, "downloads": -1, "filename": "bert-for-tf2-0.9.0.tar.gz", "has_sig": false, "md5_digest": "107f1e3a075adc67d67e2d238fe296a1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 36041, "upload_time": "2019-11-04T14:21:45", "upload_time_iso_8601": "2019-11-04T14:21:45.782789Z", "url": "https://files.pythonhosted.org/packages/69/aa/216f121617e9795f5aa52cfab6e2e0bb8d495bba6592325fdbc206d40394/bert-for-tf2-0.9.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1e0996b85b1e1ffe8a68c72e71d96c66", "sha256": "7aa11fe13c6600e083e38508a7c7250025b845008d12a41ff10a2d4b2940f3e4"}, "downloads": -1, "filename": "bert-for-tf2-0.14.4.tar.gz", "has_sig": false, "md5_digest": "1e0996b85b1e1ffe8a68c72e71d96c66", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 40553, "upload_time": "2020-04-17T16:15:58", "upload_time_iso_8601": "2020-04-17T16:15:58.877340Z", "url": "https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:45 2020"}