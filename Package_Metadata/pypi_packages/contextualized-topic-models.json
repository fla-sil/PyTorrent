{"info": {"author": "Federico Bianchi", "author_email": "f.bianchi@unibocconi.it", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "===========================\nContextualized Topic Models\n===========================\n\n\n.. image:: https://img.shields.io/pypi/v/contextualized_topic_models.svg\n        :target: https://pypi.python.org/pypi/contextualized_topic_models\n\n.. image:: https://travis-ci.com/MilaNLProc/contextualized-topic-models.svg\n        :target: https://travis-ci.com/MilaNLProc/contextualized-topic-models\n\n.. image:: https://readthedocs.org/projects/contextualized-topic-models/badge/?version=latest\n        :target: https://contextualized-topic-models.readthedocs.io/en/latest/?badge=latest\n        :alt: Documentation Status\n\n\nContextualized Topic Models (CTM) are a family of topic models that use pre-trained representations of language (e.g., BERT) to\nsupport topic modeling. See the papers for details:\n\n* `Cross-lingual Contextualized Topic Models with Zero-shot Learning` https://arxiv.org/pdf/2004.07737v1.pdf\n* `Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence` https://arxiv.org/pdf/2004.03974.pdf\n\n\n\nSoftware details:\n\n* Free software: MIT license\n* Documentation: https://contextualized-topic-models.readthedocs.io.\n* Super big shout-out to `Stephen Carrow`_ for creating the awesome https://github.com/estebandito22/PyTorchAVITM package from which we constructed the foundations of this package. We are happy to redistribute again this software under the MIT License.\n\n\n\nFeatures\n--------\n\n* Combines BERT and Neural Variational Topic Models\n* Two different methodologies: combined, where we combine BoW and BERT embeddings and contextual, that uses only BERT embeddings\n* Includes methods to create embedded representations and BoW\n* Includes evaluation metrics\n\n\nQuick Guide\n-----------\n\nInstall the package using pip\n\n.. code-block:: bash\n\n    pip install -U contextualized_topic_models\n\n\nThe contextual neural topic model can be easily instantiated using few parameters (although there is a wide range of\nparameters you can use to change the behaviour of the neural topic model). When you generate\nembeddings with BERT remember that there is a maximum length and for documents that are too long some words will be ignored.\n\nAn important aspect to take into account is which network you want to use: the one that combines BERT and the BoW or the one that just uses BERT.\nIt's easy to swap from one to the other:\n\nCombined Topic Model:\n\n.. code-block:: python\n\n    CTM(input_size=len(handler.vocab), bert_input_size=512, inference_type=\"combined\", n_components=50)\n\nFully Contextual Topic Model:\n\n.. code-block:: python\n\n    CTM(input_size=len(handler.vocab), bert_input_size=512, inference_type=\"contextual\", n_components=50)\n\n\nHere is how you can use the combined topic model. The high level API is pretty easy to use:\n\n.. code-block:: python\n\n    from contextualized_topic_models.models.ctm import CTM\n    from contextualized_topic_models.utils.data_preparation import TextHandler\n    from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file\n\n    handler = TextHandler(\"documents.txt\")\n    handler.prepare() # create vocabulary and training data\n\n    # generate BERT data\n    training_bert = bert_embeddings_from_file(\"documents.txt\", \"distiluse-base-multilingual-cased\")\n\n    training_dataset = CTMDataset(handler.bow, training_bert, handler.idx2token)\n\n    ctm = CTM(input_size=len(handler.vocab), bert_input_size=512, inference_type=\"combined\", n_components=50)\n\n    ctm.fit(training_dataset) # run the model\n\nSee the example notebook in the `contextualized_topic_models/examples` folder.\nWe have also included some of the metrics normally used in the evaluation of topic models, for example you can compute the coherence of your\ntopics using NPMI using our simple and high-level API.\n\n.. code-block:: python\n\n    from contextualized_topic_models.evaluation.measures import CoherenceNPMI\n\n    with open('documents.txt',\"r\") as fr:\n        texts = [doc.split() for doc in fr.read().splitlines()] # load text for NPMI\n\n    npmi = CoherenceNPMI(texts=texts, topics=ctm.get_topic_lists(10))\n    npmi.score()\n\n\nCross-lingual Topic Modeling\n----------------------------\n\nThe fully contextual topic model can be used for cross-lingual topic modeling! See the paper (https://arxiv.org/pdf/2004.07737v1.pdf)\n\n\n.. code-block:: python\n\n    from contextualized_topic_models.models.ctm import CTM\n    from contextualized_topic_models.utils.data_preparation import TextHandler\n    from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file\n\n    handler = TextHandler(\"english_documents.txt\")\n    handler.prepare() # create vocabulary and training data\n\n    training_bert = bert_embeddings_from_file(\"documents.txt\", \"distiluse-base-multilingual-cased\")\n\n    training_dataset = CTMDataset(handler.bow, training_bert, handler.idx2token)\n\n    ctm = CTM(input_size=len(handler.vocab), bert_input_size=512, inference_type=\"contextual\", n_components=50)\n\n    ctm.fit(training_dataset) # run the model\n\n\nPredict topics for novel documents\n\n.. code-block:: python\n\n\n    test_handler = TextHandler(\"spanish_documents.txt\")\n    test_handler.prepare() # create vocabulary and training data\n\n    # generate BERT data\n    testing_bert = bert_embeddings_from_file(\"spanish_documents.txt\", \"distiluse-base-multilingual-cased\")\n\n    testing_dataset = CTMDataset(test_handler.bow, testing_bert, test_handler.idx2token)\n    ctm.get_thetas(testing_dataset)\n\nDevelopment Team\n----------------\n\n* Federico Bianchi <f.bianchi@unibocconi.it> Bocconi University\n* Silvia Terragni <s.terragni4@campus.unimib.it> University of Milan-Bicocca\n* Dirk Hovy <dirk.hovy@unibocconi.it> Bocconi University\n\nReferences\n----------\n\nCombined BERT+BoW\n\n::\n\n    @article{bianchi2020pretraining,\n        title={Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence},\n        author={Federico Bianchi and Silvia Terragni and Dirk Hovy},\n        year={2020},\n       journal={arXiv preprint arXiv:2004.03974},\n    }\n\n\nContextual TM\n\n::\n\n    @article{bianchi2020crosslingual,\n        title={Cross-lingual Contextualized Topic Models with Zero-shot Learning},\n        author={Federico Bianchi and Silvia Terragni and Dirk Hovy and Debora Nozza and Elisabetta Fersini},\n        year={2020},\n       journal={arXiv preprint arXiv:2004.07737},\n    }\n\n\n\nCredits\n-------\n\n\nThis package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.\nTo ease the use of the library we have also incuded the `rbo`_ package, all the rights reserved to the author of that package.\n\n\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n.. _`Stephen Carrow` : https://github.com/estebandito22\n.. _`rbo` : https://github.com/dlukes/rbo\n\n\n=======\nHistory\n=======\n\n1.0.0 (2020-04-05)\n------------------\n\n* Released models with the main features implemented\n\n0.1.0 (2020-04-04)\n------------------\n\n* First release on PyPI.\n\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/MilaNLProc/contextualized_topic_models", "keywords": "contextualized_topic_models", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "contextualized-topic-models", "package_url": "https://pypi.org/project/contextualized-topic-models/", "platform": "", "project_url": "https://pypi.org/project/contextualized-topic-models/", "project_urls": {"Homepage": "https://github.com/MilaNLProc/contextualized_topic_models"}, "release_url": "https://pypi.org/project/contextualized-topic-models/1.3.1/", "requires_dist": ["wheel (==0.33.6)", "torch", "numpy", "torchvision", "gensim", "sentence-transformers", "pytest (==4.6.5)", "pytest-runner (==5.1)"], "requires_python": ">=3.5", "summary": "Contextualized Topic Models", "version": "1.3.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"contextualized-topic-models\">\n<h2>Contextualized Topic Models</h2>\n<a href=\"https://pypi.python.org/pypi/contextualized_topic_models\" rel=\"nofollow\"><img alt=\"https://img.shields.io/pypi/v/contextualized_topic_models.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0e757aba96d1445b08d4303951073b7d29d2ef81/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f636f6e7465787475616c697a65645f746f7069635f6d6f64656c732e737667\"></a>\n<a href=\"https://travis-ci.com/MilaNLProc/contextualized-topic-models\" rel=\"nofollow\"><img alt=\"https://travis-ci.com/MilaNLProc/contextualized-topic-models.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/be972af0d2666bbe828696cd68964af87f61a21d/68747470733a2f2f7472617669732d63692e636f6d2f4d696c614e4c50726f632f636f6e7465787475616c697a65642d746f7069632d6d6f64656c732e737667\"></a>\n<a href=\"https://contextualized-topic-models.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3ded5557f0574a30ed84e68acc67af57bd11e9cb/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f636f6e7465787475616c697a65642d746f7069632d6d6f64656c732f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<p>Contextualized Topic Models (CTM) are a family of topic models that use pre-trained representations of language (e.g., BERT) to\nsupport topic modeling. See the papers for details:</p>\n<ul>\n<li><cite>Cross-lingual Contextualized Topic Models with Zero-shot Learning</cite> <a href=\"https://arxiv.org/pdf/2004.07737v1.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/2004.07737v1.pdf</a></li>\n<li><cite>Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence</cite> <a href=\"https://arxiv.org/pdf/2004.03974.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/2004.03974.pdf</a></li>\n</ul>\n<p>Software details:</p>\n<ul>\n<li>Free software: MIT license</li>\n<li>Documentation: <a href=\"https://contextualized-topic-models.readthedocs.io\" rel=\"nofollow\">https://contextualized-topic-models.readthedocs.io</a>.</li>\n<li>Super big shout-out to <a href=\"https://github.com/estebandito22\" rel=\"nofollow\">Stephen Carrow</a> for creating the awesome <a href=\"https://github.com/estebandito22/PyTorchAVITM\" rel=\"nofollow\">https://github.com/estebandito22/PyTorchAVITM</a> package from which we constructed the foundations of this package. We are happy to redistribute again this software under the MIT License.</li>\n</ul>\n<div id=\"features\">\n<h3>Features</h3>\n<ul>\n<li>Combines BERT and Neural Variational Topic Models</li>\n<li>Two different methodologies: combined, where we combine BoW and BERT embeddings and contextual, that uses only BERT embeddings</li>\n<li>Includes methods to create embedded representations and BoW</li>\n<li>Includes evaluation metrics</li>\n</ul>\n</div>\n<div id=\"quick-guide\">\n<h3>Quick Guide</h3>\n<p>Install the package using pip</p>\n<pre>pip install -U contextualized_topic_models\n</pre>\n<p>The contextual neural topic model can be easily instantiated using few parameters (although there is a wide range of\nparameters you can use to change the behaviour of the neural topic model). When you generate\nembeddings with BERT remember that there is a maximum length and for documents that are too long some words will be ignored.</p>\n<p>An important aspect to take into account is which network you want to use: the one that combines BERT and the BoW or the one that just uses BERT.\nIt\u2019s easy to swap from one to the other:</p>\n<p>Combined Topic Model:</p>\n<pre><span class=\"n\">CTM</span><span class=\"p\">(</span><span class=\"n\">input_size</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">),</span> <span class=\"n\">bert_input_size</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">inference_type</span><span class=\"o\">=</span><span class=\"s2\">\"combined\"</span><span class=\"p\">,</span> <span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n</pre>\n<p>Fully Contextual Topic Model:</p>\n<pre><span class=\"n\">CTM</span><span class=\"p\">(</span><span class=\"n\">input_size</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">),</span> <span class=\"n\">bert_input_size</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">inference_type</span><span class=\"o\">=</span><span class=\"s2\">\"contextual\"</span><span class=\"p\">,</span> <span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n</pre>\n<p>Here is how you can use the combined topic model. The high level API is pretty easy to use:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">contextualized_topic_models.models.ctm</span> <span class=\"kn\">import</span> <span class=\"n\">CTM</span>\n<span class=\"kn\">from</span> <span class=\"nn\">contextualized_topic_models.utils.data_preparation</span> <span class=\"kn\">import</span> <span class=\"n\">TextHandler</span>\n<span class=\"kn\">from</span> <span class=\"nn\">contextualized_topic_models.utils.data_preparation</span> <span class=\"kn\">import</span> <span class=\"n\">bert_embeddings_from_file</span>\n\n<span class=\"n\">handler</span> <span class=\"o\">=</span> <span class=\"n\">TextHandler</span><span class=\"p\">(</span><span class=\"s2\">\"documents.txt\"</span><span class=\"p\">)</span>\n<span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">prepare</span><span class=\"p\">()</span> <span class=\"c1\"># create vocabulary and training data</span>\n\n<span class=\"c1\"># generate BERT data</span>\n<span class=\"n\">training_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert_embeddings_from_file</span><span class=\"p\">(</span><span class=\"s2\">\"documents.txt\"</span><span class=\"p\">,</span> <span class=\"s2\">\"distiluse-base-multilingual-cased\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">training_dataset</span> <span class=\"o\">=</span> <span class=\"n\">CTMDataset</span><span class=\"p\">(</span><span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">bow</span><span class=\"p\">,</span> <span class=\"n\">training_bert</span><span class=\"p\">,</span> <span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">idx2token</span><span class=\"p\">)</span>\n\n<span class=\"n\">ctm</span> <span class=\"o\">=</span> <span class=\"n\">CTM</span><span class=\"p\">(</span><span class=\"n\">input_size</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">),</span> <span class=\"n\">bert_input_size</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">inference_type</span><span class=\"o\">=</span><span class=\"s2\">\"combined\"</span><span class=\"p\">,</span> <span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n\n<span class=\"n\">ctm</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">training_dataset</span><span class=\"p\">)</span> <span class=\"c1\"># run the model</span>\n</pre>\n<p>See the example notebook in the <cite>contextualized_topic_models/examples</cite> folder.\nWe have also included some of the metrics normally used in the evaluation of topic models, for example you can compute the coherence of your\ntopics using NPMI using our simple and high-level API.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">contextualized_topic_models.evaluation.measures</span> <span class=\"kn\">import</span> <span class=\"n\">CoherenceNPMI</span>\n\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'documents.txt'</span><span class=\"p\">,</span><span class=\"s2\">\"r\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fr</span><span class=\"p\">:</span>\n    <span class=\"n\">texts</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">fr</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">splitlines</span><span class=\"p\">()]</span> <span class=\"c1\"># load text for NPMI</span>\n\n<span class=\"n\">npmi</span> <span class=\"o\">=</span> <span class=\"n\">CoherenceNPMI</span><span class=\"p\">(</span><span class=\"n\">texts</span><span class=\"o\">=</span><span class=\"n\">texts</span><span class=\"p\">,</span> <span class=\"n\">topics</span><span class=\"o\">=</span><span class=\"n\">ctm</span><span class=\"o\">.</span><span class=\"n\">get_topic_lists</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">npmi</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"cross-lingual-topic-modeling\">\n<h3>Cross-lingual Topic Modeling</h3>\n<p>The fully contextual topic model can be used for cross-lingual topic modeling! See the paper (<a href=\"https://arxiv.org/pdf/2004.07737v1.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/2004.07737v1.pdf</a>)</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">contextualized_topic_models.models.ctm</span> <span class=\"kn\">import</span> <span class=\"n\">CTM</span>\n<span class=\"kn\">from</span> <span class=\"nn\">contextualized_topic_models.utils.data_preparation</span> <span class=\"kn\">import</span> <span class=\"n\">TextHandler</span>\n<span class=\"kn\">from</span> <span class=\"nn\">contextualized_topic_models.utils.data_preparation</span> <span class=\"kn\">import</span> <span class=\"n\">bert_embeddings_from_file</span>\n\n<span class=\"n\">handler</span> <span class=\"o\">=</span> <span class=\"n\">TextHandler</span><span class=\"p\">(</span><span class=\"s2\">\"english_documents.txt\"</span><span class=\"p\">)</span>\n<span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">prepare</span><span class=\"p\">()</span> <span class=\"c1\"># create vocabulary and training data</span>\n\n<span class=\"n\">training_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert_embeddings_from_file</span><span class=\"p\">(</span><span class=\"s2\">\"documents.txt\"</span><span class=\"p\">,</span> <span class=\"s2\">\"distiluse-base-multilingual-cased\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">training_dataset</span> <span class=\"o\">=</span> <span class=\"n\">CTMDataset</span><span class=\"p\">(</span><span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">bow</span><span class=\"p\">,</span> <span class=\"n\">training_bert</span><span class=\"p\">,</span> <span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">idx2token</span><span class=\"p\">)</span>\n\n<span class=\"n\">ctm</span> <span class=\"o\">=</span> <span class=\"n\">CTM</span><span class=\"p\">(</span><span class=\"n\">input_size</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">handler</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">),</span> <span class=\"n\">bert_input_size</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">inference_type</span><span class=\"o\">=</span><span class=\"s2\">\"contextual\"</span><span class=\"p\">,</span> <span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n\n<span class=\"n\">ctm</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">training_dataset</span><span class=\"p\">)</span> <span class=\"c1\"># run the model</span>\n</pre>\n<p>Predict topics for novel documents</p>\n<pre><span class=\"n\">test_handler</span> <span class=\"o\">=</span> <span class=\"n\">TextHandler</span><span class=\"p\">(</span><span class=\"s2\">\"spanish_documents.txt\"</span><span class=\"p\">)</span>\n<span class=\"n\">test_handler</span><span class=\"o\">.</span><span class=\"n\">prepare</span><span class=\"p\">()</span> <span class=\"c1\"># create vocabulary and training data</span>\n\n<span class=\"c1\"># generate BERT data</span>\n<span class=\"n\">testing_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert_embeddings_from_file</span><span class=\"p\">(</span><span class=\"s2\">\"spanish_documents.txt\"</span><span class=\"p\">,</span> <span class=\"s2\">\"distiluse-base-multilingual-cased\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">testing_dataset</span> <span class=\"o\">=</span> <span class=\"n\">CTMDataset</span><span class=\"p\">(</span><span class=\"n\">test_handler</span><span class=\"o\">.</span><span class=\"n\">bow</span><span class=\"p\">,</span> <span class=\"n\">testing_bert</span><span class=\"p\">,</span> <span class=\"n\">test_handler</span><span class=\"o\">.</span><span class=\"n\">idx2token</span><span class=\"p\">)</span>\n<span class=\"n\">ctm</span><span class=\"o\">.</span><span class=\"n\">get_thetas</span><span class=\"p\">(</span><span class=\"n\">testing_dataset</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"development-team\">\n<h3>Development Team</h3>\n<ul>\n<li>Federico Bianchi &lt;<a href=\"mailto:f.bianchi%40unibocconi.it\">f<span>.</span>bianchi<span>@</span>unibocconi<span>.</span>it</a>&gt; Bocconi University</li>\n<li>Silvia Terragni &lt;<a href=\"mailto:s.terragni4%40campus.unimib.it\">s<span>.</span>terragni4<span>@</span>campus<span>.</span>unimib<span>.</span>it</a>&gt; University of Milan-Bicocca</li>\n<li>Dirk Hovy &lt;<a href=\"mailto:dirk.hovy%40unibocconi.it\">dirk<span>.</span>hovy<span>@</span>unibocconi<span>.</span>it</a>&gt; Bocconi University</li>\n</ul>\n</div>\n<div id=\"references\">\n<h3>References</h3>\n<p>Combined BERT+BoW</p>\n<pre>@article{bianchi2020pretraining,\n    title={Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence},\n    author={Federico Bianchi and Silvia Terragni and Dirk Hovy},\n    year={2020},\n   journal={arXiv preprint arXiv:2004.03974},\n}\n</pre>\n<p>Contextual TM</p>\n<pre>@article{bianchi2020crosslingual,\n    title={Cross-lingual Contextualized Topic Models with Zero-shot Learning},\n    author={Federico Bianchi and Silvia Terragni and Dirk Hovy and Debora Nozza and Elisabetta Fersini},\n    year={2020},\n   journal={arXiv preprint arXiv:2004.07737},\n}\n</pre>\n</div>\n<div id=\"credits\">\n<h3>Credits</h3>\n<p>This package was created with <a href=\"https://github.com/audreyr/cookiecutter\" rel=\"nofollow\">Cookiecutter</a> and the <a href=\"https://github.com/audreyr/cookiecutter-pypackage\" rel=\"nofollow\">audreyr/cookiecutter-pypackage</a> project template.\nTo ease the use of the library we have also incuded the <a href=\"https://github.com/dlukes/rbo\" rel=\"nofollow\">rbo</a> package, all the rights reserved to the author of that package.</p>\n</div>\n</div>\n<div id=\"history\">\n<h2>History</h2>\n<div id=\"id1\">\n<h3>1.0.0 (2020-04-05)</h3>\n<ul>\n<li>Released models with the main features implemented</li>\n</ul>\n</div>\n<div id=\"id2\">\n<h3>0.1.0 (2020-04-04)</h3>\n<ul>\n<li>First release on PyPI.</li>\n</ul>\n</div>\n</div>\n\n          </div>"}, "last_serial": 7039894, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "744c16f1d76cc17b788162316b0b3773", "sha256": "ac5fbbbc5ff0b65e5d5a4d7f6b96f08d9fb36f527824bc87dea07a1a4791bd95"}, "downloads": -1, "filename": "contextualized_topic_models-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "744c16f1d76cc17b788162316b0b3773", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 11333, "upload_time": "2020-04-04T19:58:29", "upload_time_iso_8601": "2020-04-04T19:58:29.167521Z", "url": "https://files.pythonhosted.org/packages/3b/ea/465b1a6cfada61ef2578e97a1a8071e55c335b013d7be8daa70d4d2eedf8/contextualized_topic_models-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "871985191d8bde1d4b5f10e70d4b1cd9", "sha256": "f5ed48464c57d700eaf09aac52e2234eb0a96af2629cd187a05d6522f11c610d"}, "downloads": -1, "filename": "contextualized_topic_models-0.1.0.tar.gz", "has_sig": false, "md5_digest": "871985191d8bde1d4b5f10e70d4b1cd9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 14498, "upload_time": "2020-04-04T19:58:31", "upload_time_iso_8601": "2020-04-04T19:58:31.372421Z", "url": "https://files.pythonhosted.org/packages/13/be/6f144f8f801254787889c1b96aec3770b5360cdaadd7be545a933629f128/contextualized_topic_models-0.1.0.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "93d753cda1a2d1093f3816bddf82b0f6", "sha256": "c918320a3098bc8dfaea1edfa65aaa816b6d61fa8819f0fef4feb6e8f47de590"}, "downloads": -1, "filename": "contextualized_topic_models-0.4.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "93d753cda1a2d1093f3816bddf82b0f6", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 18033, "upload_time": "2020-04-04T22:14:25", "upload_time_iso_8601": "2020-04-04T22:14:25.264394Z", "url": "https://files.pythonhosted.org/packages/4e/9a/4237d58745be9612a1aab16f3c28bc2a03d0e3da1742f159cdf30fd9ed86/contextualized_topic_models-0.4.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "53bafaf5ab6e9b99d28c2d83a4e98de1", "sha256": "deceeda87d8c595ca288d84e9b1164652e86971301bfa16328805e3cde2d6ac0"}, "downloads": -1, "filename": "contextualized_topic_models-0.4.2.tar.gz", "has_sig": false, "md5_digest": "53bafaf5ab6e9b99d28c2d83a4e98de1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 20717, "upload_time": "2020-04-04T22:14:26", "upload_time_iso_8601": "2020-04-04T22:14:26.437754Z", "url": "https://files.pythonhosted.org/packages/21/2f/c9e175b6632c726e6b83fa15dfecd6c0bc0955a9c54b8dd6148cacc519f3/contextualized_topic_models-0.4.2.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "5fa68d2f8cee9567f748ce120ddbdf42", "sha256": "9777643ad352b34383a7780861bb9d69f7e7ff19fcc0b6c6457f49aa56cb4f7c"}, "downloads": -1, "filename": "contextualized_topic_models-1.0.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5fa68d2f8cee9567f748ce120ddbdf42", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 18998, "upload_time": "2020-04-05T14:02:52", "upload_time_iso_8601": "2020-04-05T14:02:52.620151Z", "url": "https://files.pythonhosted.org/packages/65/fd/ad9f00fb49d53df9aedf63e3c5ff63d1e65bfb98b468f4d29cc5473577c9/contextualized_topic_models-1.0.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d7b5646365926f867b4e1040949b7d3b", "sha256": "7c56a7797cbaec4b82de0a893b6c793dac344fd9cc637dd2db604f35b06a8c06"}, "downloads": -1, "filename": "contextualized_topic_models-1.0.0.tar.gz", "has_sig": false, "md5_digest": "d7b5646365926f867b4e1040949b7d3b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 22243, "upload_time": "2020-04-05T14:02:53", "upload_time_iso_8601": "2020-04-05T14:02:53.804284Z", "url": "https://files.pythonhosted.org/packages/51/1d/32a5da861fe0570ea1d3276a9ca70fac0019339e7a8d6d0780660f27b8ae/contextualized_topic_models-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "c98ebb58a88805b411e59e26acd41b0a", "sha256": "c5ef6d0d4fcb55a46e1ecbec0c6b0f81114e7e66c6e4ab9aba4104f8a53f60bd"}, "downloads": -1, "filename": "contextualized_topic_models-1.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c98ebb58a88805b411e59e26acd41b0a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 19754, "upload_time": "2020-04-08T10:25:12", "upload_time_iso_8601": "2020-04-08T10:25:12.738144Z", "url": "https://files.pythonhosted.org/packages/d0/10/d7519b918144151b6835120cf19f48ef468bf8a8ee28f49766d4bcbfd8f6/contextualized_topic_models-1.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1a04886e2e5669c30f955e8d2f508188", "sha256": "10a0ab4d4bbb49e5948ebbe69b7a746ae22e1df3cf27ea9598f6db3a32612c0a"}, "downloads": -1, "filename": "contextualized_topic_models-1.0.1.tar.gz", "has_sig": false, "md5_digest": "1a04886e2e5669c30f955e8d2f508188", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 23583, "upload_time": "2020-04-08T10:25:14", "upload_time_iso_8601": "2020-04-08T10:25:14.036653Z", "url": "https://files.pythonhosted.org/packages/6e/c4/5bc89491e7bca5625f9a3adf47504666e597e50d0ed80920730a812b0509/contextualized_topic_models-1.0.1.tar.gz", "yanked": false}], "1.3.1": [{"comment_text": "", "digests": {"md5": "5936b577f0d2b58acad2a7009fc37adc", "sha256": "b9082ebdb72a897d2506a4dda2b8b87ee74f9c3900e42ae837c17bb8c0613f1f"}, "downloads": -1, "filename": "contextualized_topic_models-1.3.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5936b577f0d2b58acad2a7009fc37adc", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 20271, "upload_time": "2020-04-17T12:24:09", "upload_time_iso_8601": "2020-04-17T12:24:09.167251Z", "url": "https://files.pythonhosted.org/packages/c5/dd/70c15e3c73de38725b4ce8fe544743aa042e3005137ceea9dde4f486e300/contextualized_topic_models-1.3.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d643e4ba603a7b2ec9fa3acd61d5d2e3", "sha256": "1860e4e258dee2e0bb7f4e4110053c94322b564645c29149ca644be4245aa3a4"}, "downloads": -1, "filename": "contextualized_topic_models-1.3.1.tar.gz", "has_sig": false, "md5_digest": "d643e4ba603a7b2ec9fa3acd61d5d2e3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 24803, "upload_time": "2020-04-17T12:24:10", "upload_time_iso_8601": "2020-04-17T12:24:10.551265Z", "url": "https://files.pythonhosted.org/packages/89/a5/d91a96c0cb32f5201b8557b8d306769a0a57a2f5ca25a0f73693fa31cbf2/contextualized_topic_models-1.3.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5936b577f0d2b58acad2a7009fc37adc", "sha256": "b9082ebdb72a897d2506a4dda2b8b87ee74f9c3900e42ae837c17bb8c0613f1f"}, "downloads": -1, "filename": "contextualized_topic_models-1.3.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5936b577f0d2b58acad2a7009fc37adc", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 20271, "upload_time": "2020-04-17T12:24:09", "upload_time_iso_8601": "2020-04-17T12:24:09.167251Z", "url": "https://files.pythonhosted.org/packages/c5/dd/70c15e3c73de38725b4ce8fe544743aa042e3005137ceea9dde4f486e300/contextualized_topic_models-1.3.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d643e4ba603a7b2ec9fa3acd61d5d2e3", "sha256": "1860e4e258dee2e0bb7f4e4110053c94322b564645c29149ca644be4245aa3a4"}, "downloads": -1, "filename": "contextualized_topic_models-1.3.1.tar.gz", "has_sig": false, "md5_digest": "d643e4ba603a7b2ec9fa3acd61d5d2e3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 24803, "upload_time": "2020-04-17T12:24:10", "upload_time_iso_8601": "2020-04-17T12:24:10.551265Z", "url": "https://files.pythonhosted.org/packages/89/a5/d91a96c0cb32f5201b8557b8d306769a0a57a2f5ca25a0f73693fa31cbf2/contextualized_topic_models-1.3.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:32 2020"}