{"info": {"author": "Lucy Linder", "author_email": "lucy.derlin@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Phrasal\n\n## Forewords \n\n### What is it ?\n\nPhrasal is a library of tools to help gather meaningful, proper sentences from websites. \n\nWell, at least if used together. Each tool has a value of its own. \nFor example, the `Normalizer` (my favorite!) is very useful for NLP, when you have a crappy text corpus you need to clean.\nThe `MocySplitter` is a nice alternative to Moses when you need to cleverly split a stream of text into sentences, one per line. \nEtc.\n\n### Why was it developed ?\n\nI have been working on a project lately, called [SwissText](https://github.com/derlin/swisstext) that gathers Swiss German sentences from scraping the Internet (no kidding, see the LREC 2020 publication on [arXiv](https://arxiv.org/abs/1912.00159)).\nTo do so, I had to build upon existing tools and develop some of my own. \nWhile they were initially for Swiss German, I figured that it would maybe be useful in other contexts, hence this repo which is a stripped-down version of some of the SwissText modules.\n\n### How does it work ?\n\nThis repo contains implementations of four types of tools, which constitute together a pipeline:\n\n1. *converter*: extract (main) text from raw HTML;\n2. *normalizer*: normalize the raw text, including the encoding, quotes, spaces, etc.;\n3. *splitter*: split the text into chunks (potential sentences);\n4. *filterer*: filter chunks to keep only \"proper\" sentences.\n\nFor each step, I propose one or more implementations.\n\n## Tools available\n\n**HtmlConverters**\n\n* `phrasal.BsConverter` \\\nA converter built upon `BeautifulSoup` that exact text found on the HTML. \nText from code blocks, scripts or styles is ignored.\nIt deals cleverly with encodings and always delivers text in UTF-8.\n\n\n* `phrasal.JustextConverter` \\\na converter based on [`justext`](https://pypi.org/project/jusText/), that try to spot and remove boilerplate content.\nBy default, it only keeps \"good\" paragraph, that is text long enough to be a full sentences and with a low link density.\n\n**Normalizers**\n\n* `phrasal.Normalizer`, or simply `phrasal.normalize_text`\\\nNormalize some text (using a serie of homemade regexes), including: normalize spaces, replace combining diacritics by the accented letter codepoints and strip leftovers, normalize dashes and quotemarks, replace non-breakable spaces with regular ones, etc. \\\nIt can also try to fix encoding errors (see [`ftfy`](https://pypi.org/project/ftfy/)) and strip most unicode emoji symbols.\n\n**Splitters**\n\n* `phrasal.MosesSplitter`\\\nMoses' splitter [`split-sentences.perl`](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ems/support/split-sentences.perl) completely rewritten in Python. It thus perfectly mimics the behavior, while being 5x faster than calling perl from Python (approach taken by [`MosesTokenizer`](https://pypi.org/project/mosestokenizer/) for example).\n* `phrasal.MocySplitter`\\\nAn improvement upon `MosesSplitter`, which: deals more efficiently with lowercase (people are lazy on the Web), try to preserve links, can split on `:` or `;` (optional), etc.\n\n**Filterers**\n\n* `phrasal.PatternSentenceFilter`\\\nA filterer based on a list of simple rules a proper sentence should respect, such as \"*at least five words*\", \"*no S P E L L E D* words\", etc. \\\nWhat is *awesome* ? The rules are expressed in a (homemade) YAML-based syntax and are highly customizable. If you don't like the behavior, have a look at `pattern_sentence_filter.yaml` and try writing your own set of rules !\n\n\n**link_utils**\n\nThe `phrasal.link_utils` module is a simple utility to process href links found on a page. It will resolve relative links\n(given a base URL), remove duplicates, strip anchors and exclude non-HTTP/HTTPs links.\n\nTo get the list of links from a URL (i.e. `href` found on the page main content), use `extract_links`:\n```python\nimport phrasal\n\nall_links = phrasal.extract_links('https://github.com/derlin/phrasal')\n```\n\n## How to use\n\nInstall the library using:\n```bash\n# regular install, one of:\npython setup.py install \npip install .\n\n# for development, one of:\npython setup.py develop\npip install -e .\npip install -e .[showcase] # for streamlit\n```\n\n### As a library\n\n```python\nimport phrasal\n```\nDone.\n\n### From the command line\n\nEach tool contains a command line interface with different arguments. Discover it by typing:\n```bash\npython -m phrasal --help\n```\n```bash\npython -m phrasal --help\nCall one of the tools from the command line. Usage: \n   classname [other arguments specific to classname]|[-h]\n\nAllowed classname arguments:\n - BsConverter\n - JustextConverter\n - PatternSentenceFilter\n - MocySplitter\n - MosesSplitter\n - Normalizer\n```\nHere are some examples:\n```bash\npython -m phrasal JustextConverter -u https://icosys.ch/swisscrawl\n=== from URL https://icosys.ch/swisscrawl\nAs part of the SwissTranslation project, SwissCrawl is a corpus of 500,000+ Swiss German (GSW)  [...]\n[...]\n```\n```bash\npython -m phrasal PatternSentenceFilter -i <(echo 'not-a-sentence\\nYEAH !!!\\nCet outil fonctionne tr\u00e8s bien, je l\u2019utilise tous les jours.')\nCet outil fonctionne tr\u00e8s bien, je l\u2019utilise tous les jours.\n```\n```bash\npython -m phrasal Normalizer -i raw_text.txt -o clean_text.txt\n```\n\n### I just need one tool...\n\nNo problem, each tool is more or less independent. \nYou may want to simplify the code a bit (e.g. remove the interface inheritance, transform classes into static scripts, I don't know), but I hope the source code is self-explaining. \n\n## Running tests\n\nTests are using `tox` and `pytest`. The easiest way to run them is:\n```bash\npip install tox tox-venv\ntox\n```\n\n## Running the showcase\n\nA showcase using [streamlit](https://www.streamlit.io/) is included. \nIt allows you to test the full pipeline straight from your browser and also play with the different tools and options\nfrom the **Live Customizer**. Once you found what works for you, you can simply copy-paste the code snippet generated.\n\nRun the showcase locally by doing:\n```bash\npip install streamlit\nstreamlit run src/showcase/lit.py\n```\n\n\n## License\n\nThis work is licensed under Apache 2.0, so you can basically do anything with it. \n\n*However*, I would **really enjoy** it if you **credit me** somehow, either by citing my name, send me an email to say hi (I get lonely sometime, may be nice to chat), leave a star on GitHub, or any other way you think may give me strength to keep doing open-source :blush:.\n\n## Related resources\n\n* [get-html](https://pypi.org/project/get-html/) to get raw or renderer HTML (used in this repo)\n* [SwissText](https://github.com/swisstext)\n* [SwissTranslation project page](https://icosys.ch/swisscrawl)\n* :octopus::octopus::octopus::octopus::octopus::octopus::octopus::octopus: (I just love octopuses)\n* [Personal website](https://derlin.ch)\n\n## TODO\n\n* add some usecases, such as finding links, cleaning a text file, etc. add language support information", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/derlin/phrasal", "keywords": "", "license": "Apache License 2.0", "maintainer": "", "maintainer_email": "", "name": "phrasal", "package_url": "https://pypi.org/project/phrasal/", "platform": "", "project_url": "https://pypi.org/project/phrasal/", "project_urls": {"Homepage": "https://github.com/derlin/phrasal"}, "release_url": "https://pypi.org/project/phrasal/0.0.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "NLP tools to extract, normalize and filter sentences from text/HTML", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Phrasal</h1>\n<h2>Forewords</h2>\n<h3>What is it ?</h3>\n<p>Phrasal is a library of tools to help gather meaningful, proper sentences from websites.</p>\n<p>Well, at least if used together. Each tool has a value of its own.\nFor example, the <code>Normalizer</code> (my favorite!) is very useful for NLP, when you have a crappy text corpus you need to clean.\nThe <code>MocySplitter</code> is a nice alternative to Moses when you need to cleverly split a stream of text into sentences, one per line.\nEtc.</p>\n<h3>Why was it developed ?</h3>\n<p>I have been working on a project lately, called <a href=\"https://github.com/derlin/swisstext\" rel=\"nofollow\">SwissText</a> that gathers Swiss German sentences from scraping the Internet (no kidding, see the LREC 2020 publication on <a href=\"https://arxiv.org/abs/1912.00159\" rel=\"nofollow\">arXiv</a>).\nTo do so, I had to build upon existing tools and develop some of my own.\nWhile they were initially for Swiss German, I figured that it would maybe be useful in other contexts, hence this repo which is a stripped-down version of some of the SwissText modules.</p>\n<h3>How does it work ?</h3>\n<p>This repo contains implementations of four types of tools, which constitute together a pipeline:</p>\n<ol>\n<li><em>converter</em>: extract (main) text from raw HTML;</li>\n<li><em>normalizer</em>: normalize the raw text, including the encoding, quotes, spaces, etc.;</li>\n<li><em>splitter</em>: split the text into chunks (potential sentences);</li>\n<li><em>filterer</em>: filter chunks to keep only \"proper\" sentences.</li>\n</ol>\n<p>For each step, I propose one or more implementations.</p>\n<h2>Tools available</h2>\n<p><strong>HtmlConverters</strong></p>\n<ul>\n<li>\n<p><code>phrasal.BsConverter</code> <br>\nA converter built upon <code>BeautifulSoup</code> that exact text found on the HTML.\nText from code blocks, scripts or styles is ignored.\nIt deals cleverly with encodings and always delivers text in UTF-8.</p>\n</li>\n<li>\n<p><code>phrasal.JustextConverter</code> <br>\na converter based on <a href=\"https://pypi.org/project/jusText/\" rel=\"nofollow\"><code>justext</code></a>, that try to spot and remove boilerplate content.\nBy default, it only keeps \"good\" paragraph, that is text long enough to be a full sentences and with a low link density.</p>\n</li>\n</ul>\n<p><strong>Normalizers</strong></p>\n<ul>\n<li><code>phrasal.Normalizer</code>, or simply <code>phrasal.normalize_text</code><br>\nNormalize some text (using a serie of homemade regexes), including: normalize spaces, replace combining diacritics by the accented letter codepoints and strip leftovers, normalize dashes and quotemarks, replace non-breakable spaces with regular ones, etc. <br>\nIt can also try to fix encoding errors (see <a href=\"https://pypi.org/project/ftfy/\" rel=\"nofollow\"><code>ftfy</code></a>) and strip most unicode emoji symbols.</li>\n</ul>\n<p><strong>Splitters</strong></p>\n<ul>\n<li><code>phrasal.MosesSplitter</code><br>\nMoses' splitter <a href=\"https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ems/support/split-sentences.perl\" rel=\"nofollow\"><code>split-sentences.perl</code></a> completely rewritten in Python. It thus perfectly mimics the behavior, while being 5x faster than calling perl from Python (approach taken by <a href=\"https://pypi.org/project/mosestokenizer/\" rel=\"nofollow\"><code>MosesTokenizer</code></a> for example).</li>\n<li><code>phrasal.MocySplitter</code><br>\nAn improvement upon <code>MosesSplitter</code>, which: deals more efficiently with lowercase (people are lazy on the Web), try to preserve links, can split on <code>:</code> or <code>;</code> (optional), etc.</li>\n</ul>\n<p><strong>Filterers</strong></p>\n<ul>\n<li><code>phrasal.PatternSentenceFilter</code><br>\nA filterer based on a list of simple rules a proper sentence should respect, such as \"<em>at least five words</em>\", \"<em>no S P E L L E D</em> words\", etc. <br>\nWhat is <em>awesome</em> ? The rules are expressed in a (homemade) YAML-based syntax and are highly customizable. If you don't like the behavior, have a look at <code>pattern_sentence_filter.yaml</code> and try writing your own set of rules !</li>\n</ul>\n<p><strong>link_utils</strong></p>\n<p>The <code>phrasal.link_utils</code> module is a simple utility to process href links found on a page. It will resolve relative links\n(given a base URL), remove duplicates, strip anchors and exclude non-HTTP/HTTPs links.</p>\n<p>To get the list of links from a URL (i.e. <code>href</code> found on the page main content), use <code>extract_links</code>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">phrasal</span>\n\n<span class=\"n\">all_links</span> <span class=\"o\">=</span> <span class=\"n\">phrasal</span><span class=\"o\">.</span><span class=\"n\">extract_links</span><span class=\"p\">(</span><span class=\"s1\">'https://github.com/derlin/phrasal'</span><span class=\"p\">)</span>\n</pre>\n<h2>How to use</h2>\n<p>Install the library using:</p>\n<pre><span class=\"c1\"># regular install, one of:</span>\npython setup.py install \npip install .\n\n<span class=\"c1\"># for development, one of:</span>\npython setup.py develop\npip install -e .\npip install -e .<span class=\"o\">[</span>showcase<span class=\"o\">]</span> <span class=\"c1\"># for streamlit</span>\n</pre>\n<h3>As a library</h3>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">phrasal</span>\n</pre>\n<p>Done.</p>\n<h3>From the command line</h3>\n<p>Each tool contains a command line interface with different arguments. Discover it by typing:</p>\n<pre>python -m phrasal --help\n</pre>\n<pre>python -m phrasal --help\nCall one of the tools from the <span class=\"nb\">command</span> line. Usage: \n   classname <span class=\"o\">[</span>other arguments specific to classname<span class=\"o\">]</span><span class=\"p\">|</span><span class=\"o\">[</span>-h<span class=\"o\">]</span>\n\nAllowed classname arguments:\n - BsConverter\n - JustextConverter\n - PatternSentenceFilter\n - MocySplitter\n - MosesSplitter\n - Normalizer\n</pre>\n<p>Here are some examples:</p>\n<pre>python -m phrasal JustextConverter -u https://icosys.ch/swisscrawl\n<span class=\"o\">===</span> from URL https://icosys.ch/swisscrawl\nAs part of the SwissTranslation project, SwissCrawl is a corpus of <span class=\"m\">500</span>,000+ Swiss German <span class=\"o\">(</span>GSW<span class=\"o\">)</span>  <span class=\"o\">[</span>...<span class=\"o\">]</span>\n<span class=\"o\">[</span>...<span class=\"o\">]</span>\n</pre>\n<pre>python -m phrasal PatternSentenceFilter -i &lt;<span class=\"o\">(</span><span class=\"nb\">echo</span> <span class=\"s1\">'not-a-sentence\\nYEAH !!!\\nCet outil fonctionne tr\u00e8s bien, je l\u2019utilise tous les jours.'</span><span class=\"o\">)</span>\nCet outil fonctionne tr\u00e8s bien, je l\u2019utilise tous les jours.\n</pre>\n<pre>python -m phrasal Normalizer -i raw_text.txt -o clean_text.txt\n</pre>\n<h3>I just need one tool...</h3>\n<p>No problem, each tool is more or less independent.\nYou may want to simplify the code a bit (e.g. remove the interface inheritance, transform classes into static scripts, I don't know), but I hope the source code is self-explaining.</p>\n<h2>Running tests</h2>\n<p>Tests are using <code>tox</code> and <code>pytest</code>. The easiest way to run them is:</p>\n<pre>pip install tox tox-venv\ntox\n</pre>\n<h2>Running the showcase</h2>\n<p>A showcase using <a href=\"https://www.streamlit.io/\" rel=\"nofollow\">streamlit</a> is included.\nIt allows you to test the full pipeline straight from your browser and also play with the different tools and options\nfrom the <strong>Live Customizer</strong>. Once you found what works for you, you can simply copy-paste the code snippet generated.</p>\n<p>Run the showcase locally by doing:</p>\n<pre>pip install streamlit\nstreamlit run src/showcase/lit.py\n</pre>\n<h2>License</h2>\n<p>This work is licensed under Apache 2.0, so you can basically do anything with it.</p>\n<p><em>However</em>, I would <strong>really enjoy</strong> it if you <strong>credit me</strong> somehow, either by citing my name, send me an email to say hi (I get lonely sometime, may be nice to chat), leave a star on GitHub, or any other way you think may give me strength to keep doing open-source :blush:.</p>\n<h2>Related resources</h2>\n<ul>\n<li><a href=\"https://pypi.org/project/get-html/\" rel=\"nofollow\">get-html</a> to get raw or renderer HTML (used in this repo)</li>\n<li><a href=\"https://github.com/swisstext\" rel=\"nofollow\">SwissText</a></li>\n<li><a href=\"https://icosys.ch/swisscrawl\" rel=\"nofollow\">SwissTranslation project page</a></li>\n<li>:octopus::octopus::octopus::octopus::octopus::octopus::octopus::octopus: (I just love octopuses)</li>\n<li><a href=\"https://derlin.ch\" rel=\"nofollow\">Personal website</a></li>\n</ul>\n<h2>TODO</h2>\n<ul>\n<li>add some usecases, such as finding links, cleaning a text file, etc. add language support information</li>\n</ul>\n\n          </div>"}, "last_serial": 6768272, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "0ff2475f011f00bb8dc26f73a4aaf57c", "sha256": "9220c66fff1fc1235f44b4bd0b0d29461ac98c1b4700ed571cb29ba358b481fe"}, "downloads": -1, "filename": "phrasal-0.0.1.tar.gz", "has_sig": false, "md5_digest": "0ff2475f011f00bb8dc26f73a4aaf57c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27890, "upload_time": "2020-03-07T14:36:35", "upload_time_iso_8601": "2020-03-07T14:36:35.789729Z", "url": "https://files.pythonhosted.org/packages/08/a3/3aa4821af24e5bc475ad9c6d783ca31b43be037f8ccd5fb1ab00589c9154/phrasal-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0ff2475f011f00bb8dc26f73a4aaf57c", "sha256": "9220c66fff1fc1235f44b4bd0b0d29461ac98c1b4700ed571cb29ba358b481fe"}, "downloads": -1, "filename": "phrasal-0.0.1.tar.gz", "has_sig": false, "md5_digest": "0ff2475f011f00bb8dc26f73a4aaf57c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27890, "upload_time": "2020-03-07T14:36:35", "upload_time_iso_8601": "2020-03-07T14:36:35.789729Z", "url": "https://files.pythonhosted.org/packages/08/a3/3aa4821af24e5bc475ad9c6d783ca31b43be037f8ccd5fb1ab00589c9154/phrasal-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:17 2020"}