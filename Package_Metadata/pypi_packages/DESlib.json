{"info": {"author": "Rafael M. O. Cruz", "author_email": "rafaelmenelau@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: BSD License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": ".. image:: https://readthedocs.org/projects/deslib/badge/?version=latest\n    :target: http://deslib.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n\n.. image:: https://circleci.com/gh/scikit-learn-contrib/DESlib.svg?style=shield\n    :target: https://circleci.com/gh/scikit-learn-contrib/DESlib\n\n.. image:: https://travis-ci.org/scikit-learn-contrib/DESlib.svg?branch=master\n    :target: https://travis-ci.org/scikit-learn-contrib/DESlib\n\n.. image:: https://codecov.io/gh/scikit-learn-contrib/DESlib/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/scikit-learn-contrib/DESlib\n\n.. image:: https://api.codacy.com/project/badge/Grade/59500eecc5524c59b9eb2284b43ae3e6\n   :alt: Codacy Badge\n   :target: https://app.codacy.com/app/Menelau/DESlib?\n\n.. image:: https://img.shields.io/badge/License-BSD%203--Clause-blue.svg\n    :target: https://opensource.org/licenses/BSD-3-Clause\n\n.. image:: https://badges.gitter.im/DESlib/gitter.png\n    :target: https://gitter.im/deslib/Lobby\n\nDESlib\n========\n\nDESlib is an easy-to-use ensemble learning library focused on the implementation of the state-of-the-art techniques for dynamic classifier and ensemble selection.\nThe library is is based on scikit-learn_, using the same method signatures: **fit**, **predict**, **predict_proba** and **score**.\nAll dynamic selection techniques were implemented according to the definitions from [1]_.\n\nDynamic Selection:\n-------------------\n\nDynamic Selection (DS) refers to techniques in which the base classifiers are selected\ndynamically at test time, according to each new sample to be classified. Only the most competent, or an ensemble of the most competent classifiers is selected to predict\nthe label of a specific test sample. The rationale for these techniques is that not every classifier in\nthe pool is an expert in classifying all unknown samples, but rather each base classifier is an expert in\na different local region of the feature space.\n\nDS is one of the most promising MCS approaches (Multiple Classifier Systems) due to an increasing number of empirical studies\nreporting superior performance over static combination methods. Such techniques\nhave achieved better classification performance especially when dealing with small-sized and imbalanced datasets.\n\nInstallation:\n-------------\n\nThe package can be installed using pip:\n\nStable version:\n\n.. code-block:: bash\n\n    pip install deslib\n\nLatest version (under development):\n\n.. code-block:: bash\n\n    pip install git+https://github.com/scikit-learn-contrib/DESlib\n\n\nDependencies:\n-------------\n\nDESlib is tested to work with Python 3.5, 3.6 and 3.7. The dependency requirements are:\n\n* scipy(>=1.2.0)\n* numpy(>=1.10.4)\n* scikit-learn(>=0.19.0)\n\nThese dependencies are automatically installed using the pip commands above.\n\nExamples:\n---------\n\nHere we show an example using the KNORA-E method with random forest as a pool of classifiers:\n\n.. code-block:: python\n\n    from deslib.des.knora_e import KNORAE\n\n    # Train a pool of 10 classifiers\n    pool_classifiers = RandomForestClassifier(n_estimators=10)\n    pool_classifiers.fit(X_train, y_train)\n\n    # Initialize the DES model\n    knorae = KNORAE(pool_classifiers)\n\n    # Preprocess the Dynamic Selection dataset (DSEL)\n    knorae.fit(X_dsel, y_dsel)\n\n    # Predict new examples:\n    knorae.predict(X_test)\n\nThe library accepts any list of classifiers (compatible with scikit-learn) as input, including a list containing different classifier models (heterogeneous ensembles).\nMore examples on how to use the API can be found in the documentation_ and in the Examples directory.\n\nOrganization:\n-------------\n\nThe library is divided into four modules:\n\n1. deslib.des: Implementation of DES techniques (Dynamic Ensemble Selection).\n2. deslib.dcs: Implementation of DCS techniques (Dynamic Classifier Selection).\n3. deslib.static: Implementation of baseline ensemble methods.\n4. deslib.util: A collection of aggregation functions and diversity measures for ensemble of classifiers.\n\n* DES techniques currently available are:\n    1. META-DES [7]_ [8]_ [15]_\n    2. K-Nearest-Oracle-Eliminate (KNORA-E) [3]_\n    3. K-Nearest-Oracle-Union (KNORA-U) [3]_\n    4. Dynamic Ensemble Selection-Performance(DES-P) [12]_\n    5. K-Nearest-Output Profiles (KNOP) [9]_\n    6. Randomized Reference Classifier (DES-RRC) [10]_\n    7. DES Kullback-Leibler Divergence (DES-KL) [12]_\n    8. DES-Exponential [21]_\n    9. DES-Logarithmic [11]_\n    10. DES-Minimum Difference [21]_\n    11. DES-Clustering [16]_\n    12. DES-KNN [16]_\n    13. DES Multiclass Imbalance (DES-MI) [24]_\n\n* DCS techniques currently available are:\n    1. Modified Classifier Rank (Rank) [19]_\n    2. Overall Local Accuracy (OLA) [4]_\n    3. Local Class Accuracy (LCA) [4]_\n    4. Modified Local Accuracy (MLA) [23]_\n    5. Multiple Classifier Behaviour (MCB) [5]_\n    6. A Priori Selection (A Priori) [6]_\n    7. A Posteriori Selection (A Posteriori) [6]_\n\n* Baseline methods:\n    1. Oracle [20]_\n    2. Single Best [2]_\n    3. Static Selection [2]_\n    4. Stacked Classifier [25]_\n\nVariations of each DES techniques are also provided by the library (e.g., different versions of the META-DES framework).\n\nThe following techniques are also available for all methods:\n * For DES techniques, the combination of the selected classifiers can be done as Dynamic Selection (majority voting), Dynamic Weighting  (weighted majority voting) or a Hybrid (selection + weighting).\n * For all DS techniques, Dynamic Frienemy Pruning (DFP) [13]_ can be used.\n * For all DS techniques, Instance Hardness (IH) can be used to classify easy samples with a KNN and hard samples using the DS technique. More details on IH and Dynamic Selection can be found in [14]_.\n\nAs an optional requirement, the fast KNN implementation from FAISS_ can be used to speed-up the computation of the region of competence.\n\nCitation\n---------\n\nIf you use DESLib in a scientific paper, please consider citing the following paper:\n\nRafael M. O. Cruz, Luiz G. Hafemann, Robert Sabourin and George D. C. Cavalcanti `DESlib: A Dynamic ensemble selection library in Python. <https://arxiv.org/abs/1802.04967>`_ arXiv preprint arXiv:1802.04967 (2018).\n\n.. code-block:: text\n\n    @article{cruz_deslib:2018,\n        title = {{DESlib}: {A} {Dynamic} ensemble selection library in {Python}},\n        journal = {arXiv preprint arXiv:1802.04967},\n        author = {Cruz, Rafael M. O. and Hafemann, Luiz G. and Sabourin, Robert and Cavalcanti, George D. C.},\n        year = {2018}\n    }\n\nReferences:\n-----------\n\n.. [1] : R. M. O. Cruz, R. Sabourin, and G. D. Cavalcanti, \u201cDynamic classifier selection: Recent advances and perspectives,\u201d Information Fusion, vol. 41, pp. 195 \u2013 216, 2018.\n\n.. [2] : A. S. Britto, R. Sabourin, L. E. S. de Oliveira, Dynamic selection of classifiers - A comprehensive review, Pattern Recognition 47 (11) (2014) 3665\u20133680.\n\n.. [3] : A. H. R. Ko, R. Sabourin, u. S. Britto, Jr., From dynamic classifier selection to dynamic ensemble selection, Pattern Recognition 41 (2008) 1735\u20131748.\n\n.. [4] : K. Woods, W. P. Kegelmeyer, Jr., K. Bowyer, Combination of multiple classifiers using local accuracy estimates, IEEE Transactions on Pattern Analysis Machine Intelligence 19 (1997) 405\u2013410.\n\n.. [5] : G. Giacinto, F. Roli, Dynamic classifier selection based on multiple classifier behaviour, Pattern Recognition 34 (2001) 1879\u20131881.\n\n.. [6] : L. Didaci, G. Giacinto, F. Roli, G. L. Marcialis, A study on the performances of dynamic classifier selection based on local accuracy estimation, Pattern Recognition 38 (11) (2005) 2188\u20132191.\n\n.. [7] : R. M. O. Cruz, R. Sabourin, G. D. C. Cavalcanti, T. I. Ren, META-DES: A dynamic ensemble selection framework using meta-learning, Pattern Recognition 48 (5) (2015) 1925\u20131935.\n\n.. [8] : Cruz, R.M., Sabourin, R. and Cavalcanti, G.D., 2015, July. META-DES. H: a dynamic ensemble selection technique using meta-learning and a dynamic weighting approach. In Neural Networks (IJCNN), 2015 International Joint Conference on (pp. 1-8)\n\n.. [9] : P. R. Cavalin, R. Sabourin, C. Y. Suen, Dynamic selection approaches for multiple classifier systems, Neural Computing and Applications 22 (3-4) (2013) 673\u2013688.\n\n.. [10] : T.Woloszynski, M. Kurzynski, A probabilistic model of classifier competence for dynamic ensemble selection, Pattern Recognition 44 (2011) 2656\u20132668.\n\n.. [11] : T.Woloszynski, M. Kurzynski, A measure of competence based on randomized reference classifier for dynamic ensemble selection, in: International Conference on Pattern Recognition (ICPR), 2010, pp. 4194\u20134197.\n\n.. [12] : T. Woloszynski, M. Kurzynski, P. Podsiadlo, G. W. Stachowiak, A measure of competence based on random classification for dynamic ensemble selection, Information Fusion 13 (3) (2012) 207\u2013213.\n\n.. [13] : Oliveira, D.V.R., Cavalcanti, G.D.C. and Sabourin, R., Online Pruning of Base Classifiers for Dynamic Ensemble Selection, Pattern Recognition, vol. 72, December 2017, pp 44-58.\n\n.. [14] : Cruz RM, Zakane HH, Sabourin R, Cavalcanti GD. Dynamic Ensemble Selection VS K-NN: why and when Dynamic Selection obtains higher classification performance?.\n\n.. [15] : R. M. O. Cruz, R. Sabourin, G. D. C. Cavalcanti, META-DES.Oracle: Meta-learning and feature selection for dynamic ensemble selection, Information Fusion 38 (2017) 84\u2013103.Nov 30;38:84-103.\n\n.. [16] : R. G. F. Soares, A. Santana, A. M. P. Canuto, M. C. P. de Souto, Using accuracy and diversity to select classifiers to build ensembles, Proceedings of the International Joint Conference on Neural Networks (2006) 1310\u20131316.\n\n.. [17] : L. I. Kuncheva, Combining Pattern Classifiers: Methods and Algorithms, Wiley-Interscience, 2004.\n\n.. [18] : Shipp, Catherine A., and Ludmila I. Kuncheva. \"Relationships between combination methods and measures of diversity in combining classifiers.\" Information fusion 3.2 (2002): 135-148.\n\n.. [19] : M. Sabourin, A. Mitiche, D. Thomas, G. Nagy, Classifier combination for handprinted digit recognition, International Conference on Document Analysis and Recognition (1993) 163\u2013166.\n\n.. [20] : L. I. Kuncheva, A theoretical study on six classifier fusion strategies, IEEE Transactions on Pattern Analysis and Machine Intelligence 24 (2) (2002) 281\u2013286.\n\n.. [21] : B. Antosik, M. Kurzynski, New measures of classifier competence \u2013 heuristics and application to the design of multiple classifier systems., in: Computer recognition systems 4., 2011, pp. 197\u2013206.\n\n.. [22] : Smith, Michael R., Tony Martinez, and Christophe Giraud-Carrier. \"An instance level analysis of data complexity.\" Machine learning 95.2 (2014), pp 225-256.\n\n.. [23] : P. C. Smits, Multiple classifier systems for supervised remote sensing image classification based on dynamic classifier selection, IEEE Transactions on Geoscience and Remote Sensing 40 (4) (2002) 801\u2013813.\n\n.. [24] : Garc\u00eda, S., Zhang, Z.L., Altalhi, A., Alshomrani, S. and Herrera, F., \"Dynamic ensemble selection for multi-class imbalanced datasets.\" Information Sciences 445 (2018): 22-37.\n\n.. [25] : Wolpert, David H. \"Stacked generalization.\" Neural networks 5, no. 2 (1992): 241-259.\n\n.. _scikit-learn: http://scikit-learn.org/stable/\n\n.. _numpy: http://www.numpy.org/\n\n.. _scipy: https://www.scipy.org/\n\n.. _documentation: https://deslib.readthedocs.io\n\n.. _FAISS: https://github.com/facebookresearch/faiss\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Menelau/DESlib", "keywords": "", "license": "BSD 3-clause \"New\" or \"Revised License\"", "maintainer": "Rafael M. O. Cruz, L. G. Hafemann", "maintainer_email": "rafaelmenelau@gmail.com", "name": "DESlib", "package_url": "https://pypi.org/project/DESlib/", "platform": "", "project_url": "https://pypi.org/project/DESlib/", "project_urls": {"Homepage": "https://github.com/Menelau/DESlib"}, "release_url": "https://pypi.org/project/DESlib/0.3/", "requires_dist": ["scikit-learn (>=0.19.0)", "numpy (>=1.10.4)", "scipy (>=1.2.0)"], "requires_python": ">=3", "summary": "Implementation of Dynamic Ensemble Selection methods", "version": "0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"http://deslib.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b94731f60e4ca1d233a7570d977e10aafd7b7e93/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6465736c69622f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<a href=\"https://circleci.com/gh/scikit-learn-contrib/DESlib\" rel=\"nofollow\"><img alt=\"https://circleci.com/gh/scikit-learn-contrib/DESlib.svg?style=shield\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0060676dc15c88dcaf66980f28c42296a2faed68/68747470733a2f2f636972636c6563692e636f6d2f67682f7363696b69742d6c6561726e2d636f6e747269622f4445536c69622e7376673f7374796c653d736869656c64\"></a>\n<a href=\"https://travis-ci.org/scikit-learn-contrib/DESlib\" rel=\"nofollow\"><img alt=\"https://travis-ci.org/scikit-learn-contrib/DESlib.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3f724f6ec38671be39347af49008c1038e32d763/68747470733a2f2f7472617669732d63692e6f72672f7363696b69742d6c6561726e2d636f6e747269622f4445536c69622e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://codecov.io/gh/scikit-learn-contrib/DESlib\" rel=\"nofollow\"><img alt=\"https://codecov.io/gh/scikit-learn-contrib/DESlib/branch/master/graph/badge.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c38f3425347bd4d6491f67f244f4d8f1e7b3cc89/68747470733a2f2f636f6465636f762e696f2f67682f7363696b69742d6c6561726e2d636f6e747269622f4445536c69622f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<a href=\"https://app.codacy.com/app/Menelau/DESlib?\" rel=\"nofollow\"><img alt=\"Codacy Badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f6eff0045febefad88e2300bb7196eb56c9b3a31/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3539353030656563633535323463353962396562323238346234336165336536\"></a>\n<a href=\"https://opensource.org/licenses/BSD-3-Clause\" rel=\"nofollow\"><img alt=\"https://img.shields.io/badge/License-BSD%203--Clause-blue.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/110fcca60a43a8ea37b1a5bda616e639325f2f30/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\"></a>\n<a href=\"https://gitter.im/deslib/Lobby\" rel=\"nofollow\"><img alt=\"https://badges.gitter.im/DESlib/gitter.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0adecfb5bd27c4b3405b122e8fec953d14f93d82/68747470733a2f2f6261646765732e6769747465722e696d2f4445536c69622f6769747465722e706e67\"></a>\n<div id=\"deslib\">\n<h2>DESlib</h2>\n<p>DESlib is an easy-to-use ensemble learning library focused on the implementation of the state-of-the-art techniques for dynamic classifier and ensemble selection.\nThe library is is based on <a href=\"http://scikit-learn.org/stable/\" rel=\"nofollow\">scikit-learn</a>, using the same method signatures: <strong>fit</strong>, <strong>predict</strong>, <strong>predict_proba</strong> and <strong>score</strong>.\nAll dynamic selection techniques were implemented according to the definitions from <a href=\"#id30\" id=\"id1\" rel=\"nofollow\">[1]</a>.</p>\n<div id=\"dynamic-selection\">\n<h3>Dynamic Selection:</h3>\n<p>Dynamic Selection (DS) refers to techniques in which the base classifiers are selected\ndynamically at test time, according to each new sample to be classified. Only the most competent, or an ensemble of the most competent classifiers is selected to predict\nthe label of a specific test sample. The rationale for these techniques is that not every classifier in\nthe pool is an expert in classifying all unknown samples, but rather each base classifier is an expert in\na different local region of the feature space.</p>\n<p>DS is one of the most promising MCS approaches (Multiple Classifier Systems) due to an increasing number of empirical studies\nreporting superior performance over static combination methods. Such techniques\nhave achieved better classification performance especially when dealing with small-sized and imbalanced datasets.</p>\n</div>\n<div id=\"installation\">\n<h3>Installation:</h3>\n<p>The package can be installed using pip:</p>\n<p>Stable version:</p>\n<pre>pip install deslib\n</pre>\n<p>Latest version (under development):</p>\n<pre>pip install git+https://github.com/scikit-learn-contrib/DESlib\n</pre>\n</div>\n<div id=\"dependencies\">\n<h3>Dependencies:</h3>\n<p>DESlib is tested to work with Python 3.5, 3.6 and 3.7. The dependency requirements are:</p>\n<ul>\n<li>scipy(&gt;=1.2.0)</li>\n<li>numpy(&gt;=1.10.4)</li>\n<li>scikit-learn(&gt;=0.19.0)</li>\n</ul>\n<p>These dependencies are automatically installed using the pip commands above.</p>\n</div>\n<div id=\"examples\">\n<h3>Examples:</h3>\n<p>Here we show an example using the KNORA-E method with random forest as a pool of classifiers:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">deslib.des.knora_e</span> <span class=\"kn\">import</span> <span class=\"n\">KNORAE</span>\n\n<span class=\"c1\"># Train a pool of 10 classifiers</span>\n<span class=\"n\">pool_classifiers</span> <span class=\"o\">=</span> <span class=\"n\">RandomForestClassifier</span><span class=\"p\">(</span><span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"n\">pool_classifiers</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Initialize the DES model</span>\n<span class=\"n\">knorae</span> <span class=\"o\">=</span> <span class=\"n\">KNORAE</span><span class=\"p\">(</span><span class=\"n\">pool_classifiers</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Preprocess the Dynamic Selection dataset (DSEL)</span>\n<span class=\"n\">knorae</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_dsel</span><span class=\"p\">,</span> <span class=\"n\">y_dsel</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Predict new examples:</span>\n<span class=\"n\">knorae</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span>\n</pre>\n<p>The library accepts any list of classifiers (compatible with scikit-learn) as input, including a list containing different classifier models (heterogeneous ensembles).\nMore examples on how to use the API can be found in the <a href=\"https://deslib.readthedocs.io\" rel=\"nofollow\">documentation</a> and in the Examples directory.</p>\n</div>\n<div id=\"organization\">\n<h3>Organization:</h3>\n<p>The library is divided into four modules:</p>\n<ol>\n<li>deslib.des: Implementation of DES techniques (Dynamic Ensemble Selection).</li>\n<li>deslib.dcs: Implementation of DCS techniques (Dynamic Classifier Selection).</li>\n<li>deslib.static: Implementation of baseline ensemble methods.</li>\n<li>deslib.util: A collection of aggregation functions and diversity measures for ensemble of classifiers.</li>\n</ol>\n<ul>\n<li><dl>\n<dt>DES techniques currently available are:</dt>\n<dd><ol>\n<li>META-DES <a href=\"#id36\" id=\"id2\" rel=\"nofollow\">[7]</a> <a href=\"#id37\" id=\"id3\" rel=\"nofollow\">[8]</a> <a href=\"#id44\" id=\"id4\" rel=\"nofollow\">[15]</a></li>\n<li>K-Nearest-Oracle-Eliminate (KNORA-E) <a href=\"#id32\" id=\"id5\" rel=\"nofollow\">[3]</a></li>\n<li>K-Nearest-Oracle-Union (KNORA-U) <a href=\"#id32\" id=\"id6\" rel=\"nofollow\">[3]</a></li>\n<li>Dynamic Ensemble Selection-Performance(DES-P) <a href=\"#id41\" id=\"id7\" rel=\"nofollow\">[12]</a></li>\n<li>K-Nearest-Output Profiles (KNOP) <a href=\"#id38\" id=\"id8\" rel=\"nofollow\">[9]</a></li>\n<li>Randomized Reference Classifier (DES-RRC) <a href=\"#id39\" id=\"id9\" rel=\"nofollow\">[10]</a></li>\n<li>DES Kullback-Leibler Divergence (DES-KL) <a href=\"#id41\" id=\"id10\" rel=\"nofollow\">[12]</a></li>\n<li>DES-Exponential <a href=\"#id50\" id=\"id11\" rel=\"nofollow\">[21]</a></li>\n<li>DES-Logarithmic <a href=\"#id40\" id=\"id12\" rel=\"nofollow\">[11]</a></li>\n<li>DES-Minimum Difference <a href=\"#id50\" id=\"id13\" rel=\"nofollow\">[21]</a></li>\n<li>DES-Clustering <a href=\"#id45\" id=\"id14\" rel=\"nofollow\">[16]</a></li>\n<li>DES-KNN <a href=\"#id45\" id=\"id15\" rel=\"nofollow\">[16]</a></li>\n<li>DES Multiclass Imbalance (DES-MI) <a href=\"#id53\" id=\"id16\" rel=\"nofollow\">[24]</a></li>\n</ol>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt>DCS techniques currently available are:</dt>\n<dd><ol>\n<li>Modified Classifier Rank (Rank) <a href=\"#id48\" id=\"id17\" rel=\"nofollow\">[19]</a></li>\n<li>Overall Local Accuracy (OLA) <a href=\"#id33\" id=\"id18\" rel=\"nofollow\">[4]</a></li>\n<li>Local Class Accuracy (LCA) <a href=\"#id33\" id=\"id19\" rel=\"nofollow\">[4]</a></li>\n<li>Modified Local Accuracy (MLA) <a href=\"#id52\" id=\"id20\" rel=\"nofollow\">[23]</a></li>\n<li>Multiple Classifier Behaviour (MCB) <a href=\"#id34\" id=\"id21\" rel=\"nofollow\">[5]</a></li>\n<li>A Priori Selection (A Priori) <a href=\"#id35\" id=\"id22\" rel=\"nofollow\">[6]</a></li>\n<li>A Posteriori Selection (A Posteriori) <a href=\"#id35\" id=\"id23\" rel=\"nofollow\">[6]</a></li>\n</ol>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt>Baseline methods:</dt>\n<dd><ol>\n<li>Oracle <a href=\"#id49\" id=\"id24\" rel=\"nofollow\">[20]</a></li>\n<li>Single Best <a href=\"#id31\" id=\"id25\" rel=\"nofollow\">[2]</a></li>\n<li>Static Selection <a href=\"#id31\" id=\"id26\" rel=\"nofollow\">[2]</a></li>\n<li>Stacked Classifier <a href=\"#id54\" id=\"id27\" rel=\"nofollow\">[25]</a></li>\n</ol>\n</dd>\n</dl>\n</li>\n</ul>\n<p>Variations of each DES techniques are also provided by the library (e.g., different versions of the META-DES framework).</p>\n<dl>\n<dt>The following techniques are also available for all methods:</dt>\n<dd><ul>\n<li>For DES techniques, the combination of the selected classifiers can be done as Dynamic Selection (majority voting), Dynamic Weighting  (weighted majority voting) or a Hybrid (selection + weighting).</li>\n<li>For all DS techniques, Dynamic Frienemy Pruning (DFP) <a href=\"#id42\" id=\"id28\" rel=\"nofollow\">[13]</a> can be used.</li>\n<li>For all DS techniques, Instance Hardness (IH) can be used to classify easy samples with a KNN and hard samples using the DS technique. More details on IH and Dynamic Selection can be found in <a href=\"#id43\" id=\"id29\" rel=\"nofollow\">[14]</a>.</li>\n</ul>\n</dd>\n</dl>\n<p>As an optional requirement, the fast KNN implementation from <a href=\"https://github.com/facebookresearch/faiss\" rel=\"nofollow\">FAISS</a> can be used to speed-up the computation of the region of competence.</p>\n</div>\n<div id=\"citation\">\n<h3>Citation</h3>\n<p>If you use DESLib in a scientific paper, please consider citing the following paper:</p>\n<p>Rafael M. O. Cruz, Luiz G. Hafemann, Robert Sabourin and George D. C. Cavalcanti <a href=\"https://arxiv.org/abs/1802.04967\" rel=\"nofollow\">DESlib: A Dynamic ensemble selection library in Python.</a> arXiv preprint arXiv:1802.04967 (2018).</p>\n<pre>@article{cruz_deslib:2018,\n    title = {{DESlib}: {A} {Dynamic} ensemble selection library in {Python}},\n    journal = {arXiv preprint arXiv:1802.04967},\n    author = {Cruz, Rafael M. O. and Hafemann, Luiz G. and Sabourin, Robert and Cavalcanti, George D. C.},\n    year = {2018}\n}\n</pre>\n</div>\n<div id=\"references\">\n<h3>References:</h3>\n<table id=\"id30\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id1\" rel=\"nofollow\">[1]</a></td><td>: R. M. O. Cruz, R. Sabourin, and G. D. Cavalcanti, \u201cDynamic classifier selection: Recent advances and perspectives,\u201d Information Fusion, vol. 41, pp. 195 \u2013 216, 2018.</td></tr>\n</tbody>\n</table>\n<table id=\"id31\">\n<col><col>\n<tbody>\n<tr><td>[2]</td><td><em>(<a href=\"#id25\" rel=\"nofollow\">1</a>, <a href=\"#id26\" rel=\"nofollow\">2</a>)</em> : A. S. Britto, R. Sabourin, L. E. S. de Oliveira, Dynamic selection of classifiers - A comprehensive review, Pattern Recognition 47 (11) (2014) 3665\u20133680.</td></tr>\n</tbody>\n</table>\n<table id=\"id32\">\n<col><col>\n<tbody>\n<tr><td>[3]</td><td><em>(<a href=\"#id5\" rel=\"nofollow\">1</a>, <a href=\"#id6\" rel=\"nofollow\">2</a>)</em> : A. H. R. Ko, R. Sabourin, u. S. Britto, Jr., From dynamic classifier selection to dynamic ensemble selection, Pattern Recognition 41 (2008) 1735\u20131748.</td></tr>\n</tbody>\n</table>\n<table id=\"id33\">\n<col><col>\n<tbody>\n<tr><td>[4]</td><td><em>(<a href=\"#id18\" rel=\"nofollow\">1</a>, <a href=\"#id19\" rel=\"nofollow\">2</a>)</em> : K. Woods, W. P. Kegelmeyer, Jr., K. Bowyer, Combination of multiple classifiers using local accuracy estimates, IEEE Transactions on Pattern Analysis Machine Intelligence 19 (1997) 405\u2013410.</td></tr>\n</tbody>\n</table>\n<table id=\"id34\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id21\" rel=\"nofollow\">[5]</a></td><td>: G. Giacinto, F. Roli, Dynamic classifier selection based on multiple classifier behaviour, Pattern Recognition 34 (2001) 1879\u20131881.</td></tr>\n</tbody>\n</table>\n<table id=\"id35\">\n<col><col>\n<tbody>\n<tr><td>[6]</td><td><em>(<a href=\"#id22\" rel=\"nofollow\">1</a>, <a href=\"#id23\" rel=\"nofollow\">2</a>)</em> : L. Didaci, G. Giacinto, F. Roli, G. L. Marcialis, A study on the performances of dynamic classifier selection based on local accuracy estimation, Pattern Recognition 38 (11) (2005) 2188\u20132191.</td></tr>\n</tbody>\n</table>\n<table id=\"id36\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id2\" rel=\"nofollow\">[7]</a></td><td>: R. M. O. Cruz, R. Sabourin, G. D. C. Cavalcanti, T. I. Ren, META-DES: A dynamic ensemble selection framework using meta-learning, Pattern Recognition 48 (5) (2015) 1925\u20131935.</td></tr>\n</tbody>\n</table>\n<table id=\"id37\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id3\" rel=\"nofollow\">[8]</a></td><td>: Cruz, R.M., Sabourin, R. and Cavalcanti, G.D., 2015, July. META-DES. H: a dynamic ensemble selection technique using meta-learning and a dynamic weighting approach. In Neural Networks (IJCNN), 2015 International Joint Conference on (pp. 1-8)</td></tr>\n</tbody>\n</table>\n<table id=\"id38\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id8\" rel=\"nofollow\">[9]</a></td><td>: P. R. Cavalin, R. Sabourin, C. Y. Suen, Dynamic selection approaches for multiple classifier systems, Neural Computing and Applications 22 (3-4) (2013) 673\u2013688.</td></tr>\n</tbody>\n</table>\n<table id=\"id39\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id9\" rel=\"nofollow\">[10]</a></td><td>: T.Woloszynski, M. Kurzynski, A probabilistic model of classifier competence for dynamic ensemble selection, Pattern Recognition 44 (2011) 2656\u20132668.</td></tr>\n</tbody>\n</table>\n<table id=\"id40\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id12\" rel=\"nofollow\">[11]</a></td><td>: T.Woloszynski, M. Kurzynski, A measure of competence based on randomized reference classifier for dynamic ensemble selection, in: International Conference on Pattern Recognition (ICPR), 2010, pp. 4194\u20134197.</td></tr>\n</tbody>\n</table>\n<table id=\"id41\">\n<col><col>\n<tbody>\n<tr><td>[12]</td><td><em>(<a href=\"#id7\" rel=\"nofollow\">1</a>, <a href=\"#id10\" rel=\"nofollow\">2</a>)</em> : T. Woloszynski, M. Kurzynski, P. Podsiadlo, G. W. Stachowiak, A measure of competence based on random classification for dynamic ensemble selection, Information Fusion 13 (3) (2012) 207\u2013213.</td></tr>\n</tbody>\n</table>\n<table id=\"id42\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id28\" rel=\"nofollow\">[13]</a></td><td>: Oliveira, D.V.R., Cavalcanti, G.D.C. and Sabourin, R., Online Pruning of Base Classifiers for Dynamic Ensemble Selection, Pattern Recognition, vol. 72, December 2017, pp 44-58.</td></tr>\n</tbody>\n</table>\n<table id=\"id43\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id29\" rel=\"nofollow\">[14]</a></td><td>: Cruz RM, Zakane HH, Sabourin R, Cavalcanti GD. Dynamic Ensemble Selection VS K-NN: why and when Dynamic Selection obtains higher classification performance?.</td></tr>\n</tbody>\n</table>\n<table id=\"id44\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id4\" rel=\"nofollow\">[15]</a></td><td>: R. M. O. Cruz, R. Sabourin, G. D. C. Cavalcanti, META-DES.Oracle: Meta-learning and feature selection for dynamic ensemble selection, Information Fusion 38 (2017) 84\u2013103.Nov 30;38:84-103.</td></tr>\n</tbody>\n</table>\n<table id=\"id45\">\n<col><col>\n<tbody>\n<tr><td>[16]</td><td><em>(<a href=\"#id14\" rel=\"nofollow\">1</a>, <a href=\"#id15\" rel=\"nofollow\">2</a>)</em> : R. G. F. Soares, A. Santana, A. M. P. Canuto, M. C. P. de Souto, Using accuracy and diversity to select classifiers to build ensembles, Proceedings of the International Joint Conference on Neural Networks (2006) 1310\u20131316.</td></tr>\n</tbody>\n</table>\n<table id=\"id46\">\n<col><col>\n<tbody>\n<tr><td>[17]</td><td>: L. I. Kuncheva, Combining Pattern Classifiers: Methods and Algorithms, Wiley-Interscience, 2004.</td></tr>\n</tbody>\n</table>\n<table id=\"id47\">\n<col><col>\n<tbody>\n<tr><td>[18]</td><td>: Shipp, Catherine A., and Ludmila I. Kuncheva. \u201cRelationships between combination methods and measures of diversity in combining classifiers.\u201d Information fusion 3.2 (2002): 135-148.</td></tr>\n</tbody>\n</table>\n<table id=\"id48\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id17\" rel=\"nofollow\">[19]</a></td><td>: M. Sabourin, A. Mitiche, D. Thomas, G. Nagy, Classifier combination for handprinted digit recognition, International Conference on Document Analysis and Recognition (1993) 163\u2013166.</td></tr>\n</tbody>\n</table>\n<table id=\"id49\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id24\" rel=\"nofollow\">[20]</a></td><td>: L. I. Kuncheva, A theoretical study on six classifier fusion strategies, IEEE Transactions on Pattern Analysis and Machine Intelligence 24 (2) (2002) 281\u2013286.</td></tr>\n</tbody>\n</table>\n<table id=\"id50\">\n<col><col>\n<tbody>\n<tr><td>[21]</td><td><em>(<a href=\"#id11\" rel=\"nofollow\">1</a>, <a href=\"#id13\" rel=\"nofollow\">2</a>)</em> : B. Antosik, M. Kurzynski, New measures of classifier competence \u2013 heuristics and application to the design of multiple classifier systems., in: Computer recognition systems 4., 2011, pp. 197\u2013206.</td></tr>\n</tbody>\n</table>\n<table id=\"id51\">\n<col><col>\n<tbody>\n<tr><td>[22]</td><td>: Smith, Michael R., Tony Martinez, and Christophe Giraud-Carrier. \u201cAn instance level analysis of data complexity.\u201d Machine learning 95.2 (2014), pp 225-256.</td></tr>\n</tbody>\n</table>\n<table id=\"id52\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id20\" rel=\"nofollow\">[23]</a></td><td>: P. C. Smits, Multiple classifier systems for supervised remote sensing image classification based on dynamic classifier selection, IEEE Transactions on Geoscience and Remote Sensing 40 (4) (2002) 801\u2013813.</td></tr>\n</tbody>\n</table>\n<table id=\"id53\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id16\" rel=\"nofollow\">[24]</a></td><td>: Garc\u00eda, S., Zhang, Z.L., Altalhi, A., Alshomrani, S. and Herrera, F., \u201cDynamic ensemble selection for multi-class imbalanced datasets.\u201d Information Sciences 445 (2018): 22-37.</td></tr>\n</tbody>\n</table>\n<table id=\"id54\">\n<col><col>\n<tbody>\n<tr><td><a href=\"#id27\" rel=\"nofollow\">[25]</a></td><td>: Wolpert, David H. \u201cStacked generalization.\u201d Neural networks 5, no. 2 (1992): 241-259.</td></tr>\n</tbody>\n</table>\n</div>\n</div>\n\n          </div>"}, "last_serial": 4836035, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "728d9923683949e08cbdb1fc5be34e00", "sha256": "9c1c6763937cfe55775828cc8022642f774f450e3e4d215b6847e34b58b7963e"}, "downloads": -1, "filename": "DESlib-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "728d9923683949e08cbdb1fc5be34e00", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 103643, "upload_time": "2018-01-31T20:55:09", "upload_time_iso_8601": "2018-01-31T20:55:09.886163Z", "url": "https://files.pythonhosted.org/packages/f7/b6/309d2716b33ee687d49fa6c17c6396ba2d467ac265c8bbe3ed9af2405452/DESlib-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c2f097b720a06d5eef3ba56c42dc216b", "sha256": "69c4c73fa8395aebbc760c27717226f8cd12341c1aac7c1a71fc0d4c7e691cd7"}, "downloads": -1, "filename": "DESlib-0.1.2.tar.gz", "has_sig": false, "md5_digest": "c2f097b720a06d5eef3ba56c42dc216b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 94370, "upload_time": "2018-01-31T20:55:11", "upload_time_iso_8601": "2018-01-31T20:55:11.678021Z", "url": "https://files.pythonhosted.org/packages/32/00/18f9184117912f2df57ebab8c9f4613e362ff0edabf5e3b11f9dd9f25ec6/DESlib-0.1.2.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "078c698b0e3e3ac6eaa45954da6b20ed", "sha256": "6472f42be48b39207f476b4b0e37732acdad4cb1e1c0f4d6d84471efc76e3e7f"}, "downloads": -1, "filename": "DESlib-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "078c698b0e3e3ac6eaa45954da6b20ed", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 115653, "upload_time": "2018-04-24T20:24:51", "upload_time_iso_8601": "2018-04-24T20:24:51.774072Z", "url": "https://files.pythonhosted.org/packages/d3/cc/dbad116001c47a45e67ad506f79b80a0e40900b120e0de50843478034bd8/DESlib-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8af97a42fd4486b6a1cce5df8970c295", "sha256": "4e1817e3702bef1d5bb463579dfb52654ea3db226168c236481f2028339d11b2"}, "downloads": -1, "filename": "DESlib-0.2.1.tar.gz", "has_sig": false, "md5_digest": "8af97a42fd4486b6a1cce5df8970c295", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 121629, "upload_time": "2018-04-24T20:24:52", "upload_time_iso_8601": "2018-04-24T20:24:52.976035Z", "url": "https://files.pythonhosted.org/packages/e5/91/459a9bd2c32ac39ae1251f5d71b7c9e1455af32dd40dddf8953701986809/DESlib-0.2.1.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "8dbe02c1f1e2a5d5264e082ad2a72827", "sha256": "59a22796205ba2f0c6ee6190a8593bfae0b89aeb5c5c27d8383b52b20686598c"}, "downloads": -1, "filename": "DESlib-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "8dbe02c1f1e2a5d5264e082ad2a72827", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 152509, "upload_time": "2019-02-18T16:47:48", "upload_time_iso_8601": "2019-02-18T16:47:48.433835Z", "url": "https://files.pythonhosted.org/packages/4b/36/89c48ec8fbb01c41d9262453141c9aabdc243acc896e46070fedc973a08a/DESlib-0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6e385127af6241365fba40819966ffcc", "sha256": "f9f827b56a1070d9b4510b8fff20be25973ccf81136739822f22a26b57d87066"}, "downloads": -1, "filename": "DESlib-0.3.tar.gz", "has_sig": false, "md5_digest": "6e385127af6241365fba40819966ffcc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 158413, "upload_time": "2019-02-18T16:47:49", "upload_time_iso_8601": "2019-02-18T16:47:49.979146Z", "url": "https://files.pythonhosted.org/packages/bf/89/f2c9cc4086c45ee20a6b9850b2a1209fd45455701273c3d6538d04509d84/DESlib-0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8dbe02c1f1e2a5d5264e082ad2a72827", "sha256": "59a22796205ba2f0c6ee6190a8593bfae0b89aeb5c5c27d8383b52b20686598c"}, "downloads": -1, "filename": "DESlib-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "8dbe02c1f1e2a5d5264e082ad2a72827", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 152509, "upload_time": "2019-02-18T16:47:48", "upload_time_iso_8601": "2019-02-18T16:47:48.433835Z", "url": "https://files.pythonhosted.org/packages/4b/36/89c48ec8fbb01c41d9262453141c9aabdc243acc896e46070fedc973a08a/DESlib-0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6e385127af6241365fba40819966ffcc", "sha256": "f9f827b56a1070d9b4510b8fff20be25973ccf81136739822f22a26b57d87066"}, "downloads": -1, "filename": "DESlib-0.3.tar.gz", "has_sig": false, "md5_digest": "6e385127af6241365fba40819966ffcc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 158413, "upload_time": "2019-02-18T16:47:49", "upload_time_iso_8601": "2019-02-18T16:47:49.979146Z", "url": "https://files.pythonhosted.org/packages/bf/89/f2c9cc4086c45ee20a6b9850b2a1209fd45455701273c3d6538d04509d84/DESlib-0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:38:57 2020"}