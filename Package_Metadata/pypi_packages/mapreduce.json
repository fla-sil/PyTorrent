{"info": {"author": "Dustin Oprea", "author_email": "myselfasunder@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "**This project is still under active development, though largely finished. It is currently being tested in a production environment. The documentation is being incrementally completed.**\n\n\n--------\nOverview\n--------\n\n*JobX* is a Python-based MapReduce solution. The JobX project is entirely written in Python, as are the queue and KV clients. However, the actual distributed queue (*NSQ*) and distributed KV (*etcd*) are written in Go.\n\nMany of the configuration options have reasonable defaults so as to be as simple as possible to experiment with. All you need is a local instance of *NSQ* and *etcd*, which are, themselves, almost trivial to setup.\n\n\n---------------\nInstalling JobX\n---------------\n\n\nDependencies\n============\n\n- *nginx*\n- *Python 2.7 (gevent is not Python 3 compatible)*\n- *go*::\n\n    $ sudo apt-get install golang\n\n- *etcd*::\n  \n    $ git clone git@github.com:coreos/etcd.git\n    $ cd etcd\n    $ ./build\n\n    $ sudo mkdir /var/lib/etcd\n    $ sudo bin/etcd -addr=127.0.0.1:4001 -data-dir=/var/lib/etcd -name=etcd1\n\n- *nsq*::\n\n    $ sudo apt-get install gpm\n    $ mkdir ~/.go\n    $ GOPATH=~/.go go get github.com/bitly/nsq/...\n\n    $ sudo mkdir /var/lib/nsq\n    $ cd /var/lib/nsq\n    $ sudo ~/.go/bin/nsqd\n\n- Install Nginx.\n\n\nConfiguration\n=============\n\n1. Configure Nginx::\n\n    upstream mapreduce {\n        server unix:/tmp/mr.gunicorn.sock fail_timeout=0;\n    }\n\n    server {\n            listen 80;\n\n            server_name job1.domain;\n            keepalive_timeout 5;\n\n            access_log /tmp/nginx-mr-access.log;\n            error_log  /tmp/nginx-mr-error.log;\n\n            location /s {\n                root /usr/local/lib/python2.7/dist-packages/mr/resources/static;\n                try_files $uri $uri/ =404;\n            }\n\n            location / {\n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                proxy_set_header Host $http_host;\n                proxy_redirect off;\n\n                proxy_pass http://mapreduce;\n            }\n    }\n\n2. Set the following two environment variables (either in your profile, or /etc/environment)::\n\n    export MR_ETCD_HOST=127.0.0.1\n    export MR_NSQD_HOSTS=127.0.0.1\n    export MR_WORKFLOW_NAMES=test_workflow\n\n2. Create workflow::\n\n    mr_kv_workflow_create test_workflow \"Jobs that assist build and deployment.\"\n\n    Even though *test_workflow* is actually the default workflow name in the system, we explicitly specify it in the variables above so that it's clear how to for when you'll need to with your own workflow.\n\n3. Load handlers::\n\n   ..write and load handlers\n\n4. Load steps::\n\n   ..create step(s)\n\n5. Load jobs::\n\n   ..create job\n\n6. Start::\n\n    mr_start_gunicorn_dev \n\n\n------------------\nHandler Management\n------------------\n\nIn order to both alleviate the annoyance of having to maintain current copies of the sourcecode for handlers on every job worker, we store the source-code to the KV. It is syntax-checked when loaded, the metadata header is parsed, the code is compiled, and the compiled object is committed to the \"library\". There is a sync script that can be sure to push updated handler code, ignored unchanged handlers, and remove handlers for which no file is found and no steps refer.\n\nThe job workers will check the KV for updates approximately every ten-seconds, and merge them.\n\n\nShared Filesystem\n=================\n\nThere is general support for a distributed, shared filesystem between the handlers of the same workflow. The filesystem is for general use, such as prepopulating it with assets that are required for your operations, using it as a workspace for temporary files, as well as using it to deposit final artifacts, to be picked-up upon completion of the job. Any supported filesystem will share a common interface. Currently, there is only support for Tahoe LAFS.\n\nYou must define environment variables with the required parameters to enable this functionality.\n\n\nSessions\n========\n\nWhen it comes to flow, mappers receive the data (key-value pairs), first. If this data represents actual arguments, then your logic might determine what comes next dynamically. Your mapper may branch to downstream mappers in order to collect data that you require to perform your primary task, and your reducer may then act on it. However, the reducer may need access to some of the same data that your mapper had. Unfortunately, where the mapper receives data that it is free to slice and reorganize, the reducer only receives a collection of results from mappers that yielded data. Unless the mappers forwarded data down to the eventual result (potentially being of no actual use intermediary mapper), the reducer may need some of that original information to complete its task.\n\nThis is what *sessions* are for. Every mapper invocation is given a private, durable namespace in which to stash data that only the corresponding reducer will have access to. This data will be destroyed at the completion of the request like all of the other request-specific entities.\n\nThere are tools available to debug sessions, if needed.\n\n\nScope Factories\n===============\n\n*Scope factories* are a mechanism that allow you to inject variables into the global scope of each handler. A different scope factory can be defined for each workflow. Though you can inject the same variables into the scope of every handler [in the same workflow], the scope-factory will also receive the name of the handler. This allows you to provide sensitive information to some, but not all, handlers.\n\nYou must define environment variables with the required parameters to enable this functionality.\n\n\nCapabilities\n============\n\n*Capabilities* are classifications that you may define to control how jobs are assigned to workers. Every worker declares a list of offered capabality classifications, and every handler declares a \"required capability\" classification. You may use this functionality to only route operations with handlers that invoke licensed functionality to only those workers that have been adequately equipped.\n\n\nLanguage Processors\n===================\n\nHandlers can be defined in any language, as long as there's a processor defined for it that can dispatch the code to be executed, and can yield the data that is returned (all handlers are generators).\n\n\nResult Writers\n==============\n\nA *result-writer* manages how results are transmitted, and will influence what you receive in the HTTP response.\n\nCurrently, there are two:\n\n- *inline*: Return the data within the response. **This is the default.**\n- *file*: Store to a local directory.\n\n\nNotifications\n=============\n\n\nEmail Notifications\n-------------------\n\n\nHTTP Notifications\n------------------\n\n- The tool for listening to HTTP notifications.\n\n\nUncaught Exceptions\n-------------------\n\n\nHandler Examples\n================\n\n\n---------------------------\nDistributed Queue Semantics\n---------------------------\n\nThe circulatory system of JobX is *bit.ly*'s NSQ platform, a very high-volume, and easy-to-deploy, distributed queue.\n\n\n------------------------\nDistributed KV Semantics\n------------------------\n\nAll persistence is done into the *etcd* distributed, immediately-consistent KV. *etcd* is a component of *CoreOS*, and is also very easy to deploy. *etcd* stores key-value pairs, but the three-things that makes it unique are:\n\n1. It's immediately consistent across all instances.\n2. Key-value pairs can be stored heirarchically.\n3. You can long-poll on nodes while waiting for them to change, rather than polling them.\n\nAll data is manipulated as entities, which are modeled heirarchically on to the KV in functionality that was written specifically for this project. The models of this project resemble traditional RDBMS models found in Django and SQLAlchemy (to within reason, while being pragmatic and maintaining efficiency).\n\n\nEntity Types\n============\n\n- *workflow*: This is the container of all of the other entities. You may have concurrent workflows operating on the same cluster that have their own jobs, steps, and handlers defined. They are *completely* isolated at the queueing and storage levels.\n- *job*: This defines the noun that you post requests to, and the initial step.\n- *step*: This binds a mapper to a combiner (optional), and a combiner to a reducer.\n- *handler*: This defines a single body of code for a mapper, combiner, or reducer.\n- *request*: This identifies one received request, and the invocation of the first step.\n- *invocation*: This is the basic unit of operation. Every time a mapper or reducer is queued, it is given its own invocation record.\n\n\nScripts\n=======\n\n\nDirectly Reading KV Entities\n============================\n\nWhere we want to read the \"request\" entity with the given ID under the \"build\" workflow::\n\n    $ etcdctl get entities/request/build/c1ef1a0d645e9a01fae9de1b7eca412fb14372c3 | python -m json.tool \n    {\n        \"context\": {\n            \"requester_ip\": \"127.0.0.1\"\n        },\n        \"is_done\": true,\n        \"is_blocking\": true,\n        \"failed_invocation_id\": null,\n        \"invocation_id\": \"3c7494eb9f521d39e8609733a6d3988100540abb\",\n        \"job_name\": \"obfuscate_for_clients\",\n        \"workflow_name\": \"build\"\n    }\n\nNOTE: *etcd* allows for modifying KV nodes via REST, as well. However, this is \nnot recommended. Use the scripts to mitigate any potential mistakes, and to \ninvoke any validation.\n\n\n-------\nTracing\n-------\n\nWhen one of your handlers eventually starts chronically raising an exception, it'll be critical to be able to investigate it. The following tools are available, and take advantage of a common *tracing* functionality.\n\nNote that in order to be able to do anything, you need to disable request-cleanup. Otherwise, every request will be immediately queued and destroyed after a result is achieved.\n\n\nGenerating a Physical Graph Using Graphviz\n==========================================\n\n\nGenerating an Invocation Tree from the Command-Line\n===================================================\n\n\nDumping the Invocation Tree and Data from the Command-Line\n==========================================================\n\n\n--------\nAdvanced\n--------\n\n\nKV Queue Collections\n====================\n\n\nKV Tree Collections\n===================", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/dsoprea/JobX", "keywords": "mapreduce jobx jobs workers", "license": "GPL 2", "maintainer": null, "maintainer_email": null, "name": "mapreduce", "package_url": "https://pypi.org/project/mapreduce/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/mapreduce/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/dsoprea/JobX"}, "release_url": "https://pypi.org/project/mapreduce/0.3.5/", "requires_dist": null, "requires_python": null, "summary": "A Python-based, distributed MapReduce solution.", "version": "0.3.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><strong>This project is still under active development, though largely finished. It is currently being tested in a production environment. The documentation is being incrementally completed.</strong></p>\n<div id=\"overview\">\n<h2>Overview</h2>\n<p><em>JobX</em> is a Python-based MapReduce solution. The JobX project is entirely written in Python, as are the queue and KV clients. However, the actual distributed queue (<em>NSQ</em>) and distributed KV (<em>etcd</em>) are written in Go.</p>\n<p>Many of the configuration options have reasonable defaults so as to be as simple as possible to experiment with. All you need is a local instance of <em>NSQ</em> and <em>etcd</em>, which are, themselves, almost trivial to setup.</p>\n</div>\n<div id=\"installing-jobx\">\n<h2>Installing JobX</h2>\n<div id=\"dependencies\">\n<h3>Dependencies</h3>\n<ul>\n<li><p><em>nginx</em></p>\n</li>\n<li><p><em>Python 2.7 (gevent is not Python 3 compatible)</em></p>\n</li>\n<li><p><em>go</em>:</p>\n<pre>$ sudo apt-get install golang\n</pre>\n</li>\n<li><p><em>etcd</em>:</p>\n<pre>$ git clone git@github.com:coreos/etcd.git\n$ cd etcd\n$ ./build\n\n$ sudo mkdir /var/lib/etcd\n$ sudo bin/etcd -addr=127.0.0.1:4001 -data-dir=/var/lib/etcd -name=etcd1\n</pre>\n</li>\n<li><p><em>nsq</em>:</p>\n<pre>$ sudo apt-get install gpm\n$ mkdir ~/.go\n$ GOPATH=~/.go go get github.com/bitly/nsq/...\n\n$ sudo mkdir /var/lib/nsq\n$ cd /var/lib/nsq\n$ sudo ~/.go/bin/nsqd\n</pre>\n</li>\n<li><p>Install Nginx.</p>\n</li>\n</ul>\n</div>\n<div id=\"configuration\">\n<h3>Configuration</h3>\n<ol>\n<li><p>Configure Nginx:</p>\n<pre>upstream mapreduce {\n    server unix:/tmp/mr.gunicorn.sock fail_timeout=0;\n}\n\nserver {\n        listen 80;\n\n        server_name job1.domain;\n        keepalive_timeout 5;\n\n        access_log /tmp/nginx-mr-access.log;\n        error_log  /tmp/nginx-mr-error.log;\n\n        location /s {\n            root /usr/local/lib/python2.7/dist-packages/mr/resources/static;\n            try_files $uri $uri/ =404;\n        }\n\n        location / {\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header Host $http_host;\n            proxy_redirect off;\n\n            proxy_pass http://mapreduce;\n        }\n}\n</pre>\n</li>\n<li><p>Set the following two environment variables (either in your profile, or /etc/environment):</p>\n<pre>export MR_ETCD_HOST=127.0.0.1\nexport MR_NSQD_HOSTS=127.0.0.1\nexport MR_WORKFLOW_NAMES=test_workflow\n</pre>\n</li>\n</ol>\n<ol>\n<li><p>Create workflow:</p>\n<pre>mr_kv_workflow_create test_workflow \"Jobs that assist build and deployment.\"\n\nEven though *test_workflow* is actually the default workflow name in the system, we explicitly specify it in the variables above so that it's clear how to for when you'll need to with your own workflow.\n</pre>\n</li>\n<li><p>Load handlers:</p>\n<pre>..write and load handlers\n</pre>\n</li>\n<li><p>Load steps:</p>\n<pre>..create step(s)\n</pre>\n</li>\n<li><p>Load jobs:</p>\n<pre>..create job\n</pre>\n</li>\n<li><p>Start:</p>\n<pre>mr_start_gunicorn_dev\n</pre>\n</li>\n</ol>\n</div>\n</div>\n<div id=\"handler-management\">\n<h2>Handler Management</h2>\n<p>In order to both alleviate the annoyance of having to maintain current copies of the sourcecode for handlers on every job worker, we store the source-code to the KV. It is syntax-checked when loaded, the metadata header is parsed, the code is compiled, and the compiled object is committed to the \u201clibrary\u201d. There is a sync script that can be sure to push updated handler code, ignored unchanged handlers, and remove handlers for which no file is found and no steps refer.</p>\n<p>The job workers will check the KV for updates approximately every ten-seconds, and merge them.</p>\n<div id=\"shared-filesystem\">\n<h3>Shared Filesystem</h3>\n<p>There is general support for a distributed, shared filesystem between the handlers of the same workflow. The filesystem is for general use, such as prepopulating it with assets that are required for your operations, using it as a workspace for temporary files, as well as using it to deposit final artifacts, to be picked-up upon completion of the job. Any supported filesystem will share a common interface. Currently, there is only support for Tahoe LAFS.</p>\n<p>You must define environment variables with the required parameters to enable this functionality.</p>\n</div>\n<div id=\"sessions\">\n<h3>Sessions</h3>\n<p>When it comes to flow, mappers receive the data (key-value pairs), first. If this data represents actual arguments, then your logic might determine what comes next dynamically. Your mapper may branch to downstream mappers in order to collect data that you require to perform your primary task, and your reducer may then act on it. However, the reducer may need access to some of the same data that your mapper had. Unfortunately, where the mapper receives data that it is free to slice and reorganize, the reducer only receives a collection of results from mappers that yielded data. Unless the mappers forwarded data down to the eventual result (potentially being of no actual use intermediary mapper), the reducer may need some of that original information to complete its task.</p>\n<p>This is what <em>sessions</em> are for. Every mapper invocation is given a private, durable namespace in which to stash data that only the corresponding reducer will have access to. This data will be destroyed at the completion of the request like all of the other request-specific entities.</p>\n<p>There are tools available to debug sessions, if needed.</p>\n</div>\n<div id=\"scope-factories\">\n<h3>Scope Factories</h3>\n<p><em>Scope factories</em> are a mechanism that allow you to inject variables into the global scope of each handler. A different scope factory can be defined for each workflow. Though you can inject the same variables into the scope of every handler [in the same workflow], the scope-factory will also receive the name of the handler. This allows you to provide sensitive information to some, but not all, handlers.</p>\n<p>You must define environment variables with the required parameters to enable this functionality.</p>\n</div>\n<div id=\"capabilities\">\n<h3>Capabilities</h3>\n<p><em>Capabilities</em> are classifications that you may define to control how jobs are assigned to workers. Every worker declares a list of offered capabality classifications, and every handler declares a \u201crequired capability\u201d classification. You may use this functionality to only route operations with handlers that invoke licensed functionality to only those workers that have been adequately equipped.</p>\n</div>\n<div id=\"language-processors\">\n<h3>Language Processors</h3>\n<p>Handlers can be defined in any language, as long as there\u2019s a processor defined for it that can dispatch the code to be executed, and can yield the data that is returned (all handlers are generators).</p>\n</div>\n<div id=\"result-writers\">\n<h3>Result Writers</h3>\n<p>A <em>result-writer</em> manages how results are transmitted, and will influence what you receive in the HTTP response.</p>\n<p>Currently, there are two:</p>\n<ul>\n<li><em>inline</em>: Return the data within the response. <strong>This is the default.</strong></li>\n<li><em>file</em>: Store to a local directory.</li>\n</ul>\n</div>\n<div id=\"notifications\">\n<h3>Notifications</h3>\n<div id=\"email-notifications\">\n<h4>Email Notifications</h4>\n</div>\n<div id=\"http-notifications\">\n<h4>HTTP Notifications</h4>\n<ul>\n<li>The tool for listening to HTTP notifications.</li>\n</ul>\n</div>\n<div id=\"uncaught-exceptions\">\n<h4>Uncaught Exceptions</h4>\n</div>\n</div>\n<div id=\"handler-examples\">\n<h3>Handler Examples</h3>\n</div>\n</div>\n<div id=\"distributed-queue-semantics\">\n<h2>Distributed Queue Semantics</h2>\n<p>The circulatory system of JobX is <em>bit.ly</em>\u2019s NSQ platform, a very high-volume, and easy-to-deploy, distributed queue.</p>\n</div>\n<div id=\"distributed-kv-semantics\">\n<h2>Distributed KV Semantics</h2>\n<p>All persistence is done into the <em>etcd</em> distributed, immediately-consistent KV. <em>etcd</em> is a component of <em>CoreOS</em>, and is also very easy to deploy. <em>etcd</em> stores key-value pairs, but the three-things that makes it unique are:</p>\n<ol>\n<li>It\u2019s immediately consistent across all instances.</li>\n<li>Key-value pairs can be stored heirarchically.</li>\n<li>You can long-poll on nodes while waiting for them to change, rather than polling them.</li>\n</ol>\n<p>All data is manipulated as entities, which are modeled heirarchically on to the KV in functionality that was written specifically for this project. The models of this project resemble traditional RDBMS models found in Django and SQLAlchemy (to within reason, while being pragmatic and maintaining efficiency).</p>\n<div id=\"entity-types\">\n<h3>Entity Types</h3>\n<ul>\n<li><em>workflow</em>: This is the container of all of the other entities. You may have concurrent workflows operating on the same cluster that have their own jobs, steps, and handlers defined. They are <em>completely</em> isolated at the queueing and storage levels.</li>\n<li><em>job</em>: This defines the noun that you post requests to, and the initial step.</li>\n<li><em>step</em>: This binds a mapper to a combiner (optional), and a combiner to a reducer.</li>\n<li><em>handler</em>: This defines a single body of code for a mapper, combiner, or reducer.</li>\n<li><em>request</em>: This identifies one received request, and the invocation of the first step.</li>\n<li><em>invocation</em>: This is the basic unit of operation. Every time a mapper or reducer is queued, it is given its own invocation record.</li>\n</ul>\n</div>\n<div id=\"scripts\">\n<h3>Scripts</h3>\n</div>\n<div id=\"directly-reading-kv-entities\">\n<h3>Directly Reading KV Entities</h3>\n<p>Where we want to read the \u201crequest\u201d entity with the given ID under the \u201cbuild\u201d workflow:</p>\n<pre>$ etcdctl get entities/request/build/c1ef1a0d645e9a01fae9de1b7eca412fb14372c3 | python -m json.tool\n{\n    \"context\": {\n        \"requester_ip\": \"127.0.0.1\"\n    },\n    \"is_done\": true,\n    \"is_blocking\": true,\n    \"failed_invocation_id\": null,\n    \"invocation_id\": \"3c7494eb9f521d39e8609733a6d3988100540abb\",\n    \"job_name\": \"obfuscate_for_clients\",\n    \"workflow_name\": \"build\"\n}\n</pre>\n<p>NOTE: <em>etcd</em> allows for modifying KV nodes via REST, as well. However, this is\nnot recommended. Use the scripts to mitigate any potential mistakes, and to\ninvoke any validation.</p>\n</div>\n</div>\n<div id=\"tracing\">\n<h2>Tracing</h2>\n<p>When one of your handlers eventually starts chronically raising an exception, it\u2019ll be critical to be able to investigate it. The following tools are available, and take advantage of a common <em>tracing</em> functionality.</p>\n<p>Note that in order to be able to do anything, you need to disable request-cleanup. Otherwise, every request will be immediately queued and destroyed after a result is achieved.</p>\n<div id=\"generating-a-physical-graph-using-graphviz\">\n<h3>Generating a Physical Graph Using Graphviz</h3>\n</div>\n<div id=\"generating-an-invocation-tree-from-the-command-line\">\n<h3>Generating an Invocation Tree from the Command-Line</h3>\n</div>\n<div id=\"dumping-the-invocation-tree-and-data-from-the-command-line\">\n<h3>Dumping the Invocation Tree and Data from the Command-Line</h3>\n</div>\n</div>\n<div id=\"advanced\">\n<h2>Advanced</h2>\n<div id=\"kv-queue-collections\">\n<h3>KV Queue Collections</h3>\n</div>\n<div id=\"kv-tree-collections\">\n<h3>KV Tree Collections</h3>\n</div>\n</div>\n\n          </div>"}, "last_serial": 1586879, "releases": {"0.2.0": [], "0.2.3": [{"comment_text": "", "digests": {"md5": "158452a140b618c8cfbf4dde53fc4d4f", "sha256": "560a681d8c98b7ed0c7524dcc4c1498ef76067f37036a0dd22685ff4d5f5ce37"}, "downloads": -1, "filename": "mapreduce-0.2.3-py2-none-any.whl", "has_sig": false, "md5_digest": "158452a140b618c8cfbf4dde53fc4d4f", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 114615, "upload_time": "2014-09-25T22:31:14", "upload_time_iso_8601": "2014-09-25T22:31:14.624793Z", "url": "https://files.pythonhosted.org/packages/4c/3e/648f5f4b41901de3f868c361981447f0228e3a6397e2ea42fc60b331b700/mapreduce-0.2.3-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "52b0138a8268cdfd1414d2c328220b07", "sha256": "b16a3eefc038a2a047a98f49a12bd286c981a50ba9e72ab56cc6e84d98a8f9fd"}, "downloads": -1, "filename": "mapreduce-0.2.3.tar.gz", "has_sig": false, "md5_digest": "52b0138a8268cdfd1414d2c328220b07", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 57226, "upload_time": "2014-09-25T22:31:10", "upload_time_iso_8601": "2014-09-25T22:31:10.473124Z", "url": "https://files.pythonhosted.org/packages/d6/f0/38775640ae683957546ab8c5ed3f8161a364b024e95b026d702295cdb6c9/mapreduce-0.2.3.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "254fbae66ffdf41bd32edec285a5403a", "sha256": "42a56e779472f63a3958b3847a4edefa1dd77387bbfb23168c81decaa4be9431"}, "downloads": -1, "filename": "mapreduce-0.2.4-py2-none-any.whl", "has_sig": false, "md5_digest": "254fbae66ffdf41bd32edec285a5403a", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 114626, "upload_time": "2014-09-26T00:28:41", "upload_time_iso_8601": "2014-09-26T00:28:41.439091Z", "url": "https://files.pythonhosted.org/packages/be/cb/96412f48e697ea97f3654d4bccf0a758a3ba8a7a197911c70b09366a5a04/mapreduce-0.2.4-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "deb63be10ec9b080bb1771e8df6077da", "sha256": "62031c6e5fa5cb4a8a5cc49c534cb192ad8463deef71d0f0c71180255e66cd5e"}, "downloads": -1, "filename": "mapreduce-0.2.4.tar.gz", "has_sig": false, "md5_digest": "deb63be10ec9b080bb1771e8df6077da", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 57225, "upload_time": "2014-09-26T00:28:38", "upload_time_iso_8601": "2014-09-26T00:28:38.833857Z", "url": "https://files.pythonhosted.org/packages/93/5e/bf3cf38cdfc3a0c43dd9c729894977912d9947542b2f32b4b82f4aeda74d/mapreduce-0.2.4.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "05bfdadb1fb5acd7af9452ac88c84b3b", "sha256": "30b95fb21585f22be180761883cddade27b7ba0ba765d5a8b0987370f5e26349"}, "downloads": -1, "filename": "mapreduce-0.2.5-py2-none-any.whl", "has_sig": false, "md5_digest": "05bfdadb1fb5acd7af9452ac88c84b3b", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 114624, "upload_time": "2014-09-26T00:41:57", "upload_time_iso_8601": "2014-09-26T00:41:57.610750Z", "url": "https://files.pythonhosted.org/packages/2c/77/28eead7e05abebf675bd7914f14335bcafcb0f2d22595ffd866811118ff4/mapreduce-0.2.5-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a76f3e3dcd2586e91031308c2324ba7f", "sha256": "7587ebfdecee0fe0f94ea193cdc3880d41fb5ea63f06ddd6d4497767c2ce3afb"}, "downloads": -1, "filename": "mapreduce-0.2.5.tar.gz", "has_sig": false, "md5_digest": "a76f3e3dcd2586e91031308c2324ba7f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 57243, "upload_time": "2014-09-26T00:41:55", "upload_time_iso_8601": "2014-09-26T00:41:55.378345Z", "url": "https://files.pythonhosted.org/packages/b6/98/c28ad450ec0045dc16ada900b4ddff44fc9ed89fa93cb97029306cd0d7a6/mapreduce-0.2.5.tar.gz", "yanked": false}], "0.2.6": [{"comment_text": "", "digests": {"md5": "ab26fd3099fdfc2d15a85fa5f08d906e", "sha256": "45ae69e0357fc869ed73e78fe130047e9acd4208138616286f22542bbd3d1b28"}, "downloads": -1, "filename": "mapreduce-0.2.6-py2-none-any.whl", "has_sig": false, "md5_digest": "ab26fd3099fdfc2d15a85fa5f08d906e", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 115980, "upload_time": "2014-09-26T08:15:35", "upload_time_iso_8601": "2014-09-26T08:15:35.302526Z", "url": "https://files.pythonhosted.org/packages/76/c7/532593451be0481eecbca8aaa4e50a0168e9ed832e7c95c3442b54127c46/mapreduce-0.2.6-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f2965fed259b78ab7d51e04c5e4fc95c", "sha256": "43d40840a6cde087f1f37592c73775b90784f9702f0938863803d850fb32c4ea"}, "downloads": -1, "filename": "mapreduce-0.2.6.tar.gz", "has_sig": false, "md5_digest": "f2965fed259b78ab7d51e04c5e4fc95c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 57160, "upload_time": "2014-09-26T08:15:32", "upload_time_iso_8601": "2014-09-26T08:15:32.078745Z", "url": "https://files.pythonhosted.org/packages/88/62/55474269c49c79007f68548a54a1e637ecc50267e39b149d15052842b752/mapreduce-0.2.6.tar.gz", "yanked": false}], "0.2.7": [{"comment_text": "", "digests": {"md5": "86d94f84624fa883ec3cc11f2fc42950", "sha256": "afe997097c260bcd7b7780688a5ce61594919eafbda7d73c6515efc64d8a79f2"}, "downloads": -1, "filename": "mapreduce-0.2.7-py2-none-any.whl", "has_sig": false, "md5_digest": "86d94f84624fa883ec3cc11f2fc42950", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 126248, "upload_time": "2014-09-29T14:46:36", "upload_time_iso_8601": "2014-09-29T14:46:36.428857Z", "url": "https://files.pythonhosted.org/packages/d7/20/e858202b468b5fc03c796fea0cfe027273fa6123798d2d21ed99ef59adae/mapreduce-0.2.7-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fa40fa57c2606ecdcb2d7029f366411c", "sha256": "b79141447cee097804a46442b0ae909189b57972068b27a7137e853608ea0ada"}, "downloads": -1, "filename": "mapreduce-0.2.7.tar.gz", "has_sig": false, "md5_digest": "fa40fa57c2606ecdcb2d7029f366411c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 67243, "upload_time": "2014-09-29T14:46:33", "upload_time_iso_8601": "2014-09-29T14:46:33.562280Z", "url": "https://files.pythonhosted.org/packages/8e/d5/de5f5d455deba1a17eba98ca7b2860f96c5b11a17c4bc2bc95af73152639/mapreduce-0.2.7.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "8f9a226f89dec5799d0848ac1e71e418", "sha256": "eeb976a77c8e561c0984e0bf39c72e69269144f76252303cde7089d4e02424f4"}, "downloads": -1, "filename": "mapreduce-0.3.0-py2-none-any.whl", "has_sig": false, "md5_digest": "8f9a226f89dec5799d0848ac1e71e418", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 133682, "upload_time": "2014-10-20T06:30:24", "upload_time_iso_8601": "2014-10-20T06:30:24.116538Z", "url": "https://files.pythonhosted.org/packages/7d/da/b7774a71c067f710755214afa182fd40ce3b604d15ac6f52945691447374/mapreduce-0.3.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2f56052eec2b8a1fdbb7862508357cc5", "sha256": "8c72176c42b159190f1adb37ce81e44efddd04fefc6fa45f8b956cf19e64c515"}, "downloads": -1, "filename": "mapreduce-0.3.0.tar.gz", "has_sig": false, "md5_digest": "2f56052eec2b8a1fdbb7862508357cc5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 70740, "upload_time": "2014-10-20T06:30:20", "upload_time_iso_8601": "2014-10-20T06:30:20.581831Z", "url": "https://files.pythonhosted.org/packages/8f/39/77ac9593f2540cc267c2fa1f9e9d501e410716982244dfc71c26a65eabaf/mapreduce-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "372ed770b16f5a17b4280ec62e2b79e6", "sha256": "59bd4d4dad29988a60465f9dd33d84c94c5457a308b3a6cfcdd63ebb60bd51e6"}, "downloads": -1, "filename": "mapreduce-0.3.1-py2-none-any.whl", "has_sig": false, "md5_digest": "372ed770b16f5a17b4280ec62e2b79e6", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 135497, "upload_time": "2014-10-20T08:01:25", "upload_time_iso_8601": "2014-10-20T08:01:25.364851Z", "url": "https://files.pythonhosted.org/packages/b2/07/166e1775a37a985f0c761539c21f8ca8ad5ed31e0d96008ceed056d7733a/mapreduce-0.3.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "71386aaa61996842b145800569ae3e3a", "sha256": "37e85dc0948c4cd7ac9ef1b727f95e443350c7c3bc1c6519e610ca58c33ed772"}, "downloads": -1, "filename": "mapreduce-0.3.1.tar.gz", "has_sig": false, "md5_digest": "71386aaa61996842b145800569ae3e3a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 71482, "upload_time": "2014-10-20T08:01:22", "upload_time_iso_8601": "2014-10-20T08:01:22.297978Z", "url": "https://files.pythonhosted.org/packages/cb/b7/a3895ef89250b0bbe2060c988e6c1c7c6f8456121b5eb823d91b15232b75/mapreduce-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "5833ad54a0f5771bc0ba22a5dedfab31", "sha256": "aa542c8737927a672b1e491d167e8642bf90e015e67480db6dad17645c1e8054"}, "downloads": -1, "filename": "mapreduce-0.3.2-py2-none-any.whl", "has_sig": false, "md5_digest": "5833ad54a0f5771bc0ba22a5dedfab31", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 134423, "upload_time": "2014-10-20T18:19:42", "upload_time_iso_8601": "2014-10-20T18:19:42.129447Z", "url": "https://files.pythonhosted.org/packages/ee/65/46a5772df2dced8c1a90a1e6fd71b2f75d3631d6e26979bbb74fc455e726/mapreduce-0.3.2-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "15fc1efa6cb711e022d177268f4dc9c2", "sha256": "657b6815fb43b7665fdee67fac137c90f6e50e6f3a426d6f8274fdc39f9dcff9"}, "downloads": -1, "filename": "mapreduce-0.3.2.tar.gz", "has_sig": false, "md5_digest": "15fc1efa6cb711e022d177268f4dc9c2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 71435, "upload_time": "2014-10-20T18:19:39", "upload_time_iso_8601": "2014-10-20T18:19:39.279009Z", "url": "https://files.pythonhosted.org/packages/c7/56/94ffd00d14aff4ee751f9b9da50cbe0b0b8dc3f17aae675d0078ed5510c6/mapreduce-0.3.2.tar.gz", "yanked": false}], "0.3.3": [{"comment_text": "", "digests": {"md5": "bafe31ffb9a6234677febd7254d35e8d", "sha256": "f428f05b28b29d5f10ddc6f6a8ea6d1148cb2d19690c92598b12f1b8a0478c93"}, "downloads": -1, "filename": "mapreduce-0.3.3-py2-none-any.whl", "has_sig": false, "md5_digest": "bafe31ffb9a6234677febd7254d35e8d", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 134165, "upload_time": "2014-10-21T17:45:24", "upload_time_iso_8601": "2014-10-21T17:45:24.681231Z", "url": "https://files.pythonhosted.org/packages/3a/42/bbf5d3cb08d3f085ac1239ab8b3e5442f1b86980d70f75b58d58b9fd1653/mapreduce-0.3.3-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "46df27c80362a2305ffee73a7603bc20", "sha256": "76779f4090b404c836a1005ec06843499080700e848571e718c78491b5e633e6"}, "downloads": -1, "filename": "mapreduce-0.3.3.tar.gz", "has_sig": false, "md5_digest": "46df27c80362a2305ffee73a7603bc20", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 70857, "upload_time": "2014-10-21T17:45:21", "upload_time_iso_8601": "2014-10-21T17:45:21.508764Z", "url": "https://files.pythonhosted.org/packages/25/70/df9c8cc2ed8131931e2982f5535f31f5ac240bf22a587326acda24c2517b/mapreduce-0.3.3.tar.gz", "yanked": false}], "0.3.4": [{"comment_text": "", "digests": {"md5": "cc90688511a707f3abb124183d632005", "sha256": "0fee26cb8f5bb2e541958b34f6837f42e7c69ceeae25704edd88689425465595"}, "downloads": -1, "filename": "mapreduce-0.3.4-py2-none-any.whl", "has_sig": false, "md5_digest": "cc90688511a707f3abb124183d632005", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 138694, "upload_time": "2014-10-23T06:29:54", "upload_time_iso_8601": "2014-10-23T06:29:54.612227Z", "url": "https://files.pythonhosted.org/packages/ab/36/ca082bcf310ff660bfd5e262553adcf4fc886147c795fbe5dd12e2909fd1/mapreduce-0.3.4-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bf4c39faefb40c18096cca16cbe4d680", "sha256": "facda660620f086ad550ec6d346d31505f2b9798e8eacb1f58d2f4a6c964213a"}, "downloads": -1, "filename": "mapreduce-0.3.4.tar.gz", "has_sig": false, "md5_digest": "bf4c39faefb40c18096cca16cbe4d680", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74160, "upload_time": "2014-10-23T06:29:52", "upload_time_iso_8601": "2014-10-23T06:29:52.004269Z", "url": "https://files.pythonhosted.org/packages/8b/29/7d74ec072a9627ce6d038b1f6368eca2547231c21d7a31ffc7ddf1b547e4/mapreduce-0.3.4.tar.gz", "yanked": false}], "0.3.5": [{"comment_text": "", "digests": {"md5": "f5240a63f5e7d0e6f3eb1007dbc748e0", "sha256": "98b46bf523c643429691d0ae760d366148cd8ab35e6c8ff026dd07af653a4934"}, "downloads": -1, "filename": "mapreduce-0.3.5.tar.gz", "has_sig": false, "md5_digest": "f5240a63f5e7d0e6f3eb1007dbc748e0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66660, "upload_time": "2015-06-10T17:13:26", "upload_time_iso_8601": "2015-06-10T17:13:26.485764Z", "url": "https://files.pythonhosted.org/packages/8b/b7/664c7e313a73cbea3a8aec4223ce46e5e2b3bdbe9e10e961641be3189d97/mapreduce-0.3.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f5240a63f5e7d0e6f3eb1007dbc748e0", "sha256": "98b46bf523c643429691d0ae760d366148cd8ab35e6c8ff026dd07af653a4934"}, "downloads": -1, "filename": "mapreduce-0.3.5.tar.gz", "has_sig": false, "md5_digest": "f5240a63f5e7d0e6f3eb1007dbc748e0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66660, "upload_time": "2015-06-10T17:13:26", "upload_time_iso_8601": "2015-06-10T17:13:26.485764Z", "url": "https://files.pythonhosted.org/packages/8b/b7/664c7e313a73cbea3a8aec4223ce46e5e2b3bdbe9e10e961641be3189d97/mapreduce-0.3.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:57:53 2020"}