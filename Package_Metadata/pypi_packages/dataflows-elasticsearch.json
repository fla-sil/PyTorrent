{"info": {"author": "Adam Kariv", "author_email": "adam.kariv@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Web Environment", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.7", "Topic :: Internet :: WWW/HTTP :: Dynamic Content", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# dataflows-elasticsearch\n\n[![Travis](https://travis-ci.org/dataspot/dataflows-elasticsearch.svg?branch=master)](https://travis-ci.org/dataspot/dataflows-elasticsearch)\n[![Coveralls](http://img.shields.io/coveralls/dataspot/dataflows-elasticsearch.svg?branch=master)](https://coveralls.io/r/dataspot/dataflows-elasticsearch?branch=master)\n\nDataflows's processors to work with ElasticSearch\n\n## Features\n\n- `dump_to_elasticsearch` processor\n\n## Contents\n\n<!--TOC-->\n\n  - [Getting Started](#getting-started)\n    - [Installation](#installation)\n    - [Examples](#examples)\n  - [Documentation](#documentation)\n    - [dump_to_s3](#dump_to_s3)\n    - [change_acl_on_s3](#change_acl_on_s3)\n  - [Contributing](#contributing)\n  - [Changelog](#changelog)\n\n<!--TOC-->\n\n## Getting Started\n\n### Installation\n\nThe package use semantic versioning. It means that major versions  could include breaking changes. It's recommended to specify `package` version range in your `setup/requirements` file e.g. `package>=1.0,<2.0`.\n\n```bash\n$ pip install dataflows-awelasticsearchs\n```\n\n### Examples\n\nThese processors have to be used as a part of data flow. For example:\n\n```python\nflow = Flow(\n    load('data/data.csv'),\n    dump_to_s3(\n        bucket=bucket,\n        acl='private',\n        path='my/datapackage',\n        endpoint_url=os.environ['S3_ENDPOINT_URL'],\n    ),\n)\nflow.process()\n```\n\n## Documentation\n\n### dump_to_s3\n\nSaves the DataPackage to AWS S3.\n\n#### Parameters\n\n- `bucket` - Name of the bucket where DataPackage will be stored (should already be created!)\n- `acl` - ACL to provide the uploaded files. Default is 'public-read' (see [boto3 docs](http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.put_object) for more info).\n- `path` - Path (key/prefix) to the DataPackage. May contain format string available for `datapackage.json` Eg: `my/example/path/{owner}/{name}/{version}`\n- `content_type` - content type to use when storing files in S3. Defaults to text/plain (usual S3 default is binary/octet-stream but we prefer text/plain).\n- `endpoint_url` - api endpoint to allow using S3 compatible services (e.g. 'https://ams3.digitaloceanspaces.com')\n\n### change_acl_on_s3\n\nChanges ACL of object in given Bucket with given path aka prefix.\n\n#### Parameters\n\n- `bucket` - Name of the bucket where objects are stored\n- `acl` - Available options `'private'|'public-read'|'public-read-write'|'authenticated-read'|'aws-exec-read'|'bucket-owner-read'|'bucket-owner-full-control'`\n- `path` - Path (key/prefix) to the DataPackage.\n- `endpoint_url` - api endpoint to allow using S3 compatible services (e.g. 'https://ams3.digitaloceanspaces.com')\n\n## Contributing\n\nThe project follows the [Open Knowledge International coding standards](https://github.com/okfn/coding-standards).\n\nThe recommended way to get started is to create and activate a project virtual environment.\nTo install package and development dependencies into your active environment:\n\n```\n$ make install\n```\n\nTo run tests with linting and coverage:\n\n```bash\n$ make test\n```\n\nFor linting, `pylama` (configured in `pylama.ini`) is used. At this stage it's already\ninstalled into your environment and could be used separately with more fine-grained control\nas described in documentation - https://pylama.readthedocs.io/en/latest/.\n\nFor example to sort results by error type:\n\n```bash\n$ pylama --sort <path>\n```\n\nFor testing, `tox` (configured in `tox.ini`) is used.\nIt's already installed into your environment and could be used separately with more fine-grained control as described in documentation - https://testrun.org/tox/latest/.\n\nFor example to check subset of tests against Python 2 environment with increased verbosity.\nAll positional arguments and options after `--` will be passed to `py.test`:\n\n```bash\ntox -e py37 -- -v tests/<path>\n```\n\nUnder the hood `tox` uses `pytest` (configured in `pytest.ini`), `coverage`\nand `mock` packages. These packages are available only in tox envionments.\n\n## Changelog\n\nHere described only breaking and the most important changes. The full changelog and documentation for all released versions can be found in the nicely formatted [commit history](https://github.com/frictionlessdata/dataflows-aws/commits/master).\n\n#### v0.x\n\n- an initial processors implementation\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/dataspot/dataflows-elasticsearch", "keywords": "frictionless data,open data,json schema,table schema,data package,tabular data package", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "dataflows-elasticsearch", "package_url": "https://pypi.org/project/dataflows-elasticsearch/", "platform": "", "project_url": "https://pypi.org/project/dataflows-elasticsearch/", "project_urls": {"Homepage": "https://github.com/dataspot/dataflows-elasticsearch"}, "release_url": "https://pypi.org/project/dataflows-elasticsearch/0.0.3/", "requires_dist": ["six", "elasticsearch", "dataflows", "tableschema-elasticsearch", "moto[server] ; extra == 'develop'", "pytest-cov ; extra == 'develop'", "pytest ; extra == 'develop'", "pylama ; extra == 'develop'", "mock ; extra == 'develop'", "tox ; extra == 'develop'"], "requires_python": "", "summary": "A utility library for working with data flows in Python and ElasticSearch", "version": "0.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>dataflows-elasticsearch</h1>\n<p><a href=\"https://travis-ci.org/dataspot/dataflows-elasticsearch\" rel=\"nofollow\"><img alt=\"Travis\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/21acfb397d406d579ba24b7fdb1876ec6da2fa3d/68747470733a2f2f7472617669732d63692e6f72672f6461746173706f742f64617461666c6f77732d656c61737469637365617263682e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/r/dataspot/dataflows-elasticsearch?branch=master\" rel=\"nofollow\"><img alt=\"Coveralls\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/903b06706f05d369278d5d48cce84b524eff859f/687474703a2f2f696d672e736869656c64732e696f2f636f766572616c6c732f6461746173706f742f64617461666c6f77732d656c61737469637365617263682e7376673f6272616e63683d6d6173746572\"></a></p>\n<p>Dataflows's processors to work with ElasticSearch</p>\n<h2>Features</h2>\n<ul>\n<li><code>dump_to_elasticsearch</code> processor</li>\n</ul>\n<h2>Contents</h2>\n\n<ul>\n<li><a href=\"#getting-started\" rel=\"nofollow\">Getting Started</a>\n<ul>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#examples\" rel=\"nofollow\">Examples</a></li>\n</ul>\n</li>\n<li><a href=\"#documentation\" rel=\"nofollow\">Documentation</a>\n<ul>\n<li><a href=\"#dump_to_s3\" rel=\"nofollow\">dump_to_s3</a></li>\n<li><a href=\"#change_acl_on_s3\" rel=\"nofollow\">change_acl_on_s3</a></li>\n</ul>\n</li>\n<li><a href=\"#contributing\" rel=\"nofollow\">Contributing</a></li>\n<li><a href=\"#changelog\" rel=\"nofollow\">Changelog</a></li>\n</ul>\n\n<h2>Getting Started</h2>\n<h3>Installation</h3>\n<p>The package use semantic versioning. It means that major versions  could include breaking changes. It's recommended to specify <code>package</code> version range in your <code>setup/requirements</code> file e.g. <code>package&gt;=1.0,&lt;2.0</code>.</p>\n<pre>$ pip install dataflows-awelasticsearchs\n</pre>\n<h3>Examples</h3>\n<p>These processors have to be used as a part of data flow. For example:</p>\n<pre><span class=\"n\">flow</span> <span class=\"o\">=</span> <span class=\"n\">Flow</span><span class=\"p\">(</span>\n    <span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'data/data.csv'</span><span class=\"p\">),</span>\n    <span class=\"n\">dump_to_s3</span><span class=\"p\">(</span>\n        <span class=\"n\">bucket</span><span class=\"o\">=</span><span class=\"n\">bucket</span><span class=\"p\">,</span>\n        <span class=\"n\">acl</span><span class=\"o\">=</span><span class=\"s1\">'private'</span><span class=\"p\">,</span>\n        <span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'my/datapackage'</span><span class=\"p\">,</span>\n        <span class=\"n\">endpoint_url</span><span class=\"o\">=</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">'S3_ENDPOINT_URL'</span><span class=\"p\">],</span>\n    <span class=\"p\">),</span>\n<span class=\"p\">)</span>\n<span class=\"n\">flow</span><span class=\"o\">.</span><span class=\"n\">process</span><span class=\"p\">()</span>\n</pre>\n<h2>Documentation</h2>\n<h3>dump_to_s3</h3>\n<p>Saves the DataPackage to AWS S3.</p>\n<h4>Parameters</h4>\n<ul>\n<li><code>bucket</code> - Name of the bucket where DataPackage will be stored (should already be created!)</li>\n<li><code>acl</code> - ACL to provide the uploaded files. Default is 'public-read' (see <a href=\"http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.put_object\" rel=\"nofollow\">boto3 docs</a> for more info).</li>\n<li><code>path</code> - Path (key/prefix) to the DataPackage. May contain format string available for <code>datapackage.json</code> Eg: <code>my/example/path/{owner}/{name}/{version}</code></li>\n<li><code>content_type</code> - content type to use when storing files in S3. Defaults to text/plain (usual S3 default is binary/octet-stream but we prefer text/plain).</li>\n<li><code>endpoint_url</code> - api endpoint to allow using S3 compatible services (e.g. '<a href=\"https://ams3.digitaloceanspaces.com\" rel=\"nofollow\">https://ams3.digitaloceanspaces.com</a>')</li>\n</ul>\n<h3>change_acl_on_s3</h3>\n<p>Changes ACL of object in given Bucket with given path aka prefix.</p>\n<h4>Parameters</h4>\n<ul>\n<li><code>bucket</code> - Name of the bucket where objects are stored</li>\n<li><code>acl</code> - Available options <code>'private'|'public-read'|'public-read-write'|'authenticated-read'|'aws-exec-read'|'bucket-owner-read'|'bucket-owner-full-control'</code></li>\n<li><code>path</code> - Path (key/prefix) to the DataPackage.</li>\n<li><code>endpoint_url</code> - api endpoint to allow using S3 compatible services (e.g. '<a href=\"https://ams3.digitaloceanspaces.com\" rel=\"nofollow\">https://ams3.digitaloceanspaces.com</a>')</li>\n</ul>\n<h2>Contributing</h2>\n<p>The project follows the <a href=\"https://github.com/okfn/coding-standards\" rel=\"nofollow\">Open Knowledge International coding standards</a>.</p>\n<p>The recommended way to get started is to create and activate a project virtual environment.\nTo install package and development dependencies into your active environment:</p>\n<pre><code>$ make install\n</code></pre>\n<p>To run tests with linting and coverage:</p>\n<pre>$ make <span class=\"nb\">test</span>\n</pre>\n<p>For linting, <code>pylama</code> (configured in <code>pylama.ini</code>) is used. At this stage it's already\ninstalled into your environment and could be used separately with more fine-grained control\nas described in documentation - <a href=\"https://pylama.readthedocs.io/en/latest/\" rel=\"nofollow\">https://pylama.readthedocs.io/en/latest/</a>.</p>\n<p>For example to sort results by error type:</p>\n<pre>$ pylama --sort &lt;path&gt;\n</pre>\n<p>For testing, <code>tox</code> (configured in <code>tox.ini</code>) is used.\nIt's already installed into your environment and could be used separately with more fine-grained control as described in documentation - <a href=\"https://testrun.org/tox/latest/\" rel=\"nofollow\">https://testrun.org/tox/latest/</a>.</p>\n<p>For example to check subset of tests against Python 2 environment with increased verbosity.\nAll positional arguments and options after <code>--</code> will be passed to <code>py.test</code>:</p>\n<pre>tox -e py37 -- -v tests/&lt;path&gt;\n</pre>\n<p>Under the hood <code>tox</code> uses <code>pytest</code> (configured in <code>pytest.ini</code>), <code>coverage</code>\nand <code>mock</code> packages. These packages are available only in tox envionments.</p>\n<h2>Changelog</h2>\n<p>Here described only breaking and the most important changes. The full changelog and documentation for all released versions can be found in the nicely formatted <a href=\"https://github.com/frictionlessdata/dataflows-aws/commits/master\" rel=\"nofollow\">commit history</a>.</p>\n<h4>v0.x</h4>\n<ul>\n<li>an initial processors implementation</li>\n</ul>\n\n          </div>"}, "last_serial": 6350546, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "33143db1e25372f0123df77d627e9239", "sha256": "fb557c1d5f5732bdd2431093884fb9c39e6a8bd16ffe4ceb4546e7fa0c09f46e"}, "downloads": -1, "filename": "dataflows_elasticsearch-0.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "33143db1e25372f0123df77d627e9239", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 6248, "upload_time": "2019-11-25T20:25:05", "upload_time_iso_8601": "2019-11-25T20:25:05.322765Z", "url": "https://files.pythonhosted.org/packages/8a/ae/de1d224a64ed03096306d383a6d91609e59990e497dd8f2dda13487f87a0/dataflows_elasticsearch-0.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bd8bbd5afd08d99941995fbc9b76689f", "sha256": "be1d1cfd39d937dd728d0bb4d44902ddc2fcbf964fc810e5f9ee81084a235fc1"}, "downloads": -1, "filename": "dataflows-elasticsearch-0.0.1.tar.gz", "has_sig": false, "md5_digest": "bd8bbd5afd08d99941995fbc9b76689f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5128480, "upload_time": "2019-11-25T20:25:08", "upload_time_iso_8601": "2019-11-25T20:25:08.203171Z", "url": "https://files.pythonhosted.org/packages/c1/30/adad0a8962ed9b78eee737fa8b9fd61f0cd06444246af77ed8b2b547b3d2/dataflows-elasticsearch-0.0.1.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "cee4419c761983b3b7781013890c4ff0", "sha256": "b2d7fa05d1a562a7d3604d776ae51c8254ff53ae34d50638e2c0e7f7ff387df8"}, "downloads": -1, "filename": "dataflows_elasticsearch-0.0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "cee4419c761983b3b7781013890c4ff0", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 6238, "upload_time": "2019-12-23T11:58:05", "upload_time_iso_8601": "2019-12-23T11:58:05.882321Z", "url": "https://files.pythonhosted.org/packages/1e/92/d761ac3f5ea3158980a2e75a6dfac2cc2a2ba4e93d8dfaddfea76ab253ee/dataflows_elasticsearch-0.0.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "05e254816e14b037506b4f8b13f5f804", "sha256": "d71ba0ee99efd9d48ef1a79f76124aa230470b7a7b9510085944d9977b078548"}, "downloads": -1, "filename": "dataflows-elasticsearch-0.0.3.tar.gz", "has_sig": false, "md5_digest": "05e254816e14b037506b4f8b13f5f804", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5309873, "upload_time": "2019-12-23T11:58:08", "upload_time_iso_8601": "2019-12-23T11:58:08.126862Z", "url": "https://files.pythonhosted.org/packages/71/0e/bc7c3adb3b4a1efcf9f597276c2614e67ceb8aa903af8cb516b1796f43cf/dataflows-elasticsearch-0.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cee4419c761983b3b7781013890c4ff0", "sha256": "b2d7fa05d1a562a7d3604d776ae51c8254ff53ae34d50638e2c0e7f7ff387df8"}, "downloads": -1, "filename": "dataflows_elasticsearch-0.0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "cee4419c761983b3b7781013890c4ff0", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 6238, "upload_time": "2019-12-23T11:58:05", "upload_time_iso_8601": "2019-12-23T11:58:05.882321Z", "url": "https://files.pythonhosted.org/packages/1e/92/d761ac3f5ea3158980a2e75a6dfac2cc2a2ba4e93d8dfaddfea76ab253ee/dataflows_elasticsearch-0.0.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "05e254816e14b037506b4f8b13f5f804", "sha256": "d71ba0ee99efd9d48ef1a79f76124aa230470b7a7b9510085944d9977b078548"}, "downloads": -1, "filename": "dataflows-elasticsearch-0.0.3.tar.gz", "has_sig": false, "md5_digest": "05e254816e14b037506b4f8b13f5f804", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5309873, "upload_time": "2019-12-23T11:58:08", "upload_time_iso_8601": "2019-12-23T11:58:08.126862Z", "url": "https://files.pythonhosted.org/packages/71/0e/bc7c3adb3b4a1efcf9f597276c2614e67ceb8aa903af8cb516b1796f43cf/dataflows-elasticsearch-0.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:40:24 2020"}