{"info": {"author": "Manuel Gunther", "author_email": "siebenkopf@googlemail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Natural Language :: English", "Programming Language :: Python", "Programming Language :: Python :: 3", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "The Extreme Value Machine\n=========================\n\nThis package contains source code for the Extreme Value Machine (EVM).\nThe EVM is a model to compute probabilities for samples to belong to a certain class.\nIn opposition to other closed-set classification models, the EVM enables open-set classification, i.e., test samples might be of classes that are not contained in the training set.\nHence, the probability of this test sample should be low for any of the classes.\n\nThe EVM was introduced in the paper `The Extreme Value Machine <http://doi.org/10.1109/TPAMI.2017.2707495>`__.\nIf you are using the EVM in a scientific publication, please cite:\n\n.. code-block:: latex\n\n   @article{rudd2018evm,\n     author={Rudd, Ethan M. and Jain, Lalit P. and Scheirer, Walter J. and Boult, Terrance E.},\n     journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},\n     title={The Extreme Value Machine},\n     year={2018},\n     volume={40},\n     number={3},\n     pages={762-768},\n  }\n\nInstallation\n------------\n\nPip\n~~~\n\nThis package is on PyPI, so you can simply call:\n\n.. code-block:: sh\n\n   pip install evm\n\nto install this package.\nAll required dependencies will be automatically installed, too.\n\nLocal build\n~~~~~~~~~~~\n\nThe EVM package is using the setuptools installation package.\nIn order to compile the package locally, you can use:\n\n.. code-block:: sh\n\n   python setup.py build\n\nwhich will install this package into the ``build/lib`` directory.\nYou can also call:\n\n.. code-block:: sh\n\n   python setup.py install\n\nto install the package system-wide.\n\n\nUsage\n-----\n\nThis package provides mainly two classes, for different purposes.\n\nTraining a Single EVM\n~~~~~~~~~~~~~~~~~~~~~\n\nThe first class is the ``EVM`` class, which holds all information that is required to model data from a specific class of your data.\nThis is done by computing distances from all instances of your class (``positives``) to instances of other classes (``negatives``).\n\nFirst, you need to create an object of the ``EVM`` class.\nHere, you need to specify several parameters:\n\n* ``tailsize`` : An integral parameter that defines, how many negative samples are used to estimate the model parameters.\n* ``cover_threshold`` : If not ``None`` (the default), specifies the probability threshold used to eliminate extreme vectors if they are covered by other extreme vectors with that probability. If ``None``, no model reduction will be performed.\n* ``distance_multiplier`` : The multiplier to compute margin distances. ``0.5`` by default.\n* ``distance_function`` : The distance function used to compute the distance between two samples; defaults to ``scipy.spatial.distance.cosine``\n* ``include_cover_probabilities`` : (experimental) include cover probabilities that are calculated between samples of the same class. At least three samples per class are required to be able to compute cover probabilities.\n* ``log_level`` : defines the verbosity of the EVM training. Possible values: ``'info'`` (the default), ``'debug'``.\n\n.. code-block:: py\n\n   import EVM, numpy, scipy\n   evm = EVM.EVM(tailsize=10, cover_threshold = 0.7, distance_function=scipy.spatial.distance.euclidean)\n\nAfter creating an ``EVM`` object, it needs to be trained.\nYou can train the model in two different ways.\nIn the most commonly used way, you pass both positive and negative data samples of your class and other classes, respectively:\n\n.. code-block:: py\n\n   class1 = numpy.random.normal((0,0),3,(50,2))\n   class2 = numpy.random.normal((-10,10),3,(50,2))\n   class3 = numpy.random.normal((10,-10),3,(50,2))\n\n   evm.train(positives = class1, negatives = numpy.concatenate((class2, class3)))\n\nAlternatively, you can also pre-compute the distances between all positive and all negative samples and pass the distance matrix in:\n\n.. code-block:: py\n\n   distances = scipy.spatial.distance.cdist(class1, numpy.concatenate((class2, class3)), 'euclidean')\n   evm.train(positives = class1, distances = distances)\n\nNow, you can compute the probability of any data point to belong to this class (actually, the function expects a list of points, so here we have to wrap it in square brackets):\n\n.. code-block:: py\n\n   probabilities = evm.probabilities([[0,0]])[0]\n\nwhich will return the probability of inclusion for each of the extreme vectors.\nAlternatively, you might be interested in the maximum probability, i.e., the probability that the test sample belongs to your class.\nYou can call:\n\n.. code-block:: py\n\n   probability, evm_index = evm.max_probabilities([[0,0]])\n\nwhich will return the maximum probability over all extreme vectors, and the index of the extreme vectors that was the maximum.\n\nTraining Multi-Class EVMs\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe second class that you can use to train EVMs is the ``MultipleEVM`` class.\nFor a given set of samples of several classes, it will compute an EVM model for each of the classes, taking all other classes as negatives.\nThe parameters are similar to the ``EVM`` class.\n\n.. code-block:: py\n\n   mevm = EVM.MultipleEVM(tailsize=10, cover_threshold = 0.7, distance_function=scipy.spatial.distance.euclidean)\n   mevm.train([class1, class2, class3])\n\nYou can obtain the trained EVM models for each of the classes separately, if you want:\n\n.. code-block:: py\n\n   evm1 = mevm.evms[0]\n   evm2 = mevm.evms[1]\n   evm3 = mevm.evms[2]\n\nFor a given test sample, you can compute the probabilities for all extreme vectors in all EVMs by calling:\n\n.. code-block:: py\n\n   probabilities = mevm.probabilities([[0,0]])[0]\n\nSimilarly, you can compute the class with the maximum probability:\n\n.. code-block:: py\n\n   probabilities, indexes = mevm.max_probabilities([[0,0]])\n\nwhere ``indexes`` contain both the index of the maximum class, and the index of the extreme vector inside that class.\n\nParallelism\n~~~~~~~~~~~\n\nAny of the public API functions to train or test ``EVM`` or ``MultipleEVM`` have a ``parallel`` parameter, where you can specify the maximum number of parallel threads to compute the function.\n\nExample\n~~~~~~~\n\nIn the root directory of this package, there is an ``example.py`` script that will show an exemplary usage of the ``EVM`` and ``MultipleEVM`` classes in order to display probabilities obtained on a sample 2D dataset.\nYou can run the example via\n\n.. code-block:: sh\n\n   python example.py\n\nIt might run for some time, and it will create a multi-page PDF file containing 4 plots:\n\n1. Training a single EVM class from samples, and showing the extreme vectors.\n2. Provide the probability of the class for inputs in a grid in color coding.\n3. Training multiple EVM classes from the same samples, and showing the extreme vectors.\n4. Provide the maximum probability of the classes for inputs in a grid in color coding.\n\nIn the color coding, black means that none of the classes had a high probability.\nSince most of the regions are black, this indicates that the EVM has learned to classify unknown samples as unknown.\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://bitbucket.org/vastlab/evm", "keywords": "", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "EVM", "package_url": "https://pypi.org/project/EVM/", "platform": "", "project_url": "https://pypi.org/project/EVM/", "project_urls": {"Homepage": "http://bitbucket.org/vastlab/evm"}, "release_url": "https://pypi.org/project/EVM/0.1.2/", "requires_dist": null, "requires_python": "", "summary": "The Extreme Value Machine", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This package contains source code for the Extreme Value Machine (EVM).\nThe EVM is a model to compute probabilities for samples to belong to a certain class.\nIn opposition to other closed-set classification models, the EVM enables open-set classification, i.e., test samples might be of classes that are not contained in the training set.\nHence, the probability of this test sample should be low for any of the classes.</p>\n<p>The EVM was introduced in the paper <a href=\"http://doi.org/10.1109/TPAMI.2017.2707495\" rel=\"nofollow\">The Extreme Value Machine</a>.\nIf you are using the EVM in a scientific publication, please cite:</p>\n<pre> @article<span class=\"nb\">{</span>rudd2018evm,\n   author=<span class=\"nb\">{</span>Rudd, Ethan M. and Jain, Lalit P. and Scheirer, Walter J. and Boult, Terrance E.<span class=\"nb\">}</span>,\n   journal=<span class=\"nb\">{</span>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<span class=\"nb\">}</span>,\n   title=<span class=\"nb\">{</span>The Extreme Value Machine<span class=\"nb\">}</span>,\n   year=<span class=\"nb\">{</span>2018<span class=\"nb\">}</span>,\n   volume=<span class=\"nb\">{</span>40<span class=\"nb\">}</span>,\n   number=<span class=\"nb\">{</span>3<span class=\"nb\">}</span>,\n   pages=<span class=\"nb\">{</span>762-768<span class=\"nb\">}</span>,\n<span class=\"nb\">}</span>\n</pre>\n<div id=\"installation\">\n<h2>Installation</h2>\n<div id=\"pip\">\n<h3>Pip</h3>\n<p>This package is on PyPI, so you can simply call:</p>\n<pre>pip install evm\n</pre>\n<p>to install this package.\nAll required dependencies will be automatically installed, too.</p>\n</div>\n<div id=\"local-build\">\n<h3>Local build</h3>\n<p>The EVM package is using the setuptools installation package.\nIn order to compile the package locally, you can use:</p>\n<pre>python setup.py build\n</pre>\n<p>which will install this package into the <tt>build/lib</tt> directory.\nYou can also call:</p>\n<pre>python setup.py install\n</pre>\n<p>to install the package system-wide.</p>\n</div>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>This package provides mainly two classes, for different purposes.</p>\n<div id=\"training-a-single-evm\">\n<h3>Training a Single EVM</h3>\n<p>The first class is the <tt>EVM</tt> class, which holds all information that is required to model data from a specific class of your data.\nThis is done by computing distances from all instances of your class (<tt>positives</tt>) to instances of other classes (<tt>negatives</tt>).</p>\n<p>First, you need to create an object of the <tt>EVM</tt> class.\nHere, you need to specify several parameters:</p>\n<ul>\n<li><tt>tailsize</tt> : An integral parameter that defines, how many negative samples are used to estimate the model parameters.</li>\n<li><tt>cover_threshold</tt> : If not <tt>None</tt> (the default), specifies the probability threshold used to eliminate extreme vectors if they are covered by other extreme vectors with that probability. If <tt>None</tt>, no model reduction will be performed.</li>\n<li><tt>distance_multiplier</tt> : The multiplier to compute margin distances. <tt>0.5</tt> by default.</li>\n<li><tt>distance_function</tt> : The distance function used to compute the distance between two samples; defaults to <tt>scipy.spatial.distance.cosine</tt></li>\n<li><tt>include_cover_probabilities</tt> : (experimental) include cover probabilities that are calculated between samples of the same class. At least three samples per class are required to be able to compute cover probabilities.</li>\n<li><tt>log_level</tt> : defines the verbosity of the EVM training. Possible values: <tt>'info'</tt> (the default), <tt>'debug'</tt>.</li>\n</ul>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">EVM</span><span class=\"o\">,</span> <span class=\"nn\">numpy</span><span class=\"o\">,</span> <span class=\"nn\">scipy</span>\n<span class=\"n\">evm</span> <span class=\"o\">=</span> <span class=\"n\">EVM</span><span class=\"o\">.</span><span class=\"n\">EVM</span><span class=\"p\">(</span><span class=\"n\">tailsize</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">cover_threshold</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"n\">distance_function</span><span class=\"o\">=</span><span class=\"n\">scipy</span><span class=\"o\">.</span><span class=\"n\">spatial</span><span class=\"o\">.</span><span class=\"n\">distance</span><span class=\"o\">.</span><span class=\"n\">euclidean</span><span class=\"p\">)</span>\n</pre>\n<p>After creating an <tt>EVM</tt> object, it needs to be trained.\nYou can train the model in two different ways.\nIn the most commonly used way, you pass both positive and negative data samples of your class and other classes, respectively:</p>\n<pre><span class=\"n\">class1</span> <span class=\"o\">=</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">),</span><span class=\"mi\">3</span><span class=\"p\">,(</span><span class=\"mi\">50</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"n\">class2</span> <span class=\"o\">=</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"mi\">3</span><span class=\"p\">,(</span><span class=\"mi\">50</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"n\">class3</span> <span class=\"o\">=</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"mi\">3</span><span class=\"p\">,(</span><span class=\"mi\">50</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">))</span>\n\n<span class=\"n\">evm</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">positives</span> <span class=\"o\">=</span> <span class=\"n\">class1</span><span class=\"p\">,</span> <span class=\"n\">negatives</span> <span class=\"o\">=</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">concatenate</span><span class=\"p\">((</span><span class=\"n\">class2</span><span class=\"p\">,</span> <span class=\"n\">class3</span><span class=\"p\">)))</span>\n</pre>\n<p>Alternatively, you can also pre-compute the distances between all positive and all negative samples and pass the distance matrix in:</p>\n<pre><span class=\"n\">distances</span> <span class=\"o\">=</span> <span class=\"n\">scipy</span><span class=\"o\">.</span><span class=\"n\">spatial</span><span class=\"o\">.</span><span class=\"n\">distance</span><span class=\"o\">.</span><span class=\"n\">cdist</span><span class=\"p\">(</span><span class=\"n\">class1</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">concatenate</span><span class=\"p\">((</span><span class=\"n\">class2</span><span class=\"p\">,</span> <span class=\"n\">class3</span><span class=\"p\">)),</span> <span class=\"s1\">'euclidean'</span><span class=\"p\">)</span>\n<span class=\"n\">evm</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">positives</span> <span class=\"o\">=</span> <span class=\"n\">class1</span><span class=\"p\">,</span> <span class=\"n\">distances</span> <span class=\"o\">=</span> <span class=\"n\">distances</span><span class=\"p\">)</span>\n</pre>\n<p>Now, you can compute the probability of any data point to belong to this class (actually, the function expects a list of points, so here we have to wrap it in square brackets):</p>\n<pre><span class=\"n\">probabilities</span> <span class=\"o\">=</span> <span class=\"n\">evm</span><span class=\"o\">.</span><span class=\"n\">probabilities</span><span class=\"p\">([[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">]])[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n</pre>\n<p>which will return the probability of inclusion for each of the extreme vectors.\nAlternatively, you might be interested in the maximum probability, i.e., the probability that the test sample belongs to your class.\nYou can call:</p>\n<pre><span class=\"n\">probability</span><span class=\"p\">,</span> <span class=\"n\">evm_index</span> <span class=\"o\">=</span> <span class=\"n\">evm</span><span class=\"o\">.</span><span class=\"n\">max_probabilities</span><span class=\"p\">([[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">]])</span>\n</pre>\n<p>which will return the maximum probability over all extreme vectors, and the index of the extreme vectors that was the maximum.</p>\n</div>\n<div id=\"training-multi-class-evms\">\n<h3>Training Multi-Class EVMs</h3>\n<p>The second class that you can use to train EVMs is the <tt>MultipleEVM</tt> class.\nFor a given set of samples of several classes, it will compute an EVM model for each of the classes, taking all other classes as negatives.\nThe parameters are similar to the <tt>EVM</tt> class.</p>\n<pre><span class=\"n\">mevm</span> <span class=\"o\">=</span> <span class=\"n\">EVM</span><span class=\"o\">.</span><span class=\"n\">MultipleEVM</span><span class=\"p\">(</span><span class=\"n\">tailsize</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">cover_threshold</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"n\">distance_function</span><span class=\"o\">=</span><span class=\"n\">scipy</span><span class=\"o\">.</span><span class=\"n\">spatial</span><span class=\"o\">.</span><span class=\"n\">distance</span><span class=\"o\">.</span><span class=\"n\">euclidean</span><span class=\"p\">)</span>\n<span class=\"n\">mevm</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">([</span><span class=\"n\">class1</span><span class=\"p\">,</span> <span class=\"n\">class2</span><span class=\"p\">,</span> <span class=\"n\">class3</span><span class=\"p\">])</span>\n</pre>\n<p>You can obtain the trained EVM models for each of the classes separately, if you want:</p>\n<pre><span class=\"n\">evm1</span> <span class=\"o\">=</span> <span class=\"n\">mevm</span><span class=\"o\">.</span><span class=\"n\">evms</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">evm2</span> <span class=\"o\">=</span> <span class=\"n\">mevm</span><span class=\"o\">.</span><span class=\"n\">evms</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"n\">evm3</span> <span class=\"o\">=</span> <span class=\"n\">mevm</span><span class=\"o\">.</span><span class=\"n\">evms</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n</pre>\n<p>For a given test sample, you can compute the probabilities for all extreme vectors in all EVMs by calling:</p>\n<pre><span class=\"n\">probabilities</span> <span class=\"o\">=</span> <span class=\"n\">mevm</span><span class=\"o\">.</span><span class=\"n\">probabilities</span><span class=\"p\">([[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">]])[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n</pre>\n<p>Similarly, you can compute the class with the maximum probability:</p>\n<pre><span class=\"n\">probabilities</span><span class=\"p\">,</span> <span class=\"n\">indexes</span> <span class=\"o\">=</span> <span class=\"n\">mevm</span><span class=\"o\">.</span><span class=\"n\">max_probabilities</span><span class=\"p\">([[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">]])</span>\n</pre>\n<p>where <tt>indexes</tt> contain both the index of the maximum class, and the index of the extreme vector inside that class.</p>\n</div>\n<div id=\"parallelism\">\n<h3>Parallelism</h3>\n<p>Any of the public API functions to train or test <tt>EVM</tt> or <tt>MultipleEVM</tt> have a <tt>parallel</tt> parameter, where you can specify the maximum number of parallel threads to compute the function.</p>\n</div>\n<div id=\"example\">\n<h3>Example</h3>\n<p>In the root directory of this package, there is an <tt>example.py</tt> script that will show an exemplary usage of the <tt>EVM</tt> and <tt>MultipleEVM</tt> classes in order to display probabilities obtained on a sample 2D dataset.\nYou can run the example via</p>\n<pre>python example.py\n</pre>\n<p>It might run for some time, and it will create a multi-page PDF file containing 4 plots:</p>\n<ol>\n<li>Training a single EVM class from samples, and showing the extreme vectors.</li>\n<li>Provide the probability of the class for inputs in a grid in color coding.</li>\n<li>Training multiple EVM classes from the same samples, and showing the extreme vectors.</li>\n<li>Provide the maximum probability of the classes for inputs in a grid in color coding.</li>\n</ol>\n<p>In the color coding, black means that none of the classes had a high probability.\nSince most of the regions are black, this indicates that the EVM has learned to classify unknown samples as unknown.</p>\n</div>\n</div>\n\n          </div>"}, "last_serial": 3910978, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "65179531f280e21e23077a44d481304d", "sha256": "864ca9ac67683603f7037c3322d89cfba27f117de6a3131270ae655c08be26ef"}, "downloads": -1, "filename": "EVM-0.1.0.zip", "has_sig": false, "md5_digest": "65179531f280e21e23077a44d481304d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20031, "upload_time": "2018-05-30T00:04:12", "upload_time_iso_8601": "2018-05-30T00:04:12.346215Z", "url": "https://files.pythonhosted.org/packages/64/03/f81249e0782ba3026e522950e34168b719b2780187e6d7c41abf431dbe65/EVM-0.1.0.zip", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "c4990ea7a4fad6dd6aea290902436b1f", "sha256": "546528d893409b82bd4d0a8772b466ae67c1c97065260598c5913bac3f9b2c12"}, "downloads": -1, "filename": "EVM-0.1.1.zip", "has_sig": false, "md5_digest": "c4990ea7a4fad6dd6aea290902436b1f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21274, "upload_time": "2018-05-30T00:12:15", "upload_time_iso_8601": "2018-05-30T00:12:15.625112Z", "url": "https://files.pythonhosted.org/packages/31/c6/30741c9bc5137d553b2d0331697d91944463c7d5f6af964134c62a682ce8/EVM-0.1.1.zip", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "aac43e931e4ed786fe85cd9f664d79a1", "sha256": "ec8b36adbed3d326496b59e45e075d8c5a0fad0a093481a8b48ef88145a8b6ef"}, "downloads": -1, "filename": "EVM-0.1.2.zip", "has_sig": false, "md5_digest": "aac43e931e4ed786fe85cd9f664d79a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21461, "upload_time": "2018-05-30T00:17:08", "upload_time_iso_8601": "2018-05-30T00:17:08.851323Z", "url": "https://files.pythonhosted.org/packages/a5/ee/187044f9158fcfd4cf808a5ca58115a987f79228b76bf230e1abac0c2636/EVM-0.1.2.zip", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "aac43e931e4ed786fe85cd9f664d79a1", "sha256": "ec8b36adbed3d326496b59e45e075d8c5a0fad0a093481a8b48ef88145a8b6ef"}, "downloads": -1, "filename": "EVM-0.1.2.zip", "has_sig": false, "md5_digest": "aac43e931e4ed786fe85cd9f664d79a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21461, "upload_time": "2018-05-30T00:17:08", "upload_time_iso_8601": "2018-05-30T00:17:08.851323Z", "url": "https://files.pythonhosted.org/packages/a5/ee/187044f9158fcfd4cf808a5ca58115a987f79228b76bf230e1abac0c2636/EVM-0.1.2.zip", "yanked": false}], "timestamp": "Fri May  8 00:45:07 2020"}