{"info": {"author": "Daniel Kaslovsky", "author_email": "dkaslovsky@gmail.com", "bugtrack_url": null, "classifiers": ["Environment :: Console", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# ElasticBatch\n\nElasticsearch buffer for collecting and batch inserting Python data and pandas DataFrames\n\n[![Build Status](https://travis-ci.com/dkaslovsky/ElasticBatch.svg?branch=master)](https://travis-ci.com/dkaslovsky/ElasticBatch)\n[![Coverage Status](https://coveralls.io/repos/github/dkaslovsky/ElasticBatch/badge.svg?branch=master)](https://coveralls.io/github/dkaslovsky/ElasticBatch?branch=master)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ElasticBatch)\n\n## Overview\n`ElasticBatch` makes it easy to efficiently insert batches of data in the form of Python dictionaries or [pandas](https://pandas.pydata.org/) [DataFrames](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html#dataframe) into Elasticsearch.  An efficient pattern when processing data bound for [Elasticsearch](https://www.elastic.co/products/elasticsearch) is to collect data records (\"documents\") in a buffer to be bulk-inserted in batches.  `ElasticBatch` provides this functionality to ease the overhead and reduce the code involved in inserting large batches or streams of data into Elasticsearch.\n\n`ElasticBatch` has been tested with Elasticsearch 7.x, but _should_ work with earlier versions.\n\n## Features\n`ElasticBatch` implements the following features (see [Usage](#usage) for examples and more details) that allow a user to:\n- Work with documents as lists of dicts or as rows of pandas DataFrames\n- Add documents to a buffer that will automatically flush (insert its contents to Elasticsearch) when it is full\n- Interact with an intuitive interface that handles all of the underlying Elasticsearch client logic on behalf of the user\n- Track the elapsed time a document has been in the buffer, allowing a user to flush the buffer at a desired time interval even when it is not full\n- Work within a context manager that will automatically flush before exiting, alleviating the need for extra code to ensure all documents are written to the database\n- Optionally dump the buffer contents (documents) to a file before exiting due to an uncaught exception\n- Automatically add Elasticsearch metadata fields (e.g., `_index`, `_id`) to each document via user-supplied functions\n\n## Installation\nThis package is hosted on PyPI and can be installed via `pip`:\n- To install with the ability to process pandas DataFrames:\n  ```\n  $ pip install elasticbatch[pandas]\n  ```\n- For a more lightweight installation with only the ability to process native Python dicts:\n  ```\n  $ pip install elasticbatch\n  ```\nThe only dependency of the latter is `elasticsearch` whereas the former will also install `pandas` as a dependency.\n\nTo instead install from source:\n```\n$ git clone https://github.com/dkaslovsky/ElasticBatch.git\n$ cd ElasticBatch\n$ pip install \".[pandas]\"\n```\nTo install from source without the `pandas` dependency, replace the last line above with\n```\n$ pip install .\n```\n\n## Usage\n\n### Basic Usage\nStart by importing the `ElasticBuffer` class:\n```\n>>> from elasticbatch import ElasticBuffer\n```\n`ElasticBuffer` uses sensible defaults when initialized without parameters:\n```\n>>> esbuf = ElasticBuffer()\n```\nAlternatively, one can pass any of the following parameters:\n- `size`: (`int`) number of documents the buffer can hold before flushing to Elasticsearch; defaults to `5000`.\n- `client_kwargs`: (`dict`) configuration passed to the underlying `elasticsearch.Elasticsearch` client; see the Elasticsearch [documentation](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch) for all available options.\n- `bulk_kwargs`: (`dict`) configuration passed to the underlying call to `elasticsearch.helpers.bulk` for bulk insertion; see the Elasticsearch [documentation](https://elasticsearch-py.readthedocs.io/en/master/helpers.html#elasticsearch.helpers.bulk) for all available options.\n- `verbose_errs`: (`bool`) whether verbose (`True`, default) or truncated (`False`) exceptions are raised; see [Exception Handling](#exception-handling) for more details.\n- `dump_dir`: (`str`) directory to write buffer contents when exiting context due to raised Exception; defaults to `None` for not writing to file.\n- `**metadata_funcs`: (`callable`) functions to apply to each document for adding Elasticsearch metadata.; see [Automatic Elasticsearch Metadata Fields](#automatic-elasticsearch-metadata-fields) for more details.\n\nOnce initialized, `ElasticBuffer` exposes two methods, `add` and `flush`.\nUse `add` to add documents to the buffer, noting that all documents in the buffer will be flushed and inserted into Elasticsearch once the number of documents exceeds the buffer's size:\n```\n>>> docs = [\n        {'_index': 'my-index', 'a': 1, 'b': 2.1, 'c': 'xyz'},\n        {'_index': 'my-index', 'a': 3, 'b': 4.1, 'c': 'xyy'},\n        {'_index': 'my-other-index', 'a': 5, 'b': 6.1, 'c': 'zzz'},\n        {'_index': 'my-other-index', 'a': 7, 'b': 8.1, 'c': 'zyx'},\n    ]\n>>> esbuf.add(docs)\n```\nNote that all metadata fields required for indexing into Elasticsearch (e.g., `_index` above) must either be included in each document or added [programmatically](#automatic-elasticsearch-metadata-fields) via callable kwarg parameters supplied to the `ElasticBuffer` instance (see below).\n\nTo manually force a buffer flush and insert all documents to Elasticsearch, use the `flush` method which does not accept any arguments:\n```\n>>> esbuf.flush()\n```\n\nA third method, `show()`, exists mostly for debug purposes and prints all documents currently in the buffer as newline-delimited json.\n\n### pandas DataFrames\n\nOne can directly insert a pandas DataFrame into the buffer and each row will be treated as a document:\n```\n>>> import pandas as pd\n>>> df = pd.DataFrame(docs)\n>>> print(df)\n\n           _index  a    b    c\n0        my-index  1  2.1  xyz\n1        my-index  3  4.1  xyy\n2  my-other-index  5  6.1  zzz\n3  my-other-index  7  8.1  zyx\n\n>>> esbuf.add(df)\n```\nThe DataFrame's index (referring to `df.index` and __not__ the column named `_index`) is ignored unless it is named, in which case it is added as an ordinary field (column).\n\n### Context Manager\n\n`ElasticBuffer` can also be used as a context manager, offering the advantages of automatically flushing the remaining buffer contents when exiting scope as well as optionally dumping the buffer contents to a file before exiting due to an unhandled exception.\n```\n>>> with ElasticBuffer(size=100, dump_dir='/tmp') as esbuf:\n       for doc in document_stream:\n           doc = process_document(doc)  # some user-defined application-specific processing function\n           esbuf.add(doc)\n```\n\n### Elapsed Time\n\nWhen using `ElasticBuffer` in a service consuming messages from some external source, it can be important to track how long messages have been waiting in the buffer to be flushed.  In particular, a user may wish to flush, say, every hour to account for the situation where only a trickle of data is coming in and the buffer is not filling up.  `ElasticBuffer` provides the elapsed time (in seconds) that its oldest message has been in the buffer:\n```\n>>> esbuf.oldest_elapsed_time\n\n5.687833070755005  # the oldest message was inserted ~5.69 seconds ago\n```\nThis information can be used to periodically check the elapsed time of the oldest message and force a flush if it exceeds a desired threshold.\n\n### Automatic Elasticsearch Metadata Fields\n\nAn `ElasticBuffer` instance can be initialized with kwargs corresponding to callable functions to add [Elasticsearch metadata](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-fields.html) fields to each document added to the buffer:\n```\n>>> def my_index_func(doc): return 'my-index'\n>>> def my_id_func(doc): return sum(doc.values())\n\n>>> esbuf = ElasticBuffer(_index=my_index_func, _id=my_id_func)\n\n>>> docs = [\n        {'a': 1, 'b': 2},\n        {'a': 8, 'b': 9},\n    ]\n>>> esbuf.add(docs)\n\n>>> esbuf.show()\n\n{\"a\": 1, \"b\": 2, \"_index\": \"my-index\", \"_id\": 3}\n{\"a\": 8, \"b\": 9, \"_index\": \"my-index\", \"_id\": 17}\n```\nCallable kwargs add key/value pairs to each document, where the key corresponds to the name of the kwarg and the value is the function's return value.  Each function must accept one argument (the document as a dict) and return one value.  This also works for DataFrames, as they are transformed to documents (dicts) before applying the supplied metadata functions.\n\nThe key/value pairs are added to the top-level of each document.  Note that the user need not add documents with data nested under a `_source` key, as metadata fields can be handled at the same level as the data fields.  For further details, see the underlying Elasticsearch client [bulk insert](https://elasticsearch-py.readthedocs.io/en/master/helpers.html) documentation on handling of metadata fields in flat dicts.\n\n### Exception Handling\n\nFor exception handing, `ElasticBatch` provides the base exception `ElasticBatchError`:\n```\n>>> from elasticbatch import ElasticBatchError\n```\nas well as the more specific `ElasticBufferFlushError` raised on errors flushing to Elasticsearch:\n```\n>>> from elasticbatch.exceptions import ElasticBufferFlushError\n```\nElasticsearch exception messages can contain a copy of every document related to a failed bulk insertion request.  As such messages can be very large, the `verbose_errors` flag can be used to optionally truncate the error message.  When `ElasticBuffer` is initialized with `verbose_errors=True`, the entirety of the error message is returned.  When `verbose_errors=False`, a shorter, descriptive message is returned.  In both cases, the full, potentially verbose, exception is available via the `err` property on the raised `ElasticBufferFlushError`.\n\n## Tests\nTo run tests:\n```\n$ python -m unittest discover -v\n```\nThe awesome [green](https://github.com/CleanCut/green) package is also highly recommended for running tests and reporting test coverage:\n```\n$ green -vvr\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/dkaslovsky/ElasticBatch", "keywords": "elasticsearch,python,pandas,dataframes,batch-processing", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "elasticbatch", "package_url": "https://pypi.org/project/elasticbatch/", "platform": "", "project_url": "https://pypi.org/project/elasticbatch/", "project_urls": {"Homepage": "https://github.com/dkaslovsky/ElasticBatch"}, "release_url": "https://pypi.org/project/elasticbatch/1.0.0/", "requires_dist": ["elasticsearch", "pandas ; extra == 'pandas'"], "requires_python": "", "summary": "Elasticsearch buffer for collecting and batch inserting Python data and pandas DataFrames", "version": "1.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ElasticBatch</h1>\n<p>Elasticsearch buffer for collecting and batch inserting Python data and pandas DataFrames</p>\n<p><a href=\"https://travis-ci.com/dkaslovsky/ElasticBatch\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/15344245319248f45c38f9ff769a3e3b0794ecb3/68747470733a2f2f7472617669732d63692e636f6d2f646b61736c6f76736b792f456c617374696342617463682e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/dkaslovsky/ElasticBatch?branch=master\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6692677aeafd0f36821d3997884adb3b9db65005/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f646b61736c6f76736b792f456c617374696342617463682f62616467652e7376673f6272616e63683d6d6173746572\"></a>\n<img alt=\"PyPI - Python Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/299411a309b9c2be17911891b71a7f808add167c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f456c61737469634261746368\"></p>\n<h2>Overview</h2>\n<p><code>ElasticBatch</code> makes it easy to efficiently insert batches of data in the form of Python dictionaries or <a href=\"https://pandas.pydata.org/\" rel=\"nofollow\">pandas</a> <a href=\"https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html#dataframe\" rel=\"nofollow\">DataFrames</a> into Elasticsearch.  An efficient pattern when processing data bound for <a href=\"https://www.elastic.co/products/elasticsearch\" rel=\"nofollow\">Elasticsearch</a> is to collect data records (\"documents\") in a buffer to be bulk-inserted in batches.  <code>ElasticBatch</code> provides this functionality to ease the overhead and reduce the code involved in inserting large batches or streams of data into Elasticsearch.</p>\n<p><code>ElasticBatch</code> has been tested with Elasticsearch 7.x, but <em>should</em> work with earlier versions.</p>\n<h2>Features</h2>\n<p><code>ElasticBatch</code> implements the following features (see <a href=\"#usage\" rel=\"nofollow\">Usage</a> for examples and more details) that allow a user to:</p>\n<ul>\n<li>Work with documents as lists of dicts or as rows of pandas DataFrames</li>\n<li>Add documents to a buffer that will automatically flush (insert its contents to Elasticsearch) when it is full</li>\n<li>Interact with an intuitive interface that handles all of the underlying Elasticsearch client logic on behalf of the user</li>\n<li>Track the elapsed time a document has been in the buffer, allowing a user to flush the buffer at a desired time interval even when it is not full</li>\n<li>Work within a context manager that will automatically flush before exiting, alleviating the need for extra code to ensure all documents are written to the database</li>\n<li>Optionally dump the buffer contents (documents) to a file before exiting due to an uncaught exception</li>\n<li>Automatically add Elasticsearch metadata fields (e.g., <code>_index</code>, <code>_id</code>) to each document via user-supplied functions</li>\n</ul>\n<h2>Installation</h2>\n<p>This package is hosted on PyPI and can be installed via <code>pip</code>:</p>\n<ul>\n<li>To install with the ability to process pandas DataFrames:\n<pre><code>$ pip install elasticbatch[pandas]\n</code></pre>\n</li>\n<li>For a more lightweight installation with only the ability to process native Python dicts:\n<pre><code>$ pip install elasticbatch\n</code></pre>\n</li>\n</ul>\n<p>The only dependency of the latter is <code>elasticsearch</code> whereas the former will also install <code>pandas</code> as a dependency.</p>\n<p>To instead install from source:</p>\n<pre><code>$ git clone https://github.com/dkaslovsky/ElasticBatch.git\n$ cd ElasticBatch\n$ pip install \".[pandas]\"\n</code></pre>\n<p>To install from source without the <code>pandas</code> dependency, replace the last line above with</p>\n<pre><code>$ pip install .\n</code></pre>\n<h2>Usage</h2>\n<h3>Basic Usage</h3>\n<p>Start by importing the <code>ElasticBuffer</code> class:</p>\n<pre><code>&gt;&gt;&gt; from elasticbatch import ElasticBuffer\n</code></pre>\n<p><code>ElasticBuffer</code> uses sensible defaults when initialized without parameters:</p>\n<pre><code>&gt;&gt;&gt; esbuf = ElasticBuffer()\n</code></pre>\n<p>Alternatively, one can pass any of the following parameters:</p>\n<ul>\n<li><code>size</code>: (<code>int</code>) number of documents the buffer can hold before flushing to Elasticsearch; defaults to <code>5000</code>.</li>\n<li><code>client_kwargs</code>: (<code>dict</code>) configuration passed to the underlying <code>elasticsearch.Elasticsearch</code> client; see the Elasticsearch <a href=\"https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch\" rel=\"nofollow\">documentation</a> for all available options.</li>\n<li><code>bulk_kwargs</code>: (<code>dict</code>) configuration passed to the underlying call to <code>elasticsearch.helpers.bulk</code> for bulk insertion; see the Elasticsearch <a href=\"https://elasticsearch-py.readthedocs.io/en/master/helpers.html#elasticsearch.helpers.bulk\" rel=\"nofollow\">documentation</a> for all available options.</li>\n<li><code>verbose_errs</code>: (<code>bool</code>) whether verbose (<code>True</code>, default) or truncated (<code>False</code>) exceptions are raised; see <a href=\"#exception-handling\" rel=\"nofollow\">Exception Handling</a> for more details.</li>\n<li><code>dump_dir</code>: (<code>str</code>) directory to write buffer contents when exiting context due to raised Exception; defaults to <code>None</code> for not writing to file.</li>\n<li><code>**metadata_funcs</code>: (<code>callable</code>) functions to apply to each document for adding Elasticsearch metadata.; see <a href=\"#automatic-elasticsearch-metadata-fields\" rel=\"nofollow\">Automatic Elasticsearch Metadata Fields</a> for more details.</li>\n</ul>\n<p>Once initialized, <code>ElasticBuffer</code> exposes two methods, <code>add</code> and <code>flush</code>.\nUse <code>add</code> to add documents to the buffer, noting that all documents in the buffer will be flushed and inserted into Elasticsearch once the number of documents exceeds the buffer's size:</p>\n<pre><code>&gt;&gt;&gt; docs = [\n        {'_index': 'my-index', 'a': 1, 'b': 2.1, 'c': 'xyz'},\n        {'_index': 'my-index', 'a': 3, 'b': 4.1, 'c': 'xyy'},\n        {'_index': 'my-other-index', 'a': 5, 'b': 6.1, 'c': 'zzz'},\n        {'_index': 'my-other-index', 'a': 7, 'b': 8.1, 'c': 'zyx'},\n    ]\n&gt;&gt;&gt; esbuf.add(docs)\n</code></pre>\n<p>Note that all metadata fields required for indexing into Elasticsearch (e.g., <code>_index</code> above) must either be included in each document or added <a href=\"#automatic-elasticsearch-metadata-fields\" rel=\"nofollow\">programmatically</a> via callable kwarg parameters supplied to the <code>ElasticBuffer</code> instance (see below).</p>\n<p>To manually force a buffer flush and insert all documents to Elasticsearch, use the <code>flush</code> method which does not accept any arguments:</p>\n<pre><code>&gt;&gt;&gt; esbuf.flush()\n</code></pre>\n<p>A third method, <code>show()</code>, exists mostly for debug purposes and prints all documents currently in the buffer as newline-delimited json.</p>\n<h3>pandas DataFrames</h3>\n<p>One can directly insert a pandas DataFrame into the buffer and each row will be treated as a document:</p>\n<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame(docs)\n&gt;&gt;&gt; print(df)\n\n           _index  a    b    c\n0        my-index  1  2.1  xyz\n1        my-index  3  4.1  xyy\n2  my-other-index  5  6.1  zzz\n3  my-other-index  7  8.1  zyx\n\n&gt;&gt;&gt; esbuf.add(df)\n</code></pre>\n<p>The DataFrame's index (referring to <code>df.index</code> and <strong>not</strong> the column named <code>_index</code>) is ignored unless it is named, in which case it is added as an ordinary field (column).</p>\n<h3>Context Manager</h3>\n<p><code>ElasticBuffer</code> can also be used as a context manager, offering the advantages of automatically flushing the remaining buffer contents when exiting scope as well as optionally dumping the buffer contents to a file before exiting due to an unhandled exception.</p>\n<pre><code>&gt;&gt;&gt; with ElasticBuffer(size=100, dump_dir='/tmp') as esbuf:\n       for doc in document_stream:\n           doc = process_document(doc)  # some user-defined application-specific processing function\n           esbuf.add(doc)\n</code></pre>\n<h3>Elapsed Time</h3>\n<p>When using <code>ElasticBuffer</code> in a service consuming messages from some external source, it can be important to track how long messages have been waiting in the buffer to be flushed.  In particular, a user may wish to flush, say, every hour to account for the situation where only a trickle of data is coming in and the buffer is not filling up.  <code>ElasticBuffer</code> provides the elapsed time (in seconds) that its oldest message has been in the buffer:</p>\n<pre><code>&gt;&gt;&gt; esbuf.oldest_elapsed_time\n\n5.687833070755005  # the oldest message was inserted ~5.69 seconds ago\n</code></pre>\n<p>This information can be used to periodically check the elapsed time of the oldest message and force a flush if it exceeds a desired threshold.</p>\n<h3>Automatic Elasticsearch Metadata Fields</h3>\n<p>An <code>ElasticBuffer</code> instance can be initialized with kwargs corresponding to callable functions to add <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-fields.html\" rel=\"nofollow\">Elasticsearch metadata</a> fields to each document added to the buffer:</p>\n<pre><code>&gt;&gt;&gt; def my_index_func(doc): return 'my-index'\n&gt;&gt;&gt; def my_id_func(doc): return sum(doc.values())\n\n&gt;&gt;&gt; esbuf = ElasticBuffer(_index=my_index_func, _id=my_id_func)\n\n&gt;&gt;&gt; docs = [\n        {'a': 1, 'b': 2},\n        {'a': 8, 'b': 9},\n    ]\n&gt;&gt;&gt; esbuf.add(docs)\n\n&gt;&gt;&gt; esbuf.show()\n\n{\"a\": 1, \"b\": 2, \"_index\": \"my-index\", \"_id\": 3}\n{\"a\": 8, \"b\": 9, \"_index\": \"my-index\", \"_id\": 17}\n</code></pre>\n<p>Callable kwargs add key/value pairs to each document, where the key corresponds to the name of the kwarg and the value is the function's return value.  Each function must accept one argument (the document as a dict) and return one value.  This also works for DataFrames, as they are transformed to documents (dicts) before applying the supplied metadata functions.</p>\n<p>The key/value pairs are added to the top-level of each document.  Note that the user need not add documents with data nested under a <code>_source</code> key, as metadata fields can be handled at the same level as the data fields.  For further details, see the underlying Elasticsearch client <a href=\"https://elasticsearch-py.readthedocs.io/en/master/helpers.html\" rel=\"nofollow\">bulk insert</a> documentation on handling of metadata fields in flat dicts.</p>\n<h3>Exception Handling</h3>\n<p>For exception handing, <code>ElasticBatch</code> provides the base exception <code>ElasticBatchError</code>:</p>\n<pre><code>&gt;&gt;&gt; from elasticbatch import ElasticBatchError\n</code></pre>\n<p>as well as the more specific <code>ElasticBufferFlushError</code> raised on errors flushing to Elasticsearch:</p>\n<pre><code>&gt;&gt;&gt; from elasticbatch.exceptions import ElasticBufferFlushError\n</code></pre>\n<p>Elasticsearch exception messages can contain a copy of every document related to a failed bulk insertion request.  As such messages can be very large, the <code>verbose_errors</code> flag can be used to optionally truncate the error message.  When <code>ElasticBuffer</code> is initialized with <code>verbose_errors=True</code>, the entirety of the error message is returned.  When <code>verbose_errors=False</code>, a shorter, descriptive message is returned.  In both cases, the full, potentially verbose, exception is available via the <code>err</code> property on the raised <code>ElasticBufferFlushError</code>.</p>\n<h2>Tests</h2>\n<p>To run tests:</p>\n<pre><code>$ python -m unittest discover -v\n</code></pre>\n<p>The awesome <a href=\"https://github.com/CleanCut/green\" rel=\"nofollow\">green</a> package is also highly recommended for running tests and reporting test coverage:</p>\n<pre><code>$ green -vvr\n</code></pre>\n\n          </div>"}, "last_serial": 6329450, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "b10d50cd2d2e1e168a0980b2b10e2fa3", "sha256": "812e8a6d3c02e50339a79cacc45622851d58005697ea65fe0658987dc0ec4c38"}, "downloads": -1, "filename": "elasticbatch-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "b10d50cd2d2e1e168a0980b2b10e2fa3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 14038, "upload_time": "2019-12-19T03:12:03", "upload_time_iso_8601": "2019-12-19T03:12:03.302791Z", "url": "https://files.pythonhosted.org/packages/51/5c/2a3d38415d89b8e32dedfd011a1b49913de0048082bff506e2bd21e99718/elasticbatch-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4b50f2606bc1e48e066fcd5ae69e503c", "sha256": "a69d461d6ab9fb8bd9a8621e8adcb2b7956a23bac75f0eb146bee6bef5672039"}, "downloads": -1, "filename": "elasticbatch-1.0.0.tar.gz", "has_sig": false, "md5_digest": "4b50f2606bc1e48e066fcd5ae69e503c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12363, "upload_time": "2019-12-19T03:12:05", "upload_time_iso_8601": "2019-12-19T03:12:05.998640Z", "url": "https://files.pythonhosted.org/packages/ac/47/2e9888fe3bf5fa15bd430436eb754337660162ba11635dd865ffe626ef02/elasticbatch-1.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b10d50cd2d2e1e168a0980b2b10e2fa3", "sha256": "812e8a6d3c02e50339a79cacc45622851d58005697ea65fe0658987dc0ec4c38"}, "downloads": -1, "filename": "elasticbatch-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "b10d50cd2d2e1e168a0980b2b10e2fa3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 14038, "upload_time": "2019-12-19T03:12:03", "upload_time_iso_8601": "2019-12-19T03:12:03.302791Z", "url": "https://files.pythonhosted.org/packages/51/5c/2a3d38415d89b8e32dedfd011a1b49913de0048082bff506e2bd21e99718/elasticbatch-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4b50f2606bc1e48e066fcd5ae69e503c", "sha256": "a69d461d6ab9fb8bd9a8621e8adcb2b7956a23bac75f0eb146bee6bef5672039"}, "downloads": -1, "filename": "elasticbatch-1.0.0.tar.gz", "has_sig": false, "md5_digest": "4b50f2606bc1e48e066fcd5ae69e503c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12363, "upload_time": "2019-12-19T03:12:05", "upload_time_iso_8601": "2019-12-19T03:12:05.998640Z", "url": "https://files.pythonhosted.org/packages/ac/47/2e9888fe3bf5fa15bd430436eb754337660162ba11635dd865ffe626ef02/elasticbatch-1.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:05 2020"}