{"info": {"author": "Ryan Turner", "author_email": "rdturnermtl@github.com", "bugtrack_url": null, "classifiers": [], "description": "Installation\n============\n\n.. image:: https://api.travis-ci.com/uber/bayesmark.png?token=RSemjpisB7uiZv78DVwd&branch=master\n   :target: https://travis-ci.com/uber/bayesmark\n   :alt:\n\nThis project provides a benchmark framework to easily compare Bayesian optimization methods on real machine learning tasks.\n\nThis project is experimental and the APIs are not considered stable.\n\nThis Bayesian optimization (BO) benchmark framework requires a few easy steps for setup. It can be run either on a local machine (in serial) or prepare a *commands file* to run on a cluster as parallel experiments (dry run mode).\n\nOnly ``Python>=3.6`` is officially supported, but older versions of Python likely work as well.\n\nThe core package itself can be installed with:\n\n.. code-block:: bash\n\n   pip install bayesmark\n\nHowever, to also require installation of all the \"built in\" optimizers for evaluation, run:\n\n.. code-block:: bash\n\n   pip install bayesmark[optimizers]\n\nIt is also possible to use the same pinned dependencies we used in testing by `installing from the repo <#install-in-editable-mode>`_.\n\nBuilding an environment to run the included notebooks can be done with:\n\n.. code-block:: bash\n\n   pip install bayesmark[notebooks]\n\nOr, ``bayesmark[optimizers,notebooks]`` can be used.\n\nA quick example of running the benchmark is `here <#example>`_.\n\nNon-pip dependencies\n--------------------\n\nTo be able to install ``opentuner`` some system level (non-pip) dependencies must be installed. This can be done with:\n\n.. code-block:: bash\n\n   sudo apt-get install libsqlite3-0\n   sudo apt-get install libsqlite3-dev\n\nOn Ubuntu, this results in:\n\n.. code-block:: console\n\n   > dpkg -l | grep libsqlite\n   ii  libsqlite3-0:amd64    3.11.0-1ubuntu1  amd64  SQLite 3 shared library\n   ii  libsqlite3-dev:amd64  3.11.0-1ubuntu1  amd64  SQLite 3 development files\n\nThe environment should now all be setup to run the BO benchmark.\n\nRunning\n=======\n\nNow we can run each step of the experiments. First, we run all combinations and then run some quick commands to analyze the output.\n\nLaunch the experiments\n----------------------\n\nThe experiments are run using the experiment launcher, which has the following interface:\n\n.. code-block::\n\n   usage: bayesmark-launch [-h] [-dir DB_ROOT] [-odir OPTIMIZER_ROOT] [-v] [-u UUID]\n                     [-dr DATA_ROOT] [-b DB] [-o OPTIMIZER [OPTIMIZER ...]]\n                     [-d DATA [DATA ...]]\n                     [-c [{DT,MLP-adam,MLP-sgd,RF,SVM,ada,kNN,lasso,linear} ...]]\n                     [-m [{acc,mae,mse,nll} ...]] [-n N_CALLS]\n                     [-p N_SUGGEST] [-r N_REPEAT] [-nj N_JOBS] [-ofile JOBS_FILE]\n\nThe arguments are:\n\n.. code-block::\n\n     -h, --help            show this help message and exit\n     -dir DB_ROOT, -db-root DB_ROOT\n                           root directory for all benchmark experiments output\n     -odir OPTIMIZER_ROOT, --opt-root OPTIMIZER_ROOT\n                           Directory with optimization wrappers\n     -v, --verbose         print the study logs to console\n     -u UUID, --uuid UUID  length 32 hex UUID for this experiment\n     -dr DATA_ROOT, --data-root DATA_ROOT\n                           root directory for all custom csv files\n     -b DB, --db DB        database ID of this benchmark experiment\n     -o OPTIMIZER [OPTIMIZER ...], --opt OPTIMIZER [OPTIMIZER ...]\n                           optimizers to use\n     -d DATA [DATA ...], --data DATA [DATA ...]\n                           data sets to use\n     -c, --classifier [{DT,MLP-adam,MLP-sgd,RF,SVM,ada,kNN,lasso,linear} ...]\n                           classifiers to use\n     -m, --metric [{acc,mae,mse,nll} ...]\n                           scoring metric to use\n     -n N_CALLS, --calls N_CALLS\n                           number of function evaluations\n     -p N_SUGGEST, --suggestions N_SUGGEST\n                           number of suggestions to provide in parallel\n     -r N_REPEAT, --repeat N_REPEAT\n                           number of repetitions of each study\n     -nj N_JOBS, --num-jobs N_JOBS\n                           number of jobs to put in the dry run file, the default\n                           0 value disables dry run (real run)\n     -ofile JOBS_FILE, --jobs-file JOBS_FILE\n                           a jobs file with all commands to be run\n\nThe output files will be placed in ``[DB_ROOT]/[DBID]``. If ``DBID`` is not specified, it will be a randomly created subdirectory with a new name to avoid overwriting previous experiments. The path to ``DBID`` is shown at the beginning of ``stdout`` when running ``bayesmark-launch``. In general, let the launcher create and setup ``DBID`` unless you are appending to a previous experiment, in which case, specify the existing ``DBID``.\n\nThe launcher's sequence of commands can be accessed programmatically via `.experiment_launcher.gen_commands`. The individual experiments can be launched programmatically via `.experiment.run_sklearn_study`.\n\nSelecting the experiments\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nA list of optimizers, classifiers, data sets, and metrics can be listed using the ``-o``/``-c``/``-d``/``-m`` commands, respectively. If not specified, the program launches all possible options.\n\nSelecting the optimizer\n^^^^^^^^^^^^^^^^^^^^^^^\n\nA few different open source optimizers have been included as an example and are considered the \"built-in\" optimizers. The original repos are shown in the `Links <#links>`_.\n\nThe data argument ``-o`` allows a list containing the \"built-in\" optimizers:\n\n.. code-block::\n\n   \"HyperOpt\", \"Nevergrad-OnePlusOne\", \"OpenTuner-BanditA\", \"OpenTuner-GA\", \"OpenTuner-GA-DE\", \"PySOT\", \"RandomSearch\", \"Scikit-GBRT-Hedge\", \"Scikit-GP-Hedge\", \"Scikit-GP-LCB\"\n\nor, one can specify a user-defined optimizer. The class containing an optimizer conforming to the API must be found in in the folder specified by ``--opt-root``. Additionally, a configuration defining each optimizer must be defined in ``[OPT_ROOT]/config.json``. The ``--opt-root`` and ``config.json`` may be omitted if only built-in optimizers are used.\n\nAdditional details for providing a new optimizer are found in `adding a new optimizer <#adding-a-new-optimizer>`_.\n\nSelecting the data set\n^^^^^^^^^^^^^^^^^^^^^^\n\nBy default, this benchmark uses the `sklearn example data sets <https://scikit-learn.org/stable/datasets/index.html#toy-datasets>`_ as the \"built-in\" data sets for use in ML model tuning problems.\n\nThe data argument ``-d`` allows a list containing the \"built-in\" data sets:\n\n.. code-block::\n\n   \"breast\", \"digits\", \"iris\", \"wine\", \"boston\", \"diabetes\"\n\nor, it can refer to a custom ``csv`` file, which is the name of file in the folder specified by ``--data-root``. It also follows the convention that regression data sets start with ``reg-`` and classification data sets start with ``clf-``. For example, the classification data set in ``[DATA_ROOT]/clf-foo.csv`` is specified with ``-d clf-foo``.\n\nThe ``csv`` file can be anything readable by pandas, but we assume the final column is the target and all other columns are features. The target column should be integer for classification data and float for regression. The features should float (or ``str`` for categorical variable columns). See ``bayesmark.data.load_data`` for more information.\n\nDry run for cluster jobs\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nIt is also possible to do a \"dry run\" of the launcher by specifying a value for ``--num-jobs`` greater than zero. For example, if ``--num-jobs 50`` is provided, a text file listing 50 commands to run is produced, with one command (job) per line. This is useful when preparing a list of commands to run later on a cluster.\n\nA dry run will generate a command file (e.g., ``jobs.txt``) like the following (with a meta-data header). Each line corresponds to a command that can be used as a job on a different worker:\n\n.. code-block::\n\n   # running: {'--uuid': None, '-db-root': '/foo', '--opt-root': '/example_opt_root', '--data-root': None, '--db': 'bo_example_folder', '--opt': ['RandomSearch', 'PySOT'], '--data': None, '--classifier': ['SVM', 'DT'], '--metric': None, '--calls': 15, '--suggestions': 1, '--repeat': 3, '--num-jobs': 50, '--jobs-file': '/jobs.txt', '--verbose': False, 'dry_run': True, 'rev': '9a14ef2', 'opt_rev': None}\n   # cmd: python bayesmark-launch -n 15 -r 3 -dir foo -o RandomSearch PySOT -c SVM DT -nj 50 -b bo_example_folder\n   job_e2b63a9_00 bayesmark-exp -c SVM -d diabetes -o PySOT -u 079a155f03095d2ba414a5d2cedde08c -m mse -n 15 -p 1 -dir foo -b bo_example_folder && bayesmark-exp -c SVM -d boston -o RandomSearch -u 400e4c0be8295ad59db22d9b5f31d153 -m mse -n 15 -p 1 -dir foo -b bo_example_folder && bayesmark-exp -c SVM -d digits -o RandomSearch -u fe73a2aa960a5e3f8d78bfc4bcf51428 -m acc -n 15 -p 1 -dir foo -b bo_example_folder\n   job_e2b63a9_01 bayesmark-exp -c DT -d diabetes -o PySOT -u db1d9297948554e096006c172a0486fb -m mse -n 15 -p 1 -dir foo -b bo_example_folder && bayesmark-exp -c SVM -d boston -o RandomSearch -u 7148f690ed6a543890639cc59db8320b -m mse -n 15 -p 1 -dir foo -b bo_example_folder && bayesmark-exp -c SVM -d breast -o PySOT -u 72c104ba1b6d5bb8a546b0064a7c52b1 -m nll -n 15 -p 1 -dir foo -b bo_example_folder\n   job_e2b63a9_02 bayesmark-exp -c SVM -d iris -o PySOT -u cc63b2c1e4315a9aac0f5f7b496bfb0f -m nll -n 15 -p 1 -dir foo -b bo_example_folder && bayesmark-exp -c DT -d breast -o RandomSearch -u aec62e1c8b5552e6b12836f0c59c1681 -m nll -n 15 -p 1 -dir foo -b bo_example_folder && bayesmark-exp -c DT -d digits -o RandomSearch -u 4d0a175d56105b6bb3055c3b62937b2d -m acc -n 15 -p 1 -dir foo -b bo_example_folder\n   ...\n\nThis package does not have built in support for deploying these jobs on a cluster or cloud environment (.e.g., AWS).\n\nThe UUID argument\n^^^^^^^^^^^^^^^^^\n\nThe ``UUID`` is a 32-char hex string used as a master random seed which we use to draw random seeds for the experiments. If ``UUID`` is not specified a version 4 UUID is generated. The used UUID is displayed at the beginning of ``stdout``. In general, the ``UUID`` should not specified/re-used except for debugging because it violates the assumption that the experiment UUIDs are unique.\n\nAggregate results\n-----------------\n\nNext to aggregate all the experiment files into combined (json) files we need to run the aggregation command:\n\n.. code-block::\n\n   usage: bayesmark-agg [-h] [-dir DB_ROOT] [-odir OPTIMIZER_ROOT] [-v] -b DB [-rv]\n\nThe arguments are:\n\n.. code-block::\n\n     -h, --help            show this help message and exit\n     -dir DB_ROOT, -db-root DB_ROOT\n                           root directory for all benchmark experiments output\n     -odir OPTIMIZER_ROOT, --opt-root OPTIMIZER_ROOT\n                           Directory with optimization wrappers\n     -v, --verbose         print the study logs to console\n     -b DB, --db DB        database ID of this benchmark experiment\n     -rv, --ravel          ravel all studies to store batch suggestions as if\n                           they were serial\n\nThe ``DB_ROOT`` must match the folder from the launcher ``bayesmark-launch``, and ``DBID`` must match that displayed from the launcher as well. The aggregate files are found in ``[DB_ROOT]/[DBID]/derived``.\n\nThe result aggregation can be done programmatically via `.experiment_aggregate.concat_experiments`.\n\nAnalyze and summarize results\n-----------------------------\n\nFinally, to run a statistical analysis presenting a summary of the experiments we run\n\n.. code-block::\n\n   usage: bayesmark-anal [-h] [-dir DB_ROOT] [-odir OPTIMIZER_ROOT] [-v] -b DB\n\nThe arguments are:\n\n.. code-block::\n\n     -h, --help            show this help message and exit\n     -dir DB_ROOT, -db-root DB_ROOT\n                           root directory for all benchmark experiments output\n     -odir OPTIMIZER_ROOT, --opt-root OPTIMIZER_ROOT\n                           Directory with optimization wrappers\n     -v, --verbose         print the study logs to console\n     -b DB, --db DB        database ID of this benchmark experiment\n\nThe ``DB_ROOT`` must match the folder from the launcher ``bayesmark-launch``, and ``DBID`` must match that displayed from the launcher as well. The aggregate files are found in ``[DB_ROOT]/[DBID]/derived``.\n\nThe ``bayesmark-anal`` command looks for a ``baseline.json`` file in ``[DB_ROOT]/[DBID]/derived``, which states the best possible and random search performance. If no such file is present, ``bayesmark-anal`` automatically calls ``bayesmark-baseline`` to build it. The baselines are inferred from the random search performance in the logs. The baseline values are considered fixed (not random) quantities when ``bayesmark-anal`` builds confidence intervals. Therefore, we allow the user to leave them fixed and do not rebuild them when ``bayesmark-anal`` is called if a baselines file is already present.\n\nThe result analysis can be done programmatically via `.experiment_analysis.compute_aggregates`, and the baseline computation via `.experiment_baseline.compute_baseline`.\n\nSee `how-scoring-works` for more information on how the scores are computed and aggregated.\n\nExample\n-------\n\nAfter finishing the setup (environment) a small-scale serial can be run as follows:\n\n.. code-block:: console\n\n   > # setup\n   > DB_ROOT=./notebooks  # path/to/where/you/put/results\n   > DBID=bo_example_folder\n   > mkdir $DB_ROOT\n   > # experiments\n   > bayesmark-launch -n 15 -r 3 -dir $DB_ROOT -b $DBID -o RandomSearch PySOT -c SVM DT -v\n   Supply --uuid 3adc3182635e44ea96969d267591f034 to reproduce this run.\n   Supply --dbid bo_example_folder to append to this experiment or reproduce jobs file.\n   User must ensure equal reps of each optimizer for unbiased results\n   -c DT -d boston -o PySOT -u a1b287b450385ad09b2abd7582f404a2 -m mae -n 15 -p 1 -dir /notebooks -b bo_example_folder\n   -c DT -d boston -o PySOT -u 63746599ae3f5111a96942d930ba1898 -m mse -n 15 -p 1 -dir /notebooks -b bo_example_folder\n   -c DT -d boston -o RandomSearch -u 8ba16c880ef45b27ba0909199ab7aa8a -m mae -n 15 -p 1 -dir /notebooks -b bo_example_folder\n   ...\n   0 failures of benchmark script after 144 studies.\n   done\n   > # aggregate\n   > bayesmark-agg -dir $DB_ROOT -b $DBID\n   > # analyze\n   > bayesmark-anal -dir $DB_ROOT -b $DBID -v\n   ...\n   median score @ 15:\n   optimizer\n   PySOT_0.2.3_9b766b6           0.330404\n   RandomSearch_0.0.1_9b766b6    0.961829\n   mean score @ 15:\n   optimizer\n   PySOT_0.2.3_9b766b6           0.124262\n   RandomSearch_0.0.1_9b766b6    0.256422\n   normed mean score @ 15:\n   optimizer\n   PySOT_0.2.3_9b766b6           0.475775\n   RandomSearch_0.0.1_9b766b6    0.981787\n   done\n\nThe aggregate result files (i.e., ``summary.json``) will now be available in ``$DB_ROOT/$DBID/derived``. However, this will be high variance since it was from only 3 trials and only to 15 function evaluations.\n\nPlotting and notebooks\n----------------------\n\nPlotting the quantitative results found in ``$DB_ROOT/$DBID/derived`` can be done using the notebooks found in the ``notebooks/`` folder of the git repository. The notebook ``plot_mean_score.ipynb`` generates plots for aggregate scores averaging over all problems. The notebook ``plot_test_case.ipynb`` generates plots for each test problem.\n\nTo use the notebooks, first copy over the ``notebooks/`` folder from git repository.\n\nTo setup the kernel for running the notebooks use:\n\n.. code-block:: bash\n\n   virtualenv bobm_ipynb --python=python3.6\n   source ./bobm_ipynb/bin/activate\n   pip install bayesmark[notebooks]\n   python -m ipykernel install --name=bobm_ipynb --user\n\nNow, the notebooks for plotting can be run with the command ``jupyter notebook`` and selecting the kernel ``bobm_ipynb``.\n\nIt is also possible to convert the notebooks to an HTML report at the command line using ``nbconvert``. For example, use the command:\n\n.. code-block:: bash\n\n   jupyter nbconvert --to html --execute notebooks/plot_mean_score.ipynb\n\nThe output file will be in ``./notebooks/plot_mean_score.html``. See the ``nbconvert`` `documentation page <https://nbconvert.readthedocs.io/en/latest/usage.html#supported-output-formats>`_ for more output formats. By default, the notebooks look in ``./notebooks/bo_example_folder/`` for the ``summary.json`` from ``bayesmark-anal``.\n\nTo run ``plot_test_case.ipynb`` use the command:\n\n.. code-block:: bash\n\n   jupyter nbconvert --to html --execute notebooks/plot_test_case.ipynb --ExecutePreprocessor.timeout=600\n\nThe ``--ExecutePreprocessor.timeout=600`` timeout increase is needed due to the large number of plots being generated. The output will be in ``./notebooks/plot_test_case.html``.\n\nAdding a new optimizer\n======================\n\nAll optimizers in this benchmark are required to follow the interface specified of the ``AbstractOptimizer`` class in ``bayesmark.abstract_optimizer``. In general, this requires creating a wrapper class around the new optimizer. The wrapper classes must all be placed in a folder referred to by the ``--opt-root`` argument. This folder must also contain the ``config.json`` folder.\n\nThe interface is simple, one must merely implement the ``suggest`` and ``observe`` functions. The ``suggest`` function generates new guesses for evaluating the function. Once evaluated, the function evaluations are passed to the ``observe`` function. The objective function is *not* evaluated by the optimizer class. The objective function is evaluated on outside and results are passed to ``observe``. This is the correct setup for Bayesian optimization because:\n\n* We can observe/try inputs that were never suggested\n* We can ignore suggestions\n* The objective function may not be something as simple as a Python function\n\nSo passing the function as an argument as is done in ``scipy.optimization`` is artificially restrictive.\n\nThe implementation of the wrapper will look like the following:\n\n.. code-block:: python\n\n   from bayesmark.abstract_optimizer import AbstractOptimizer\n   from bayesmark.experiment import experiment_main\n\n\n   class NewOptimizerName(AbstractOptimizer):\n       # Used for determining the version number of package used\n       primary_import = \"name of import used e.g, opentuner\"\n\n       def __init__(self, api_config, optional_arg_foo=None, optional_arg_bar=None):\n           \"\"\"Build wrapper class to use optimizer in benchmark.\n\n           Parameters\n           ----------\n           api_config : dict-like of dict-like\n               Configuration of the optimization variables. See API description.\n           \"\"\"\n           AbstractOptimizer.__init__(self, api_config)\n           # Do whatever other setup is needed\n           # ...\n\n       def suggest(self, n_suggestions=1):\n           \"\"\"Get suggestion from the optimizer.\n\n           Parameters\n           ----------\n           n_suggestions : int\n               Desired number of parallel suggestions in the output\n\n           Returns\n           -------\n           next_guess : list of dict\n               List of `n_suggestions` suggestions to evaluate the objective\n               function. Each suggestion is a dictionary where each key\n               corresponds to a parameter being optimized.\n           \"\"\"\n           # Do whatever is needed to get the parallel guesses\n           # ...\n           return x_guess\n\n       def observe(self, X, y):\n           \"\"\"Feed an observation back.\n\n           Parameters\n           ----------\n           X : list of dict-like\n               Places where the objective function has already been evaluated.\n               Each suggestion is a dictionary where each key corresponds to a\n               parameter being optimized.\n           y : array-like, shape (n,)\n               Corresponding values where objective has been evaluated\n           \"\"\"\n           # Update the model with new objective function observations\n           # ...\n           # No return statement needed\n\n\n   if __name__ == \"__main__\":\n       # This is the entry point for experiments, so pass the class to experiment_main to use this optimizer.\n       # This statement must be included in the wrapper class file:\n       experiment_main(NewOptimizerName)\n\nDepending on the API of the optimizer being wrapped, building this wrapper class may only or require a few lines of code, or be a total pain.\n\nThe config file\n---------------\n\nEach optimizer wrapper can have multiple configurations, which is each referred to as a different optimizer in the benchmark. For example, the JSON config file will have entries as follows:\n\n.. code-block:: json\n\n   {\n       \"OpenTuner-BanditA-New\": [\n           \"opentuner_optimizer.py\",\n           {\"techniques\": [\"AUCBanditMetaTechniqueA\"]}\n       ],\n       \"OpenTuner-GA-DE-New\": [\n           \"opentuner_optimizer.py\",\n           {\"techniques\": [\"PSO_GA_DE\"]}\n       ],\n       \"OpenTuner-GA-New\": [\n           \"opentuner_optimizer.py\",\n           {\"techniques\": [\"PSO_GA_Bandit\"]}\n       ]\n   }\n\nBasically, the entries are ``\"name_of_strategy\": [\"file_with_class\", {kwargs_for_the_constructor}]``. Here, ``OpenTuner-BanditA``, ``OpenTuner-GA-DE``, and ``OpenTuner-GA`` are all treated as different optimizers by the benchmark even though the all use the same class from ``opentuner_optimizer.py``.\n\nThis ``config.json`` must be in the same folder as the optimizer classes (e.g., ``opentuner_optimizer.py``).\n\nRunning with a new optimizer\n----------------------------\n\nTo run the benchmarks using a new optimizer, simply provide its name (from ``config.json``) in the ``-o`` list. The ``--opt-root`` argument must be specified in this case. For example, the launch command from the `example <#example>`_ becomes:\n\n.. code-block:: bash\n\n   bayesmark-launch -n 15 -r 3 -dir $DB_ROOT -b $DBID -o RandomSearch PySOT-New -c SVM DT --opt-root ./example_opt_root -v\n\nHere, we are using the example ``PySOT-New`` wrapper from the ``example_opt_root`` folder in the git repo. It is equivalent to the builtin ``PySOT``, but gives an example of how to provide a new custom optimizer.\n\nContributing\n============\n\nThe following instructions have been tested with Python 3.6.8 on Ubuntu (16.04.5 LTS).\n\nInstall in editable mode\n------------------------\n\nFirst, define the variables for the paths we will use:\n\n.. code-block:: bash\n\n   GIT=/path/to/where/you/put/repos\n   ENVS=/path/to/where/you/put/virtualenvs\n\nThen clone the repo in your git directory ``$GIT``:\n\n.. code-block:: bash\n\n   cd $GIT\n   git clone https://github.com/uber/bayesmark.git\n\nInside your virtual environments folder ``$ENVS``, make the environment:\n\n.. code-block:: bash\n\n   cd $ENVS\n   virtualenv bayesmark --python=python3.6\n   source $ENVS/bayesmark/bin/activate\n\nNow we can install the pip dependencies. Move back into your git directory and run\n\n.. code-block:: bash\n\n   cd $GIT/bayesmark\n   pip install -r requirements/base.txt\n   pip install -r requirements/optimizers.txt\n   pip install -e .  # Install the benchmark itself\n\nYou may want to run ``pip install -U pip`` first if you have an old version of ``pip``. The file ``optimizers.txt`` contains the dependencies for all the optimizers used in the benchmark. The analysis and aggregation programs can be run using only the requirements in ``base.txt``.\n\nContributor tools\n-----------------\n\nFirst, we need to setup some needed tools:\n\n.. code-block:: bash\n\n   cd $ENVS\n   virtualenv bayesmark_tools --python=python3.6\n   source $ENVS/bayesmark_tools/bin/activate\n   pip install -r $GIT/bayesmark/requirements/tools.txt\n\nTo install the pre-commit hooks for contributing run (in the ``bayesmark_tools`` environment):\n\n.. code-block:: bash\n\n   cd $GIT/bayesmark\n   pre-commit install\n\nTo rebuild the requirements, we can run:\n\n.. code-block:: bash\n\n   cd $GIT/bayesmark\n   # Get py files from notebooks to analyze\n   jupyter nbconvert --to script notebooks/*.ipynb\n   # Generate the .in files (but pins to latest, which we might not want)\n   pipreqs bayesmark/ --ignore bayesmark/builtin_opt/ --savepath requirements/base.in\n   pipreqs test/ --savepath requirements/test.in\n   pipreqs bayesmark/builtin_opt/ --savepath requirements/optimizers.in\n   pipreqs notebooks/ --savepath requirements/ipynb.in\n   pipreqs docs/ --savepath requirements/docs.in\n   # Regenerate the .txt files from .in files\n   pip-compile-multi --no-upgrade\n\nGenerating the documentation\n----------------------------\n\nFirst setup the environment for building with ``Sphinx``:\n\n.. code-block:: bash\n\n   cd $ENVS\n   virtualenv bayesmark_docs --python=python3.6\n   source $ENVS/bayesmark_docs/bin/activate\n   pip install -r $GIT/bayesmark/requirements/docs.txt\n\nThen we can do the build:\n\n.. code-block:: bash\n\n   cd $GIT/bayesmark/docs\n   make all\n   open _build/html/index.html\n\nDocumentation will be available in all formats in ``Makefile``. Use ``make html`` to only generate the HTML documentation.\n\nRunning the tests\n-----------------\n\nThe tests for this package can be run with:\n\n.. code-block:: bash\n\n   cd $GIT/bayesmark\n   ./test.sh\n\nThe script creates a conda environment using the requirements found in ``requirements/test.txt``.\n\nThe ``test.sh`` script *must* be run from a *clean* git repo.\n\nOr if we only want to run the unit tests and not check the adequacy of the requirements files, one can use\n\n.. code-block:: bash\n\n   # Setup environment\n   cd $ENVS\n   virtualenv bayesmark_test --python=python3.6\n   source $ENVS/bayesmark_test/bin/activate\n   pip install -r $GIT/bayesmark/requirements/test.txt\n   pip install -e $GIT/bayesmark\n   # Now run tests\n   cd $GIT/bayesmark/\n   pytest test/ -s -v --hypothesis-seed=0 --disable-pytest-warnings --cov=bayesmark --cov-report html\n\nA code coverage report will also be produced in ``$GIT/bayesmark/htmlcov/index.html``.\n\nDeployment\n----------\n\nThe wheel (tar ball) for deployment as a pip installable package can be built using the script:\n\n.. code-block:: bash\n\n   cd $GIT/bayesmark/\n   ./build_wheel.sh\n\nLinks\n=====\n\nThe `source <https://github.com/uber/bayesmark>`_ is hosted on GitHub.\n\nThe `documentation <https://bayesmark.readthedocs.io/en/latest/>`_ is hosted at Read the Docs.\n\nInstallable from `PyPI <https://pypi.org/project/bayesmark/>`_.\n\nThe builtin optimizers are wrappers on the following projects:\n\n* `HyperOpt <https://github.com/hyperopt/hyperopt>`_\n* `Nevergrad <https://github.com/facebookresearch/nevergrad>`_\n* `OpenTuner <https://github.com/jansel/opentuner>`_\n* `PySOT <https://github.com/dme65/pySOT>`_\n* `Scikit-optimize <https://github.com/scikit-optimize/scikit-optimize>`_\n\nLicense\n=======\n\nThis project is licensed under the Apache 2 License - see the LICENSE file for details.", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/uber/bayesmark/", "keywords": "", "license": "Apache v2", "maintainer": "", "maintainer_email": "", "name": "bayesmark", "package_url": "https://pypi.org/project/bayesmark/", "platform": "any", "project_url": "https://pypi.org/project/bayesmark/", "project_urls": {"Homepage": "https://github.com/uber/bayesmark/"}, "release_url": "https://pypi.org/project/bayesmark/0.0.4/", "requires_dist": null, "requires_python": "", "summary": "Bayesian optimization benchmark system", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"installation\">\n<h2>Installation</h2>\n<a href=\"https://travis-ci.com/uber/bayesmark\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/45780ca0dae5f81e73e8198481dd89eae27e9a06/68747470733a2f2f6170692e7472617669732d63692e636f6d2f756265722f62617965736d61726b2e706e673f746f6b656e3d5253656d6a706973423775695a76373844567764266272616e63683d6d6173746572\"></a>\n<p>This project provides a benchmark framework to easily compare Bayesian optimization methods on real machine learning tasks.</p>\n<p>This project is experimental and the APIs are not considered stable.</p>\n<p>This Bayesian optimization (BO) benchmark framework requires a few easy steps for setup. It can be run either on a local machine (in serial) or prepare a <em>commands file</em> to run on a cluster as parallel experiments (dry run mode).</p>\n<p>Only <tt><span class=\"pre\">Python&gt;=3.6</span></tt> is officially supported, but older versions of Python likely work as well.</p>\n<p>The core package itself can be installed with:</p>\n<pre>pip install bayesmark\n</pre>\n<p>However, to also require installation of all the \u201cbuilt in\u201d optimizers for evaluation, run:</p>\n<pre>pip install bayesmark<span class=\"o\">[</span>optimizers<span class=\"o\">]</span>\n</pre>\n<p>It is also possible to use the same pinned dependencies we used in testing by <a href=\"#install-in-editable-mode\" rel=\"nofollow\">installing from the repo</a>.</p>\n<p>Building an environment to run the included notebooks can be done with:</p>\n<pre>pip install bayesmark<span class=\"o\">[</span>notebooks<span class=\"o\">]</span>\n</pre>\n<p>Or, <tt>bayesmark[optimizers,notebooks]</tt> can be used.</p>\n<p>A quick example of running the benchmark is <a href=\"#example\" rel=\"nofollow\">here</a>.</p>\n<div id=\"non-pip-dependencies\">\n<h3>Non-pip dependencies</h3>\n<p>To be able to install <tt>opentuner</tt> some system level (non-pip) dependencies must be installed. This can be done with:</p>\n<pre>sudo apt-get install libsqlite3-0\nsudo apt-get install libsqlite3-dev\n</pre>\n<p>On Ubuntu, this results in:</p>\n<pre><span class=\"gp\">&gt;</span> dpkg -l <span class=\"p\">|</span> grep libsqlite\n<span class=\"go\">ii  libsqlite3-0:amd64    3.11.0-1ubuntu1  amd64  SQLite 3 shared library\nii  libsqlite3-dev:amd64  3.11.0-1ubuntu1  amd64  SQLite 3 development files</span>\n</pre>\n<p>The environment should now all be setup to run the BO benchmark.</p>\n</div>\n</div>\n<div id=\"running\">\n<h2>Running</h2>\n<p>Now we can run each step of the experiments. First, we run all combinations and then run some quick commands to analyze the output.</p>\n<div id=\"launch-the-experiments\">\n<h3>Launch the experiments</h3>\n<p>The experiments are run using the experiment launcher, which has the following interface:</p>\n<pre>usage: bayesmark-launch [-h] [-dir DB_ROOT] [-odir OPTIMIZER_ROOT] [-v] [-u UUID]\n                  [-dr DATA_ROOT] [-b DB] [-o OPTIMIZER [OPTIMIZER ...]]\n                  [-d DATA [DATA ...]]\n                  [-c [{DT,MLP-adam,MLP-sgd,RF,SVM,ada,kNN,lasso,linear} ...]]\n                  [-m [{acc,mae,mse,nll} ...]] [-n N_CALLS]\n                  [-p N_SUGGEST] [-r N_REPEAT] [-nj N_JOBS] [-ofile JOBS_FILE]\n</pre>\n<p>The arguments are:</p>\n<pre>-h, --help            show this help message and exit\n-dir DB_ROOT, -db-root DB_ROOT\n                      root directory for all benchmark experiments output\n-odir OPTIMIZER_ROOT, --opt-root OPTIMIZER_ROOT\n                      Directory with optimization wrappers\n-v, --verbose         print the study logs to console\n-u UUID, --uuid UUID  length 32 hex UUID for this experiment\n-dr DATA_ROOT, --data-root DATA_ROOT\n                      root directory for all custom csv files\n-b DB, --db DB        database ID of this benchmark experiment\n-o OPTIMIZER [OPTIMIZER ...], --opt OPTIMIZER [OPTIMIZER ...]\n                      optimizers to use\n-d DATA [DATA ...], --data DATA [DATA ...]\n                      data sets to use\n-c, --classifier [{DT,MLP-adam,MLP-sgd,RF,SVM,ada,kNN,lasso,linear} ...]\n                      classifiers to use\n-m, --metric [{acc,mae,mse,nll} ...]\n                      scoring metric to use\n-n N_CALLS, --calls N_CALLS\n                      number of function evaluations\n-p N_SUGGEST, --suggestions N_SUGGEST\n                      number of suggestions to provide in parallel\n-r N_REPEAT, --repeat N_REPEAT\n                      number of repetitions of each study\n-nj N_JOBS, --num-jobs N_JOBS\n                      number of jobs to put in the dry run file, the default\n                      0 value disables dry run (real run)\n-ofile JOBS_FILE, --jobs-file JOBS_FILE\n                      a jobs file with all commands to be run\n</pre>\n<p>The output files will be placed in <tt><span class=\"pre\">[DB_ROOT]/[DBID]</span></tt>. If <tt>DBID</tt> is not specified, it will be a randomly created subdirectory with a new name to avoid overwriting previous experiments. The path to <tt>DBID</tt> is shown at the beginning of <tt>stdout</tt> when running <tt><span class=\"pre\">bayesmark-launch</span></tt>. In general, let the launcher create and setup <tt>DBID</tt> unless you are appending to a previous experiment, in which case, specify the existing <tt>DBID</tt>.</p>\n<p>The launcher\u2019s sequence of commands can be accessed programmatically via <cite>.experiment_launcher.gen_commands</cite>. The individual experiments can be launched programmatically via <cite>.experiment.run_sklearn_study</cite>.</p>\n<div id=\"selecting-the-experiments\">\n<h4>Selecting the experiments</h4>\n<p>A list of optimizers, classifiers, data sets, and metrics can be listed using the <tt><span class=\"pre\">-o</span></tt>/<tt><span class=\"pre\">-c</span></tt>/<tt><span class=\"pre\">-d</span></tt>/<tt><span class=\"pre\">-m</span></tt> commands, respectively. If not specified, the program launches all possible options.</p>\n</div>\n<div id=\"selecting-the-optimizer\">\n<h4>Selecting the optimizer</h4>\n<p>A few different open source optimizers have been included as an example and are considered the \u201cbuilt-in\u201d optimizers. The original repos are shown in the <a href=\"#links\" rel=\"nofollow\">Links</a>.</p>\n<p>The data argument <tt><span class=\"pre\">-o</span></tt> allows a list containing the \u201cbuilt-in\u201d optimizers:</p>\n<pre>\"HyperOpt\", \"Nevergrad-OnePlusOne\", \"OpenTuner-BanditA\", \"OpenTuner-GA\", \"OpenTuner-GA-DE\", \"PySOT\", \"RandomSearch\", \"Scikit-GBRT-Hedge\", \"Scikit-GP-Hedge\", \"Scikit-GP-LCB\"\n</pre>\n<p>or, one can specify a user-defined optimizer. The class containing an optimizer conforming to the API must be found in in the folder specified by <tt><span class=\"pre\">--opt-root</span></tt>. Additionally, a configuration defining each optimizer must be defined in <tt><span class=\"pre\">[OPT_ROOT]/config.json</span></tt>. The <tt><span class=\"pre\">--opt-root</span></tt> and <tt>config.json</tt> may be omitted if only built-in optimizers are used.</p>\n<p>Additional details for providing a new optimizer are found in <a href=\"#adding-a-new-optimizer\" rel=\"nofollow\">adding a new optimizer</a>.</p>\n</div>\n<div id=\"selecting-the-data-set\">\n<h4>Selecting the data set</h4>\n<p>By default, this benchmark uses the <a href=\"https://scikit-learn.org/stable/datasets/index.html#toy-datasets\" rel=\"nofollow\">sklearn example data sets</a> as the \u201cbuilt-in\u201d data sets for use in ML model tuning problems.</p>\n<p>The data argument <tt><span class=\"pre\">-d</span></tt> allows a list containing the \u201cbuilt-in\u201d data sets:</p>\n<pre>\"breast\", \"digits\", \"iris\", \"wine\", \"boston\", \"diabetes\"\n</pre>\n<p>or, it can refer to a custom <tt>csv</tt> file, which is the name of file in the folder specified by <tt><span class=\"pre\">--data-root</span></tt>. It also follows the convention that regression data sets start with <tt>reg-</tt> and classification data sets start with <tt>clf-</tt>. For example, the classification data set in <tt><span class=\"pre\">[DATA_ROOT]/clf-foo.csv</span></tt> is specified with <tt><span class=\"pre\">-d</span> <span class=\"pre\">clf-foo</span></tt>.</p>\n<p>The <tt>csv</tt> file can be anything readable by pandas, but we assume the final column is the target and all other columns are features. The target column should be integer for classification data and float for regression. The features should float (or <tt>str</tt> for categorical variable columns). See <tt>bayesmark.data.load_data</tt> for more information.</p>\n</div>\n<div id=\"dry-run-for-cluster-jobs\">\n<h4>Dry run for cluster jobs</h4>\n<p>It is also possible to do a \u201cdry run\u201d of the launcher by specifying a value for <tt><span class=\"pre\">--num-jobs</span></tt> greater than zero. For example, if <tt><span class=\"pre\">--num-jobs</span> 50</tt> is provided, a text file listing 50 commands to run is produced, with one command (job) per line. This is useful when preparing a list of commands to run later on a cluster.</p>\n<p>A dry run will generate a command file (e.g., <tt>jobs.txt</tt>) like the following (with a meta-data header). Each line corresponds to a command that can be used as a job on a different worker:</p>\n<pre># running: {'--uuid': None, '-db-root': '/foo', '--opt-root': '/example_opt_root', '--data-root': None, '--db': 'bo_example_folder', '--opt': ['RandomSearch', 'PySOT'], '--data': None, '--classifier': ['SVM', 'DT'], '--metric': None, '--calls': 15, '--suggestions': 1, '--repeat': 3, '--num-jobs': 50, '--jobs-file': '/jobs.txt', '--verbose': False, 'dry_run': True, 'rev': '9a14ef2', 'opt_rev': None}\n# cmd: python bayesmark-launch -n 15 -r 3 -dir foo -o RandomSearch PySOT -c SVM DT -nj 50 -b bo_example_folder\njob_e2b63a9_00 bayesmark-exp -c SVM -d diabetes -o PySOT -u 079a155f03095d2ba414a5d2cedde08c -m mse -n 15 -p 1 -dir foo -b bo_example_folder &amp;&amp; bayesmark-exp -c SVM -d boston -o RandomSearch -u 400e4c0be8295ad59db22d9b5f31d153 -m mse -n 15 -p 1 -dir foo -b bo_example_folder &amp;&amp; bayesmark-exp -c SVM -d digits -o RandomSearch -u fe73a2aa960a5e3f8d78bfc4bcf51428 -m acc -n 15 -p 1 -dir foo -b bo_example_folder\njob_e2b63a9_01 bayesmark-exp -c DT -d diabetes -o PySOT -u db1d9297948554e096006c172a0486fb -m mse -n 15 -p 1 -dir foo -b bo_example_folder &amp;&amp; bayesmark-exp -c SVM -d boston -o RandomSearch -u 7148f690ed6a543890639cc59db8320b -m mse -n 15 -p 1 -dir foo -b bo_example_folder &amp;&amp; bayesmark-exp -c SVM -d breast -o PySOT -u 72c104ba1b6d5bb8a546b0064a7c52b1 -m nll -n 15 -p 1 -dir foo -b bo_example_folder\njob_e2b63a9_02 bayesmark-exp -c SVM -d iris -o PySOT -u cc63b2c1e4315a9aac0f5f7b496bfb0f -m nll -n 15 -p 1 -dir foo -b bo_example_folder &amp;&amp; bayesmark-exp -c DT -d breast -o RandomSearch -u aec62e1c8b5552e6b12836f0c59c1681 -m nll -n 15 -p 1 -dir foo -b bo_example_folder &amp;&amp; bayesmark-exp -c DT -d digits -o RandomSearch -u 4d0a175d56105b6bb3055c3b62937b2d -m acc -n 15 -p 1 -dir foo -b bo_example_folder\n...\n</pre>\n<p>This package does not have built in support for deploying these jobs on a cluster or cloud environment (.e.g., AWS).</p>\n</div>\n<div id=\"the-uuid-argument\">\n<h4>The UUID argument</h4>\n<p>The <tt>UUID</tt> is a 32-char hex string used as a master random seed which we use to draw random seeds for the experiments. If <tt>UUID</tt> is not specified a version 4 UUID is generated. The used UUID is displayed at the beginning of <tt>stdout</tt>. In general, the <tt>UUID</tt> should not specified/re-used except for debugging because it violates the assumption that the experiment UUIDs are unique.</p>\n</div>\n</div>\n<div id=\"aggregate-results\">\n<h3>Aggregate results</h3>\n<p>Next to aggregate all the experiment files into combined (json) files we need to run the aggregation command:</p>\n<pre>usage: bayesmark-agg [-h] [-dir DB_ROOT] [-odir OPTIMIZER_ROOT] [-v] -b DB [-rv]\n</pre>\n<p>The arguments are:</p>\n<pre>-h, --help            show this help message and exit\n-dir DB_ROOT, -db-root DB_ROOT\n                      root directory for all benchmark experiments output\n-odir OPTIMIZER_ROOT, --opt-root OPTIMIZER_ROOT\n                      Directory with optimization wrappers\n-v, --verbose         print the study logs to console\n-b DB, --db DB        database ID of this benchmark experiment\n-rv, --ravel          ravel all studies to store batch suggestions as if\n                      they were serial\n</pre>\n<p>The <tt>DB_ROOT</tt> must match the folder from the launcher <tt><span class=\"pre\">bayesmark-launch</span></tt>, and <tt>DBID</tt> must match that displayed from the launcher as well. The aggregate files are found in <tt><span class=\"pre\">[DB_ROOT]/[DBID]/derived</span></tt>.</p>\n<p>The result aggregation can be done programmatically via <cite>.experiment_aggregate.concat_experiments</cite>.</p>\n</div>\n<div id=\"analyze-and-summarize-results\">\n<h3>Analyze and summarize results</h3>\n<p>Finally, to run a statistical analysis presenting a summary of the experiments we run</p>\n<pre>usage: bayesmark-anal [-h] [-dir DB_ROOT] [-odir OPTIMIZER_ROOT] [-v] -b DB\n</pre>\n<p>The arguments are:</p>\n<pre>-h, --help            show this help message and exit\n-dir DB_ROOT, -db-root DB_ROOT\n                      root directory for all benchmark experiments output\n-odir OPTIMIZER_ROOT, --opt-root OPTIMIZER_ROOT\n                      Directory with optimization wrappers\n-v, --verbose         print the study logs to console\n-b DB, --db DB        database ID of this benchmark experiment\n</pre>\n<p>The <tt>DB_ROOT</tt> must match the folder from the launcher <tt><span class=\"pre\">bayesmark-launch</span></tt>, and <tt>DBID</tt> must match that displayed from the launcher as well. The aggregate files are found in <tt><span class=\"pre\">[DB_ROOT]/[DBID]/derived</span></tt>.</p>\n<p>The <tt><span class=\"pre\">bayesmark-anal</span></tt> command looks for a <tt>baseline.json</tt> file in <tt><span class=\"pre\">[DB_ROOT]/[DBID]/derived</span></tt>, which states the best possible and random search performance. If no such file is present, <tt><span class=\"pre\">bayesmark-anal</span></tt> automatically calls <tt><span class=\"pre\">bayesmark-baseline</span></tt> to build it. The baselines are inferred from the random search performance in the logs. The baseline values are considered fixed (not random) quantities when <tt><span class=\"pre\">bayesmark-anal</span></tt> builds confidence intervals. Therefore, we allow the user to leave them fixed and do not rebuild them when <tt><span class=\"pre\">bayesmark-anal</span></tt> is called if a baselines file is already present.</p>\n<p>The result analysis can be done programmatically via <cite>.experiment_analysis.compute_aggregates</cite>, and the baseline computation via <cite>.experiment_baseline.compute_baseline</cite>.</p>\n<p>See <cite>how-scoring-works</cite> for more information on how the scores are computed and aggregated.</p>\n</div>\n<div id=\"example\">\n<h3>Example</h3>\n<p>After finishing the setup (environment) a small-scale serial can be run as follows:</p>\n<pre><span class=\"gp\">&gt;</span> <span class=\"c1\"># setup\n</span><span class=\"gp\">&gt;</span><span class=\"c1\"></span> <span class=\"nv\">DB_ROOT</span><span class=\"o\">=</span>./notebooks  <span class=\"c1\"># path/to/where/you/put/results\n</span><span class=\"gp\">&gt;</span><span class=\"c1\"></span> <span class=\"nv\">DBID</span><span class=\"o\">=</span>bo_example_folder\n<span class=\"gp\">&gt;</span> mkdir <span class=\"nv\">$DB_ROOT</span>\n<span class=\"gp\">&gt;</span> <span class=\"c1\"># experiments\n</span><span class=\"gp\">&gt;</span><span class=\"c1\"></span> bayesmark-launch -n <span class=\"m\">15</span> -r <span class=\"m\">3</span> -dir <span class=\"nv\">$DB_ROOT</span> -b <span class=\"nv\">$DBID</span> -o RandomSearch PySOT -c SVM DT -v\n<span class=\"go\">Supply --uuid 3adc3182635e44ea96969d267591f034 to reproduce this run.\nSupply --dbid bo_example_folder to append to this experiment or reproduce jobs file.\nUser must ensure equal reps of each optimizer for unbiased results\n-c DT -d boston -o PySOT -u a1b287b450385ad09b2abd7582f404a2 -m mae -n 15 -p 1 -dir /notebooks -b bo_example_folder\n-c DT -d boston -o PySOT -u 63746599ae3f5111a96942d930ba1898 -m mse -n 15 -p 1 -dir /notebooks -b bo_example_folder\n-c DT -d boston -o RandomSearch -u 8ba16c880ef45b27ba0909199ab7aa8a -m mae -n 15 -p 1 -dir /notebooks -b bo_example_folder\n...\n0 failures of benchmark script after 144 studies.\ndone\n</span><span class=\"gp\">&gt;</span> <span class=\"c1\"># aggregate\n</span><span class=\"gp\">&gt;</span><span class=\"c1\"></span> bayesmark-agg -dir <span class=\"nv\">$DB_ROOT</span> -b <span class=\"nv\">$DBID</span>\n<span class=\"gp\">&gt;</span> <span class=\"c1\"># analyze\n</span><span class=\"gp\">&gt;</span><span class=\"c1\"></span> bayesmark-anal -dir <span class=\"nv\">$DB_ROOT</span> -b <span class=\"nv\">$DBID</span> -v\n<span class=\"go\">...\nmedian score @ 15:\noptimizer\nPySOT_0.2.3_9b766b6           0.330404\nRandomSearch_0.0.1_9b766b6    0.961829\nmean score @ 15:\noptimizer\nPySOT_0.2.3_9b766b6           0.124262\nRandomSearch_0.0.1_9b766b6    0.256422\nnormed mean score @ 15:\noptimizer\nPySOT_0.2.3_9b766b6           0.475775\nRandomSearch_0.0.1_9b766b6    0.981787\ndone</span>\n</pre>\n<p>The aggregate result files (i.e., <tt>summary.json</tt>) will now be available in <tt><span class=\"pre\">$DB_ROOT/$DBID/derived</span></tt>. However, this will be high variance since it was from only 3 trials and only to 15 function evaluations.</p>\n</div>\n<div id=\"plotting-and-notebooks\">\n<h3>Plotting and notebooks</h3>\n<p>Plotting the quantitative results found in <tt><span class=\"pre\">$DB_ROOT/$DBID/derived</span></tt> can be done using the notebooks found in the <tt>notebooks/</tt> folder of the git repository. The notebook <tt>plot_mean_score.ipynb</tt> generates plots for aggregate scores averaging over all problems. The notebook <tt>plot_test_case.ipynb</tt> generates plots for each test problem.</p>\n<p>To use the notebooks, first copy over the <tt>notebooks/</tt> folder from git repository.</p>\n<p>To setup the kernel for running the notebooks use:</p>\n<pre>virtualenv bobm_ipynb --python<span class=\"o\">=</span>python3.6\n<span class=\"nb\">source</span> ./bobm_ipynb/bin/activate\npip install bayesmark<span class=\"o\">[</span>notebooks<span class=\"o\">]</span>\npython -m ipykernel install --name<span class=\"o\">=</span>bobm_ipynb --user\n</pre>\n<p>Now, the notebooks for plotting can be run with the command <tt>jupyter notebook</tt> and selecting the kernel <tt>bobm_ipynb</tt>.</p>\n<p>It is also possible to convert the notebooks to an HTML report at the command line using <tt>nbconvert</tt>. For example, use the command:</p>\n<pre>jupyter nbconvert --to html --execute notebooks/plot_mean_score.ipynb\n</pre>\n<p>The output file will be in <tt>./notebooks/plot_mean_score.html</tt>. See the <tt>nbconvert</tt> <a href=\"https://nbconvert.readthedocs.io/en/latest/usage.html#supported-output-formats\" rel=\"nofollow\">documentation page</a> for more output formats. By default, the notebooks look in <tt>./notebooks/bo_example_folder/</tt> for the <tt>summary.json</tt> from <tt><span class=\"pre\">bayesmark-anal</span></tt>.</p>\n<p>To run <tt>plot_test_case.ipynb</tt> use the command:</p>\n<pre>jupyter nbconvert --to html --execute notebooks/plot_test_case.ipynb --ExecutePreprocessor.timeout<span class=\"o\">=</span><span class=\"m\">600</span>\n</pre>\n<p>The <tt><span class=\"pre\">--ExecutePreprocessor.timeout=600</span></tt> timeout increase is needed due to the large number of plots being generated. The output will be in <tt>./notebooks/plot_test_case.html</tt>.</p>\n</div>\n</div>\n<div id=\"id1\">\n<h2>Adding a new optimizer</h2>\n<p>All optimizers in this benchmark are required to follow the interface specified of the <tt>AbstractOptimizer</tt> class in <tt>bayesmark.abstract_optimizer</tt>. In general, this requires creating a wrapper class around the new optimizer. The wrapper classes must all be placed in a folder referred to by the <tt><span class=\"pre\">--opt-root</span></tt> argument. This folder must also contain the <tt>config.json</tt> folder.</p>\n<p>The interface is simple, one must merely implement the <tt>suggest</tt> and <tt>observe</tt> functions. The <tt>suggest</tt> function generates new guesses for evaluating the function. Once evaluated, the function evaluations are passed to the <tt>observe</tt> function. The objective function is <em>not</em> evaluated by the optimizer class. The objective function is evaluated on outside and results are passed to <tt>observe</tt>. This is the correct setup for Bayesian optimization because:</p>\n<ul>\n<li>We can observe/try inputs that were never suggested</li>\n<li>We can ignore suggestions</li>\n<li>The objective function may not be something as simple as a Python function</li>\n</ul>\n<p>So passing the function as an argument as is done in <tt>scipy.optimization</tt> is artificially restrictive.</p>\n<p>The implementation of the wrapper will look like the following:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">bayesmark.abstract_optimizer</span> <span class=\"kn\">import</span> <span class=\"n\">AbstractOptimizer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">bayesmark.experiment</span> <span class=\"kn\">import</span> <span class=\"n\">experiment_main</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">NewOptimizerName</span><span class=\"p\">(</span><span class=\"n\">AbstractOptimizer</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Used for determining the version number of package used</span>\n    <span class=\"n\">primary_import</span> <span class=\"o\">=</span> <span class=\"s2\">\"name of import used e.g, opentuner\"</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">api_config</span><span class=\"p\">,</span> <span class=\"n\">optional_arg_foo</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">optional_arg_bar</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Build wrapper class to use optimizer in benchmark.\n\n        Parameters\n        ----------\n        api_config : dict-like of dict-like\n            Configuration of the optimization variables. See API description.\n        \"\"\"</span>\n        <span class=\"n\">AbstractOptimizer</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">api_config</span><span class=\"p\">)</span>\n        <span class=\"c1\"># Do whatever other setup is needed</span>\n        <span class=\"c1\"># ...</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">suggest</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">n_suggestions</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Get suggestion from the optimizer.\n\n        Parameters\n        ----------\n        n_suggestions : int\n            Desired number of parallel suggestions in the output\n\n        Returns\n        -------\n        next_guess : list of dict\n            List of `n_suggestions` suggestions to evaluate the objective\n            function. Each suggestion is a dictionary where each key\n            corresponds to a parameter being optimized.\n        \"\"\"</span>\n        <span class=\"c1\"># Do whatever is needed to get the parallel guesses</span>\n        <span class=\"c1\"># ...</span>\n        <span class=\"k\">return</span> <span class=\"n\">x_guess</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">observe</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Feed an observation back.\n\n        Parameters\n        ----------\n        X : list of dict-like\n            Places where the objective function has already been evaluated.\n            Each suggestion is a dictionary where each key corresponds to a\n            parameter being optimized.\n        y : array-like, shape (n,)\n            Corresponding values where objective has been evaluated\n        \"\"\"</span>\n        <span class=\"c1\"># Update the model with new objective function observations</span>\n        <span class=\"c1\"># ...</span>\n        <span class=\"c1\"># No return statement needed</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">\"__main__\"</span><span class=\"p\">:</span>\n    <span class=\"c1\"># This is the entry point for experiments, so pass the class to experiment_main to use this optimizer.</span>\n    <span class=\"c1\"># This statement must be included in the wrapper class file:</span>\n    <span class=\"n\">experiment_main</span><span class=\"p\">(</span><span class=\"n\">NewOptimizerName</span><span class=\"p\">)</span>\n</pre>\n<p>Depending on the API of the optimizer being wrapped, building this wrapper class may only or require a few lines of code, or be a total pain.</p>\n<div id=\"the-config-file\">\n<h3>The config file</h3>\n<p>Each optimizer wrapper can have multiple configurations, which is each referred to as a different optimizer in the benchmark. For example, the JSON config file will have entries as follows:</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"nt\">\"OpenTuner-BanditA-New\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"opentuner_optimizer.py\"</span><span class=\"p\">,</span>\n        <span class=\"p\">{</span><span class=\"nt\">\"techniques\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"AUCBanditMetaTechniqueA\"</span><span class=\"p\">]}</span>\n    <span class=\"p\">],</span>\n    <span class=\"nt\">\"OpenTuner-GA-DE-New\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"opentuner_optimizer.py\"</span><span class=\"p\">,</span>\n        <span class=\"p\">{</span><span class=\"nt\">\"techniques\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"PSO_GA_DE\"</span><span class=\"p\">]}</span>\n    <span class=\"p\">],</span>\n    <span class=\"nt\">\"OpenTuner-GA-New\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"opentuner_optimizer.py\"</span><span class=\"p\">,</span>\n        <span class=\"p\">{</span><span class=\"nt\">\"techniques\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"PSO_GA_Bandit\"</span><span class=\"p\">]}</span>\n    <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</pre>\n<p>Basically, the entries are <tt>\"name_of_strategy\": [\"file_with_class\", {kwargs_for_the_constructor}]</tt>. Here, <tt><span class=\"pre\">OpenTuner-BanditA</span></tt>, <tt><span class=\"pre\">OpenTuner-GA-DE</span></tt>, and <tt><span class=\"pre\">OpenTuner-GA</span></tt> are all treated as different optimizers by the benchmark even though the all use the same class from <tt>opentuner_optimizer.py</tt>.</p>\n<p>This <tt>config.json</tt> must be in the same folder as the optimizer classes (e.g., <tt>opentuner_optimizer.py</tt>).</p>\n</div>\n<div id=\"running-with-a-new-optimizer\">\n<h3>Running with a new optimizer</h3>\n<p>To run the benchmarks using a new optimizer, simply provide its name (from <tt>config.json</tt>) in the <tt><span class=\"pre\">-o</span></tt> list. The <tt><span class=\"pre\">--opt-root</span></tt> argument must be specified in this case. For example, the launch command from the <a href=\"#example\" rel=\"nofollow\">example</a> becomes:</p>\n<pre>bayesmark-launch -n <span class=\"m\">15</span> -r <span class=\"m\">3</span> -dir <span class=\"nv\">$DB_ROOT</span> -b <span class=\"nv\">$DBID</span> -o RandomSearch PySOT-New -c SVM DT --opt-root ./example_opt_root -v\n</pre>\n<p>Here, we are using the example <tt><span class=\"pre\">PySOT-New</span></tt> wrapper from the <tt>example_opt_root</tt> folder in the git repo. It is equivalent to the builtin <tt>PySOT</tt>, but gives an example of how to provide a new custom optimizer.</p>\n</div>\n</div>\n<div id=\"contributing\">\n<h2>Contributing</h2>\n<p>The following instructions have been tested with Python 3.6.8 on Ubuntu (16.04.5 LTS).</p>\n<div id=\"install-in-editable-mode\">\n<h3>Install in editable mode</h3>\n<p>First, define the variables for the paths we will use:</p>\n<pre><span class=\"nv\">GIT</span><span class=\"o\">=</span>/path/to/where/you/put/repos\n<span class=\"nv\">ENVS</span><span class=\"o\">=</span>/path/to/where/you/put/virtualenvs\n</pre>\n<p>Then clone the repo in your git directory <tt>$GIT</tt>:</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$GIT</span>\ngit clone https://github.com/uber/bayesmark.git\n</pre>\n<p>Inside your virtual environments folder <tt>$ENVS</tt>, make the environment:</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$ENVS</span>\nvirtualenv bayesmark --python<span class=\"o\">=</span>python3.6\n<span class=\"nb\">source</span> <span class=\"nv\">$ENVS</span>/bayesmark/bin/activate\n</pre>\n<p>Now we can install the pip dependencies. Move back into your git directory and run</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$GIT</span>/bayesmark\npip install -r requirements/base.txt\npip install -r requirements/optimizers.txt\npip install -e .  <span class=\"c1\"># Install the benchmark itself</span>\n</pre>\n<p>You may want to run <tt>pip install <span class=\"pre\">-U</span> pip</tt> first if you have an old version of <tt>pip</tt>. The file <tt>optimizers.txt</tt> contains the dependencies for all the optimizers used in the benchmark. The analysis and aggregation programs can be run using only the requirements in <tt>base.txt</tt>.</p>\n</div>\n<div id=\"contributor-tools\">\n<h3>Contributor tools</h3>\n<p>First, we need to setup some needed tools:</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$ENVS</span>\nvirtualenv bayesmark_tools --python<span class=\"o\">=</span>python3.6\n<span class=\"nb\">source</span> <span class=\"nv\">$ENVS</span>/bayesmark_tools/bin/activate\npip install -r <span class=\"nv\">$GIT</span>/bayesmark/requirements/tools.txt\n</pre>\n<p>To install the pre-commit hooks for contributing run (in the <tt>bayesmark_tools</tt> environment):</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$GIT</span>/bayesmark\npre-commit install\n</pre>\n<p>To rebuild the requirements, we can run:</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$GIT</span>/bayesmark\n<span class=\"c1\"># Get py files from notebooks to analyze\n</span>jupyter nbconvert --to script notebooks/*.ipynb\n<span class=\"c1\"># Generate the .in files (but pins to latest, which we might not want)\n</span>pipreqs bayesmark/ --ignore bayesmark/builtin_opt/ --savepath requirements/base.in\npipreqs test/ --savepath requirements/test.in\npipreqs bayesmark/builtin_opt/ --savepath requirements/optimizers.in\npipreqs notebooks/ --savepath requirements/ipynb.in\npipreqs docs/ --savepath requirements/docs.in\n<span class=\"c1\"># Regenerate the .txt files from .in files\n</span>pip-compile-multi --no-upgrade\n</pre>\n</div>\n<div id=\"generating-the-documentation\">\n<h3>Generating the documentation</h3>\n<p>First setup the environment for building with <tt>Sphinx</tt>:</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$ENVS</span>\nvirtualenv bayesmark_docs --python<span class=\"o\">=</span>python3.6\n<span class=\"nb\">source</span> <span class=\"nv\">$ENVS</span>/bayesmark_docs/bin/activate\npip install -r <span class=\"nv\">$GIT</span>/bayesmark/requirements/docs.txt\n</pre>\n<p>Then we can do the build:</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$GIT</span>/bayesmark/docs\nmake all\nopen _build/html/index.html\n</pre>\n<p>Documentation will be available in all formats in <tt>Makefile</tt>. Use <tt>make html</tt> to only generate the HTML documentation.</p>\n</div>\n<div id=\"running-the-tests\">\n<h3>Running the tests</h3>\n<p>The tests for this package can be run with:</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$GIT</span>/bayesmark\n./test.sh\n</pre>\n<p>The script creates a conda environment using the requirements found in <tt>requirements/test.txt</tt>.</p>\n<p>The <tt>test.sh</tt> script <em>must</em> be run from a <em>clean</em> git repo.</p>\n<p>Or if we only want to run the unit tests and not check the adequacy of the requirements files, one can use</p>\n<pre><span class=\"c1\"># Setup environment\n</span><span class=\"nb\">cd</span> <span class=\"nv\">$ENVS</span>\nvirtualenv bayesmark_test --python<span class=\"o\">=</span>python3.6\n<span class=\"nb\">source</span> <span class=\"nv\">$ENVS</span>/bayesmark_test/bin/activate\npip install -r <span class=\"nv\">$GIT</span>/bayesmark/requirements/test.txt\npip install -e <span class=\"nv\">$GIT</span>/bayesmark\n<span class=\"c1\"># Now run tests\n</span><span class=\"nb\">cd</span> <span class=\"nv\">$GIT</span>/bayesmark/\npytest test/ -s -v --hypothesis-seed<span class=\"o\">=</span><span class=\"m\">0</span> --disable-pytest-warnings --cov<span class=\"o\">=</span>bayesmark --cov-report html\n</pre>\n<p>A code coverage report will also be produced in <tt>$GIT/bayesmark/htmlcov/index.html</tt>.</p>\n</div>\n<div id=\"deployment\">\n<h3>Deployment</h3>\n<p>The wheel (tar ball) for deployment as a pip installable package can be built using the script:</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$GIT</span>/bayesmark/\n./build_wheel.sh\n</pre>\n</div>\n</div>\n<div id=\"id3\">\n<h2>Links</h2>\n<p>The <a href=\"https://github.com/uber/bayesmark\" rel=\"nofollow\">source</a> is hosted on GitHub.</p>\n<p>The <a href=\"https://bayesmark.readthedocs.io/en/latest/\" rel=\"nofollow\">documentation</a> is hosted at Read the Docs.</p>\n<p>Installable from <a href=\"https://pypi.org/project/bayesmark/\" rel=\"nofollow\">PyPI</a>.</p>\n<p>The builtin optimizers are wrappers on the following projects:</p>\n<ul>\n<li><a href=\"https://github.com/hyperopt/hyperopt\" rel=\"nofollow\">HyperOpt</a></li>\n<li><a href=\"https://github.com/facebookresearch/nevergrad\" rel=\"nofollow\">Nevergrad</a></li>\n<li><a href=\"https://github.com/jansel/opentuner\" rel=\"nofollow\">OpenTuner</a></li>\n<li><a href=\"https://github.com/dme65/pySOT\" rel=\"nofollow\">PySOT</a></li>\n<li><a href=\"https://github.com/scikit-optimize/scikit-optimize\" rel=\"nofollow\">Scikit-optimize</a></li>\n</ul>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>This project is licensed under the Apache 2 License - see the LICENSE file for details.</p>\n</div>\n\n          </div>"}, "last_serial": 5925892, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "8de237a91643245c4229b74645378f64", "sha256": "dc9607a970f81ad84f41a03246cc7ac74e4a7b9503fcd8e6534b7a16f1281d4f"}, "downloads": -1, "filename": "bayesmark-0.0.0.tar.gz", "has_sig": false, "md5_digest": "8de237a91643245c4229b74645378f64", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 87520, "upload_time": "2019-09-28T16:44:56", "upload_time_iso_8601": "2019-09-28T16:44:56.418126Z", "url": "https://files.pythonhosted.org/packages/e2/03/feae67ff9bae8d207dd942c10104bcff374e69919e49128c2701764f37cb/bayesmark-0.0.0.tar.gz", "yanked": false}], "0.0.1": [{"comment_text": "", "digests": {"md5": "1dacbac7c018d6d690ada539fb69b49c", "sha256": "78b9aa6415eba42b0863c3e465a7c8e3f109cf73a1064c19e02f0ac1dbdd9b2c"}, "downloads": -1, "filename": "bayesmark-0.0.1.tar.gz", "has_sig": false, "md5_digest": "1dacbac7c018d6d690ada539fb69b49c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 88070, "upload_time": "2019-09-28T19:33:40", "upload_time_iso_8601": "2019-09-28T19:33:40.187115Z", "url": "https://files.pythonhosted.org/packages/45/e3/d9dd7310d9d3cc9a034c2af0fd48843d944660657d48ecb124b7fe27ce75/bayesmark-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "4dec637e6bfb8b767208657c590bf88a", "sha256": "7c490c6949694c160d47b3468e3e5c0e0691313d45daee80b02f0093d62ce7be"}, "downloads": -1, "filename": "bayesmark-0.0.2.tar.gz", "has_sig": false, "md5_digest": "4dec637e6bfb8b767208657c590bf88a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 87687, "upload_time": "2019-09-29T22:29:11", "upload_time_iso_8601": "2019-09-29T22:29:11.808131Z", "url": "https://files.pythonhosted.org/packages/ef/50/614fedf6a9d3133be991159f52df1177e4275f760c428fbb25b8ae055b3d/bayesmark-0.0.2.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "675e6990150403924fecc7d7c41e8a87", "sha256": "47e1d720cae6fa5a8f7efdefc3b2f04e590ad9451b39b4a867084dce72108094"}, "downloads": -1, "filename": "bayesmark-0.0.4.tar.gz", "has_sig": false, "md5_digest": "675e6990150403924fecc7d7c41e8a87", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 88180, "upload_time": "2019-10-03T23:56:39", "upload_time_iso_8601": "2019-10-03T23:56:39.353658Z", "url": "https://files.pythonhosted.org/packages/0c/ce/3bedf37d4a83174d0553af9d41f407628aff8bd7ff7a9f7031ae2075bc43/bayesmark-0.0.4.tar.gz", "yanked": false}], "0.0.4rc6": [{"comment_text": "", "digests": {"md5": "f1071aeb7def84cd65c92a26cdcd1a53", "sha256": "db7a37cd17dc78401cdd0b3397a04ed6d8c344bd84c7f6f00ad8afd27435ae61"}, "downloads": -1, "filename": "bayesmark-0.0.4rc6.tar.gz", "has_sig": false, "md5_digest": "f1071aeb7def84cd65c92a26cdcd1a53", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 88186, "upload_time": "2019-10-03T19:33:07", "upload_time_iso_8601": "2019-10-03T19:33:07.439610Z", "url": "https://files.pythonhosted.org/packages/ad/ec/630729bcaf0ea539368a58802cff0aef5c5161558917407f489c29847826/bayesmark-0.0.4rc6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "675e6990150403924fecc7d7c41e8a87", "sha256": "47e1d720cae6fa5a8f7efdefc3b2f04e590ad9451b39b4a867084dce72108094"}, "downloads": -1, "filename": "bayesmark-0.0.4.tar.gz", "has_sig": false, "md5_digest": "675e6990150403924fecc7d7c41e8a87", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 88180, "upload_time": "2019-10-03T23:56:39", "upload_time_iso_8601": "2019-10-03T23:56:39.353658Z", "url": "https://files.pythonhosted.org/packages/0c/ce/3bedf37d4a83174d0553af9d41f407628aff8bd7ff7a9f7031ae2075bc43/bayesmark-0.0.4.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:14:41 2020"}