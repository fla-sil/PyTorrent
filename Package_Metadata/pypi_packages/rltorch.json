{"info": {"author": "Toshiki Watanabe", "author_email": "watanabe.toshiki@outlook.jp", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy"], "description": "\n# rltorch(WIP)\nrltorch provides a simple framework for reinforcement learning in PyTorch. You can easily implement distributed RL algorithms.\n\n## Installation\nInstall rltorch from source.\n```\ngit clone https://github.com/ku2482/rltorch.git\ncd rltorch\npip install -e .\n```\n\nYou can also install using PyPI.\n```\npip install rltorch\n```\n\n## Examples\n### Ape-X\nYou can implement Ape-X[[1]](#references) agent like this example [here](https://github.com/ku2482/rltorch/blob/master/examples/atari/apex.py).\n\n```\npython examples/atari/apex.py \\\n[--env_id str(default MsPacmanNoFrameskip-v4)] \\\n[--num_actors int(default 4)] [--cuda (optional)] \\\n[--seed int(default 0)]\n```\n\n### Soft Actor-Critic\nYou can implement Soft Actor-Critic[[2, 3]](#references) agent like this example [here](https://github.com/ku2482/rltorch/blob/master/examples/mujoco/sac.py). Note that you need [a license](https://www.roboti.us/license.html) and [mujoco_py](https://github.com/openai/mujoco-py) to be installed.\n\n```\npython examples/mujoco/sac.py \\\n[--env_id str(default HalfCheetah-v2)] \\\n[--num_actors int(default 1)] \\\n[--cuda (optional)] [--seed int(default 0)]\n```\n\n### SAC-Discrete\nYou can implement SAC-Discrete[[4]](#references) agent like this example [here](https://github.com/ku2482/rltorch/blob/master/examples/atari/sac_discrete.py).\n\n```\npython examples/mujoco/sac.py \\\n[--env_id str(default MsPacmanNoFrameskip-v4)] \\\n[--num_actors int(default 4)] \\\n[--cuda (optional)] [--seed int(default 0)]\n```\n\n## References\n[[1]](https://arxiv.org/abs/1803.00933) Horgan, Dan, et al. \"Distributed prioritized experience replay.\" arXiv preprint arXiv:1803.00933 (2018).\n\n[[2]](https://arxiv.org/abs/1801.01290) Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018).\n\n[[3]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).\n\n[[4]](https://arxiv.org/abs/1910.07207) Christodoulou, Petros. \"Soft Actor-Critic for Discrete Action Settings.\" arXiv preprint arXiv:1910.07207 (2019).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ku2482/rltorch", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "rltorch", "package_url": "https://pypi.org/project/rltorch/", "platform": "", "project_url": "https://pypi.org/project/rltorch/", "project_urls": {"Homepage": "https://github.com/ku2482/rltorch"}, "release_url": "https://pypi.org/project/rltorch/0.1.0/", "requires_dist": ["numpy", "torch", "gym"], "requires_python": ">=3.6.0", "summary": "A simple framework for distributed reinforcement learning in PyTorch.", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>rltorch(WIP)</h1>\n<p>rltorch provides a simple framework for reinforcement learning in PyTorch. You can easily implement distributed RL algorithms.</p>\n<h2>Installation</h2>\n<p>Install rltorch from source.</p>\n<pre><code>git clone https://github.com/ku2482/rltorch.git\ncd rltorch\npip install -e .\n</code></pre>\n<p>You can also install using PyPI.</p>\n<pre><code>pip install rltorch\n</code></pre>\n<h2>Examples</h2>\n<h3>Ape-X</h3>\n<p>You can implement Ape-X<a href=\"#references\" rel=\"nofollow\">[1]</a> agent like this example <a href=\"https://github.com/ku2482/rltorch/blob/master/examples/atari/apex.py\" rel=\"nofollow\">here</a>.</p>\n<pre><code>python examples/atari/apex.py \\\n[--env_id str(default MsPacmanNoFrameskip-v4)] \\\n[--num_actors int(default 4)] [--cuda (optional)] \\\n[--seed int(default 0)]\n</code></pre>\n<h3>Soft Actor-Critic</h3>\n<p>You can implement Soft Actor-Critic<a href=\"#references\" rel=\"nofollow\">[2, 3]</a> agent like this example <a href=\"https://github.com/ku2482/rltorch/blob/master/examples/mujoco/sac.py\" rel=\"nofollow\">here</a>. Note that you need <a href=\"https://www.roboti.us/license.html\" rel=\"nofollow\">a license</a> and <a href=\"https://github.com/openai/mujoco-py\" rel=\"nofollow\">mujoco_py</a> to be installed.</p>\n<pre><code>python examples/mujoco/sac.py \\\n[--env_id str(default HalfCheetah-v2)] \\\n[--num_actors int(default 1)] \\\n[--cuda (optional)] [--seed int(default 0)]\n</code></pre>\n<h3>SAC-Discrete</h3>\n<p>You can implement SAC-Discrete<a href=\"#references\" rel=\"nofollow\">[4]</a> agent like this example <a href=\"https://github.com/ku2482/rltorch/blob/master/examples/atari/sac_discrete.py\" rel=\"nofollow\">here</a>.</p>\n<pre><code>python examples/mujoco/sac.py \\\n[--env_id str(default MsPacmanNoFrameskip-v4)] \\\n[--num_actors int(default 4)] \\\n[--cuda (optional)] [--seed int(default 0)]\n</code></pre>\n<h2>References</h2>\n<p><a href=\"https://arxiv.org/abs/1803.00933\" rel=\"nofollow\">[1]</a> Horgan, Dan, et al. \"Distributed prioritized experience replay.\" arXiv preprint arXiv:1803.00933 (2018).</p>\n<p><a href=\"https://arxiv.org/abs/1801.01290\" rel=\"nofollow\">[2]</a> Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018).</p>\n<p><a href=\"https://arxiv.org/abs/1812.05905\" rel=\"nofollow\">[3]</a> Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).</p>\n<p><a href=\"https://arxiv.org/abs/1910.07207\" rel=\"nofollow\">[4]</a> Christodoulou, Petros. \"Soft Actor-Critic for Discrete Action Settings.\" arXiv preprint arXiv:1910.07207 (2019).</p>\n\n          </div>"}, "last_serial": 6078967, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "60109ce86f560f895e21e51c8a4e142e", "sha256": "13cf0b130d060c36e090a30a95ca86021db1bb007cff94bc76a45df0f838d616"}, "downloads": -1, "filename": "rltorch-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "60109ce86f560f895e21e51c8a4e142e", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 30584, "upload_time": "2019-11-05T03:53:10", "upload_time_iso_8601": "2019-11-05T03:53:10.789275Z", "url": "https://files.pythonhosted.org/packages/b2/09/3f758be9a10a023e3fc5ff983f8b50eb21fee8b4b9c0a74fe6c45780c579/rltorch-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3457efd9ab7f08f9e972f736d2b40dd8", "sha256": "e21cd38f049deb596184afa9f3727bece03aba8c8aee66ac4c9e16bc4250bb58"}, "downloads": -1, "filename": "rltorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "3457efd9ab7f08f9e972f736d2b40dd8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 15984, "upload_time": "2019-11-05T03:53:12", "upload_time_iso_8601": "2019-11-05T03:53:12.531776Z", "url": "https://files.pythonhosted.org/packages/3d/ea/60356ec31f901b7d4ab44ab9aa68ad81fe6f64be78308a58455ee7397fe5/rltorch-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "60109ce86f560f895e21e51c8a4e142e", "sha256": "13cf0b130d060c36e090a30a95ca86021db1bb007cff94bc76a45df0f838d616"}, "downloads": -1, "filename": "rltorch-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "60109ce86f560f895e21e51c8a4e142e", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 30584, "upload_time": "2019-11-05T03:53:10", "upload_time_iso_8601": "2019-11-05T03:53:10.789275Z", "url": "https://files.pythonhosted.org/packages/b2/09/3f758be9a10a023e3fc5ff983f8b50eb21fee8b4b9c0a74fe6c45780c579/rltorch-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3457efd9ab7f08f9e972f736d2b40dd8", "sha256": "e21cd38f049deb596184afa9f3727bece03aba8c8aee66ac4c9e16bc4250bb58"}, "downloads": -1, "filename": "rltorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "3457efd9ab7f08f9e972f736d2b40dd8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 15984, "upload_time": "2019-11-05T03:53:12", "upload_time_iso_8601": "2019-11-05T03:53:12.531776Z", "url": "https://files.pythonhosted.org/packages/3d/ea/60356ec31f901b7d4ab44ab9aa68ad81fe6f64be78308a58455ee7397fe5/rltorch-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:02:15 2020"}