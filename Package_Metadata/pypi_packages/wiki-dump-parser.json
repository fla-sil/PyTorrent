{"info": {"author": "Abel 'Akronix' Serrano Juste", "author_email": "akronix5@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "wiki dump parser\n================\n\nA simple but fast python script that reads the XML dump of a wiki and\noutput the processed data in a CSV file.\n\n`All revisions history of a mediawiki wiki can be backed up as an XML\nfile, known as a XML\ndump. <https://www.mediawiki.org/wiki/Manual:Backing_up_a_wiki#Backup_the_content_of_the_wiki_(XML_dump)>`__\nThis file is a record of all the edits made in a wiki with all the\ncorresponding data regarding date, page, author and the full content\nwithin the edit.\n\nVery often we just want the metadata for the edit regarding date, author\nand page; and therefore, we do not need the content of the edit, which\nby far the longest piece of data.\n\nThis script converts this very long XML dump in csv files much smaller\nand easiest to read and work with. It takes care of\n\nUsage\n-----\n\nInstall the package using pip:\n\n``pip install wiki_dump_parser``\n\nThen, use it directly from command line:\n\n``python -m wiki_dump_parser <dump.xml>``\n\nOr from python code:\n\n.. code:: python\n\n    import wiki_dump_parser as parser\n    parser.xml_to_csv('dump.xml')\n\nThe output csv files should be loaded using '\\|' as an escape character\nfor quoting string. An example to load the output file \"dump.csv\"\ngenerated by this script using pandas would be:\n\n.. code:: python\n\n    df = pd.read_csv('dump.csv', quotechar='|', index_col = False)\n    df['timestamp'] = pd.to_datetime(df['timestamp'],format='%Y-%m-%dT%H:%M:%SZ')\n\nDependencies\n------------\n\n-  python 3\n\n*Yes, nothing more.*\n\nHow to get a wiki history dump\n------------------------------\n\nThere are several ways to get the wiki dump:\n\n-  If you have access to the server, follow the `instructions in the\n   mediawiki\n   docs <https://www.mediawiki.org/wiki/Manual:Backing_up_a_wiki#Backup_the_content_of_the_wiki_(XML_dump)>`__.\n-  For **Wikia wikis** and `many other\n   domains <https://github.com/Grasia/wiki-scripts/tree/master/wiki_dump_downloader#domains-tested>`__,\n   you can use our in-house developed script made to accomplish this\n   task. It is straightforward to use and very fast on it.\n-  **Wikimedia project wikis**: For wikis belonging to the Wikimedia\n   project, you already have a regular updated repo with all the dumps\n   here: http://dumps.wikimedia.org. `Select your target wiki from the\n   list <https://dumps.wikimedia.org/backup-index-bydb.html>`__ and\n   download the complete edit history dump and uncompress it.\n-  For **other wikis**, like self-hosted wikis, you should use the\n   wikiteam's dumpgenerator.py script. You have a simple tutorial `in\n   their\n   wiki <https://github.com/WikiTeam/wikiteam/wiki/Tutorial#I_have_no_shell_access_to_server>`__.\n   Its usage is very straightforward and the script is well maintained.\n   Remember to use the --xml option to download the full history dump.", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Grasia/wiki-scripts/tree/master/wiki_dump_parser", "keywords": "wiki dump parser Wikia xml csv pandas proccessing history data", "license": "AGPL-3.0", "maintainer": "", "maintainer_email": "", "name": "wiki-dump-parser", "package_url": "https://pypi.org/project/wiki-dump-parser/", "platform": "", "project_url": "https://pypi.org/project/wiki-dump-parser/", "project_urls": {"Homepage": "https://github.com/Grasia/wiki-scripts/tree/master/wiki_dump_parser"}, "release_url": "https://pypi.org/project/wiki-dump-parser/2.0.1/", "requires_dist": null, "requires_python": ">=3", "summary": "A simple but fast python script that reads the XML dump of a     wiki and output the processed data in a CSV file.", "version": "2.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>A simple but fast python script that reads the XML dump of a wiki and\noutput the processed data in a CSV file.</p>\n<p><a href=\"https://www.mediawiki.org/wiki/Manual:Backing_up_a_wiki#Backup_the_content_of_the_wiki_(XML_dump)\" rel=\"nofollow\">All revisions history of a mediawiki wiki can be backed up as an XML\nfile, known as a XML\ndump.</a>\nThis file is a record of all the edits made in a wiki with all the\ncorresponding data regarding date, page, author and the full content\nwithin the edit.</p>\n<p>Very often we just want the metadata for the edit regarding date, author\nand page; and therefore, we do not need the content of the edit, which\nby far the longest piece of data.</p>\n<p>This script converts this very long XML dump in csv files much smaller\nand easiest to read and work with. It takes care of</p>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>Install the package using pip:</p>\n<p><tt>pip install wiki_dump_parser</tt></p>\n<p>Then, use it directly from command line:</p>\n<p><tt>python <span class=\"pre\">-m</span> wiki_dump_parser &lt;dump.xml&gt;</tt></p>\n<p>Or from python code:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">wiki_dump_parser</span> <span class=\"k\">as</span> <span class=\"nn\">parser</span>\n<span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">xml_to_csv</span><span class=\"p\">(</span><span class=\"s1\">'dump.xml'</span><span class=\"p\">)</span>\n</pre>\n<p>The output csv files should be loaded using \u2018|\u2019 as an escape character\nfor quoting string. An example to load the output file \u201cdump.csv\u201d\ngenerated by this script using pandas would be:</p>\n<pre><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s1\">'dump.csv'</span><span class=\"p\">,</span> <span class=\"n\">quotechar</span><span class=\"o\">=</span><span class=\"s1\">'|'</span><span class=\"p\">,</span> <span class=\"n\">index_col</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">'timestamp'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">to_datetime</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">'timestamp'</span><span class=\"p\">],</span><span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"s1\">'%Y-%m-</span><span class=\"si\">%d</span><span class=\"s1\">T%H:%M:%SZ'</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"dependencies\">\n<h2>Dependencies</h2>\n<ul>\n<li>python 3</li>\n</ul>\n<p><em>Yes, nothing more.</em></p>\n</div>\n<div id=\"how-to-get-a-wiki-history-dump\">\n<h2>How to get a wiki history dump</h2>\n<p>There are several ways to get the wiki dump:</p>\n<ul>\n<li>If you have access to the server, follow the <a href=\"https://www.mediawiki.org/wiki/Manual:Backing_up_a_wiki#Backup_the_content_of_the_wiki_(XML_dump)\" rel=\"nofollow\">instructions in the\nmediawiki\ndocs</a>.</li>\n<li>For <strong>Wikia wikis</strong> and <a href=\"https://github.com/Grasia/wiki-scripts/tree/master/wiki_dump_downloader#domains-tested\" rel=\"nofollow\">many other\ndomains</a>,\nyou can use our in-house developed script made to accomplish this\ntask. It is straightforward to use and very fast on it.</li>\n<li><strong>Wikimedia project wikis</strong>: For wikis belonging to the Wikimedia\nproject, you already have a regular updated repo with all the dumps\nhere: <a href=\"http://dumps.wikimedia.org\" rel=\"nofollow\">http://dumps.wikimedia.org</a>. <a href=\"https://dumps.wikimedia.org/backup-index-bydb.html\" rel=\"nofollow\">Select your target wiki from the\nlist</a> and\ndownload the complete edit history dump and uncompress it.</li>\n<li>For <strong>other wikis</strong>, like self-hosted wikis, you should use the\nwikiteam\u2019s dumpgenerator.py script. You have a simple tutorial <a href=\"https://github.com/WikiTeam/wikiteam/wiki/Tutorial#I_have_no_shell_access_to_server\" rel=\"nofollow\">in\ntheir\nwiki</a>.\nIts usage is very straightforward and the script is well maintained.\nRemember to use the \u2013xml option to download the full history dump.</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 4694724, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "eb06bf5c229e60361d668c08ae41948c", "sha256": "3f1dfadb390ca27000ef877a080a3b9ec2552e8b8d0af3eaa1541f3a230e02c7"}, "downloads": -1, "filename": "wiki_dump_parser-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "eb06bf5c229e60361d668c08ae41948c", "packagetype": "bdist_wheel", "python_version": "3.5", "requires_python": null, "size": 15141, "upload_time": "2018-10-17T11:26:25", "upload_time_iso_8601": "2018-10-17T11:26:25.822153Z", "url": "https://files.pythonhosted.org/packages/b7/25/266fa6407c83131f931922ca963d096d47861f3f5517f3fa717f3a26b19c/wiki_dump_parser-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5239fa63715ca2ff1770d78c27696947", "sha256": "f516738ad605b44495cacb31a36ec79c99e4c91c3051d4994713ddc73a5a537c"}, "downloads": -1, "filename": "wiki_dump_parser-1.0.0.tar.gz", "has_sig": false, "md5_digest": "5239fa63715ca2ff1770d78c27696947", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2620, "upload_time": "2018-10-17T11:26:27", "upload_time_iso_8601": "2018-10-17T11:26:27.522968Z", "url": "https://files.pythonhosted.org/packages/a2/6c/d688294c355f96bcb365d12b1b32e758ff74d4ceeb591542cccb8d22a5e8/wiki_dump_parser-1.0.0.tar.gz", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "e5cfa3e037c838650554b7576c4dbc71", "sha256": "1cf7885408b7032510b79210271922d9e5bde357361fa86be0f84b50c33e7e43"}, "downloads": -1, "filename": "wiki_dump_parser-2.0.0.tar.gz", "has_sig": false, "md5_digest": "e5cfa3e037c838650554b7576c4dbc71", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4121, "upload_time": "2019-01-14T15:28:56", "upload_time_iso_8601": "2019-01-14T15:28:56.366642Z", "url": "https://files.pythonhosted.org/packages/79/a9/e26a0e1077d641c0f7cf3018d1d5d4ce17657822b5ca1485e05539189527/wiki_dump_parser-2.0.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "b3b982c2e2665e217d409c50fe5cbd34", "sha256": "05d6a6e2af0d7faf57b4d69f7155a6028d24991a80361b22c8f8848b580842f7"}, "downloads": -1, "filename": "wiki_dump_parser-2.0.1.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "b3b982c2e2665e217d409c50fe5cbd34", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 5628, "upload_time": "2019-01-14T15:42:27", "upload_time_iso_8601": "2019-01-14T15:42:27.361789Z", "url": "https://files.pythonhosted.org/packages/79/ff/e06e6bfa775e6e2cffe1945ac85fbca22bf9c0c177cabd39109a9a5c11b6/wiki_dump_parser-2.0.1.linux-x86_64.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b3b982c2e2665e217d409c50fe5cbd34", "sha256": "05d6a6e2af0d7faf57b4d69f7155a6028d24991a80361b22c8f8848b580842f7"}, "downloads": -1, "filename": "wiki_dump_parser-2.0.1.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "b3b982c2e2665e217d409c50fe5cbd34", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 5628, "upload_time": "2019-01-14T15:42:27", "upload_time_iso_8601": "2019-01-14T15:42:27.361789Z", "url": "https://files.pythonhosted.org/packages/79/ff/e06e6bfa775e6e2cffe1945ac85fbca22bf9c0c177cabd39109a9a5c11b6/wiki_dump_parser-2.0.1.linux-x86_64.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:29:19 2020"}