{"info": {"author": "Bogdan Kulynych, Mohammad Yaghini", "author_email": "hello@bogdankulynych.me, mohammad.yaghini@epfl.ch", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: Implementation :: CPython", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "###\nmia\n###\n\n|pypi| |license| |build_status| |docs_status| |zenodo|\n\n.. |pypi| image:: https://img.shields.io/pypi/v/mia.svg\n   :target: https://pypi.org/project/mia/\n   :alt: PyPI version\n\n.. |build_status| image:: https://travis-ci.org/spring-epfl/mia.svg?branch=master\n   :target: https://travis-ci.org/spring-epfl/mia\n   :alt: Build status\n\n.. |docs_status| image:: https://readthedocs.org/projects/mia-lib/badge/?version=latest\n   :target: https://mia-lib.readthedocs.io/?badge=latest\n   :alt: Documentation status\n\n.. |license| image:: https://img.shields.io/pypi/l/mia.svg\n   :target: https://pypi.org/project/mia/\n   :alt: License\n\n.. |zenodo| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.1433744.svg\n   :target: https://zenodo.org/record/1433744\n   :alt: Citing with the Zenodo\n\nA library for running membership inference attacks (MIA) against machine learning models. Check out\nthe `documentation <https://mia-lib.rtfd.io>`_.\n\n.. description-marker-do-not-remove\n\nThese are attacks against privacy of the training data. In MIA, an attacker tries to guess whether a\ngiven example was used during training of a target model or not, only by querying the model. See\nmore in the paper by `Shokri et al <https://arxiv.org/abs/1610.05820>`_. Currently, you can use the\nlibrary to evaluate the robustness of your Keras or PyTorch models to MIA.\n\nFeatures:\n\n* Implements the original shadow model `attack <https://arxiv.org/abs/1610.05820>`_\n* Is customizable, can use any scikit learn's ``Estimator``-like object as a shadow or attack model\n* Is tested with Keras and PyTorch\n\n.. getting-started-marker-do-not-remove\n\n===============\nGetting started\n===============\n\nYou can install mia from PyPI:\n\n.. code-block::  bash\n\n    pip install mia\n\n.. usage-marker-do-not-remove\n\n=====\nUsage \n=====\n\nShokri et al. attack\n====================\n\nSee the `full runnable example\n<https://github.com/spring-epfl/mia/tree/master/examples/cifar10.py>`_.  Read the details of the\nattack in the `paper <https://arxiv.org/abs/1610.05820>`_.\n\nLet ``target_model_fn()`` return the target model architecture as a scikit-like classifier. The\nattack is white-box, meaning the attacker is assumed to know the architecture. Let ``NUM_CLASSES``\nbe the number of classes of the classification problem.\n\nFirst, the attacker needs to train several *shadow models* \u2014that mimick the target model\u2014\non different datasets sampled from the original data distribution. The following code snippet\ninitializes a *shadow model bundle*, and runs the training of the shadows. For each shadow model,\n``2 * SHADOW_DATASET_SIZE`` examples are sampled without replacement from the full attacker's\ndataset.  Half of them will be used for control, and the other half for training of the shadow model.\n\n.. code-block::  python\n\n    from mia.estimators import ShadowModelBundle\n\n    smb = ShadowModelBundle(\n        target_model_fn,\n        shadow_dataset_size=SHADOW_DATASET_SIZE,\n        num_models=NUM_MODELS,\n    )\n    X_shadow, y_shadow = smb.fit_transform(attacker_X_train, attacker_y_train)\n\n``fit_transform`` returns *attack data* ``X_shadow, y_shadow``. Each row in ``X_shadow`` is a\nconcatenated vector consisting of the prediction vector of a shadow model for an example from the\noriginal dataset, and the example's class (one-hot encoded). Its shape is hence ``(2 *\nSHADOW_DATASET_SIZE, 2 * NUM_CLASSES)``. Each label in ``y_shadow`` is zero if a corresponding\nexample was \"out\" of the training dataset of the shadow model (control), or one, if it was \"in\" the\ntraining.\n\nmia provides a class to train a bundle of attack models, one model per class. ``attack_model_fn()``\nis supposed to return a scikit-like classifier that takes a vector of model predictions ``(NUM_CLASSES, )``,\nand returns whether an example with these predictions was in the training, or out.\n\n.. code-block::  python\n\n    from mia.estimators import AttackModelBundle\n\n    amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n    amb.fit(X_shadow, y_shadow)\n\nIn place of the ``AttackModelBundle`` one can use any binary classifier that takes ``(2 *\nNUM_CLASSES, )``-shape examples (as explained above, the first half of an input is the prediction\nvector from a model, the second half is the true class of a corresponding example).\n\nTo evaluate the attack, one must encode the data in the above-mentioned format. Let ``target_model`` be\nthe target model, ``data_in`` the data (tuple ``X, y``) that was used in the training of the target model, and\n``data_out`` the data that was not used in the training.\n\n.. code-block::  python\n\n    from mia.estimators import prepare_attack_data    \n\n    attack_test_data, real_membership_labels = prepare_attack_data(\n        target_model, data_in, data_out\n    )\n\n    attack_guesses = amb.predict(attack_test_data)\n    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n\n.. misc-marker-do-not-remove\n\n======\nCiting\n======\n\n.. code-block::\n\n   @misc{mia,\n     author       = {Bogdan Kulynych and\n                     Mohammad Yaghini},\n     title        = {{mia: A library for running membership inference \n                      attacks against ML models}},\n     month        = sep,\n     year         = 2018,\n     doi          = {10.5281/zenodo.1433744},\n     url          = {https://doi.org/10.5281/zenodo.1433744}\n   }\n\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/spring-epfl/mia", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "mia", "package_url": "https://pypi.org/project/mia/", "platform": "", "project_url": "https://pypi.org/project/mia/", "project_urls": {"Homepage": "https://github.com/spring-epfl/mia"}, "release_url": "https://pypi.org/project/mia/0.1.2/", "requires_dist": ["numpy", "scipy", "scikit-learn", "torch", "tqdm", "pytest; extra == 'dev'", "pytest-lazy-fixture; extra == 'dev'", "tensorflow; extra == 'dev'", "skorch; extra == 'dev'", "keras; extra == 'dev'", "sphinx; extra == 'dev'", "sphinx-rtd-theme; extra == 'dev'", "black; extra == 'dev'", "pytest; extra == 'test'", "pytest-lazy-fixture; extra == 'test'", "tensorflow; extra == 'test'", "skorch; extra == 'test'", "keras; extra == 'test'"], "requires_python": "", "summary": "A library for running membership inference attacks against ML models", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://pypi.org/project/mia/\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a976c83130aa3ca4964b04788995610953aa7d42/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6d69612e737667\"></a> <a href=\"https://pypi.org/project/mia/\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/53ce47d8e8d840d9f9ba5675ec3791af1a38a764/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6d69612e737667\"></a> <a href=\"https://travis-ci.org/spring-epfl/mia\" rel=\"nofollow\"><img alt=\"Build status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/662c63539924ae1352c4a0d4ab71e84c5cc8f6b7/68747470733a2f2f7472617669732d63692e6f72672f737072696e672d6570666c2f6d69612e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://mia-lib.readthedocs.io/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f05e23b5dee8f40be99351354edde6a43727fe66/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6d69612d6c69622f62616467652f3f76657273696f6e3d6c6174657374\"></a> <a href=\"https://zenodo.org/record/1433744\" rel=\"nofollow\"><img alt=\"Citing with the Zenodo\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/dc5b977f515eeac2c027e7d2aae35e74be67665a/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e313433333734342e737667\"></a></p>\n<p>A library for running membership inference attacks (MIA) against machine learning models. Check out\nthe <a href=\"https://mia-lib.rtfd.io\" rel=\"nofollow\">documentation</a>.</p>\n<p>These are attacks against privacy of the training data. In MIA, an attacker tries to guess whether a\ngiven example was used during training of a target model or not, only by querying the model. See\nmore in the paper by <a href=\"https://arxiv.org/abs/1610.05820\" rel=\"nofollow\">Shokri et al</a>. Currently, you can use the\nlibrary to evaluate the robustness of your Keras or PyTorch models to MIA.</p>\n<p>Features:</p>\n<ul>\n<li>Implements the original shadow model <a href=\"https://arxiv.org/abs/1610.05820\" rel=\"nofollow\">attack</a></li>\n<li>Is customizable, can use any scikit learn\u2019s <tt>Estimator</tt>-like object as a shadow or attack model</li>\n<li>Is tested with Keras and PyTorch</li>\n</ul>\n<div id=\"getting-started\">\n<h2>Getting started</h2>\n<p>You can install mia from PyPI:</p>\n<pre>pip install mia\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<h2 id=\"shokri-et-al-attack\"><span class=\"section-subtitle\">Shokri et al. attack</span></h2>\n<p>See the <a href=\"https://github.com/spring-epfl/mia/tree/master/examples/cifar10.py\" rel=\"nofollow\">full runnable example</a>.  Read the details of the\nattack in the <a href=\"https://arxiv.org/abs/1610.05820\" rel=\"nofollow\">paper</a>.</p>\n<p>Let <tt>target_model_fn()</tt> return the target model architecture as a scikit-like classifier. The\nattack is white-box, meaning the attacker is assumed to know the architecture. Let <tt>NUM_CLASSES</tt>\nbe the number of classes of the classification problem.</p>\n<p>First, the attacker needs to train several <em>shadow models</em> \u2014that mimick the target model\u2014\non different datasets sampled from the original data distribution. The following code snippet\ninitializes a <em>shadow model bundle</em>, and runs the training of the shadows. For each shadow model,\n<tt>2 * SHADOW_DATASET_SIZE</tt> examples are sampled without replacement from the full attacker\u2019s\ndataset.  Half of them will be used for control, and the other half for training of the shadow model.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">mia.estimators</span> <span class=\"kn\">import</span> <span class=\"n\">ShadowModelBundle</span>\n\n<span class=\"n\">smb</span> <span class=\"o\">=</span> <span class=\"n\">ShadowModelBundle</span><span class=\"p\">(</span>\n    <span class=\"n\">target_model_fn</span><span class=\"p\">,</span>\n    <span class=\"n\">shadow_dataset_size</span><span class=\"o\">=</span><span class=\"n\">SHADOW_DATASET_SIZE</span><span class=\"p\">,</span>\n    <span class=\"n\">num_models</span><span class=\"o\">=</span><span class=\"n\">NUM_MODELS</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">X_shadow</span><span class=\"p\">,</span> <span class=\"n\">y_shadow</span> <span class=\"o\">=</span> <span class=\"n\">smb</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">attacker_X_train</span><span class=\"p\">,</span> <span class=\"n\">attacker_y_train</span><span class=\"p\">)</span>\n</pre>\n<p><tt>fit_transform</tt> returns <em>attack data</em> <tt>X_shadow, y_shadow</tt>. Each row in <tt>X_shadow</tt> is a\nconcatenated vector consisting of the prediction vector of a shadow model for an example from the\noriginal dataset, and the example\u2019s class (one-hot encoded). Its shape is hence <tt>(2 *\nSHADOW_DATASET_SIZE, 2 * NUM_CLASSES)</tt>. Each label in <tt>y_shadow</tt> is zero if a corresponding\nexample was \u201cout\u201d of the training dataset of the shadow model (control), or one, if it was \u201cin\u201d the\ntraining.</p>\n<p>mia provides a class to train a bundle of attack models, one model per class. <tt>attack_model_fn()</tt>\nis supposed to return a scikit-like classifier that takes a vector of model predictions <tt>(NUM_CLASSES, )</tt>,\nand returns whether an example with these predictions was in the training, or out.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">mia.estimators</span> <span class=\"kn\">import</span> <span class=\"n\">AttackModelBundle</span>\n\n<span class=\"n\">amb</span> <span class=\"o\">=</span> <span class=\"n\">AttackModelBundle</span><span class=\"p\">(</span><span class=\"n\">attack_model_fn</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"n\">NUM_CLASSES</span><span class=\"p\">)</span>\n<span class=\"n\">amb</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_shadow</span><span class=\"p\">,</span> <span class=\"n\">y_shadow</span><span class=\"p\">)</span>\n</pre>\n<p>In place of the <tt>AttackModelBundle</tt> one can use any binary classifier that takes <tt>(2 *\nNUM_CLASSES, )</tt>-shape examples (as explained above, the first half of an input is the prediction\nvector from a model, the second half is the true class of a corresponding example).</p>\n<p>To evaluate the attack, one must encode the data in the above-mentioned format. Let <tt>target_model</tt> be\nthe target model, <tt>data_in</tt> the data (tuple <tt>X, y</tt>) that was used in the training of the target model, and\n<tt>data_out</tt> the data that was not used in the training.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">mia.estimators</span> <span class=\"kn\">import</span> <span class=\"n\">prepare_attack_data</span>\n\n<span class=\"n\">attack_test_data</span><span class=\"p\">,</span> <span class=\"n\">real_membership_labels</span> <span class=\"o\">=</span> <span class=\"n\">prepare_attack_data</span><span class=\"p\">(</span>\n    <span class=\"n\">target_model</span><span class=\"p\">,</span> <span class=\"n\">data_in</span><span class=\"p\">,</span> <span class=\"n\">data_out</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">attack_guesses</span> <span class=\"o\">=</span> <span class=\"n\">amb</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">attack_test_data</span><span class=\"p\">)</span>\n<span class=\"n\">attack_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">attack_guesses</span> <span class=\"o\">==</span> <span class=\"n\">real_membership_labels</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"citing\">\n<h2>Citing</h2>\n<pre>@misc{mia,\n  author       = {Bogdan Kulynych and\n                  Mohammad Yaghini},\n  title        = {{mia: A library for running membership inference\n                   attacks against ML models}},\n  month        = sep,\n  year         = 2018,\n  doi          = {10.5281/zenodo.1433744},\n  url          = {https://doi.org/10.5281/zenodo.1433744}\n}\n</pre>\n</div>\n\n          </div>"}, "last_serial": 4315812, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "177bf610890a7d3c60d1cc66830e65ae", "sha256": "40a6cd9a74099033e8216ed01d07f64c2af0d599eea28d96e837682d44153798"}, "downloads": -1, "filename": "mia-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "177bf610890a7d3c60d1cc66830e65ae", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10142, "upload_time": "2018-09-23T18:28:13", "upload_time_iso_8601": "2018-09-23T18:28:13.842135Z", "url": "https://files.pythonhosted.org/packages/e8/1b/232e9de9f96887d1c7add2a3226ca02f2e3ea5c1c92803e833bbd405720f/mia-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "52a1ac440fcdcfb7fd720aea27b272a0", "sha256": "33de8d5e9246d2ac2cab55ead798cd39c42fb6fcc7a009f0464ca4a95f87bb64"}, "downloads": -1, "filename": "mia-0.1.1.tar.gz", "has_sig": false, "md5_digest": "52a1ac440fcdcfb7fd720aea27b272a0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15890, "upload_time": "2018-09-23T18:28:15", "upload_time_iso_8601": "2018-09-23T18:28:15.305791Z", "url": "https://files.pythonhosted.org/packages/e1/cb/2b2ba3712896fd293fe58c34a2d00ac178c25b85263c7632448f8bc75517/mia-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "684eaac8c7d551bf2d48c1210dcc0916", "sha256": "f170384300f1e08898237cf7f43582fb2b95c0ea1ac56c2c8198e18f58633154"}, "downloads": -1, "filename": "mia-0.1.2-py2-none-any.whl", "has_sig": false, "md5_digest": "684eaac8c7d551bf2d48c1210dcc0916", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 13470, "upload_time": "2018-09-27T11:00:15", "upload_time_iso_8601": "2018-09-27T11:00:15.582712Z", "url": "https://files.pythonhosted.org/packages/ff/2c/9027be252e23650230be310f9b7886fc2f9f5bed77afe89542441a44c8ce/mia-0.1.2-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ea7ab0440a729e549018171836ee3fc2", "sha256": "118606dfe9f8cbd8ecb6edcd1231d5360decbf4ff02d90a1696df73958580a55"}, "downloads": -1, "filename": "mia-0.1.2.tar.gz", "has_sig": false, "md5_digest": "ea7ab0440a729e549018171836ee3fc2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17240, "upload_time": "2018-09-27T11:00:17", "upload_time_iso_8601": "2018-09-27T11:00:17.263832Z", "url": "https://files.pythonhosted.org/packages/7d/12/f149a7cd43e49725921e9884363aa3cbfea8a49c319a944eb71d48973fa9/mia-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "684eaac8c7d551bf2d48c1210dcc0916", "sha256": "f170384300f1e08898237cf7f43582fb2b95c0ea1ac56c2c8198e18f58633154"}, "downloads": -1, "filename": "mia-0.1.2-py2-none-any.whl", "has_sig": false, "md5_digest": "684eaac8c7d551bf2d48c1210dcc0916", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 13470, "upload_time": "2018-09-27T11:00:15", "upload_time_iso_8601": "2018-09-27T11:00:15.582712Z", "url": "https://files.pythonhosted.org/packages/ff/2c/9027be252e23650230be310f9b7886fc2f9f5bed77afe89542441a44c8ce/mia-0.1.2-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ea7ab0440a729e549018171836ee3fc2", "sha256": "118606dfe9f8cbd8ecb6edcd1231d5360decbf4ff02d90a1696df73958580a55"}, "downloads": -1, "filename": "mia-0.1.2.tar.gz", "has_sig": false, "md5_digest": "ea7ab0440a729e549018171836ee3fc2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17240, "upload_time": "2018-09-27T11:00:17", "upload_time_iso_8601": "2018-09-27T11:00:17.263832Z", "url": "https://files.pythonhosted.org/packages/7d/12/f149a7cd43e49725921e9884363aa3cbfea8a49c319a944eb71d48973fa9/mia-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:55:23 2020"}