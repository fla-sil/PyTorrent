{"info": {"author": "Ozzy", "author_email": "cfhamlet@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3 :: Only", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy"], "description": "# os-rq-scrapy\n\n[![Build Status](https://www.travis-ci.org/cfhamlet/os-rq-scrapy.svg?branch=master)](https://www.travis-ci.org/cfhamlet/os-rq-scrapy)\n[![codecov](https://codecov.io/gh/cfhamlet/os-rq-scrapy/branch/master/graph/badge.svg)](https://codecov.io/gh/cfhamlet/os-rq-scrapy)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/os-rq-scrapy.svg)](https://pypi.python.org/pypi/os-rq-scrapy)\n[![PyPI](https://img.shields.io/pypi/v/os-rq-scrapy.svg)](https://pypi.python.org/pypi/os-rq-scrapy)\n\n\nA framework for [Scrapy](https://github.com/scrapy/scrapy) working with [os-rq-pod](https://github.com/cfhamlet/os-rq-pod) and [os-rq-hub](https://github.com/cfhamlet/os-rq-hub) to build [\"broad crawls\"](https://docs.scrapy.org/en/latest/topics/broad-crawls.html) system.\n\nAs you know, Scrapy is a very popular python crawler framework. It is suit for \"focused crawl\", start from several URLs of specific sites, fetch html, extract and save \"structured data\" also with patternd links to crawl recursively. But for large scale, long time crawling especially \"broad crawls\", scrapy is incompetent. Basically, you have to decouple the whole crawling system into several sub-systems, high-performance full-featured distributed fetcher, task scheduler, html extractor, link database, data storage, proxy and a lot of auxiliary equipments. It will be more complex when your system is for multi-tenancy.\n\nThe os-rq-scrapy and [os-rq-pod](https://github.com/cfhamlet/os-rq-pod) project are basic components to build \"broad crawls\" system. The core conceptions are very simple, os-rq-pod is multi-sites request queue have http api to recieve requests. os-rq-scrapy is fetcher, getting requests from os-rq-pod and crawl multi-sites concurrently.  [os-rq-hub](https://github.com/cfhamlet/os-rq-hub) can also be used to connect multi pod and scrapy instances to work simultaneously.\n\n\n## Requirements\n\n* Python 3.6+ (pypy3.6+)\n* [Scrapy](https://github.com/scrapy/scrapy) 2.0\n\nextra requirements:\n\n* [ujson](https://github.com/ultrajson/ultrajson), for json performance\n\n## Install\n\n```\npip install os-rq-scrapy\n```\n\n## Usage\n\n### Command line\n\n``rq-scrapy`` command enhance the basic ``scrapy`` command. When RQ_API is configured, the ``crawl`` subcommand will run on rq mode, get requests from rq.\n\n## Unit Tests\n\n```\ntox\n```\n\n## License\n\nMIT licensed.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/cfhamlet/os-rq-scrapy", "keywords": "", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "os-rq-scrapy", "package_url": "https://pypi.org/project/os-rq-scrapy/", "platform": "", "project_url": "https://pypi.org/project/os-rq-scrapy/", "project_urls": {"Homepage": "https://github.com/cfhamlet/os-rq-scrapy"}, "release_url": "https://pypi.org/project/os-rq-scrapy/0.0.7/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Scrapy for Request Queue", "version": "0.0.7", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>os-rq-scrapy</h1>\n<p><a href=\"https://www.travis-ci.org/cfhamlet/os-rq-scrapy\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e6843cd7bb0063a2cd97045892ff2fb6c3082540/68747470733a2f2f7777772e7472617669732d63692e6f72672f636668616d6c65742f6f732d72712d7363726170792e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://codecov.io/gh/cfhamlet/os-rq-scrapy\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9ef789e87a838ad069e84e90e6599745dcebca93/68747470733a2f2f636f6465636f762e696f2f67682f636668616d6c65742f6f732d72712d7363726170792f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/os-rq-scrapy\" rel=\"nofollow\"><img alt=\"PyPI - Python Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7859ec7fe776b981d7a39796bc59ded15f223fa5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6f732d72712d7363726170792e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/os-rq-scrapy\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/50a3d287fce30d75e4e8ca28717331c4ca2b8a2e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6f732d72712d7363726170792e737667\"></a></p>\n<p>A framework for <a href=\"https://github.com/scrapy/scrapy\" rel=\"nofollow\">Scrapy</a> working with <a href=\"https://github.com/cfhamlet/os-rq-pod\" rel=\"nofollow\">os-rq-pod</a> and <a href=\"https://github.com/cfhamlet/os-rq-hub\" rel=\"nofollow\">os-rq-hub</a> to build <a href=\"https://docs.scrapy.org/en/latest/topics/broad-crawls.html\" rel=\"nofollow\">\"broad crawls\"</a> system.</p>\n<p>As you know, Scrapy is a very popular python crawler framework. It is suit for \"focused crawl\", start from several URLs of specific sites, fetch html, extract and save \"structured data\" also with patternd links to crawl recursively. But for large scale, long time crawling especially \"broad crawls\", scrapy is incompetent. Basically, you have to decouple the whole crawling system into several sub-systems, high-performance full-featured distributed fetcher, task scheduler, html extractor, link database, data storage, proxy and a lot of auxiliary equipments. It will be more complex when your system is for multi-tenancy.</p>\n<p>The os-rq-scrapy and <a href=\"https://github.com/cfhamlet/os-rq-pod\" rel=\"nofollow\">os-rq-pod</a> project are basic components to build \"broad crawls\" system. The core conceptions are very simple, os-rq-pod is multi-sites request queue have http api to recieve requests. os-rq-scrapy is fetcher, getting requests from os-rq-pod and crawl multi-sites concurrently.  <a href=\"https://github.com/cfhamlet/os-rq-hub\" rel=\"nofollow\">os-rq-hub</a> can also be used to connect multi pod and scrapy instances to work simultaneously.</p>\n<h2>Requirements</h2>\n<ul>\n<li>Python 3.6+ (pypy3.6+)</li>\n<li><a href=\"https://github.com/scrapy/scrapy\" rel=\"nofollow\">Scrapy</a> 2.0</li>\n</ul>\n<p>extra requirements:</p>\n<ul>\n<li><a href=\"https://github.com/ultrajson/ultrajson\" rel=\"nofollow\">ujson</a>, for json performance</li>\n</ul>\n<h2>Install</h2>\n<pre><code>pip install os-rq-scrapy\n</code></pre>\n<h2>Usage</h2>\n<h3>Command line</h3>\n<p><code>rq-scrapy</code> command enhance the basic <code>scrapy</code> command. When RQ_API is configured, the <code>crawl</code> subcommand will run on rq mode, get requests from rq.</p>\n<h2>Unit Tests</h2>\n<pre><code>tox\n</code></pre>\n<h2>License</h2>\n<p>MIT licensed.</p>\n\n          </div>"}, "last_serial": 6990617, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "9fd395ac1331cd02c4f8c77bebb94c02", "sha256": "b5ab12a431c11ade577b6f0dd419c5218a8c976c9b35b85f9788d4a43775652c"}, "downloads": -1, "filename": "os-rq-scrapy-0.0.1.tar.gz", "has_sig": false, "md5_digest": "9fd395ac1331cd02c4f8c77bebb94c02", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11746, "upload_time": "2020-03-02T15:41:56", "upload_time_iso_8601": "2020-03-02T15:41:56.213015Z", "url": "https://files.pythonhosted.org/packages/9d/1e/ac40f9265d0390479011c895d638e85ae1a01eed6ab4d39c80272744434f/os-rq-scrapy-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "88a0de2a816d7fed932506cfb39d6c30", "sha256": "d387ff1ce10bb516cc0b4cc7bfbd71c91eb8c56a0787d8463b51124837b932d9"}, "downloads": -1, "filename": "os-rq-scrapy-0.0.2.tar.gz", "has_sig": false, "md5_digest": "88a0de2a816d7fed932506cfb39d6c30", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13809, "upload_time": "2020-03-05T07:59:46", "upload_time_iso_8601": "2020-03-05T07:59:46.154373Z", "url": "https://files.pythonhosted.org/packages/f6/29/55252f3f2cb5e26ae11a7f95de586e0e3683545b41b3907d33cd67e72210/os-rq-scrapy-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "1ea9dd9cda8242a3f59683c9a0512c07", "sha256": "758c5605e165ff70caf6ed0de078a5525cb13094ccaaae51d3b86d2250c995ff"}, "downloads": -1, "filename": "os-rq-scrapy-0.0.3.tar.gz", "has_sig": false, "md5_digest": "1ea9dd9cda8242a3f59683c9a0512c07", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13890, "upload_time": "2020-03-05T14:19:10", "upload_time_iso_8601": "2020-03-05T14:19:10.204148Z", "url": "https://files.pythonhosted.org/packages/20/65/3c017feaa1a23f85452381e109cf8154b797b43f91375ae2f7819057b23c/os-rq-scrapy-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "46554a20ef970f23ce7df8cc7c13a626", "sha256": "2e58bb6ad3ffab59f088769238f73485688d1f3d713392a7ae4a51a35c35a350"}, "downloads": -1, "filename": "os-rq-scrapy-0.0.4.tar.gz", "has_sig": false, "md5_digest": "46554a20ef970f23ce7df8cc7c13a626", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13883, "upload_time": "2020-03-06T12:01:39", "upload_time_iso_8601": "2020-03-06T12:01:39.287929Z", "url": "https://files.pythonhosted.org/packages/14/f1/be5c385ecb9c0b8a047eb1d7a16e573173943852e6685f56bd606fd72882/os-rq-scrapy-0.0.4.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "85e419b51165f25cb98b0e8b2762621e", "sha256": "8829d6396d5655b239de3f99dc6a7f99a1273fa5939ca9fa3c0d27114e833c72"}, "downloads": -1, "filename": "os-rq-scrapy-0.0.6.tar.gz", "has_sig": false, "md5_digest": "85e419b51165f25cb98b0e8b2762621e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13931, "upload_time": "2020-03-10T08:18:00", "upload_time_iso_8601": "2020-03-10T08:18:00.645138Z", "url": "https://files.pythonhosted.org/packages/c5/af/2f7a8a7817252fe1b0594828532fb2b72f44763d85c9d2e547e42f923255/os-rq-scrapy-0.0.6.tar.gz", "yanked": false}], "0.0.7": [{"comment_text": "", "digests": {"md5": "7499d2ce97696fdade6b8e122ddb5c96", "sha256": "3ada9612a96b6e11e1f887fa5e7b189ac31f1e39347cae46c327750fd6812034"}, "downloads": -1, "filename": "os-rq-scrapy-0.0.7.tar.gz", "has_sig": false, "md5_digest": "7499d2ce97696fdade6b8e122ddb5c96", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13936, "upload_time": "2020-04-10T03:50:11", "upload_time_iso_8601": "2020-04-10T03:50:11.683733Z", "url": "https://files.pythonhosted.org/packages/00/ca/974fdc645ff6521677b6629fcf0726fe5df1e9fa79b1461fb720c455c67f/os-rq-scrapy-0.0.7.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "7499d2ce97696fdade6b8e122ddb5c96", "sha256": "3ada9612a96b6e11e1f887fa5e7b189ac31f1e39347cae46c327750fd6812034"}, "downloads": -1, "filename": "os-rq-scrapy-0.0.7.tar.gz", "has_sig": false, "md5_digest": "7499d2ce97696fdade6b8e122ddb5c96", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13936, "upload_time": "2020-04-10T03:50:11", "upload_time_iso_8601": "2020-04-10T03:50:11.683733Z", "url": "https://files.pythonhosted.org/packages/00/ca/974fdc645ff6521677b6629fcf0726fe5df1e9fa79b1461fb720c455c67f/os-rq-scrapy-0.0.7.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:01:00 2020"}