{"info": {"author": "Fran\u00e7ois-Guillaume Fernandez", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Mathematics", "Topic :: Software Development", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "\n# Torchcam: class activation explorer\n\n[![License](https://img.shields.io/badge/License-MIT-brightgreen.svg)](LICENSE) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/5124b1dff75e4e9cbb68136516605032)](https://www.codacy.com/manual/frgfm/torch-cam?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=frgfm/torch-cam&amp;utm_campaign=Badge_Grade)  ![Build Status](https://github.com/frgfm/torch-cam/workflows/python-package/badge.svg) [![codecov](https://codecov.io/gh/frgfm/torch-cam/branch/master/graph/badge.svg)](https://codecov.io/gh/frgfm/torch-cam) [![Docs](https://img.shields.io/badge/docs-available-blue.svg)](https://frgfm.github.io/torch-cam)  [![Pypi](https://img.shields.io/badge/pypi-v0.1.0-blue.svg)](https://pypi.org/project/torchcam/) \n\nSimple way to leverage the class-specific activation of convolutional layers in PyTorch.\n\n\n\n## Table of Contents\n\n* [Getting Started](#getting-started)\n  * [Prerequisites](#prerequisites)\n  * [Installation](#installation)\n* [Usage](#usage)\n* [Technical Roadmap](#technical-roadmap)\n* [Documentation](#documentation)\n* [Contributing](#contributing)\n* [Credits](#credits)\n* [License](#license)\n\n\n\n## Getting started\n\n### Prerequisites\n\n- Python 3.6 (or more recent)\n- [pip](https://pip.pypa.io/en/stable/)\n\n### Installation\n\nYou can install the package using [pypi](https://pypi.org/project/torch-cam/) as follows:\n\n```shell\npip install torchcam\n```\n\n\n\n## Usage\n\nYou can find a detailed example below to retrieve the CAM of a specific class on a resnet architecture.\n\n```python\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.models import resnet50\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms.functional import to_pil_image\nfrom torchcam.cams import CAM, GradCAM, GradCAMpp\nfrom torchcam.utils import overlay_mask\n\n\n# Pretrained imagenet model\nmodel = resnet50(pretrained=True)\n# Specify layer to hook and fully connected\nconv_layer = 'layer4'\n\n# Hook the corresponding layer in the model\ngradcam = GradCAMpp(model, conv_layer)\n\n# Get a dog image\nURL = 'https://www.woopets.fr/assets/races/000/066/big-portrait/border-collie.jpg'\nresponse = requests.get(URL)\n\n# Forward an image\npil_img = Image.open(BytesIO(response.content), mode='r').convert('RGB')\npreprocess = transforms.Compose([\n   transforms.Resize((224,224)),\n   transforms.ToTensor(),\n   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\nimg_tensor = preprocess(pil_img)\nout = model(img_tensor.unsqueeze(0))\n\n# Select the class index\nclasses = {int(key):value for (key, value)\n          in requests.get('https://s3.amazonaws.com/outcome-blog/imagenet/labels.json').json().items()}\nclass_idx = 232\n\n# Use the hooked data to compute activation map\nactivation_maps = gradcam(out, class_idx)\n# Convert it to PIL image\n# The indexing below means first image in batch\nheatmap = to_pil_image(activation_maps[0].cpu().numpy(), mode='F')\n\n# Plot the result\nresult = overlay_mask(pil_img, heatmap)\nplt.imshow(result); plt.axis('off'); plt.title(classes.get(class_idx)); plt.tight_layout; plt.show()\n```\n\n![gradcam_sample](static/images/gradcam_sample.png)\n\n\n\n\n\n## Technical roadmap\n\nThe project is currently under development, here are the objectives for the next releases:\n\n- [x] Parallel CAMs: enable batch processing.\n- [ ] Benchmark: compare class activation map computations for different architectures.\n- [ ] Signature improvement: retrieve automatically the last convolutional layer.\n- [ ] Refine RPN: create a region proposal network using CAM.\n- [ ] Task transfer: turn a well-trained classifier into an object detector.\n\n\n\n## Documentation\n\nThe full package documentation is available [here](https://frgfm.github.io/torch-cam/) for detailed specifications. The documentation was built with [Sphinx](sphinx-doc.org) using a [theme](github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](readthedocs.org).\n\n\n\n## Contributing\n\nPlease refer to `CONTRIBUTING` if you wish to contribute to this project.\n\n\n\n## Credits\n\nThis project is developed and maintained by the repo owner, but the implementation was based on the following precious papers:\n\n- [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150): the original CAM paper\n- [Grad-CAM](https://arxiv.org/abs/1610.02391): GradCAM paper, generalizing CAM to models without global average pooling. \n- [Grad-CAM++](https://arxiv.org/abs/1710.11063): improvement of GradCAM++ for more accurate pixel-level contribution to the activation.\n\n\n\n## License\n\nDistributed under the MIT License. See `LICENSE` for more information.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/frgfm/torch-cam/tags", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/frgfm/torch-cam", "keywords": "pytorch,deep learning,cnn,convolution,activation,gradcam", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "torchcam", "package_url": "https://pypi.org/project/torchcam/", "platform": "", "project_url": "https://pypi.org/project/torchcam/", "project_urls": {"Download": "https://github.com/frgfm/torch-cam/tags", "Homepage": "https://github.com/frgfm/torch-cam"}, "release_url": "https://pypi.org/project/torchcam/0.1.0/", "requires_dist": ["torch (>=1.1.0)", "numpy (>=1.14.0)", "pillow (>=5.0.0)", "matplotlib (>=3.0.0)"], "requires_python": ">=3.6.0", "summary": "Class activation maps for your PyTorch CNN models", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Torchcam: class activation explorer</h1>\n<p><a href=\"LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/13402995be86cde517cc34ba2cc02d3de74b86c4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d627269676874677265656e2e737667\"></a> <a href=\"https://www.codacy.com/manual/frgfm/torch-cam?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=frgfm/torch-cam&amp;utm_campaign=Badge_Grade\" rel=\"nofollow\"><img alt=\"Codacy Badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0f88ff03b94412720d539feed478a3a8e1be979a/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3531323462316466663735653465396362623638313336353136363035303332\"></a>  <img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3b76f0913723ee793b6ad1eb4cfd848363ca39a7/68747470733a2f2f6769746875622e636f6d2f667267666d2f746f7263682d63616d2f776f726b666c6f77732f707974686f6e2d7061636b6167652f62616467652e737667\"> <a href=\"https://codecov.io/gh/frgfm/torch-cam\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/de562192c70fb4ee57a385210b6a1274109d8371/68747470733a2f2f636f6465636f762e696f2f67682f667267666d2f746f7263682d63616d2f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a> <a href=\"https://frgfm.github.io/torch-cam\" rel=\"nofollow\"><img alt=\"Docs\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/80e1a75e270226d8519827e9772efdc9697c54a8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d617661696c61626c652d626c75652e737667\"></a>  <a href=\"https://pypi.org/project/torchcam/\" rel=\"nofollow\"><img alt=\"Pypi\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2ec92d29ae938c2adcf887902c39e5a41f9c7423/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707970692d76302e312e302d626c75652e737667\"></a></p>\n<p>Simple way to leverage the class-specific activation of convolutional layers in PyTorch.</p>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#getting-started\" rel=\"nofollow\">Getting Started</a>\n<ul>\n<li><a href=\"#prerequisites\" rel=\"nofollow\">Prerequisites</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n</ul>\n</li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a></li>\n<li><a href=\"#technical-roadmap\" rel=\"nofollow\">Technical Roadmap</a></li>\n<li><a href=\"#documentation\" rel=\"nofollow\">Documentation</a></li>\n<li><a href=\"#contributing\" rel=\"nofollow\">Contributing</a></li>\n<li><a href=\"#credits\" rel=\"nofollow\">Credits</a></li>\n<li><a href=\"#license\" rel=\"nofollow\">License</a></li>\n</ul>\n<h2>Getting started</h2>\n<h3>Prerequisites</h3>\n<ul>\n<li>Python 3.6 (or more recent)</li>\n<li><a href=\"https://pip.pypa.io/en/stable/\" rel=\"nofollow\">pip</a></li>\n</ul>\n<h3>Installation</h3>\n<p>You can install the package using <a href=\"https://pypi.org/project/torch-cam/\" rel=\"nofollow\">pypi</a> as follows:</p>\n<pre>pip install torchcam\n</pre>\n<h2>Usage</h2>\n<p>You can find a detailed example below to retrieve the CAM of a specific class on a resnet architecture.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n<span class=\"kn\">from</span> <span class=\"nn\">io</span> <span class=\"kn\">import</span> <span class=\"n\">BytesIO</span>\n<span class=\"kn\">from</span> <span class=\"nn\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torchvision.models</span> <span class=\"kn\">import</span> <span class=\"n\">resnet50</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torchvision.transforms</span> <span class=\"kn\">import</span> <span class=\"n\">transforms</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torchvision.transforms.functional</span> <span class=\"kn\">import</span> <span class=\"n\">to_pil_image</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torchcam.cams</span> <span class=\"kn\">import</span> <span class=\"n\">CAM</span><span class=\"p\">,</span> <span class=\"n\">GradCAM</span><span class=\"p\">,</span> <span class=\"n\">GradCAMpp</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torchcam.utils</span> <span class=\"kn\">import</span> <span class=\"n\">overlay_mask</span>\n\n\n<span class=\"c1\"># Pretrained imagenet model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">resnet50</span><span class=\"p\">(</span><span class=\"n\">pretrained</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># Specify layer to hook and fully connected</span>\n<span class=\"n\">conv_layer</span> <span class=\"o\">=</span> <span class=\"s1\">'layer4'</span>\n\n<span class=\"c1\"># Hook the corresponding layer in the model</span>\n<span class=\"n\">gradcam</span> <span class=\"o\">=</span> <span class=\"n\">GradCAMpp</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">conv_layer</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Get a dog image</span>\n<span class=\"n\">URL</span> <span class=\"o\">=</span> <span class=\"s1\">'https://www.woopets.fr/assets/races/000/066/big-portrait/border-collie.jpg'</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">URL</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Forward an image</span>\n<span class=\"n\">pil_img</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"n\">BytesIO</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">),</span> <span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">'r'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"s1\">'RGB'</span><span class=\"p\">)</span>\n<span class=\"n\">preprocess</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span>\n   <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Resize</span><span class=\"p\">((</span><span class=\"mi\">224</span><span class=\"p\">,</span><span class=\"mi\">224</span><span class=\"p\">)),</span>\n   <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span>\n   <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">([</span><span class=\"mf\">0.485</span><span class=\"p\">,</span> <span class=\"mf\">0.456</span><span class=\"p\">,</span> <span class=\"mf\">0.406</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.229</span><span class=\"p\">,</span> <span class=\"mf\">0.224</span><span class=\"p\">,</span> <span class=\"mf\">0.225</span><span class=\"p\">])</span>\n<span class=\"p\">])</span>\n<span class=\"n\">img_tensor</span> <span class=\"o\">=</span> <span class=\"n\">preprocess</span><span class=\"p\">(</span><span class=\"n\">pil_img</span><span class=\"p\">)</span>\n<span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">img_tensor</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Select the class index</span>\n<span class=\"n\">classes</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">):</span><span class=\"n\">value</span> <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"p\">)</span>\n          <span class=\"ow\">in</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">json</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">()}</span>\n<span class=\"n\">class_idx</span> <span class=\"o\">=</span> <span class=\"mi\">232</span>\n\n<span class=\"c1\"># Use the hooked data to compute activation map</span>\n<span class=\"n\">activation_maps</span> <span class=\"o\">=</span> <span class=\"n\">gradcam</span><span class=\"p\">(</span><span class=\"n\">out</span><span class=\"p\">,</span> <span class=\"n\">class_idx</span><span class=\"p\">)</span>\n<span class=\"c1\"># Convert it to PIL image</span>\n<span class=\"c1\"># The indexing below means first image in batch</span>\n<span class=\"n\">heatmap</span> <span class=\"o\">=</span> <span class=\"n\">to_pil_image</span><span class=\"p\">(</span><span class=\"n\">activation_maps</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">cpu</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">(),</span> <span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">'F'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Plot the result</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">overlay_mask</span><span class=\"p\">(</span><span class=\"n\">pil_img</span><span class=\"p\">,</span> <span class=\"n\">heatmap</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">);</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"p\">(</span><span class=\"s1\">'off'</span><span class=\"p\">);</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"n\">classes</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">class_idx</span><span class=\"p\">));</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">;</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</pre>\n<p><img alt=\"gradcam_sample\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/25e7231b0ee3f6bbb06db6d5def313f8b9e9e29b/7374617469632f696d616765732f6772616463616d5f73616d706c652e706e67\"></p>\n<h2>Technical roadmap</h2>\n<p>The project is currently under development, here are the objectives for the next releases:</p>\n<ul>\n<li>[x] Parallel CAMs: enable batch processing.</li>\n<li>[ ] Benchmark: compare class activation map computations for different architectures.</li>\n<li>[ ] Signature improvement: retrieve automatically the last convolutional layer.</li>\n<li>[ ] Refine RPN: create a region proposal network using CAM.</li>\n<li>[ ] Task transfer: turn a well-trained classifier into an object detector.</li>\n</ul>\n<h2>Documentation</h2>\n<p>The full package documentation is available <a href=\"https://frgfm.github.io/torch-cam/\" rel=\"nofollow\">here</a> for detailed specifications. The documentation was built with <a href=\"sphinx-doc.org\" rel=\"nofollow\">Sphinx</a> using a <a href=\"github.com/readthedocs/sphinx_rtd_theme\" rel=\"nofollow\">theme</a> provided by <a href=\"readthedocs.org\" rel=\"nofollow\">Read the Docs</a>.</p>\n<h2>Contributing</h2>\n<p>Please refer to <code>CONTRIBUTING</code> if you wish to contribute to this project.</p>\n<h2>Credits</h2>\n<p>This project is developed and maintained by the repo owner, but the implementation was based on the following precious papers:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1512.04150\" rel=\"nofollow\">Learning Deep Features for Discriminative Localization</a>: the original CAM paper</li>\n<li><a href=\"https://arxiv.org/abs/1610.02391\" rel=\"nofollow\">Grad-CAM</a>: GradCAM paper, generalizing CAM to models without global average pooling.</li>\n<li><a href=\"https://arxiv.org/abs/1710.11063\" rel=\"nofollow\">Grad-CAM++</a>: improvement of GradCAM++ for more accurate pixel-level contribution to the activation.</li>\n</ul>\n<h2>License</h2>\n<p>Distributed under the MIT License. See <code>LICENSE</code> for more information.</p>\n\n          </div>"}, "last_serial": 6869896, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "ac26b21c8fefabd16f9a67a071607c0b", "sha256": "46a587b9fde563172660601dd8d5617881dbd6a18bf19487cfb0dbc9c5713068"}, "downloads": -1, "filename": "torchcam-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ac26b21c8fefabd16f9a67a071607c0b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 7935, "upload_time": "2020-03-24T01:35:41", "upload_time_iso_8601": "2020-03-24T01:35:41.426867Z", "url": "https://files.pythonhosted.org/packages/ba/bb/3637e7f8187ad688c19fbb0f466c860a8b29ba40c515fd5ba28c2902d326/torchcam-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "573c667829e415196585f36265510e7c", "sha256": "1319fa2d4e5bd9f55893f564bcbc8ef4608d2e373fb08f0648642bc64eefabf5"}, "downloads": -1, "filename": "torchcam-0.1.0.tar.gz", "has_sig": false, "md5_digest": "573c667829e415196585f36265510e7c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 8184, "upload_time": "2020-03-24T01:35:43", "upload_time_iso_8601": "2020-03-24T01:35:43.201271Z", "url": "https://files.pythonhosted.org/packages/c0/57/cdf4c2ddb47a19504abcc6ec66667692cae653a3079ac040dcb3321a91fa/torchcam-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ac26b21c8fefabd16f9a67a071607c0b", "sha256": "46a587b9fde563172660601dd8d5617881dbd6a18bf19487cfb0dbc9c5713068"}, "downloads": -1, "filename": "torchcam-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ac26b21c8fefabd16f9a67a071607c0b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 7935, "upload_time": "2020-03-24T01:35:41", "upload_time_iso_8601": "2020-03-24T01:35:41.426867Z", "url": "https://files.pythonhosted.org/packages/ba/bb/3637e7f8187ad688c19fbb0f466c860a8b29ba40c515fd5ba28c2902d326/torchcam-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "573c667829e415196585f36265510e7c", "sha256": "1319fa2d4e5bd9f55893f564bcbc8ef4608d2e373fb08f0648642bc64eefabf5"}, "downloads": -1, "filename": "torchcam-0.1.0.tar.gz", "has_sig": false, "md5_digest": "573c667829e415196585f36265510e7c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 8184, "upload_time": "2020-03-24T01:35:43", "upload_time_iso_8601": "2020-03-24T01:35:43.201271Z", "url": "https://files.pythonhosted.org/packages/c0/57/cdf4c2ddb47a19504abcc6ec66667692cae653a3079ac040dcb3321a91fa/torchcam-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:35 2020"}