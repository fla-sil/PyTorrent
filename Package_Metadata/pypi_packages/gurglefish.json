{"info": {"author": "Marshall L Smith Jr", "author_email": "marshallsmithjr@gmail.com", "bugtrack_url": null, "classifiers": ["Environment :: Console", "Intended Audience :: System Administrators", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Natural Language :: English", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows :: Windows 10", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3", "Topic :: Database"], "description": "\n### Gurglefish \n#### Salesforce Archiver\n\n---\n\nBackup your Salesforce sobject data to Postgres and keep in sync.\n\n#### Features\n\n* One-way data snapshots from Salesforce to Postgres.\n* Simple CLI interface.\n* Dynamic creation of equivalent database table for your selected sobjects.\n* Multiprocessing-enabled for up to 4 concurrent table snapshots.\n* Automatic creation and maintenance of indexes:\n    * Primary key index on ID column\n    * Master/Detail and Lookup field IDs.\n    * ExternalId fields.\n* Automatic detection of sobject field additions/removals and alteration of table structure to match.\n* Cloud-ready for Amazon RDS and Azure.\n* Synchronization of record additions/changes/deletions since last run.\n* Scrubbing of hard deleted records can be disabled on a per-table basis and on the commandline.\n* Logging of sync statistics for each table.\n* Export feature to enable faster initial data loading using native Postgres load file format.\n* Fast field mapping using code generation.\n* Schema artifacts saved in a format easily consumed by custom tooling.\n* Tested over 18 months continuous running in a production environment.\n\n#### Installation\n\n**Requirements**:\n* Python 3.6 or higher\n* Postgresql 9.6 or higher\n\n```bash\npip3 install --user gurglefish\n```\n\nTo update:\n\n```bash\npip3 install --upgrade gurglefish\n```\n\nIn order to use the _export_ feature to initially populate very large tables (recommended) you must\ninstall the Postgresql CLI client for your system.\n\nExamples:\n\n> Ubuntu 18: ```sudo apt install postgres-client```\n\n> CentOS 7: ```sudo apt install postgres96-client```\n\nUse your distribution package search tool (yum, apt, dnf) to find the correct name and install.\n\n#### Configuration\n\n> **NOTE:** This tool only reads from Salesforce and writes to the database/schema you have configured. However, it is\nstill very important to secure your accounts appropriately to protect against system invasion. In other words,\n**security is your responsibility**. \n\n**Requirements**:\n\n* An API-enabled Salesforce user account with *read-only* access to all sobjects to sync. NOTE that sobjects belonging to managed packages may require a license to access.\n* A configured Connected App in your Salesforce org accessible by the user account\n* A PostgreSQL database and user login with appropriate permission to create and alter tables and indexes only in a schema of your choice.\n\nCreate a directory called _/var/lib/gurglefish_.  This is the root of all host storage used by the tool.\nMake sure permissions are set accordingly so the running user has r/w privileges. **This directory tree must be protected\nappropriately as it may contain unencrypted table exports**.\n\n```bash\nsudo mkdir /var/lib/gurglefish\nsudo chmod +rwx /var/lib/gurglefish  # set permissions according to your security needs\n```\n\nIf you want to use a different directory, or a mount point, just create a symbolic link to your location.\nExample: ```sudo ln -s /mnt/my-other-storage/sfarchives /var/lib/gurglefish```\n\nNow create the configuration file (connections.ini) that provides login credentials to both your Postgres database and Salesforce organization. It is a standard INI file and can contain definitions for multiple database-org relationships. Put the file in /var/lib/gurglefish.\nIt will look something like this (for a single database and org):\n\n```ini\n[prod]\nid=prod\nlogin=my-api-user@myorg.com\npassword=password+securitytoken\nconsumer_key=key from connected app\nconsumer_secret=secret from connected app\nauthurl=https://my.domain.salesforce.com\n\ndbvendor=postgresql\ndbname=gurglefish\nschema=public\ndbuser=dbadmin\ndbpass=dbadmin123\ndbhost=192.168.2.61\ndbport=5432\n\nthreads=2\n```\n> **NOTE**: Protect this file from prying eyes. You obviously don't want these credentials stolen.\n\nThe settings are mostly self explanatory:\n\n* Make sure to include your _password_ **and** security token if you have not whitelisted your IP with Salesforce,\notherwise the token can be omitted.\n* The _authurl_ selects either your Salesforce production URL or _https://test.salesforce.com_ for sandboxes.\n* Currently, the only supported _dbvendor_ is postgresql.\n* The _schema_ can be custom, or *public* (the default). If the database is to be shared with other critical data it is highly recommended to isolate in a custom schema (see postgresql docs).\n* Use _threads_ with caution.  You can have at most 4, as this is a Salesforce-imposed limitation. But the real bottleneck could be your database server.  Without custom database tuning, or running on a small platform, you should stick with 1 or 2 threads.  Move up to 4 only when you are certain the database isn't a bottleneck.\n\n#### Getting Started\n\nNow that you are (hopefully) configured correctly you can pull down a list of sobjects and decide which ones to sync.\n\nUsing the example configuration above:\n```bash\ngurglefish prod --init\n```\n\nIf the Salesforce login was successful you will see the new directory _/var/lib/gurglefish/db/prod_. This is the root where all configuration and export data will be stored for this connection.\n\nUnder _db/prod_ you should see **config.json**.  Open it in your favorite editor.  You will see entries for all sobjects your user account has access to here.\n\n> Note: if you do not see sobjects you know exist, your probably don't have permissions to access them or you need a specific license assigned to your account (in the case of commercial managed packages).\n\nYou are free to edit this file as you see fit but make sure it remains valid JSON.  When a new sobject is detected a new entry will be added to this file for it.\n\nExample:\n\n```json\n{\n    \"configuration\": {\n        \"sobjects\": [\n            {\n                \"name\": \"account\",\n                \"enabled\": false,\n                \"auto_scrub\": \"always\"\n            },\n            {\n                \"name\": \"account_vetting__c\",\n                \"enabled\": false\n            },\n            {\n                \"name\": \"account_addresses__c\",\n                \"enabled\": false\n            }\n          ]\n     }\n }\n```\nFor each sobject you want to sync, set the \"enabled\" value to **true**.  Or from the CLI use `gurglefish prod --enable sobject_name__c`. Use `--disable` to stop further syncs.\n\nFor each sobject you want to auto detect and cleanup of deleted records, set \"auto_scrub\" to \"always\". But this comes at a cost of API calls and slows down the overall syncing process.  \n\nAlternately, you can schedule a run once a day, or some other interval, to perform the scrub.  Late a night is a good choice.\n\nSample crontab (global scrub once a day at 1am):\n\n```crontab\n0 9,13,15,17,19 * * 1-5\tgurglefish prod --sync >/tmp/sync.log\n0 1 * * 1-5\t        gurglefish prod --sync --scrub >/tmp/sync.log\n```\n\n\nSave the file. \n\nYou are ready to start pulling data.  But some choices need to be made first.\n\n#### The Initial Data Pull\n\n> This is a good time to discuss the topic of Saleforce API limits.  For each run, for each table, one metadata API call is made to detect schema changes and one query is issues to pull down changed records, giving a minimum of 2 per table per run.  If you run snapshots every 2 hours on 20 tables, that's 12 x 20 x 2 = 480/day **minimum**. I say _minimum_ because this is best-case scenario where there are less than a few hundred changes. With larger data queries, Salesforce returns the data in \"chunks\" and API users are required to call back to Salesforce to retrieve the next chunk, until are all retrieved.  So for sobjects with a lot of activity, like User, Account, Lead, Opportunity, etc, there could be hundreds of calls for each run.\n\n> Fortunately, Gurglefish reports to you at the end of a run the total number of API calls consumed so you can keep an eye on it. You can compare to the documented limits [here](https://developer.salesforce.com/docs/atlas.en-us.salesforce_app_limits_cheatsheet.meta/salesforce_app_limits_cheatsheet/salesforce_app_limits_platform_api.htm). So, for example, if you have an Enterprise license and 65+ users you already have the maximum of 1,000,000 calls rolling 24-hour window.\n\n> And remember, you are sharing those limits with your other integrations.\n\n\nThe initial load of data could take quite a bit of time depending on the number of sobjects enabled, number of fields, and number of records. But as a frame of reference, I've seen it process about 200 records per second on an Opportunity table that has over 800 fields.\n\nThe recommendation is to just enable a couple of sobjects to start, give it a run to make sure all is going well.  You can then go back and enable other sobjects as you need.  In other words, split up your work. You will consume the most API calls during initial loads so space it out if needed.\n\n**Use standard snapshots**\nFor any new table load you can stick with standard synchronization/snapshots. Gurglefish will see you are syncing a new table and pull down all records.  Once the initial load is finished, subsequent runs will only pull down the changes.\nSnapshot can be interrupted - they will resume where they left off on the next run.\n\n**Use native exports**\nAnother option is to use the _--export_ feature to dump all sobject records into a postgres native loadable format.  This file can then be loaded using _--load_, usually in under a minute.  Exported files are saved under the __exports/__ folder and compressed.\n\n> NOTE: Exported files are not useful for archiving or backups as their formats are integrally tied to the current schema of their sobject/table.  If that schema changes the exports are not usable. This is a postgres restriction and is the tradeoff for lightning fast loads. You can remove these files after loading.\n\n**Use the Salesforce bulk API**\n_This is intended as a last-resort edge case_.\nGurglefish will detect if the SOQL required to retrieve data is longer than 16k and inform you to switch to the bulk API to handle it. Honestly, if you have a table that wide you should rethink your design.\nTo enable just add \"bulkapi\":true to the sobject in config.json.  All sync requires going forward will use the Salesforce Bulk API, which in some cases is slower if you have lots of scheduled bulk jobs pending.  Gurglefish will wait up to 10 minutes for the job to start, then time out if it doesn't.\n\n#### Running\n\n```bash\n\tgurglefish prod --sync\n```\n\nSeriously, could it be any easier?\n\nGurglefish will automatically create any missing tables and indexes in postgres you elected to sync from Salesforce.\n\n#### Snapshot Frequency\n\nIt is up to you if you want to schedule automatic runs via **cron** or other mechanism.  Currently, all tables will snapshot on each run - there are not individually customizable run schedules by table. However, this feature is on the roadmap.\n\n#### Statistics\n\nGurglefish logs statistics for each table on each run to 2 tables:  _gf_mdata_sync_jobs_ (master) and _gf_mdata_sync_stats_ (detail) You are free to query these as you like for reporting, auditing, etc. Job statitics are kept for 2 months and cleaned out as they expire.  So if you want to keep them around longer you should make provisions to sync them elsewhere. A custom trigger to replicate inserts to a longer-term set of tables is a good idea.\n\nAlso, a record of automatic schema changes is recorded in _gf_mdata_schema_chg_. So whenever a new or dropped column is detected on an sobject it is recorded here.  This table is never purged.\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/mlsmithjr/gurglefish", "keywords": "salesforce sobject database synchronization snapshots postgres postgresql", "license": "", "maintainer": "", "maintainer_email": "", "name": "gurglefish", "package_url": "https://pypi.org/project/gurglefish/", "platform": "", "project_url": "https://pypi.org/project/gurglefish/", "project_urls": {"Homepage": "https://github.com/mlsmithjr/gurglefish"}, "release_url": "https://pypi.org/project/gurglefish/1.1.9/", "requires_dist": ["arrow (==0.13.1)", "fastcache (==1.0.2)", "psycopg2-binary (==2.8)", "python-dateutil (==2.8.0)", "pyyaml (==5.1)", "requests (==2.21.0)"], "requires_python": ">=3.6", "summary": "Sync and maintain Salesforce sobjects in a Postgres database", "version": "1.1.9", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h3>Gurglefish</h3>\n<h4>Salesforce Archiver</h4>\n<hr>\n<p>Backup your Salesforce sobject data to Postgres and keep in sync.</p>\n<h4>Features</h4>\n<ul>\n<li>One-way data snapshots from Salesforce to Postgres.</li>\n<li>Simple CLI interface.</li>\n<li>Dynamic creation of equivalent database table for your selected sobjects.</li>\n<li>Multiprocessing-enabled for up to 4 concurrent table snapshots.</li>\n<li>Automatic creation and maintenance of indexes:\n<ul>\n<li>Primary key index on ID column</li>\n<li>Master/Detail and Lookup field IDs.</li>\n<li>ExternalId fields.</li>\n</ul>\n</li>\n<li>Automatic detection of sobject field additions/removals and alteration of table structure to match.</li>\n<li>Cloud-ready for Amazon RDS and Azure.</li>\n<li>Synchronization of record additions/changes/deletions since last run.</li>\n<li>Scrubbing of hard deleted records can be disabled on a per-table basis and on the commandline.</li>\n<li>Logging of sync statistics for each table.</li>\n<li>Export feature to enable faster initial data loading using native Postgres load file format.</li>\n<li>Fast field mapping using code generation.</li>\n<li>Schema artifacts saved in a format easily consumed by custom tooling.</li>\n<li>Tested over 18 months continuous running in a production environment.</li>\n</ul>\n<h4>Installation</h4>\n<p><strong>Requirements</strong>:</p>\n<ul>\n<li>Python 3.6 or higher</li>\n<li>Postgresql 9.6 or higher</li>\n</ul>\n<pre>pip3 install --user gurglefish\n</pre>\n<p>To update:</p>\n<pre>pip3 install --upgrade gurglefish\n</pre>\n<p>In order to use the <em>export</em> feature to initially populate very large tables (recommended) you must\ninstall the Postgresql CLI client for your system.</p>\n<p>Examples:</p>\n<blockquote>\n<p>Ubuntu 18: <code>sudo apt install postgres-client</code></p>\n</blockquote>\n<blockquote>\n<p>CentOS 7: <code>sudo apt install postgres96-client</code></p>\n</blockquote>\n<p>Use your distribution package search tool (yum, apt, dnf) to find the correct name and install.</p>\n<h4>Configuration</h4>\n<blockquote>\n<p><strong>NOTE:</strong> This tool only reads from Salesforce and writes to the database/schema you have configured. However, it is\nstill very important to secure your accounts appropriately to protect against system invasion. In other words,\n<strong>security is your responsibility</strong>.</p>\n</blockquote>\n<p><strong>Requirements</strong>:</p>\n<ul>\n<li>An API-enabled Salesforce user account with <em>read-only</em> access to all sobjects to sync. NOTE that sobjects belonging to managed packages may require a license to access.</li>\n<li>A configured Connected App in your Salesforce org accessible by the user account</li>\n<li>A PostgreSQL database and user login with appropriate permission to create and alter tables and indexes only in a schema of your choice.</li>\n</ul>\n<p>Create a directory called <em>/var/lib/gurglefish</em>.  This is the root of all host storage used by the tool.\nMake sure permissions are set accordingly so the running user has r/w privileges. <strong>This directory tree must be protected\nappropriately as it may contain unencrypted table exports</strong>.</p>\n<pre>sudo mkdir /var/lib/gurglefish\nsudo chmod +rwx /var/lib/gurglefish  <span class=\"c1\"># set permissions according to your security needs</span>\n</pre>\n<p>If you want to use a different directory, or a mount point, just create a symbolic link to your location.\nExample: <code>sudo ln -s /mnt/my-other-storage/sfarchives /var/lib/gurglefish</code></p>\n<p>Now create the configuration file (connections.ini) that provides login credentials to both your Postgres database and Salesforce organization. It is a standard INI file and can contain definitions for multiple database-org relationships. Put the file in /var/lib/gurglefish.\nIt will look something like this (for a single database and org):</p>\n<pre><span class=\"k\">[prod]</span>\n<span class=\"na\">id</span><span class=\"o\">=</span><span class=\"s\">prod</span>\n<span class=\"na\">login</span><span class=\"o\">=</span><span class=\"s\">my-api-user@myorg.com</span>\n<span class=\"na\">password</span><span class=\"o\">=</span><span class=\"s\">password+securitytoken</span>\n<span class=\"na\">consumer_key</span><span class=\"o\">=</span><span class=\"s\">key from connected app</span>\n<span class=\"na\">consumer_secret</span><span class=\"o\">=</span><span class=\"s\">secret from connected app</span>\n<span class=\"na\">authurl</span><span class=\"o\">=</span><span class=\"s\">https://my.domain.salesforce.com</span>\n\n<span class=\"na\">dbvendor</span><span class=\"o\">=</span><span class=\"s\">postgresql</span>\n<span class=\"na\">dbname</span><span class=\"o\">=</span><span class=\"s\">gurglefish</span>\n<span class=\"na\">schema</span><span class=\"o\">=</span><span class=\"s\">public</span>\n<span class=\"na\">dbuser</span><span class=\"o\">=</span><span class=\"s\">dbadmin</span>\n<span class=\"na\">dbpass</span><span class=\"o\">=</span><span class=\"s\">dbadmin123</span>\n<span class=\"na\">dbhost</span><span class=\"o\">=</span><span class=\"s\">192.168.2.61</span>\n<span class=\"na\">dbport</span><span class=\"o\">=</span><span class=\"s\">5432</span>\n\n<span class=\"na\">threads</span><span class=\"o\">=</span><span class=\"s\">2</span>\n</pre>\n<blockquote>\n<p><strong>NOTE</strong>: Protect this file from prying eyes. You obviously don't want these credentials stolen.</p>\n</blockquote>\n<p>The settings are mostly self explanatory:</p>\n<ul>\n<li>Make sure to include your <em>password</em> <strong>and</strong> security token if you have not whitelisted your IP with Salesforce,\notherwise the token can be omitted.</li>\n<li>The <em>authurl</em> selects either your Salesforce production URL or <em>https://test.salesforce.com</em> for sandboxes.</li>\n<li>Currently, the only supported <em>dbvendor</em> is postgresql.</li>\n<li>The <em>schema</em> can be custom, or <em>public</em> (the default). If the database is to be shared with other critical data it is highly recommended to isolate in a custom schema (see postgresql docs).</li>\n<li>Use <em>threads</em> with caution.  You can have at most 4, as this is a Salesforce-imposed limitation. But the real bottleneck could be your database server.  Without custom database tuning, or running on a small platform, you should stick with 1 or 2 threads.  Move up to 4 only when you are certain the database isn't a bottleneck.</li>\n</ul>\n<h4>Getting Started</h4>\n<p>Now that you are (hopefully) configured correctly you can pull down a list of sobjects and decide which ones to sync.</p>\n<p>Using the example configuration above:</p>\n<pre>gurglefish prod --init\n</pre>\n<p>If the Salesforce login was successful you will see the new directory <em>/var/lib/gurglefish/db/prod</em>. This is the root where all configuration and export data will be stored for this connection.</p>\n<p>Under <em>db/prod</em> you should see <strong>config.json</strong>.  Open it in your favorite editor.  You will see entries for all sobjects your user account has access to here.</p>\n<blockquote>\n<p>Note: if you do not see sobjects you know exist, your probably don't have permissions to access them or you need a specific license assigned to your account (in the case of commercial managed packages).</p>\n</blockquote>\n<p>You are free to edit this file as you see fit but make sure it remains valid JSON.  When a new sobject is detected a new entry will be added to this file for it.</p>\n<p>Example:</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"nt\">\"configuration\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"sobjects\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span>\n                <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"account\"</span><span class=\"p\">,</span>\n                <span class=\"nt\">\"enabled\"</span><span class=\"p\">:</span> <span class=\"kc\">false</span><span class=\"p\">,</span>\n                <span class=\"nt\">\"auto_scrub\"</span><span class=\"p\">:</span> <span class=\"s2\">\"always\"</span>\n            <span class=\"p\">},</span>\n            <span class=\"p\">{</span>\n                <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"account_vetting__c\"</span><span class=\"p\">,</span>\n                <span class=\"nt\">\"enabled\"</span><span class=\"p\">:</span> <span class=\"kc\">false</span>\n            <span class=\"p\">},</span>\n            <span class=\"p\">{</span>\n                <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"account_addresses__c\"</span><span class=\"p\">,</span>\n                <span class=\"nt\">\"enabled\"</span><span class=\"p\">:</span> <span class=\"kc\">false</span>\n            <span class=\"p\">}</span>\n          <span class=\"p\">]</span>\n     <span class=\"p\">}</span>\n <span class=\"p\">}</span>\n</pre>\n<p>For each sobject you want to sync, set the \"enabled\" value to <strong>true</strong>.  Or from the CLI use <code>gurglefish prod --enable sobject_name__c</code>. Use <code>--disable</code> to stop further syncs.</p>\n<p>For each sobject you want to auto detect and cleanup of deleted records, set \"auto_scrub\" to \"always\". But this comes at a cost of API calls and slows down the overall syncing process.</p>\n<p>Alternately, you can schedule a run once a day, or some other interval, to perform the scrub.  Late a night is a good choice.</p>\n<p>Sample crontab (global scrub once a day at 1am):</p>\n<pre>0 9,13,15,17,19 * * 1-5\tgurglefish prod --sync &gt;/tmp/sync.log\n0 1 * * 1-5\t        gurglefish prod --sync --scrub &gt;/tmp/sync.log\n</pre>\n<p>Save the file.</p>\n<p>You are ready to start pulling data.  But some choices need to be made first.</p>\n<h4>The Initial Data Pull</h4>\n<blockquote>\n<p>This is a good time to discuss the topic of Saleforce API limits.  For each run, for each table, one metadata API call is made to detect schema changes and one query is issues to pull down changed records, giving a minimum of 2 per table per run.  If you run snapshots every 2 hours on 20 tables, that's 12 x 20 x 2 = 480/day <strong>minimum</strong>. I say <em>minimum</em> because this is best-case scenario where there are less than a few hundred changes. With larger data queries, Salesforce returns the data in \"chunks\" and API users are required to call back to Salesforce to retrieve the next chunk, until are all retrieved.  So for sobjects with a lot of activity, like User, Account, Lead, Opportunity, etc, there could be hundreds of calls for each run.</p>\n</blockquote>\n<blockquote>\n<p>Fortunately, Gurglefish reports to you at the end of a run the total number of API calls consumed so you can keep an eye on it. You can compare to the documented limits <a href=\"https://developer.salesforce.com/docs/atlas.en-us.salesforce_app_limits_cheatsheet.meta/salesforce_app_limits_cheatsheet/salesforce_app_limits_platform_api.htm\" rel=\"nofollow\">here</a>. So, for example, if you have an Enterprise license and 65+ users you already have the maximum of 1,000,000 calls rolling 24-hour window.</p>\n</blockquote>\n<blockquote>\n<p>And remember, you are sharing those limits with your other integrations.</p>\n</blockquote>\n<p>The initial load of data could take quite a bit of time depending on the number of sobjects enabled, number of fields, and number of records. But as a frame of reference, I've seen it process about 200 records per second on an Opportunity table that has over 800 fields.</p>\n<p>The recommendation is to just enable a couple of sobjects to start, give it a run to make sure all is going well.  You can then go back and enable other sobjects as you need.  In other words, split up your work. You will consume the most API calls during initial loads so space it out if needed.</p>\n<p><strong>Use standard snapshots</strong>\nFor any new table load you can stick with standard synchronization/snapshots. Gurglefish will see you are syncing a new table and pull down all records.  Once the initial load is finished, subsequent runs will only pull down the changes.\nSnapshot can be interrupted - they will resume where they left off on the next run.</p>\n<p><strong>Use native exports</strong>\nAnother option is to use the <em>--export</em> feature to dump all sobject records into a postgres native loadable format.  This file can then be loaded using <em>--load</em>, usually in under a minute.  Exported files are saved under the <strong>exports/</strong> folder and compressed.</p>\n<blockquote>\n<p>NOTE: Exported files are not useful for archiving or backups as their formats are integrally tied to the current schema of their sobject/table.  If that schema changes the exports are not usable. This is a postgres restriction and is the tradeoff for lightning fast loads. You can remove these files after loading.</p>\n</blockquote>\n<p><strong>Use the Salesforce bulk API</strong>\n<em>This is intended as a last-resort edge case</em>.\nGurglefish will detect if the SOQL required to retrieve data is longer than 16k and inform you to switch to the bulk API to handle it. Honestly, if you have a table that wide you should rethink your design.\nTo enable just add \"bulkapi\":true to the sobject in config.json.  All sync requires going forward will use the Salesforce Bulk API, which in some cases is slower if you have lots of scheduled bulk jobs pending.  Gurglefish will wait up to 10 minutes for the job to start, then time out if it doesn't.</p>\n<h4>Running</h4>\n<pre>\tgurglefish prod --sync\n</pre>\n<p>Seriously, could it be any easier?</p>\n<p>Gurglefish will automatically create any missing tables and indexes in postgres you elected to sync from Salesforce.</p>\n<h4>Snapshot Frequency</h4>\n<p>It is up to you if you want to schedule automatic runs via <strong>cron</strong> or other mechanism.  Currently, all tables will snapshot on each run - there are not individually customizable run schedules by table. However, this feature is on the roadmap.</p>\n<h4>Statistics</h4>\n<p>Gurglefish logs statistics for each table on each run to 2 tables:  <em>gf_mdata_sync_jobs</em> (master) and <em>gf_mdata_sync_stats</em> (detail) You are free to query these as you like for reporting, auditing, etc. Job statitics are kept for 2 months and cleaned out as they expire.  So if you want to keep them around longer you should make provisions to sync them elsewhere. A custom trigger to replicate inserts to a longer-term set of tables is a good idea.</p>\n<p>Also, a record of automatic schema changes is recorded in <em>gf_mdata_schema_chg</em>. So whenever a new or dropped column is detected on an sobject it is recorded here.  This table is never purged.</p>\n\n          </div>"}, "last_serial": 6937981, "releases": {"1.1.0": [{"comment_text": "", "digests": {"md5": "4262416d2b06ea932ffa0934ae84d582", "sha256": "ce2603ebbf701e087a1d4be8410758b7e874b583c4528b9f926518f774e4200b"}, "downloads": -1, "filename": "gurglefish-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "4262416d2b06ea932ffa0934ae84d582", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52134, "upload_time": "2019-04-09T00:22:55", "upload_time_iso_8601": "2019-04-09T00:22:55.842938Z", "url": "https://files.pythonhosted.org/packages/16/c6/4b24cd5c4a9a794298788ccc387904fa8f2587f9d9068e6d1492ffb62411/gurglefish-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9348d0c495c36e62859c373aedbdd291", "sha256": "bdf8d15b40fa0f0d4894b8b1a977557b3a6acd2932b41a26d4cb0ef43125b631"}, "downloads": -1, "filename": "gurglefish-1.1.0.tar.gz", "has_sig": false, "md5_digest": "9348d0c495c36e62859c373aedbdd291", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 38666, "upload_time": "2019-04-09T00:22:58", "upload_time_iso_8601": "2019-04-09T00:22:58.054839Z", "url": "https://files.pythonhosted.org/packages/48/44/e38e9afd3faec8a767fbec9dc699593010f35a2dfc3156cb0c4dc63962ab/gurglefish-1.1.0.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "12b09ef76f0d9231670f18e989356b2b", "sha256": "34df9b084fca1d432c761c81abe92b75525cac25ebede537b44a84cf59dd2e3e"}, "downloads": -1, "filename": "gurglefish-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "12b09ef76f0d9231670f18e989356b2b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52259, "upload_time": "2019-04-11T13:32:01", "upload_time_iso_8601": "2019-04-11T13:32:01.846793Z", "url": "https://files.pythonhosted.org/packages/4b/6e/2255211469debc635e3f373b7fdbb3fc985419cbdba6c6c15dea447b3927/gurglefish-1.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f341e785b7d3a61664660fc552f9a792", "sha256": "ffa4675bd873cda4ba8a46f8b15fe06257b7a47fcc106a4ece4fb1520325392c"}, "downloads": -1, "filename": "gurglefish-1.1.1.tar.gz", "has_sig": false, "md5_digest": "f341e785b7d3a61664660fc552f9a792", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 38823, "upload_time": "2019-04-11T13:32:03", "upload_time_iso_8601": "2019-04-11T13:32:03.840461Z", "url": "https://files.pythonhosted.org/packages/2d/7e/29c1e2db9e49aa704dd38465edf385dc57008bee7bb15c9925f8270be7dc/gurglefish-1.1.1.tar.gz", "yanked": false}], "1.1.2": [{"comment_text": "", "digests": {"md5": "acce3062496620b890a43ae1ce42278b", "sha256": "76907226adc472d8716cd4314578b171d1da15f8f71be37ba526b9b4b2c7152f"}, "downloads": -1, "filename": "gurglefish-1.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "acce3062496620b890a43ae1ce42278b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52260, "upload_time": "2019-04-11T23:39:36", "upload_time_iso_8601": "2019-04-11T23:39:36.464157Z", "url": "https://files.pythonhosted.org/packages/46/4f/5514ef505ba6f7031915bca9ae831251286292ec64c897734ae58f7ddf4f/gurglefish-1.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0b548578b391de756ec941d18ee789a7", "sha256": "e388f58e9d00fa0258a6ddf2b864c49495b8378eb182fef37f603e5b9a6f3d93"}, "downloads": -1, "filename": "gurglefish-1.1.2.tar.gz", "has_sig": false, "md5_digest": "0b548578b391de756ec941d18ee789a7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 38842, "upload_time": "2019-04-11T23:39:37", "upload_time_iso_8601": "2019-04-11T23:39:37.877544Z", "url": "https://files.pythonhosted.org/packages/e3/80/4230ce72bb84efb93e8e55962d0233b2af9f97c88f854bfd6d2a408eb7b6/gurglefish-1.1.2.tar.gz", "yanked": false}], "1.1.3": [{"comment_text": "", "digests": {"md5": "45ef392fd7943d398212b4bf7392fffc", "sha256": "b3b6ddeaf976a9dad8c44d7ec15499ab52346fd13a4c8feefca5d7ef77c7d2a9"}, "downloads": -1, "filename": "gurglefish-1.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "45ef392fd7943d398212b4bf7392fffc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52460, "upload_time": "2019-08-23T19:03:27", "upload_time_iso_8601": "2019-08-23T19:03:27.833842Z", "url": "https://files.pythonhosted.org/packages/57/91/d6d47b369712b1bc59f70221ff8f83694f7281ad5fa482bf4e8a5b37880c/gurglefish-1.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8e51b2f2b456fad745b35575e3c92051", "sha256": "a257e86651643806a97fb28c2334eac809f63779d2a330e3b960a1d6079bf303"}, "downloads": -1, "filename": "gurglefish-1.1.3.tar.gz", "has_sig": false, "md5_digest": "8e51b2f2b456fad745b35575e3c92051", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32304, "upload_time": "2019-08-23T19:03:29", "upload_time_iso_8601": "2019-08-23T19:03:29.896028Z", "url": "https://files.pythonhosted.org/packages/3b/fc/131d6944c8ce96e0186c9d7e8b41ffbda92746aedf29994569163455ecdd/gurglefish-1.1.3.tar.gz", "yanked": false}], "1.1.4": [{"comment_text": "", "digests": {"md5": "822ec99dd537969d68b87efc08b6ed1b", "sha256": "63368f55eaa9bddd6898710ea4f88fed4247b4786635e49c8fcb75c0ec7ae64e"}, "downloads": -1, "filename": "gurglefish-1.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "822ec99dd537969d68b87efc08b6ed1b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52736, "upload_time": "2019-08-26T16:48:59", "upload_time_iso_8601": "2019-08-26T16:48:59.175611Z", "url": "https://files.pythonhosted.org/packages/70/9d/8074cf725f5e252e09cba60300d4f7496d4962b0b29dad71957336fed431/gurglefish-1.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b5074727bb0228501e69efb73a756a71", "sha256": "7d5a3dca91595b13efce2cab36ec618f643d4993a107bc28daef15f301f4f973"}, "downloads": -1, "filename": "gurglefish-1.1.4.tar.gz", "has_sig": false, "md5_digest": "b5074727bb0228501e69efb73a756a71", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32786, "upload_time": "2019-08-26T16:49:00", "upload_time_iso_8601": "2019-08-26T16:49:00.532562Z", "url": "https://files.pythonhosted.org/packages/3d/d2/9ee823299c756425ae136c2864c85c1dfee22915c0e21d503674a76c5304/gurglefish-1.1.4.tar.gz", "yanked": false}], "1.1.5": [{"comment_text": "", "digests": {"md5": "d0dc813087a6baf1ec314d76700fb4b6", "sha256": "4f7cf49a02b510f539a974c64ee508d361c98247c3f5cd42e363f83057ea4605"}, "downloads": -1, "filename": "gurglefish-1.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "d0dc813087a6baf1ec314d76700fb4b6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52733, "upload_time": "2019-08-26T16:50:51", "upload_time_iso_8601": "2019-08-26T16:50:51.468535Z", "url": "https://files.pythonhosted.org/packages/ff/99/a2b075e92d8972f21b4a942d05332e3e56f6d9b93e4d1788855978c5203f/gurglefish-1.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ae4bec65ec4c76d1ee0dd3b68a446e73", "sha256": "c2074658684b09681cfddbd6d436f8bdf1187710fb9c9d347408c492fef02701"}, "downloads": -1, "filename": "gurglefish-1.1.5.tar.gz", "has_sig": false, "md5_digest": "ae4bec65ec4c76d1ee0dd3b68a446e73", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32782, "upload_time": "2019-08-26T16:50:53", "upload_time_iso_8601": "2019-08-26T16:50:53.401048Z", "url": "https://files.pythonhosted.org/packages/42/5f/122b55e21be151df6f0bf4359ed778fd6f9d63e566626d3d16576d350e65/gurglefish-1.1.5.tar.gz", "yanked": false}], "1.1.6": [{"comment_text": "", "digests": {"md5": "b7f9bbab90eec40aadfb0cb91e8d4045", "sha256": "b68e499331d246c0dd47424e23a7bcec517bef29b5bb1158bc89dd76c11a272e"}, "downloads": -1, "filename": "gurglefish-1.1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "b7f9bbab90eec40aadfb0cb91e8d4045", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52766, "upload_time": "2019-09-17T16:16:48", "upload_time_iso_8601": "2019-09-17T16:16:48.079681Z", "url": "https://files.pythonhosted.org/packages/d7/df/001f2664ee78d7b47dfed578258d7a6102e238553b288c3f4daa9c73d20d/gurglefish-1.1.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "88c34e725723a22d4632e767c374caf5", "sha256": "88cf2d2c6f798956a4baea763e6f5b1e168d17aa34010ceabd37ad0dfdb14d6e"}, "downloads": -1, "filename": "gurglefish-1.1.6.tar.gz", "has_sig": false, "md5_digest": "88c34e725723a22d4632e767c374caf5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32878, "upload_time": "2019-09-17T16:16:49", "upload_time_iso_8601": "2019-09-17T16:16:49.761544Z", "url": "https://files.pythonhosted.org/packages/83/27/dc9c030b9d6222aaff3b586f5e78343cb7f0c98e4c71251b6c6b0886f931/gurglefish-1.1.6.tar.gz", "yanked": false}], "1.1.7": [{"comment_text": "", "digests": {"md5": "ec1d49780ca56b614aca4c507631b2f9", "sha256": "ee32c4b70471437a1c3b36438f45987b64c3d4ed14798abd7e658642b4a5497e"}, "downloads": -1, "filename": "gurglefish-1.1.7-py3-none-any.whl", "has_sig": false, "md5_digest": "ec1d49780ca56b614aca4c507631b2f9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52793, "upload_time": "2019-11-07T22:13:41", "upload_time_iso_8601": "2019-11-07T22:13:41.235194Z", "url": "https://files.pythonhosted.org/packages/8a/39/621a79333e1bf7ab240e8fca2434cdd2d176057368241883869ff0f208f0/gurglefish-1.1.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6dd75ddbcb2dcd43ec52c2c135092515", "sha256": "679289b62096109761c2abc166a955f901bcedc402cb5a1a27300ad7a18f2ac9"}, "downloads": -1, "filename": "gurglefish-1.1.7.tar.gz", "has_sig": false, "md5_digest": "6dd75ddbcb2dcd43ec52c2c135092515", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32898, "upload_time": "2019-11-07T22:13:42", "upload_time_iso_8601": "2019-11-07T22:13:42.741503Z", "url": "https://files.pythonhosted.org/packages/22/c4/a277e94c99453fe0d93c7e951bd121414dd126db00508f750b83f63e3ca9/gurglefish-1.1.7.tar.gz", "yanked": false}], "1.1.8": [{"comment_text": "", "digests": {"md5": "7aa9f01394c6e7071ca624eafa0eb4fd", "sha256": "495f43881e0559750b159582ad9145b2beefec052680aea3b271da6fd697d471"}, "downloads": -1, "filename": "gurglefish-1.1.8-py3-none-any.whl", "has_sig": false, "md5_digest": "7aa9f01394c6e7071ca624eafa0eb4fd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 53285, "upload_time": "2020-04-01T23:43:00", "upload_time_iso_8601": "2020-04-01T23:43:00.367142Z", "url": "https://files.pythonhosted.org/packages/d5/0b/d92ae3151a83cb805269fd9cb51659a05138940d79947ac2bd94cea132c2/gurglefish-1.1.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a4c06677643fa36730b60e445882690a", "sha256": "7ddc3e3a3b53b0bfc0b50c9b97493cbbde9430c42330ed913659bd4458c662bb"}, "downloads": -1, "filename": "gurglefish-1.1.8.tar.gz", "has_sig": false, "md5_digest": "a4c06677643fa36730b60e445882690a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 44947, "upload_time": "2020-04-01T23:43:01", "upload_time_iso_8601": "2020-04-01T23:43:01.768194Z", "url": "https://files.pythonhosted.org/packages/5f/1a/e30148f8287582b2549e009673e132a62814f35054c9e1103f175344e6a8/gurglefish-1.1.8.tar.gz", "yanked": false}], "1.1.9": [{"comment_text": "", "digests": {"md5": "dbab0d030e973c0f21814c10de3cb1df", "sha256": "5b8cf76b4ee651de879588c3464521fceca6176a724ebc6835194ec08d18f0fe"}, "downloads": -1, "filename": "gurglefish-1.1.9-py3-none-any.whl", "has_sig": false, "md5_digest": "dbab0d030e973c0f21814c10de3cb1df", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 53286, "upload_time": "2020-04-02T18:55:08", "upload_time_iso_8601": "2020-04-02T18:55:08.524146Z", "url": "https://files.pythonhosted.org/packages/51/a9/b85d3e5495a6b608af94895d458be9aa32905fcc80c84d9b75b72c6499da/gurglefish-1.1.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d5a216637cc1b98c8c35759b39b40a11", "sha256": "db5c404b5f0bb7a7634f1ef7c818be56af40e3521b37692bfd9d66aa60cb0a65"}, "downloads": -1, "filename": "gurglefish-1.1.9.tar.gz", "has_sig": false, "md5_digest": "d5a216637cc1b98c8c35759b39b40a11", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 44943, "upload_time": "2020-04-02T18:55:09", "upload_time_iso_8601": "2020-04-02T18:55:09.622966Z", "url": "https://files.pythonhosted.org/packages/03/41/5da3c34dfdfe5281b45843c2db320723b1465792a2535b7f42f95011aff9/gurglefish-1.1.9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "dbab0d030e973c0f21814c10de3cb1df", "sha256": "5b8cf76b4ee651de879588c3464521fceca6176a724ebc6835194ec08d18f0fe"}, "downloads": -1, "filename": "gurglefish-1.1.9-py3-none-any.whl", "has_sig": false, "md5_digest": "dbab0d030e973c0f21814c10de3cb1df", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 53286, "upload_time": "2020-04-02T18:55:08", "upload_time_iso_8601": "2020-04-02T18:55:08.524146Z", "url": "https://files.pythonhosted.org/packages/51/a9/b85d3e5495a6b608af94895d458be9aa32905fcc80c84d9b75b72c6499da/gurglefish-1.1.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d5a216637cc1b98c8c35759b39b40a11", "sha256": "db5c404b5f0bb7a7634f1ef7c818be56af40e3521b37692bfd9d66aa60cb0a65"}, "downloads": -1, "filename": "gurglefish-1.1.9.tar.gz", "has_sig": false, "md5_digest": "d5a216637cc1b98c8c35759b39b40a11", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 44943, "upload_time": "2020-04-02T18:55:09", "upload_time_iso_8601": "2020-04-02T18:55:09.622966Z", "url": "https://files.pythonhosted.org/packages/03/41/5da3c34dfdfe5281b45843c2db320723b1465792a2535b7f42f95011aff9/gurglefish-1.1.9.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:53:10 2020"}