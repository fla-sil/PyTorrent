{"info": {"author": "Scrapinghub Inc", "author_email": "info@scrapinghub.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3 :: Only"], "description": "====================================\nScrapy & Autoextract API integration\n====================================\n\n.. image:: https://img.shields.io/pypi/v/scrapy-autoextract.svg\n   :target: https://pypi.org/project/scrapy-autoextract/\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/scrapy-autoextract.svg\n    :target: https://pypi.org/project/scrapy-autoextract/\n    :alt: Supported Python Versions\n\n.. image:: https://travis-ci.org/scrapinghub/scrapy-autoextract.svg?branch=master\n    :target: https://travis-ci.org/scrapinghub/scrapy-autoextract\n    :alt: Build Status\n\n\nThis library integrates ScrapingHub's AI Enabled Automatic Data Extraction\ninto a Scrapy spider using a downloader middleware.\nThe middleware adds the result of AutoExtract to ``response.meta['autoextract']``\nfor consumption by the spider.\n\n\nInstallation\n============\n\n::\n\n    pip install scrapy-autoextract\n\nscrapy-autoextract requires Python 3.5+\n\n\nConfiguration\n=============\n\nAdd the AutoExtract downloader middleware in the settings file::\n\n    DOWNLOADER_MIDDLEWARES = {\n        'scrapy_autoextract.AutoExtractMiddleware': 543,\n    }\n\nNote that this should be the last downloader middleware to be executed.\n\n\nUsage\n=====\n\nThe middleware is opt-in and can be explicitly enabled per request,\nwith the ``{'autoextract': {'enabled': True}}`` request meta.\nAll the options below can be set either in the project settings file,\nor just for specific spiders, in the ``custom_settings`` dict.\n\nAvailable settings:\n\n- ``AUTOEXTRACT_USER`` [mandatory] is your AutoExtract API key\n- ``AUTOEXTRACT_URL`` [optional] the AutoExtract service url. Defaults to autoextract.scrapinghub.com.\n- ``AUTOEXTRACT_TIMEOUT`` [optional] sets the response timeout from AutoExtract. Defaults to 660 seconds.\n  Can also be defined by setting the \"download_timeout\" in the request.meta.\n- ``AUTOEXTRACT_PAGE_TYPE`` [mandatory] defines the kind of document to be extracted.\n  Current available options are `\"product\"` and `\"article\"`.\n  Can also be defined on ``spider.page_type``, or ``{'autoextract': {'pageType': '...'}}`` request meta.\n  This is required for the AutoExtract classifier to know what kind of page needs to be extracted.\n- `extra` [optional] allows sending extra payload data to your AutoExtract request.\n  Must be specified as ``{'autoextract': {'extra': {}}}`` request meta and must be a dict.\n\n\nWithin the spider, consuming the AutoExtract result is as easy as::\n\n    def parse(self, response):\n        yield response.meta['autoextract']\n\n\nLimitations\n===========\n\nWhen using the AutoExtract middleware, there are some limitations.\n\n* The incoming spider request is rendered by AutoExtract, not just downloaded by Scrapy,\n  which can change the result - the IP is different, headers are different, etc.\n* Only GET requests are supported\n* Custom headers and cookies are not supported (i.e. Scrapy features to set them don't work)\n* Proxies are not supported (they would work incorrectly,\n  sitting between Scrapy and AutoExtract, instead of AutoExtract and website)\n* AutoThrottle extension can work incorrectly for AutoExtract requests,\n  because AutoExtract timing can be much larger than time required to download a page,\n  so it's best to use ``AUTHTHROTTLE_ENABLED=False`` in the settings.\n* Redirects are handled by AutoExtract, not by Scrapy,\n  so these kinds of middlewares might have no effect\n* Retries should be disabled, because AutoExtract handles them internally\n  (use ``RETRY_ENABLED=False`` in the settings)\n  There is an exception, if there are too many requests sent in\n  a short amount of time and AutoExtract returns HTTP code 429.\n  For that case it's best to use ``RETRY_HTTP_CODES=[429]``.\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/scrapinghub/scrapy-autoextract", "keywords": "scrapy autoextract middleware", "license": "", "maintainer": "Scrapinghub Inc", "maintainer_email": "info@scrapinghub.com", "name": "scrapy-autoextract", "package_url": "https://pypi.org/project/scrapy-autoextract/", "platform": "", "project_url": "https://pypi.org/project/scrapy-autoextract/", "project_urls": {"Homepage": "https://github.com/scrapinghub/scrapy-autoextract"}, "release_url": "https://pypi.org/project/scrapy-autoextract/0.4/", "requires_dist": null, "requires_python": "", "summary": "Scrapinghub AutoExtract API integration for Scrapy", "version": "0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://pypi.org/project/scrapy-autoextract/\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4899d5ba13254e3746fc4c6cd1aaf02463de8caf/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363726170792d6175746f657874726163742e737667\"></a>\n<a href=\"https://pypi.org/project/scrapy-autoextract/\" rel=\"nofollow\"><img alt=\"Supported Python Versions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74f6daa7eefa8603885a50412b77e980e8e458ea/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363726170792d6175746f657874726163742e737667\"></a>\n<a href=\"https://travis-ci.org/scrapinghub/scrapy-autoextract\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a31512c74b9dc246867578de91c74df92827c701/68747470733a2f2f7472617669732d63692e6f72672f7363726170696e676875622f7363726170792d6175746f657874726163742e7376673f6272616e63683d6d6173746572\"></a>\n<p>This library integrates ScrapingHub\u2019s AI Enabled Automatic Data Extraction\ninto a Scrapy spider using a downloader middleware.\nThe middleware adds the result of AutoExtract to <tt><span class=\"pre\">response.meta['autoextract']</span></tt>\nfor consumption by the spider.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<pre>pip install scrapy-autoextract\n</pre>\n<p>scrapy-autoextract requires Python 3.5+</p>\n</div>\n<div id=\"configuration\">\n<h2>Configuration</h2>\n<p>Add the AutoExtract downloader middleware in the settings file:</p>\n<pre>DOWNLOADER_MIDDLEWARES = {\n    'scrapy_autoextract.AutoExtractMiddleware': 543,\n}\n</pre>\n<p>Note that this should be the last downloader middleware to be executed.</p>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>The middleware is opt-in and can be explicitly enabled per request,\nwith the <tt>{'autoextract': {'enabled': True}}</tt> request meta.\nAll the options below can be set either in the project settings file,\nor just for specific spiders, in the <tt>custom_settings</tt> dict.</p>\n<p>Available settings:</p>\n<ul>\n<li><tt>AUTOEXTRACT_USER</tt> [mandatory] is your AutoExtract API key</li>\n<li><tt>AUTOEXTRACT_URL</tt> [optional] the AutoExtract service url. Defaults to autoextract.scrapinghub.com.</li>\n<li><tt>AUTOEXTRACT_TIMEOUT</tt> [optional] sets the response timeout from AutoExtract. Defaults to 660 seconds.\nCan also be defined by setting the \u201cdownload_timeout\u201d in the request.meta.</li>\n<li><tt>AUTOEXTRACT_PAGE_TYPE</tt> [mandatory] defines the kind of document to be extracted.\nCurrent available options are <cite>\u201cproduct\u201d</cite> and <cite>\u201carticle\u201d</cite>.\nCan also be defined on <tt>spider.page_type</tt>, or <tt>{'autoextract': {'pageType': <span class=\"pre\">'...'}}</span></tt> request meta.\nThis is required for the AutoExtract classifier to know what kind of page needs to be extracted.</li>\n<li><cite>extra</cite> [optional] allows sending extra payload data to your AutoExtract request.\nMust be specified as <tt>{'autoextract': {'extra': <span class=\"pre\">{}}}</span></tt> request meta and must be a dict.</li>\n</ul>\n<p>Within the spider, consuming the AutoExtract result is as easy as:</p>\n<pre>def parse(self, response):\n    yield response.meta['autoextract']\n</pre>\n</div>\n<div id=\"limitations\">\n<h2>Limitations</h2>\n<p>When using the AutoExtract middleware, there are some limitations.</p>\n<ul>\n<li>The incoming spider request is rendered by AutoExtract, not just downloaded by Scrapy,\nwhich can change the result - the IP is different, headers are different, etc.</li>\n<li>Only GET requests are supported</li>\n<li>Custom headers and cookies are not supported (i.e. Scrapy features to set them don\u2019t work)</li>\n<li>Proxies are not supported (they would work incorrectly,\nsitting between Scrapy and AutoExtract, instead of AutoExtract and website)</li>\n<li>AutoThrottle extension can work incorrectly for AutoExtract requests,\nbecause AutoExtract timing can be much larger than time required to download a page,\nso it\u2019s best to use <tt>AUTHTHROTTLE_ENABLED=False</tt> in the settings.</li>\n<li>Redirects are handled by AutoExtract, not by Scrapy,\nso these kinds of middlewares might have no effect</li>\n<li>Retries should be disabled, because AutoExtract handles them internally\n(use <tt>RETRY_ENABLED=False</tt> in the settings)\nThere is an exception, if there are too many requests sent in\na short amount of time and AutoExtract returns HTTP code 429.\nFor that case it\u2019s best to use <tt><span class=\"pre\">RETRY_HTTP_CODES=[429]</span></tt>.</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 6544185, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "6671c0eb467f0259df37da03cbeba290", "sha256": "4a45724527d67ba77670f67ea4221f9070794f6e6e648a898d6cfe14276219b3"}, "downloads": -1, "filename": "scrapy_autoextract-0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "6671c0eb467f0259df37da03cbeba290", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 8158, "upload_time": "2019-10-22T10:05:48", "upload_time_iso_8601": "2019-10-22T10:05:48.103806Z", "url": "https://files.pythonhosted.org/packages/0a/f3/501e69ba3bd9f8eff2b6f5c457d40be0a7044f6d59eeb2ece39cd180e49d/scrapy_autoextract-0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "38718acc04040b63b69754c1a097ca4a", "sha256": "7f8ee98091d343ed9352eb37f48a7bb16819608fb793c49cb422a156a187f96b"}, "downloads": -1, "filename": "scrapy-autoextract-0.1.tar.gz", "has_sig": false, "md5_digest": "38718acc04040b63b69754c1a097ca4a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7114, "upload_time": "2019-10-22T10:05:51", "upload_time_iso_8601": "2019-10-22T10:05:51.606010Z", "url": "https://files.pythonhosted.org/packages/c2/e5/abcd25f30b6bdd10b41123dc880070a770586c44ad626b3070f2889ae6d9/scrapy-autoextract-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "b1176fa0612af452bff99c2dda03fadd", "sha256": "1c55d448e4bc6cdb008661d60ab7345c3982c97859d0d8aad6f1aebaf37da252"}, "downloads": -1, "filename": "scrapy_autoextract-0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "b1176fa0612af452bff99c2dda03fadd", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 8574, "upload_time": "2019-12-09T15:15:58", "upload_time_iso_8601": "2019-12-09T15:15:58.099036Z", "url": "https://files.pythonhosted.org/packages/1b/74/982850957d4e5aeaf8cb9f75568cfa913d61eab263b23a0f86bde8f0474d/scrapy_autoextract-0.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a0ecd4473d3649f8f14f8a1a6e336563", "sha256": "dfc7863a9f18ac0523684c4ccf7504089c9b66787e88d7371f6a7a07762a580e"}, "downloads": -1, "filename": "scrapy-autoextract-0.2.tar.gz", "has_sig": false, "md5_digest": "a0ecd4473d3649f8f14f8a1a6e336563", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7588, "upload_time": "2019-12-09T15:15:59", "upload_time_iso_8601": "2019-12-09T15:15:59.721691Z", "url": "https://files.pythonhosted.org/packages/87/37/5a8a3a00ff1b6e6e70e9c49815a09587ee2fd7ff012a187e9ce8fd50fb0f/scrapy-autoextract-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "8e0e6b2425607aebd650a0b78a6686c8", "sha256": "6d406a180219115f1fabf702beee3f43362d81a1299084a0347f2cd10e9c9325"}, "downloads": -1, "filename": "scrapy_autoextract-0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "8e0e6b2425607aebd650a0b78a6686c8", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 8983, "upload_time": "2019-12-10T16:04:31", "upload_time_iso_8601": "2019-12-10T16:04:31.390860Z", "url": "https://files.pythonhosted.org/packages/60/c7/b1492f1abb90a8f1c486063375e6dab71240d239342dc05ef874b2d4fa61/scrapy_autoextract-0.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "999d1f73543eff23693fe3b6386297a4", "sha256": "a44ccceafa8f32241ca579f9eee9f289c31281fcaba55047ac650d24a77ed4ca"}, "downloads": -1, "filename": "scrapy-autoextract-0.3.tar.gz", "has_sig": false, "md5_digest": "999d1f73543eff23693fe3b6386297a4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7925, "upload_time": "2019-12-10T16:04:34", "upload_time_iso_8601": "2019-12-10T16:04:34.729799Z", "url": "https://files.pythonhosted.org/packages/99/ec/237c35806c449d80247c98847d9daaa44edd72314c85feaf109755df31bb/scrapy-autoextract-0.3.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "cf5b837cb129b0c715150b8032e95e7e", "sha256": "13e70fcd1368501d1b4c1026fced4595ad6bb9745c7571cf7c1ac5948ea7025d"}, "downloads": -1, "filename": "scrapy_autoextract-0.3.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "cf5b837cb129b0c715150b8032e95e7e", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 9199, "upload_time": "2019-12-17T16:23:19", "upload_time_iso_8601": "2019-12-17T16:23:19.214009Z", "url": "https://files.pythonhosted.org/packages/2f/fd/6644893a8a9e17d573d17ec2725c26cabb2d16d3f46b8a82ac1dd7dec8b6/scrapy_autoextract-0.3.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "26ba538ce2debf1e0c9b409d9ae04cad", "sha256": "f8810800a376a109acbeadc4217f993b664e91ded9e9ae0f7be9f2369e6f2f6a"}, "downloads": -1, "filename": "scrapy-autoextract-0.3.1.tar.gz", "has_sig": false, "md5_digest": "26ba538ce2debf1e0c9b409d9ae04cad", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8085, "upload_time": "2019-12-17T16:23:20", "upload_time_iso_8601": "2019-12-17T16:23:20.271361Z", "url": "https://files.pythonhosted.org/packages/0d/a6/59e4a730d167ba86385e02057611ed50de15ac35edd39cf33d305c407564/scrapy-autoextract-0.3.1.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "343411f75ce26e7a34bb953fe9fcf0d6", "sha256": "c593799fe15b06ec56af751d89af7880ddb38cb8bbe3364125dc53505eb2ff82"}, "downloads": -1, "filename": "scrapy_autoextract-0.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "343411f75ce26e7a34bb953fe9fcf0d6", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 9172, "upload_time": "2020-01-30T15:49:40", "upload_time_iso_8601": "2020-01-30T15:49:40.349991Z", "url": "https://files.pythonhosted.org/packages/00/0b/cd50987386dca5f35a383594ba8e27d8a51e96cab58dd330ad8012995618/scrapy_autoextract-0.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6de56ca42d2fb5c3f17906afa384a7a6", "sha256": "d814eb2d53db50b2dd08d35dfe34c387c79f27d2b13223c67d2ba828ceaeb101"}, "downloads": -1, "filename": "scrapy-autoextract-0.4.tar.gz", "has_sig": false, "md5_digest": "6de56ca42d2fb5c3f17906afa384a7a6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8077, "upload_time": "2020-01-30T15:49:41", "upload_time_iso_8601": "2020-01-30T15:49:41.366502Z", "url": "https://files.pythonhosted.org/packages/43/8b/105e5dc07fa20c041857f3008d13e3a54d6195dce6d47c1bc6a17e89a2f6/scrapy-autoextract-0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "343411f75ce26e7a34bb953fe9fcf0d6", "sha256": "c593799fe15b06ec56af751d89af7880ddb38cb8bbe3364125dc53505eb2ff82"}, "downloads": -1, "filename": "scrapy_autoextract-0.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "343411f75ce26e7a34bb953fe9fcf0d6", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 9172, "upload_time": "2020-01-30T15:49:40", "upload_time_iso_8601": "2020-01-30T15:49:40.349991Z", "url": "https://files.pythonhosted.org/packages/00/0b/cd50987386dca5f35a383594ba8e27d8a51e96cab58dd330ad8012995618/scrapy_autoextract-0.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6de56ca42d2fb5c3f17906afa384a7a6", "sha256": "d814eb2d53db50b2dd08d35dfe34c387c79f27d2b13223c67d2ba828ceaeb101"}, "downloads": -1, "filename": "scrapy-autoextract-0.4.tar.gz", "has_sig": false, "md5_digest": "6de56ca42d2fb5c3f17906afa384a7a6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8077, "upload_time": "2020-01-30T15:49:41", "upload_time_iso_8601": "2020-01-30T15:49:41.366502Z", "url": "https://files.pythonhosted.org/packages/43/8b/105e5dc07fa20c041857f3008d13e3a54d6195dce6d47c1bc6a17e89a2f6/scrapy-autoextract-0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:49 2020"}