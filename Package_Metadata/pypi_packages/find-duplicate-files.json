{"info": {"author": "hp310780", "author_email": "", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3"], "description": "# FindDuplicateFiles\n\n![FindDuplicateFiles](https://github.com/hp310780/FindDuplicateFiles/workflows/Python%20application/badge.svg)\n\nA fast and efficient way to find duplicate files in a directory. Installable as a command line interface \n(please see Installing below).\n\nThis module will walk the given directory tree and then group files by size \n(indicating potential duplicate content) followed by comparing the hash of the file.\nThis hash can be chunked by passing in a chunk arg. This will compute an initial hash for a chunk of the file \nbefore then computing the full hash if the first hash matched, thus avoiding computing\nexpensive hashes on large files.\n\n### Prerequisites\n\n* Python 3.6.5\n\n### Installing\n\n```\n> pip3 install find-duplicate-files\n> find_duplicate_files --dir /path/to/dir --chunk 2\n```\nTo run as a Python module:\n```\nimport find_duplicate_files\n# required arg: dir, optional: chunk\nfind_duplicate_files.find_duplicate_files(\"/path/to/dir\", chunk=1)\n```\n\n## Running the tests\n\nTo run the tests, please use the following commands:\n\n```\n> cd <FindDuplicateFiles directory>\n> pytest\n```\n\n## Test Data\n\nThe test data provided takes the following form - \n* tests/test_data/TestFindDuplicateFilesByHash: 5 .txt files of equal size (29 bytes). 1.txt and 3.txt are the same content. 4.txt and 5.txt are the same content. 2.txt is different contents (but the same size). Used to verify the find_duplicate_files.find_duplicate_files_by_hash function.\n* tests/test_data/TestGenerateHash/1.txt: 1 .txt file with which to compare the outcome of find_duplicate_files.generate_hash to.\n\n## Performance\n\nAn optional performance script to compare the performance of hashing the full file versus the chunked approach when finding duplicate files. Outputs performance metrics.\nTo run:\n```\n> cd <FindDuplicateFiles/metrics directory>\n> python performance.py\n```\nExample output:\n```\nMethod 1 - Generate full hash returns correct duplicates.Time 0.006515709001178038\nMethod 2 - Generate chunked hash returns correct duplicates.Time 0.006872908999866922\n```\n\n## Benchmarking\n| Attempt | #1 | #2 | #3 | #4 |\n| :---: | :---: | :---: | :---:| :---: |\n| Chunk Size | 1 | 1 | 8 | 8 |\n| Seconds | 5.4 | 4.16 | 3.25 | 3.27 |\n\nTest Data: 10.9gb, 3653 files, 128 duplicates, largest file ~156mb\n\n## Further Optimisations\n* Investigate optimal chunk size given common file type\n* Investigate threading for performance\n* Investigate different hashing algorithms\n* Investigate recursive chunking - i.e. Eliminating files that differ\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hp310780/FindDuplicateFiles", "keywords": "", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "find-duplicate-files", "package_url": "https://pypi.org/project/find-duplicate-files/", "platform": "", "project_url": "https://pypi.org/project/find-duplicate-files/", "project_urls": {"Homepage": "https://github.com/hp310780/FindDuplicateFiles"}, "release_url": "https://pypi.org/project/find-duplicate-files/2.1.0/", "requires_dist": null, "requires_python": "", "summary": "Module to find duplicate files in a directory", "version": "2.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>FindDuplicateFiles</h1>\n<p><img alt=\"FindDuplicateFiles\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1ba2edea5e309c0bd0c0b3d00969584df641b744/68747470733a2f2f6769746875622e636f6d2f68703331303738302f46696e644475706c696361746546696c65732f776f726b666c6f77732f507974686f6e2532306170706c69636174696f6e2f62616467652e737667\"></p>\n<p>A fast and efficient way to find duplicate files in a directory. Installable as a command line interface\n(please see Installing below).</p>\n<p>This module will walk the given directory tree and then group files by size\n(indicating potential duplicate content) followed by comparing the hash of the file.\nThis hash can be chunked by passing in a chunk arg. This will compute an initial hash for a chunk of the file\nbefore then computing the full hash if the first hash matched, thus avoiding computing\nexpensive hashes on large files.</p>\n<h3>Prerequisites</h3>\n<ul>\n<li>Python 3.6.5</li>\n</ul>\n<h3>Installing</h3>\n<pre><code>&gt; pip3 install find-duplicate-files\n&gt; find_duplicate_files --dir /path/to/dir --chunk 2\n</code></pre>\n<p>To run as a Python module:</p>\n<pre><code>import find_duplicate_files\n# required arg: dir, optional: chunk\nfind_duplicate_files.find_duplicate_files(\"/path/to/dir\", chunk=1)\n</code></pre>\n<h2>Running the tests</h2>\n<p>To run the tests, please use the following commands:</p>\n<pre><code>&gt; cd &lt;FindDuplicateFiles directory&gt;\n&gt; pytest\n</code></pre>\n<h2>Test Data</h2>\n<p>The test data provided takes the following form -</p>\n<ul>\n<li>tests/test_data/TestFindDuplicateFilesByHash: 5 .txt files of equal size (29 bytes). 1.txt and 3.txt are the same content. 4.txt and 5.txt are the same content. 2.txt is different contents (but the same size). Used to verify the find_duplicate_files.find_duplicate_files_by_hash function.</li>\n<li>tests/test_data/TestGenerateHash/1.txt: 1 .txt file with which to compare the outcome of find_duplicate_files.generate_hash to.</li>\n</ul>\n<h2>Performance</h2>\n<p>An optional performance script to compare the performance of hashing the full file versus the chunked approach when finding duplicate files. Outputs performance metrics.\nTo run:</p>\n<pre><code>&gt; cd &lt;FindDuplicateFiles/metrics directory&gt;\n&gt; python performance.py\n</code></pre>\n<p>Example output:</p>\n<pre><code>Method 1 - Generate full hash returns correct duplicates.Time 0.006515709001178038\nMethod 2 - Generate chunked hash returns correct duplicates.Time 0.006872908999866922\n</code></pre>\n<h2>Benchmarking</h2>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Attempt</th>\n<th align=\"center\">#1</th>\n<th align=\"center\">#2</th>\n<th align=\"center\">#3</th>\n<th align=\"center\">#4</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">Chunk Size</td>\n<td align=\"center\">1</td>\n<td align=\"center\">1</td>\n<td align=\"center\">8</td>\n<td align=\"center\">8</td>\n</tr>\n<tr>\n<td align=\"center\">Seconds</td>\n<td align=\"center\">5.4</td>\n<td align=\"center\">4.16</td>\n<td align=\"center\">3.25</td>\n<td align=\"center\">3.27</td>\n</tr></tbody></table>\n<p>Test Data: 10.9gb, 3653 files, 128 duplicates, largest file ~156mb</p>\n<h2>Further Optimisations</h2>\n<ul>\n<li>Investigate optimal chunk size given common file type</li>\n<li>Investigate threading for performance</li>\n<li>Investigate different hashing algorithms</li>\n<li>Investigate recursive chunking - i.e. Eliminating files that differ</li>\n</ul>\n\n          </div>"}, "last_serial": 7025276, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "7765f675d74c0c770cf92a0a6f8b7d78", "sha256": "a676a0f045933b05605ca9f3c6bffa34b0a5f57c074bbac35598cde09ec24d51"}, "downloads": -1, "filename": "find_duplicate_files-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "7765f675d74c0c770cf92a0a6f8b7d78", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5182, "upload_time": "2019-04-21T14:30:27", "upload_time_iso_8601": "2019-04-21T14:30:27.141411Z", "url": "https://files.pythonhosted.org/packages/ac/26/e9858b58abaef46c7ee7262c8a542e812e7def4e7af24c236046755ced80/find_duplicate_files-1.0.0-py3-none-any.whl", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "0cc05138e1e91016779e76809ba03de3", "sha256": "d8555dfe4dbbf5d059b73a1dff78190ad66733af8ca9c2b5cc23a093b2a28128"}, "downloads": -1, "filename": "find_duplicate_files-2.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0cc05138e1e91016779e76809ba03de3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 3593, "upload_time": "2020-04-15T15:06:01", "upload_time_iso_8601": "2020-04-15T15:06:01.392482Z", "url": "https://files.pythonhosted.org/packages/1e/2f/99dc6aee00d537d7658525ab4efb8b22f2f87ea360df8c7b63053309cc52/find_duplicate_files-2.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "89eadf61f22d351d814a8a0549aef945", "sha256": "b5ecffa9576677dac3da134f5b30d7eef94991e275bd0bf199df8cbd2c529287"}, "downloads": -1, "filename": "find-duplicate-files-2.0.0.tar.gz", "has_sig": false, "md5_digest": "89eadf61f22d351d814a8a0549aef945", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2683, "upload_time": "2020-04-15T15:06:04", "upload_time_iso_8601": "2020-04-15T15:06:04.285636Z", "url": "https://files.pythonhosted.org/packages/f8/78/3cb6c581e4bd0e98afa36d1c7ea269eb7f3551e3a7a9c5915ec81757cd31/find-duplicate-files-2.0.0.tar.gz", "yanked": false}], "2.1.0": [{"comment_text": "", "digests": {"md5": "51bb917a52db3eab59637b5c7ec6fbcd", "sha256": "43bac5d2ab316aed2f511a51b136c61ca610f522b0783aa09bf3012bfc3e614d"}, "downloads": -1, "filename": "find_duplicate_files-2.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "51bb917a52db3eab59637b5c7ec6fbcd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5772, "upload_time": "2020-04-15T15:06:02", "upload_time_iso_8601": "2020-04-15T15:06:02.896432Z", "url": "https://files.pythonhosted.org/packages/7b/87/1aee9d6ff909ae4afff45bb8ba8e18bb676d58072542498c700e672c0088/find_duplicate_files-2.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9483ae99fcce5337070e256e4cf9a4ce", "sha256": "11a4c742720e49883901a5be4447225d4a67a9e1e67f9446127f90ffda96a339"}, "downloads": -1, "filename": "find-duplicate-files-2.1.0.tar.gz", "has_sig": false, "md5_digest": "9483ae99fcce5337070e256e4cf9a4ce", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4483, "upload_time": "2020-04-15T15:06:05", "upload_time_iso_8601": "2020-04-15T15:06:05.520280Z", "url": "https://files.pythonhosted.org/packages/6b/8c/6f8da202a65d2cf6fa82b17eef165ed306623f65e3c4c1479e3727946a8b/find-duplicate-files-2.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "51bb917a52db3eab59637b5c7ec6fbcd", "sha256": "43bac5d2ab316aed2f511a51b136c61ca610f522b0783aa09bf3012bfc3e614d"}, "downloads": -1, "filename": "find_duplicate_files-2.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "51bb917a52db3eab59637b5c7ec6fbcd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5772, "upload_time": "2020-04-15T15:06:02", "upload_time_iso_8601": "2020-04-15T15:06:02.896432Z", "url": "https://files.pythonhosted.org/packages/7b/87/1aee9d6ff909ae4afff45bb8ba8e18bb676d58072542498c700e672c0088/find_duplicate_files-2.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9483ae99fcce5337070e256e4cf9a4ce", "sha256": "11a4c742720e49883901a5be4447225d4a67a9e1e67f9446127f90ffda96a339"}, "downloads": -1, "filename": "find-duplicate-files-2.1.0.tar.gz", "has_sig": false, "md5_digest": "9483ae99fcce5337070e256e4cf9a4ce", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4483, "upload_time": "2020-04-15T15:06:05", "upload_time_iso_8601": "2020-04-15T15:06:05.520280Z", "url": "https://files.pythonhosted.org/packages/6b/8c/6f8da202a65d2cf6fa82b17eef165ed306623f65e3c4c1479e3727946a8b/find-duplicate-files-2.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:42:25 2020"}