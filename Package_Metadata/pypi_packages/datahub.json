{"info": {"author": "Lukasz Szybalski", "author_email": "szybalski@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "DataHub\n=======\n\n* Datahub is a tool that allows faster download/crawl, parse, load, and visualize of data. It achieves this by allowing you to divide each step into its own work folders. In each work folder you get a sample files that you can start coding in.\n* Datahub is for people who found some interesting data source for them, they want to download it, parse it, load it into database, provide some documentation, and visualize it. Datahub will speed up the process by creating folder for each of these actions. You will create all the programs from our base default template and move on to analyzing the data in no time.\n\n\n6 Rules for Data Privacy\n========================\n\n 1. Sensitive, and possibly inaccurate, information may not be used against people in financial, political, employment, and health-care settings.\n 2. All information should not be forcing anybody to hide or protect them self against improper information use that significantly limits persons ability to exercise his/her right to freedom of association.\n 3. Implement a basic form of information accountability by tracking identifying information that identifies a person or corporation and could be used to held that person/corporation accountable for the compliance.\n 4. There should be no restriction on use of data unless specified by laws and these privacy rules.\n 5. Privacy is protected not by limiting the collection of data, but by placing strict rules on how the data may be used. Data that can be used in financial, political, employment, and health-care settings  cannot be used for marketing and other profiling. Strict penalties should be imposed by for the breach of these use limitations. Actions that involve financial, political, employment, and health-care settings decision must be justified with reference to the specific data on which the decision was based. If the person/corporation discovers that the data is inaccurate, he or she may demand that it be corrected. Stiff financial penalties should be imposed against the agency that does not make the appropriate corrections.\n 6. Achieve greater information accountability only by making better use of the information that is collected, retaining the data that is necessary to hold data users responsible for policy compliance. Build the system that encourages compliance, and maximizes the possibility of accountability of violations. Technology should supplant the rules because users are aware of what they are and because they know there will be consequences, after the fact.\n\n\n\nInstall DataHub\n===============\n\nThe best way to get started with datahub is to install it in the following way.\nSetup virtualenv which will keep the installation in a separate directory::\n\n virtualenv --no-site-packages datahubENV\n New python executable in datahubENV/bin/python\n Installing setuptools............done.\n \n source datahubENV/bin/activate \n\n\n\n\nCreate DataHub based project\n============================\n\nDatahub is a paster template so you run it as follows::\n\n paster create --list-templates\n paster create -t datahub\n\nYou should see something like this::\n\n paster create -t datahub\n\nSelected and implied templates::\n\n  PasteScript#basic_package  A basic setuptools-enabled package\n  datahub#datahub            DataHub is a tool to help you datamine(crawl, parse, and load) any data.\n\n Enter project name: myproject\n Variables:\n   egg:      myproject\n   package:  myproject\n   project:  myproject\n Enter version (Version (like 0.1)) ['']: 0.1\n Enter description (One-line description of the package) ['']: my project\n Enter long_description (Multi-line description (in reST)) ['']: this is long description\n Enter keywords (Space-separated keywords/tags) ['']: datahub dataprocess gov\n Enter author (Author name) ['']: myname\n Enter author_email (Author email) ['']: myemail\n Enter url (URL of homepage) ['']: mywebsite\n Enter license_name (License name) ['']: gpl\n Enter zip_safe (True/False: if the package can be distributed as a .zip file) [False]: \n Creating template basic_package\n Creating directory ./myproject\n   Recursing into +package+\n     Creating ./myproject/myproject/\n     Copying __init__.py to ./myproject/myproject/__init__.py\n   Copying setup.cfg to ./myproject/setup.cfg\n   Copying setup.py_tmpl to ./myproject/setup.py\n Creating template datahub\n   Recursing into +package+\n     Copying README.txt_tmpl to ./myproject/myproject/README.txt\n     Recursing into crawl\n       Creating ./myproject/myproject/crawl/\n       Copying Readme.txt_tmpl to ./myproject/myproject/crawl/Readme.txt\n       Copying __init__.py to ./myproject/myproject/crawl/__init__.py\n       Copying crawl.sh to ./myproject/myproject/crawl/crawl.sh\n       Copying download.sh to ./myproject/myproject/crawl/download.sh\n       Copying download_list.txt_tmpl to ./myproject/myproject/crawl/download_list.txt\n       Copying harvestman-+package+.xml to ./myproject/myproject/crawl/harvestman-myproject.xml\n     Recursing into hdf5\n       Creating ./myproject/myproject/hdf5/\n       Copying READEM_hdf5.txt_tmpl to ./myproject/myproject/hdf5/READEM_hdf5.txt\n       Copying __init__.py to ./myproject/myproject/hdf5/__init__.py\n     Recursing into load\n       Creating ./myproject/myproject/load/\n       Copying __init__.py to ./myproject/myproject/load/__init__.py\n       Copying load.py to ./myproject/myproject/load/load.py\n       Copying load.sh to ./myproject/myproject/load/load.sh\n       Copying model.py to ./myproject/myproject/load/model.py\n     Recursing into parse\n       Creating ./myproject/myproject/parse/\n       Copying __init__.py to ./myproject/myproject/parse/__init__.py\n       Copying parse.sh_tmpl to ./myproject/myproject/parse/parse.sh\n     Copying process.sh_tmpl to ./myproject/myproject/process.sh\n     Recursing into wiki\n       Creating ./myproject/myproject/wiki/\n       Copying REAME.wiki_tmpl to ./myproject/myproject/wiki/REAME.wiki\n Running /home/lucas/tmp/lmENV/bin/python setup.py egg_info\n Manually creating paster_plugins.txt (deprecated! pass a paster_plugins keyword to setup() instead)\n Adding datahub to paster_plugins.txt\n\nGo into the myproject folder and start coding.\nThe folder structure looks like this::\n\n myproject\n |-- myproject\n |   |-- README.txt\n |   |-- __init__.py\n |   |-- crawl\n |   |   |-- Readme.txt\n |   |   |-- __init__.py\n |   |   |-- crawl.sh\n |   |   |-- download.sh\n |   |   |-- download_list.txt\n |   |   `-- harvestman-myproject.xml\n |   |-- hdf5\n |   |   |-- READEM_hdf5.txt\n |   |   `-- __init__.py\n |   |-- load\n |   |   |-- __init__.py\n |   |   |-- load.py\n |   |   |-- load.sh\n |   |   `-- model.py\n |   |-- parse\n |   |   |-- __init__.py\n |   |   `-- parse.sh\n |   |-- process.sh\n |   `-- wiki\n |       `-- REAME.wiki\n |-- myproject.egg-info\n |   |-- PKG-INFO\n |   |-- SOURCES.txt\n |   |-- dependency_links.txt\n |   |-- entry_points.txt\n |   |-- not-zip-safe\n |   |-- paster_plugins.txt\n |   `-- top_level.txt\n |-- setup.cfg\n `-- setup.py\n \n\nGet stared with your data project\n=================================\n\ncrawl\n~~~~~\n\nCrawl folder is where you crawl data. You have two choices as far as downloading. For each choice there are pre-build files, so just follow this:\n \nwget\n----\n\nWith wget you can download the files if the list of files is not big. There is a download_list.txt that will hold the url you want to download. You can specify wild cards like `*.zip`, `*.pdf`, `*.txt` etc. Download.sh is a shell script that calls wget and downloads files. By default it will only download files if they are newer then what you downloaded, and it will only download the missing parts. This saves your bandwidth and does not re-download the whole files each time.\n\nThe only thing you need to do is edit download_list.txt::\n\n cd crawl\n #Edit download_list.txt and add url of files you want to download\n vi download_list.txt\n sh download.sh\n\nYou second options is harvestman, see docs.\n\nparse\n~~~~~\n\nParse is where you parse the files. This is a gray area where you take control. This can be as simple as unziping a file, or writing a simple script to replace some names, or some extensive parsing program. It all depends on the project data. Add the code to parse.sh, or write your own parser and add the running code to parse.sh so that later you just run::\n \n sh parse.sh\n\nload\n~~~~\n\nLoad is where you load your data to the database. There is a laod.py file that has a sample database structure for 4 columns. You can use that file as your starting point. It has everything from defining new columns, to setting up database and reading over a csv file in parse folder and uplaoding it to database. Read the load.sh and load.py files and make special not where it says [CHANGE]. These are the portion where you changed the namem, add columns, tell it where the file is. When all done, just run it::\n\n sh load.sh\n\nprocess.sh\n~~~~~~~~~~\n\nAbove all folders there is a file called process.sh. This files has a build in structure to go into crawl folder and start crawl.sh, then go into parse and run parse.sh, and then go into load and run load.sh script. With this file you control the whole process. When its all ready, a user can get your project, install any necessary programs and just simply run::\n\n sh process.sh\n\nThis will crawl, parse, and load the data.\n\n\nEnjoy.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://launchpad.net/datahub", "keywords": "datahub datamining csv tsv mdb crawl harvest webdata gov public data", "license": "LGPL", "maintainer": "", "maintainer_email": "", "name": "datahub", "package_url": "https://pypi.org/project/datahub/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/datahub/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://launchpad.net/datahub"}, "release_url": "https://pypi.org/project/datahub/0.8.90dev/", "requires_dist": null, "requires_python": null, "summary": "Data Hub", "version": "0.8.90dev", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"datahub\">\n<h2>DataHub</h2>\n<ul>\n<li>Datahub is a tool that allows faster download/crawl, parse, load, and visualize of data. It achieves this by allowing you to divide each step into its own work folders. In each work folder you get a sample files that you can start coding in.</li>\n<li>Datahub is for people who found some interesting data source for them, they want to download it, parse it, load it into database, provide some documentation, and visualize it. Datahub will speed up the process by creating folder for each of these actions. You will create all the programs from our base default template and move on to analyzing the data in no time.</li>\n</ul>\n</div>\n<div id=\"rules-for-data-privacy\">\n<h2>6 Rules for Data Privacy</h2>\n<blockquote>\n<ol>\n<li>Sensitive, and possibly inaccurate, information may not be used against people in financial, political, employment, and health-care settings.</li>\n<li>All information should not be forcing anybody to hide or protect them self against improper information use that significantly limits persons ability to exercise his/her right to freedom of association.</li>\n<li>Implement a basic form of information accountability by tracking identifying information that identifies a person or corporation and could be used to held that person/corporation accountable for the compliance.</li>\n<li>There should be no restriction on use of data unless specified by laws and these privacy rules.</li>\n<li>Privacy is protected not by limiting the collection of data, but by placing strict rules on how the data may be used. Data that can be used in financial, political, employment, and health-care settings  cannot be used for marketing and other profiling. Strict penalties should be imposed by for the breach of these use limitations. Actions that involve financial, political, employment, and health-care settings decision must be justified with reference to the specific data on which the decision was based. If the person/corporation discovers that the data is inaccurate, he or she may demand that it be corrected. Stiff financial penalties should be imposed against the agency that does not make the appropriate corrections.</li>\n<li>Achieve greater information accountability only by making better use of the information that is collected, retaining the data that is necessary to hold data users responsible for policy compliance. Build the system that encourages compliance, and maximizes the possibility of accountability of violations. Technology should supplant the rules because users are aware of what they are and because they know there will be consequences, after the fact.</li>\n</ol>\n</blockquote>\n</div>\n<div id=\"install-datahub\">\n<h2>Install DataHub</h2>\n<p>The best way to get started with datahub is to install it in the following way.\nSetup virtualenv which will keep the installation in a separate directory:</p>\n<pre>virtualenv --no-site-packages datahubENV\nNew python executable in datahubENV/bin/python\nInstalling setuptools............done.\n\nsource datahubENV/bin/activate\n</pre>\n</div>\n<div id=\"create-datahub-based-project\">\n<h2>Create DataHub based project</h2>\n<p>Datahub is a paster template so you run it as follows:</p>\n<pre>paster create --list-templates\npaster create -t datahub\n</pre>\n<p>You should see something like this:</p>\n<pre>paster create -t datahub\n</pre>\n<p>Selected and implied templates:</p>\n<pre> PasteScript#basic_package  A basic setuptools-enabled package\n datahub#datahub            DataHub is a tool to help you datamine(crawl, parse, and load) any data.\n\nEnter project name: myproject\nVariables:\n  egg:      myproject\n  package:  myproject\n  project:  myproject\nEnter version (Version (like 0.1)) ['']: 0.1\nEnter description (One-line description of the package) ['']: my project\nEnter long_description (Multi-line description (in reST)) ['']: this is long description\nEnter keywords (Space-separated keywords/tags) ['']: datahub dataprocess gov\nEnter author (Author name) ['']: myname\nEnter author_email (Author email) ['']: myemail\nEnter url (URL of homepage) ['']: mywebsite\nEnter license_name (License name) ['']: gpl\nEnter zip_safe (True/False: if the package can be distributed as a .zip file) [False]:\nCreating template basic_package\nCreating directory ./myproject\n  Recursing into +package+\n    Creating ./myproject/myproject/\n    Copying __init__.py to ./myproject/myproject/__init__.py\n  Copying setup.cfg to ./myproject/setup.cfg\n  Copying setup.py_tmpl to ./myproject/setup.py\nCreating template datahub\n  Recursing into +package+\n    Copying README.txt_tmpl to ./myproject/myproject/README.txt\n    Recursing into crawl\n      Creating ./myproject/myproject/crawl/\n      Copying Readme.txt_tmpl to ./myproject/myproject/crawl/Readme.txt\n      Copying __init__.py to ./myproject/myproject/crawl/__init__.py\n      Copying crawl.sh to ./myproject/myproject/crawl/crawl.sh\n      Copying download.sh to ./myproject/myproject/crawl/download.sh\n      Copying download_list.txt_tmpl to ./myproject/myproject/crawl/download_list.txt\n      Copying harvestman-+package+.xml to ./myproject/myproject/crawl/harvestman-myproject.xml\n    Recursing into hdf5\n      Creating ./myproject/myproject/hdf5/\n      Copying READEM_hdf5.txt_tmpl to ./myproject/myproject/hdf5/READEM_hdf5.txt\n      Copying __init__.py to ./myproject/myproject/hdf5/__init__.py\n    Recursing into load\n      Creating ./myproject/myproject/load/\n      Copying __init__.py to ./myproject/myproject/load/__init__.py\n      Copying load.py to ./myproject/myproject/load/load.py\n      Copying load.sh to ./myproject/myproject/load/load.sh\n      Copying model.py to ./myproject/myproject/load/model.py\n    Recursing into parse\n      Creating ./myproject/myproject/parse/\n      Copying __init__.py to ./myproject/myproject/parse/__init__.py\n      Copying parse.sh_tmpl to ./myproject/myproject/parse/parse.sh\n    Copying process.sh_tmpl to ./myproject/myproject/process.sh\n    Recursing into wiki\n      Creating ./myproject/myproject/wiki/\n      Copying REAME.wiki_tmpl to ./myproject/myproject/wiki/REAME.wiki\nRunning /home/lucas/tmp/lmENV/bin/python setup.py egg_info\nManually creating paster_plugins.txt (deprecated! pass a paster_plugins keyword to setup() instead)\nAdding datahub to paster_plugins.txt\n</pre>\n<p>Go into the myproject folder and start coding.\nThe folder structure looks like this:</p>\n<pre>myproject\n|-- myproject\n|   |-- README.txt\n|   |-- __init__.py\n|   |-- crawl\n|   |   |-- Readme.txt\n|   |   |-- __init__.py\n|   |   |-- crawl.sh\n|   |   |-- download.sh\n|   |   |-- download_list.txt\n|   |   `-- harvestman-myproject.xml\n|   |-- hdf5\n|   |   |-- READEM_hdf5.txt\n|   |   `-- __init__.py\n|   |-- load\n|   |   |-- __init__.py\n|   |   |-- load.py\n|   |   |-- load.sh\n|   |   `-- model.py\n|   |-- parse\n|   |   |-- __init__.py\n|   |   `-- parse.sh\n|   |-- process.sh\n|   `-- wiki\n|       `-- REAME.wiki\n|-- myproject.egg-info\n|   |-- PKG-INFO\n|   |-- SOURCES.txt\n|   |-- dependency_links.txt\n|   |-- entry_points.txt\n|   |-- not-zip-safe\n|   |-- paster_plugins.txt\n|   `-- top_level.txt\n|-- setup.cfg\n`-- setup.py\n</pre>\n</div>\n<div id=\"get-stared-with-your-data-project\">\n<h2>Get stared with your data project</h2>\n<div id=\"crawl\">\n<h3>crawl</h3>\n<p>Crawl folder is where you crawl data. You have two choices as far as downloading. For each choice there are pre-build files, so just follow this:</p>\n<div id=\"wget\">\n<h4>wget</h4>\n<p>With wget you can download the files if the list of files is not big. There is a download_list.txt that will hold the url you want to download. You can specify wild cards like <cite>*.zip</cite>, <cite>*.pdf</cite>, <cite>*.txt</cite> etc. Download.sh is a shell script that calls wget and downloads files. By default it will only download files if they are newer then what you downloaded, and it will only download the missing parts. This saves your bandwidth and does not re-download the whole files each time.</p>\n<p>The only thing you need to do is edit download_list.txt:</p>\n<pre>cd crawl\n#Edit download_list.txt and add url of files you want to download\nvi download_list.txt\nsh download.sh\n</pre>\n<p>You second options is harvestman, see docs.</p>\n</div>\n</div>\n<div id=\"parse\">\n<h3>parse</h3>\n<p>Parse is where you parse the files. This is a gray area where you take control. This can be as simple as unziping a file, or writing a simple script to replace some names, or some extensive parsing program. It all depends on the project data. Add the code to parse.sh, or write your own parser and add the running code to parse.sh so that later you just run:</p>\n<pre>sh parse.sh\n</pre>\n</div>\n<div id=\"load\">\n<h3>load</h3>\n<p>Load is where you load your data to the database. There is a laod.py file that has a sample database structure for 4 columns. You can use that file as your starting point. It has everything from defining new columns, to setting up database and reading over a csv file in parse folder and uplaoding it to database. Read the load.sh and load.py files and make special not where it says [CHANGE]. These are the portion where you changed the namem, add columns, tell it where the file is. When all done, just run it:</p>\n<pre>sh load.sh\n</pre>\n</div>\n<div id=\"process-sh\">\n<h3>process.sh</h3>\n<p>Above all folders there is a file called process.sh. This files has a build in structure to go into crawl folder and start crawl.sh, then go into parse and run parse.sh, and then go into load and run load.sh script. With this file you control the whole process. When its all ready, a user can get your project, install any necessary programs and just simply run:</p>\n<pre>sh process.sh\n</pre>\n<p>This will crawl, parse, and load the data.</p>\n<p>Enjoy.</p>\n</div>\n</div>\n\n          </div>"}, "last_serial": 788795, "releases": {"0.7.79dev": [{"comment_text": "", "digests": {"md5": "f649cfcac29ad94aeb7ca51ab0ee1cc9", "sha256": "75b6190b59f2093ad93ed20fd159ab32d737bda822a15bb1412ce71642de7616"}, "downloads": -1, "filename": "datahub-0.7.79dev.tar.gz", "has_sig": false, "md5_digest": "f649cfcac29ad94aeb7ca51ab0ee1cc9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5807, "upload_time": "2008-12-17T04:30:47", "upload_time_iso_8601": "2008-12-17T04:30:47Z", "url": "https://files.pythonhosted.org/packages/93/6d/32d0641885b0ace600d2767908cc8dc0210c78d28024cbd97f33f4d72aca/datahub-0.7.79dev.tar.gz", "yanked": false}], "0.7.84dev": [{"comment_text": "", "digests": {"md5": "230b3a64cf42fce1d8ef35334858e5fc", "sha256": "340fa4df6c122f9d3b85518d3b2060b7d9993fbb315a16b582d8c01e521d136f"}, "downloads": -1, "filename": "datahub-0.7.84dev.tar.gz", "has_sig": false, "md5_digest": "230b3a64cf42fce1d8ef35334858e5fc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9364, "upload_time": "2009-09-02T05:19:14", "upload_time_iso_8601": "2009-09-02T05:19:14.776559Z", "url": "https://files.pythonhosted.org/packages/55/26/dab31fefc2d1fc7c606482b2b1cccaf232587a1ecc9e8cc1a0f362edbde1/datahub-0.7.84dev.tar.gz", "yanked": false}], "0.8.89dev": [{"comment_text": "", "digests": {"md5": "d62c699ffc08f548d28e247b5185933e", "sha256": "8b4ca8ee4f3c1a0e1d94c0816fa0c389578dbfc92c064fa2507c101075f1c5c3"}, "downloads": -1, "filename": "datahub-0.8.89dev.tar.gz", "has_sig": false, "md5_digest": "d62c699ffc08f548d28e247b5185933e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10589, "upload_time": "2009-11-29T21:53:03", "upload_time_iso_8601": "2009-11-29T21:53:03.733100Z", "url": "https://files.pythonhosted.org/packages/2d/94/48307989b5ad324509eacb5d46625aec0cca23001ff7c979d0e2b77b46e2/datahub-0.8.89dev.tar.gz", "yanked": false}], "0.8.90dev": [{"comment_text": "", "digests": {"md5": "add0f9e35741ce056f9ed1afe70fbb0e", "sha256": "cf158a4a100b44dfcbedb7b03f9df4029e00379021657e02f76743cec7ffc1a9"}, "downloads": -1, "filename": "datahub-0.8.90dev.tar.gz", "has_sig": false, "md5_digest": "add0f9e35741ce056f9ed1afe70fbb0e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11140, "upload_time": "2009-12-02T16:50:40", "upload_time_iso_8601": "2009-12-02T16:50:40.491484Z", "url": "https://files.pythonhosted.org/packages/e2/fa/8ce118aa8df1b56237068b2ee79e91236da6a5cdcd06a17274385e6dcab1/datahub-0.8.90dev.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "add0f9e35741ce056f9ed1afe70fbb0e", "sha256": "cf158a4a100b44dfcbedb7b03f9df4029e00379021657e02f76743cec7ffc1a9"}, "downloads": -1, "filename": "datahub-0.8.90dev.tar.gz", "has_sig": false, "md5_digest": "add0f9e35741ce056f9ed1afe70fbb0e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11140, "upload_time": "2009-12-02T16:50:40", "upload_time_iso_8601": "2009-12-02T16:50:40.491484Z", "url": "https://files.pythonhosted.org/packages/e2/fa/8ce118aa8df1b56237068b2ee79e91236da6a5cdcd06a17274385e6dcab1/datahub-0.8.90dev.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:40:22 2020"}