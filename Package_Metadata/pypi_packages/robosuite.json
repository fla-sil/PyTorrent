{"info": {"author": "Yuke Zhu, Ajay Mandlekar, Jiren Zhu, Joan Creus-Costa, Anchit Gupta", "author_email": "yukez@cs.stanford.edu", "bugtrack_url": null, "classifiers": [], "description": "# Surreal Robotics Suite\n\n\nSurreal Robotics Suite is a tookit and simulation benchmark powered by the [MuJoCo physics engine](http://mujoco.org/) for reproducible robotics research. The current release concentrates on reinforcement learning for robot manipulation. This library is designed to smoothly interoperate with the [Surreal Distributed Reinforcement Learning Framework](https://github.com/SurrealAI/Surreal).\n\nReinforcement learning has been a powerful and generic tool in robotics. Reinforcement learning combined with deep neural networks, i.e. *deep reinforcement learning* (DRL), has achieved some exciting successes in a variety of robot control problems. However, the challenges of reproducibility and replicability in DRL and robotics have impaired research progress. Our goal is to provide an accessible set of benchmarking tasks that facilitates a fair and rigorus evaluation and improves our understanding of new methods.\n\nThis framework was originally developed since late 2017 by researchers in [Stanford Vision and Learning Lab](http://svl.stanford.edu/) (SVL) as an internal tool for robot learning research. Today it is actively maintained and used for robotics research projects in SVL.\n\nThis release of Surreal Robotics Suite contains a set of benchmarking manipulation tasks and a modularized design of APIs for building new environments. We highlight these primary features below:\n\n* [**standardized tasks**](robosuite/environments): a set of single-arm and bimanual manipulation tasks of large diversity and varying complexity.\n* [**procedural generation**](robosuite/models): modularized APIs for programmatically creating new scenes and new tasks as a combinations of robot models, arenas, and parameterized 3D objects;\n* [**controller modes**](robosuite/controllers): a selection of controller types to command the robots, such as joint velocity control, inverse kinematics control, and 3D motion devices for teleoperation;\n* **multi-modal sensors**: heterogeneous types of sensory signals, including low-level physical states, RGB cameras, depth maps, and proprioception;\n* [**human demonstrations**](docs/demonstrations.md): utilities for collecting human demonstrations, replaying demonstration datasets, and leveraging demonstration data for learning.\n\n## Installation\nSurreal Robotics Suite officially supports Mac OS X and Linux on Python 3.5 or 3.7. It can be run with an on-screen display for visualization or in a headless mode for model training, with or without a GPU.\n\nThe base installation requires the MuJoCo physics engine (with [mujoco-py](https://github.com/openai/mujoco-py), refer to link for troubleshooting the installation and further instructions) and [numpy](http://www.numpy.org/). To avoid interfering with system packages, it is recommended to install it under a virtual environment by first running `virtualenv -p python3 . && source bin/activate`.\n\nFirst download MuJoCo 2.0 ([Linux](https://www.roboti.us/download/mujoco200_linux.zip) and [Mac OS X](https://www.roboti.us/download/mujoco200_macos.zip)) and unzip its contents into `~/.mujoco/mujoco200`, and copy your MuJoCo license key `~/.mujoco/mjkey.txt`. You can obtain a license key from [here](https://www.roboti.us/license.html).\n   - For Linux, you will need to install some packages to build `mujoco-py` (sourced from [here](https://github.com/openai/mujoco-py/blob/master/Dockerfile), with a couple missing packages added). If using `apt`, the required installation command is:\n     ```sh\n     $ sudo apt install curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev \\\n             libosmesa6-dev software-properties-common net-tools unzip vim \\\n             virtualenv wget xpra xserver-xorg-dev libglfw3-dev patchelf\n     ```\n     Note that for older versions of Ubuntu (e.g., 14.04) there's no libglfw3 package, in which case you need to `export LD_LIBRARY_PATH=$HOME/.mujoco/mujoco200/bin` before proceeding to the next step.\n\n### Install from pip\n1. After setting up mujoco, robosuite can be installed with\n```sh\n    $ pip install robosuite\n```\n\n2. Test your installation with\n```sh\n    $ python -m robosuite.demo\n```\n\n### Install from source\n1. Clone the robosuite repository\n```sh \n    $ git clone https://github.com/StanfordVL/robosuite.git\n    $ cd robosuite\n```\n\n2. Install the base requirements with\n   ```sh\n   $ pip3 install -r requirements.txt\n   ```\n   This will also install our library as an editable package, such that local changes will be reflected elsewhere without having to reinstall the package.\n\n3. (Optional) We also provide add-on functionalities, such as [OpenAI Gym](https://github.com/openai/gym) [interfaces](robosuite/wrappers/gym_wrapper.py), [inverse kinematics controllers](robosuite/wrappers/ik_wrapper.py) powered by [PyBullet](http://bulletphysics.org), and [teleoperation](robosuite/scripts/demo_spacemouse_ik_control.py) with [SpaceMouse](https://www.3dconnexion.com/products/spacemouse.html) devices (Mac OS X only). To enable these additional features, please install the extra dependencies by running\n   ```sh\n   $ pip3 install -r requirements-extra.txt\n   ```\n\n4. Test your installation with\n```sh\n    $ python robosuite/demo.py\n```\n\n## Quick Start\nThe APIs we provide to interact with our environments are simple and similar to the ones used by [OpenAI Gym](https://github.com/openai/gym/). Below is a minimalistic example of how to interact with an environment.\n\n```python\nimport numpy as np\nimport robosuite as suite\n\n# create environment instance\nenv = suite.make(\"SawyerLift\", has_renderer=True)\n\n# reset the environment\nenv.reset()\n\nfor i in range(1000):\n    action = np.random.randn(env.dof)  # sample random action\n    obs, reward, done, info = env.step(action)  # take action in the environment\n    env.render()  # render on display\n````\nThe `step()` function takes an `action` as input and returns a tuple of `(obs, reward, done, info)` where `obs` is an `OrderedDict` containing observations `[(name_string, np.array), ...]`, `reward` is the immediate reward obtained per step, `done` is a Boolean flag indicating if the episode has terminated and `info` is a dictionary which contains additional metadata.\n\nThere are other parameters which can be configured for each environment. They provide functionalities such as headless rendering, getting pixel observations, changing camera settings, using reward shaping, and adding extra low-level observations. Please refer to [this page](robosuite/environments/README.md) and the [environment classes](robosuite/environments) for further details.\n\nSample scripts that showcase various features of the Surreal Robotics Suite are available at [robosuite/scripts](robosuite/scripts). The purpose of each script and usage instructions can be found at the beginning of each file.\n\n## Building Your Own Environments\nA manipulation `task` typically involves the participation of a `robot` with `gripper`s as its end-effectors, an `arena` (workspace), and `object`s that the robot interacts with. Our APIs in [Models](robosuite/models) provide a toolkit of composing these modularized elements into a scene, which can be loaded in MuJoCo for simulation. To build your own environments, you are recommended to take a look at the [environment classes](robosuite/environments) which have used these APIs to define a set of standardized manipulation tasks. You can also find detailed documentations about [creating a custom object](docs/creating_object.md) and [creating a custom environment](docs/creating_environment.md).\n\n## Human Demonstrations\n\n### Collecting Human Demonstrations\n\nWe provide teleoperation utilities that allow users to control the robots with input devices, such as the keyboard and the [SpaceMouse](https://www.3dconnexion.com/spacemouse_compact/en/). Such functionality allows us to collect a dataset of human demonstrations for learning. We provide an example script to illustrate how to collect demonstrations. Our [collect_human_demonstrations](robosuite/scripts/collect_human_demonstrations.py) script takes the following arguments:\n\n- `directory:` path to a folder for where to store the pickle file of collected demonstrations\n- `environment:` name of the environment you would like to collect the demonstrations for\n- `device:` either \"keyboard\" or \"spacemouse\"\n\nOur twin project [RoboTurk](http://roboturk.stanford.edu) has collected pilot datasets of more than a thousand demonstrations for two tasks in our Suite via crowdsourcing. You can find detailed information about the [RoboTurk datasets](docs/demonstrations.md#roboturk-dataset) and [demonstration collection](docs/demonstrations.md#collecting-your-own-demonstrations) here.\n\n### Replaying Human Demonstrations\n\nWe have included an example script that illustrates how demonstrations can be loaded and played back. Our [playback_demonstrations_from_hdf5](robosuite/scripts/playback_demonstrations_from_hdf5.py) script selects demonstration episodes at random from a demonstration pickle file and replays them. We have included some sample demonstrations for each task at `models/assets/demonstrations`.\n\n### Using Demonstrations for Learning\n\n[Several](https://arxiv.org/abs/1802.09564) [prior](https://arxiv.org/abs/1807.06919) [works](https://arxiv.org/abs/1804.02717) have demonstrated the effectiveness of altering the start state distribution of training episodes for learning RL policies. We provide a generic utility for setting various types of learning curriculums which dictate how to sample from demonstration episodes when doing an environment reset. For more information see the `DemoSamplerWrapper` class. We have provided an example of how to use this wrapper along with a demonstration pickle file in the [demo_learning_curriculum](robosuite/scripts/demo_learning_curriculum.py) script.\n\n## Citations\nPlease cite [Surreal](http://surreal.stanford.edu) if you use this repository in your publications:\n```\n@inproceedings{corl2018surreal,\n  title={SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark},\n  author={Fan, Linxi and Zhu, Yuke and Zhu, Jiren and Liu, Zihua and Zeng, Orien and Gupta, Anchit and Creus-Costa, Joan and Savarese, Silvio and Fei-Fei, Li},\n  booktitle={Conference on Robot Learning},\n  year={2018}\n}\n```\n\nPlease also cite [RoboTurk](http://roboturk.stanford.edu) if you use the demonstration datasets:\n```\n@inproceedings{corl2018roboturk,\n  title={RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation},\n  author={Mandlekar, Ajay and Zhu, Yuke and Garg, Animesh and Booher, Jonathan and Spero, Max and Tung, Albert and Gao, Julian and Emmons, John and Gupta, Anchit and Orbay, Emre and Savarese, Silvio and Fei-Fei, Li},\n  booktitle={Conference on Robot Learning},\n  year={2018}\n}\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/StanfordVL/robosuite", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "robosuite", "package_url": "https://pypi.org/project/robosuite/", "platform": "", "project_url": "https://pypi.org/project/robosuite/", "project_urls": {"Homepage": "https://github.com/StanfordVL/robosuite"}, "release_url": "https://pypi.org/project/robosuite/0.3.0/", "requires_dist": ["numpy (>=1.13.3)", "mujoco-py (==2.0.2.2)"], "requires_python": ">=3", "summary": "Surreal Robotics Suite: Standardized and Accessible Robot Manipulation Benchmark in Physics Simulation", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Surreal Robotics Suite</h1>\n<p>Surreal Robotics Suite is a tookit and simulation benchmark powered by the <a href=\"http://mujoco.org/\" rel=\"nofollow\">MuJoCo physics engine</a> for reproducible robotics research. The current release concentrates on reinforcement learning for robot manipulation. This library is designed to smoothly interoperate with the <a href=\"https://github.com/SurrealAI/Surreal\" rel=\"nofollow\">Surreal Distributed Reinforcement Learning Framework</a>.</p>\n<p>Reinforcement learning has been a powerful and generic tool in robotics. Reinforcement learning combined with deep neural networks, i.e. <em>deep reinforcement learning</em> (DRL), has achieved some exciting successes in a variety of robot control problems. However, the challenges of reproducibility and replicability in DRL and robotics have impaired research progress. Our goal is to provide an accessible set of benchmarking tasks that facilitates a fair and rigorus evaluation and improves our understanding of new methods.</p>\n<p>This framework was originally developed since late 2017 by researchers in <a href=\"http://svl.stanford.edu/\" rel=\"nofollow\">Stanford Vision and Learning Lab</a> (SVL) as an internal tool for robot learning research. Today it is actively maintained and used for robotics research projects in SVL.</p>\n<p>This release of Surreal Robotics Suite contains a set of benchmarking manipulation tasks and a modularized design of APIs for building new environments. We highlight these primary features below:</p>\n<ul>\n<li><a href=\"robosuite/environments\" rel=\"nofollow\"><strong>standardized tasks</strong></a>: a set of single-arm and bimanual manipulation tasks of large diversity and varying complexity.</li>\n<li><a href=\"robosuite/models\" rel=\"nofollow\"><strong>procedural generation</strong></a>: modularized APIs for programmatically creating new scenes and new tasks as a combinations of robot models, arenas, and parameterized 3D objects;</li>\n<li><a href=\"robosuite/controllers\" rel=\"nofollow\"><strong>controller modes</strong></a>: a selection of controller types to command the robots, such as joint velocity control, inverse kinematics control, and 3D motion devices for teleoperation;</li>\n<li><strong>multi-modal sensors</strong>: heterogeneous types of sensory signals, including low-level physical states, RGB cameras, depth maps, and proprioception;</li>\n<li><a href=\"docs/demonstrations.md\" rel=\"nofollow\"><strong>human demonstrations</strong></a>: utilities for collecting human demonstrations, replaying demonstration datasets, and leveraging demonstration data for learning.</li>\n</ul>\n<h2>Installation</h2>\n<p>Surreal Robotics Suite officially supports Mac OS X and Linux on Python 3.5 or 3.7. It can be run with an on-screen display for visualization or in a headless mode for model training, with or without a GPU.</p>\n<p>The base installation requires the MuJoCo physics engine (with <a href=\"https://github.com/openai/mujoco-py\" rel=\"nofollow\">mujoco-py</a>, refer to link for troubleshooting the installation and further instructions) and <a href=\"http://www.numpy.org/\" rel=\"nofollow\">numpy</a>. To avoid interfering with system packages, it is recommended to install it under a virtual environment by first running <code>virtualenv -p python3 . &amp;&amp; source bin/activate</code>.</p>\n<p>First download MuJoCo 2.0 (<a href=\"https://www.roboti.us/download/mujoco200_linux.zip\" rel=\"nofollow\">Linux</a> and <a href=\"https://www.roboti.us/download/mujoco200_macos.zip\" rel=\"nofollow\">Mac OS X</a>) and unzip its contents into <code>~/.mujoco/mujoco200</code>, and copy your MuJoCo license key <code>~/.mujoco/mjkey.txt</code>. You can obtain a license key from <a href=\"https://www.roboti.us/license.html\" rel=\"nofollow\">here</a>.</p>\n<ul>\n<li>For Linux, you will need to install some packages to build <code>mujoco-py</code> (sourced from <a href=\"https://github.com/openai/mujoco-py/blob/master/Dockerfile\" rel=\"nofollow\">here</a>, with a couple missing packages added). If using <code>apt</code>, the required installation command is:\n<pre>$ sudo apt install curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev <span class=\"se\">\\</span>\n        libosmesa6-dev software-properties-common net-tools unzip vim <span class=\"se\">\\</span>\n        virtualenv wget xpra xserver-xorg-dev libglfw3-dev patchelf\n</pre>\nNote that for older versions of Ubuntu (e.g., 14.04) there's no libglfw3 package, in which case you need to <code>export LD_LIBRARY_PATH=$HOME/.mujoco/mujoco200/bin</code> before proceeding to the next step.</li>\n</ul>\n<h3>Install from pip</h3>\n<ol>\n<li>After setting up mujoco, robosuite can be installed with</li>\n</ol>\n<pre>    $ pip install robosuite\n</pre>\n<ol>\n<li>Test your installation with</li>\n</ol>\n<pre>    $ python -m robosuite.demo\n</pre>\n<h3>Install from source</h3>\n<ol>\n<li>Clone the robosuite repository</li>\n</ol>\n<pre>    $ git clone https://github.com/StanfordVL/robosuite.git\n    $ <span class=\"nb\">cd</span> robosuite\n</pre>\n<ol>\n<li>\n<p>Install the base requirements with</p>\n<pre>$ pip3 install -r requirements.txt\n</pre>\n<p>This will also install our library as an editable package, such that local changes will be reflected elsewhere without having to reinstall the package.</p>\n</li>\n<li>\n<p>(Optional) We also provide add-on functionalities, such as <a href=\"https://github.com/openai/gym\" rel=\"nofollow\">OpenAI Gym</a> <a href=\"robosuite/wrappers/gym_wrapper.py\" rel=\"nofollow\">interfaces</a>, <a href=\"robosuite/wrappers/ik_wrapper.py\" rel=\"nofollow\">inverse kinematics controllers</a> powered by <a href=\"http://bulletphysics.org\" rel=\"nofollow\">PyBullet</a>, and <a href=\"robosuite/scripts/demo_spacemouse_ik_control.py\" rel=\"nofollow\">teleoperation</a> with <a href=\"https://www.3dconnexion.com/products/spacemouse.html\" rel=\"nofollow\">SpaceMouse</a> devices (Mac OS X only). To enable these additional features, please install the extra dependencies by running</p>\n<pre>$ pip3 install -r requirements-extra.txt\n</pre>\n</li>\n<li>\n<p>Test your installation with</p>\n</li>\n</ol>\n<pre>    $ python robosuite/demo.py\n</pre>\n<h2>Quick Start</h2>\n<p>The APIs we provide to interact with our environments are simple and similar to the ones used by <a href=\"https://github.com/openai/gym/\" rel=\"nofollow\">OpenAI Gym</a>. Below is a minimalistic example of how to interact with an environment.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">robosuite</span> <span class=\"k\">as</span> <span class=\"nn\">suite</span>\n\n<span class=\"c1\"># create environment instance</span>\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">suite</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s2\">\"SawyerLift\"</span><span class=\"p\">,</span> <span class=\"n\">has_renderer</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># reset the environment</span>\n<span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">):</span>\n    <span class=\"n\">action</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">dof</span><span class=\"p\">)</span>  <span class=\"c1\"># sample random action</span>\n    <span class=\"n\">obs</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">info</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"p\">)</span>  <span class=\"c1\"># take action in the environment</span>\n    <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">render</span><span class=\"p\">()</span>  <span class=\"c1\"># render on display</span>\n</pre>\n<p>The <code>step()</code> function takes an <code>action</code> as input and returns a tuple of <code>(obs, reward, done, info)</code> where <code>obs</code> is an <code>OrderedDict</code> containing observations <code>[(name_string, np.array), ...]</code>, <code>reward</code> is the immediate reward obtained per step, <code>done</code> is a Boolean flag indicating if the episode has terminated and <code>info</code> is a dictionary which contains additional metadata.</p>\n<p>There are other parameters which can be configured for each environment. They provide functionalities such as headless rendering, getting pixel observations, changing camera settings, using reward shaping, and adding extra low-level observations. Please refer to <a href=\"robosuite/environments/README.md\" rel=\"nofollow\">this page</a> and the <a href=\"robosuite/environments\" rel=\"nofollow\">environment classes</a> for further details.</p>\n<p>Sample scripts that showcase various features of the Surreal Robotics Suite are available at <a href=\"robosuite/scripts\" rel=\"nofollow\">robosuite/scripts</a>. The purpose of each script and usage instructions can be found at the beginning of each file.</p>\n<h2>Building Your Own Environments</h2>\n<p>A manipulation <code>task</code> typically involves the participation of a <code>robot</code> with <code>gripper</code>s as its end-effectors, an <code>arena</code> (workspace), and <code>object</code>s that the robot interacts with. Our APIs in <a href=\"robosuite/models\" rel=\"nofollow\">Models</a> provide a toolkit of composing these modularized elements into a scene, which can be loaded in MuJoCo for simulation. To build your own environments, you are recommended to take a look at the <a href=\"robosuite/environments\" rel=\"nofollow\">environment classes</a> which have used these APIs to define a set of standardized manipulation tasks. You can also find detailed documentations about <a href=\"docs/creating_object.md\" rel=\"nofollow\">creating a custom object</a> and <a href=\"docs/creating_environment.md\" rel=\"nofollow\">creating a custom environment</a>.</p>\n<h2>Human Demonstrations</h2>\n<h3>Collecting Human Demonstrations</h3>\n<p>We provide teleoperation utilities that allow users to control the robots with input devices, such as the keyboard and the <a href=\"https://www.3dconnexion.com/spacemouse_compact/en/\" rel=\"nofollow\">SpaceMouse</a>. Such functionality allows us to collect a dataset of human demonstrations for learning. We provide an example script to illustrate how to collect demonstrations. Our <a href=\"robosuite/scripts/collect_human_demonstrations.py\" rel=\"nofollow\">collect_human_demonstrations</a> script takes the following arguments:</p>\n<ul>\n<li><code>directory:</code> path to a folder for where to store the pickle file of collected demonstrations</li>\n<li><code>environment:</code> name of the environment you would like to collect the demonstrations for</li>\n<li><code>device:</code> either \"keyboard\" or \"spacemouse\"</li>\n</ul>\n<p>Our twin project <a href=\"http://roboturk.stanford.edu\" rel=\"nofollow\">RoboTurk</a> has collected pilot datasets of more than a thousand demonstrations for two tasks in our Suite via crowdsourcing. You can find detailed information about the <a href=\"docs/demonstrations.md#roboturk-dataset\" rel=\"nofollow\">RoboTurk datasets</a> and <a href=\"docs/demonstrations.md#collecting-your-own-demonstrations\" rel=\"nofollow\">demonstration collection</a> here.</p>\n<h3>Replaying Human Demonstrations</h3>\n<p>We have included an example script that illustrates how demonstrations can be loaded and played back. Our <a href=\"robosuite/scripts/playback_demonstrations_from_hdf5.py\" rel=\"nofollow\">playback_demonstrations_from_hdf5</a> script selects demonstration episodes at random from a demonstration pickle file and replays them. We have included some sample demonstrations for each task at <code>models/assets/demonstrations</code>.</p>\n<h3>Using Demonstrations for Learning</h3>\n<p><a href=\"https://arxiv.org/abs/1802.09564\" rel=\"nofollow\">Several</a> <a href=\"https://arxiv.org/abs/1807.06919\" rel=\"nofollow\">prior</a> <a href=\"https://arxiv.org/abs/1804.02717\" rel=\"nofollow\">works</a> have demonstrated the effectiveness of altering the start state distribution of training episodes for learning RL policies. We provide a generic utility for setting various types of learning curriculums which dictate how to sample from demonstration episodes when doing an environment reset. For more information see the <code>DemoSamplerWrapper</code> class. We have provided an example of how to use this wrapper along with a demonstration pickle file in the <a href=\"robosuite/scripts/demo_learning_curriculum.py\" rel=\"nofollow\">demo_learning_curriculum</a> script.</p>\n<h2>Citations</h2>\n<p>Please cite <a href=\"http://surreal.stanford.edu\" rel=\"nofollow\">Surreal</a> if you use this repository in your publications:</p>\n<pre><code>@inproceedings{corl2018surreal,\n  title={SURREAL: Open-Source Reinforcement Learning Framework and Robot Manipulation Benchmark},\n  author={Fan, Linxi and Zhu, Yuke and Zhu, Jiren and Liu, Zihua and Zeng, Orien and Gupta, Anchit and Creus-Costa, Joan and Savarese, Silvio and Fei-Fei, Li},\n  booktitle={Conference on Robot Learning},\n  year={2018}\n}\n</code></pre>\n<p>Please also cite <a href=\"http://roboturk.stanford.edu\" rel=\"nofollow\">RoboTurk</a> if you use the demonstration datasets:</p>\n<pre><code>@inproceedings{corl2018roboturk,\n  title={RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation},\n  author={Mandlekar, Ajay and Zhu, Yuke and Garg, Animesh and Booher, Jonathan and Spero, Max and Tung, Albert and Gao, Julian and Emmons, John and Gupta, Anchit and Orbay, Emre and Savarese, Silvio and Fei-Fei, Li},\n  booktitle={Conference on Robot Learning},\n  year={2018}\n}\n</code></pre>\n\n          </div>"}, "last_serial": 6271594, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "3572061ef9f33922725f12bd45961da6", "sha256": "18433524311766d0312bdb5b00a0aaafdd72506c234feae038ba27b1c1b57706"}, "downloads": -1, "filename": "robosuite-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3572061ef9f33922725f12bd45961da6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1293, "upload_time": "2018-10-04T03:22:30", "upload_time_iso_8601": "2018-10-04T03:22:30.991725Z", "url": "https://files.pythonhosted.org/packages/4a/ea/9735ed63b43e947e8348fc436245b00595dde3717f22571d29f642b702e9/robosuite-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "58cd625bd652da7df4706a26d1124dbd", "sha256": "fb68f12b9c45c01dbf63a813085d2d5517ec3f7263fecf66f55006a1b9b3b89a"}, "downloads": -1, "filename": "robosuite-0.0.0.tar.gz", "has_sig": false, "md5_digest": "58cd625bd652da7df4706a26d1124dbd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1039, "upload_time": "2018-10-04T03:22:32", "upload_time_iso_8601": "2018-10-04T03:22:32.161046Z", "url": "https://files.pythonhosted.org/packages/fd/5c/021c67965fe88f5ecb33c50bc0b9d2d2f3c2723066abed9da81d57072794/robosuite-0.0.0.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "d07d81bf089e4c58a6d2720a6682c9be", "sha256": "497e4f089f69cd00fcc6587d5d53bfdc50de30081034081c207ae0ac742fe4d2"}, "downloads": -1, "filename": "robosuite-0.1.0-py3.7.egg", "has_sig": false, "md5_digest": "d07d81bf089e4c58a6d2720a6682c9be", "packagetype": "bdist_egg", "python_version": "3.7", "requires_python": ">=3", "size": 238205, "upload_time": "2018-10-26T21:24:00", "upload_time_iso_8601": "2018-10-26T21:24:00.414243Z", "url": "https://files.pythonhosted.org/packages/4c/13/faca8042f9dccabf1142c4fa737772ba86d6b158a4fd5b8108a50e769c13/robosuite-0.1.0-py3.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "d6605774820866e070c0426a82861580", "sha256": "167940101c37b2a590676d60659a0a1ca59a71cc583ab5e6b9c025c23ebcec7c"}, "downloads": -1, "filename": "robosuite-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d6605774820866e070c0426a82861580", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 54210182, "upload_time": "2018-10-26T21:23:56", "upload_time_iso_8601": "2018-10-26T21:23:56.840640Z", "url": "https://files.pythonhosted.org/packages/5e/95/33534a97790056e9ead6c68f5aa307b21a86504c895d04a8a52e6f5134ab/robosuite-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "458267f00f3f65e7e625e5a99d885208", "sha256": "85da282ce25ed27953ecbcfa575fb639765dd8cb753fa27ab26ddeaacf57cbf5"}, "downloads": -1, "filename": "robosuite-0.1.0.tar.gz", "has_sig": false, "md5_digest": "458267f00f3f65e7e625e5a99d885208", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 54053222, "upload_time": "2018-10-26T21:24:20", "upload_time_iso_8601": "2018-10-26T21:24:20.528236Z", "url": "https://files.pythonhosted.org/packages/32/5e/cda6590b394a68510b9e657f4e843e563c9f156b165bb4ee68f74044c932/robosuite-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "fba45b13fffb8c9426cd0dd85ff2f492", "sha256": "57177d31cf3816a3c2066bfdc51f8a1587f60f880c9a4ebda39616a9387cc15d"}, "downloads": -1, "filename": "robosuite-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "fba45b13fffb8c9426cd0dd85ff2f492", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 54211151, "upload_time": "2019-12-09T07:06:00", "upload_time_iso_8601": "2019-12-09T07:06:00.895023Z", "url": "https://files.pythonhosted.org/packages/dd/0c/260da8e33a616f7ec538ee40cba03de8bdac6b91e016e0077c71b860ce70/robosuite-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6c24d70a667605c5675600c160a2d669", "sha256": "cb8c59a285f5dbd6e281381565fb23e7e721e02e668ee51d66476b5757ded6a9"}, "downloads": -1, "filename": "robosuite-0.2.0.tar.gz", "has_sig": false, "md5_digest": "6c24d70a667605c5675600c160a2d669", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 54072368, "upload_time": "2019-12-09T07:06:33", "upload_time_iso_8601": "2019-12-09T07:06:33.421338Z", "url": "https://files.pythonhosted.org/packages/cb/c2/f672fe7022a46022cb42e449c664e519a04d9472fd6479f752dda4d9fb0c/robosuite-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "622f4f8ef2f379dac8a9ce83265a6806", "sha256": "7b7fc816b47886ff489d1c22bc5dbcefaa9c10592f3aadc4f80336099ee50d11"}, "downloads": -1, "filename": "robosuite-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "622f4f8ef2f379dac8a9ce83265a6806", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 54215427, "upload_time": "2019-12-10T08:50:13", "upload_time_iso_8601": "2019-12-10T08:50:13.912394Z", "url": "https://files.pythonhosted.org/packages/b8/fd/4f6d6bc38134f14494f2e8032aca360a93fc362e5c859869a0dd787b1ec3/robosuite-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "89cae9b7c78ca8a972daa63f9ea4ee8d", "sha256": "35ebba87d69179fcd208091e12c463ef3df56c6f4f57f48d74ae0f52aad0073a"}, "downloads": -1, "filename": "robosuite-0.3.0.tar.gz", "has_sig": false, "md5_digest": "89cae9b7c78ca8a972daa63f9ea4ee8d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 54076826, "upload_time": "2019-12-10T08:50:47", "upload_time_iso_8601": "2019-12-10T08:50:47.550352Z", "url": "https://files.pythonhosted.org/packages/d0/6e/c19b46ffc86d0243f26120028bc3b30f2ce32c1723ccbc5dc4473e549370/robosuite-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "622f4f8ef2f379dac8a9ce83265a6806", "sha256": "7b7fc816b47886ff489d1c22bc5dbcefaa9c10592f3aadc4f80336099ee50d11"}, "downloads": -1, "filename": "robosuite-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "622f4f8ef2f379dac8a9ce83265a6806", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 54215427, "upload_time": "2019-12-10T08:50:13", "upload_time_iso_8601": "2019-12-10T08:50:13.912394Z", "url": "https://files.pythonhosted.org/packages/b8/fd/4f6d6bc38134f14494f2e8032aca360a93fc362e5c859869a0dd787b1ec3/robosuite-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "89cae9b7c78ca8a972daa63f9ea4ee8d", "sha256": "35ebba87d69179fcd208091e12c463ef3df56c6f4f57f48d74ae0f52aad0073a"}, "downloads": -1, "filename": "robosuite-0.3.0.tar.gz", "has_sig": false, "md5_digest": "89cae9b7c78ca8a972daa63f9ea4ee8d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 54076826, "upload_time": "2019-12-10T08:50:47", "upload_time_iso_8601": "2019-12-10T08:50:47.550352Z", "url": "https://files.pythonhosted.org/packages/d0/6e/c19b46ffc86d0243f26120028bc3b30f2ce32c1723ccbc5dc4473e549370/robosuite-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:02:00 2020"}