{"info": {"author": "Ivan Lopez", "author_email": "ivan@askai.net", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: POSIX", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# Spark Networks DevOps coding challenge\n\nhttps://gitlab.com/askainet/spark-networks-code-challenge\n\nRequirements\n------------\n\n- Python >= 3.4\n\nInstall\n-------\n\nFrom public Pypi repository:\n\n```\npip install sparkd\n```\n\nDemo\n----\n\n```\ndocker build . -t sparkd\n\ndocker run -it -e RETRIES=5 sparkd demo.sh\n```\n\nUsage\n-----\n\n```\nusage: sparkd [-h] [--version] [-l LOGFILE]\n              [--command-logfile COMMAND_LOGFILE] [-n NAME] [-v] [-d]\n              [-r RETRIES] [-i RETRY_INTERVAL] [-c CHECK_INTERVAL]\n              [command] [arguments [arguments ...]]\n\nRun any command as a daemon and supervise it\n\npositional arguments:\n  command               The command to run as a daemon to supervise\n  arguments             Arguments to the command\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             Show version\n  -l LOGFILE, --logfile LOGFILE\n                        Set the logfile for the Sparkd supervisor\n  --command-logfile COMMAND_LOGFILE\n                        Set the logfile for the command to supervise\n  -n NAME, --name NAME  Set the name of this Sparkd instance\n  -v, --verbose         Enable verbose logging\n  -d, --debug           Enable debug logging\n  -r RETRIES, --retries RETRIES\n                        Number of retries to restart the process\n  -i RETRY_INTERVAL, --retry-interval RETRY_INTERVAL\n                        Seconds to wait between retries to restart the process\n  -c CHECK_INTERVAL, --check-interval CHECK_INTERVAL\n                        Seconds to wait between checking process status\n```\n\nDeveloping\n----------\n\n### Tests\n\nSome basic tests have been added to have coverage only for the `Process` class,\njust to demonstrate how to use `unittest` with Python.\n\n```\nmake test\n```\n\n### Linting\n\nLinting with `pylint` to keep a nice and healthy code.\n\n```\nmake lint\n```\n\n### Publish to Pypi\n\nPackage and publish to Pypi repository.\n\n```\nmake dist\n```\n\n### Install from source\n\n```\nmake install\n```\n\n### Documentation\n\nDocumentation is built using `sphinx` based on docstrings present in the code.\n\nReadthedocs integration is configured to automatically generate and upload\ndocs to https://sparkd.readthedocs.io/en/latest/ on updates to master branch.\n\nManual local docs generation is possible running `make docs`", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://gitlab.com/askainet/spark-networks-code-challenge", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "sparkd", "package_url": "https://pypi.org/project/sparkd/", "platform": "", "project_url": "https://pypi.org/project/sparkd/", "project_urls": {"Homepage": "https://gitlab.com/askainet/spark-networks-code-challenge"}, "release_url": "https://pypi.org/project/sparkd/0.0.1b0/", "requires_dist": null, "requires_python": ">=3.4, <4", "summary": "Spark Networks DevOps coding challenge", "version": "0.0.1b0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Spark Networks DevOps coding challenge</h1>\n<p><a href=\"https://gitlab.com/askainet/spark-networks-code-challenge\" rel=\"nofollow\">https://gitlab.com/askainet/spark-networks-code-challenge</a></p>\n<h2>Requirements</h2>\n<ul>\n<li>Python &gt;= 3.4</li>\n</ul>\n<h2>Install</h2>\n<p>From public Pypi repository:</p>\n<pre><code>pip install sparkd\n</code></pre>\n<h2>Demo</h2>\n<pre><code>docker build . -t sparkd\n\ndocker run -it -e RETRIES=5 sparkd demo.sh\n</code></pre>\n<h2>Usage</h2>\n<pre><code>usage: sparkd [-h] [--version] [-l LOGFILE]\n              [--command-logfile COMMAND_LOGFILE] [-n NAME] [-v] [-d]\n              [-r RETRIES] [-i RETRY_INTERVAL] [-c CHECK_INTERVAL]\n              [command] [arguments [arguments ...]]\n\nRun any command as a daemon and supervise it\n\npositional arguments:\n  command               The command to run as a daemon to supervise\n  arguments             Arguments to the command\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             Show version\n  -l LOGFILE, --logfile LOGFILE\n                        Set the logfile for the Sparkd supervisor\n  --command-logfile COMMAND_LOGFILE\n                        Set the logfile for the command to supervise\n  -n NAME, --name NAME  Set the name of this Sparkd instance\n  -v, --verbose         Enable verbose logging\n  -d, --debug           Enable debug logging\n  -r RETRIES, --retries RETRIES\n                        Number of retries to restart the process\n  -i RETRY_INTERVAL, --retry-interval RETRY_INTERVAL\n                        Seconds to wait between retries to restart the process\n  -c CHECK_INTERVAL, --check-interval CHECK_INTERVAL\n                        Seconds to wait between checking process status\n</code></pre>\n<h2>Developing</h2>\n<h3>Tests</h3>\n<p>Some basic tests have been added to have coverage only for the <code>Process</code> class,\njust to demonstrate how to use <code>unittest</code> with Python.</p>\n<pre><code>make test\n</code></pre>\n<h3>Linting</h3>\n<p>Linting with <code>pylint</code> to keep a nice and healthy code.</p>\n<pre><code>make lint\n</code></pre>\n<h3>Publish to Pypi</h3>\n<p>Package and publish to Pypi repository.</p>\n<pre><code>make dist\n</code></pre>\n<h3>Install from source</h3>\n<pre><code>make install\n</code></pre>\n<h3>Documentation</h3>\n<p>Documentation is built using <code>sphinx</code> based on docstrings present in the code.</p>\n<p>Readthedocs integration is configured to automatically generate and upload\ndocs to <a href=\"https://sparkd.readthedocs.io/en/latest/\" rel=\"nofollow\">https://sparkd.readthedocs.io/en/latest/</a> on updates to master branch.</p>\n<p>Manual local docs generation is possible running <code>make docs</code></p>\n\n          </div>"}, "last_serial": 4234987, "releases": {"0.0.1b0": [{"comment_text": "", "digests": {"md5": "1d998064185a76a3dcb2b4aeb815e4b1", "sha256": "1f5fcfbefeacc2586f2330f51a8078b003f0ffecc1a0412c6590760b0d1e03e4"}, "downloads": -1, "filename": "sparkd-0.0.1b0.tar.gz", "has_sig": false, "md5_digest": "1d998064185a76a3dcb2b4aeb815e4b1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4, <4", "size": 8756, "upload_time": "2018-09-03T17:29:06", "upload_time_iso_8601": "2018-09-03T17:29:06.289882Z", "url": "https://files.pythonhosted.org/packages/40/34/536223496344f0c46e0d8344257b4773982a6ef0b812a20714ac5278f11f/sparkd-0.0.1b0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1d998064185a76a3dcb2b4aeb815e4b1", "sha256": "1f5fcfbefeacc2586f2330f51a8078b003f0ffecc1a0412c6590760b0d1e03e4"}, "downloads": -1, "filename": "sparkd-0.0.1b0.tar.gz", "has_sig": false, "md5_digest": "1d998064185a76a3dcb2b4aeb815e4b1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4, <4", "size": 8756, "upload_time": "2018-09-03T17:29:06", "upload_time_iso_8601": "2018-09-03T17:29:06.289882Z", "url": "https://files.pythonhosted.org/packages/40/34/536223496344f0c46e0d8344257b4773982a6ef0b812a20714ac5278f11f/sparkd-0.0.1b0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:05:58 2020"}