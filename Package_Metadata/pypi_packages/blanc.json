{"info": {"author": "Primer AI", "author_email": "blanc@primer.ai", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# BLANC\nThis is the reference implementation of BLANC-help and BLANC-tune as defined in [Fill in the BLANC: Human-free quality estimation of document summaries](https://arxiv.org/abs/2002.09836). \n\nBLANC is a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document's text. We present evidence that BLANC scores have at least as good correlation with human evaluations as do the ROUGE family of summary quality measurements. And unlike ROUGE, the BLANC method does not require human-written reference summaries, allowing for fully human-free summary quality estimation.\n\n## Setup\n1. Install Python 3.6 or higher\n2. Install with `pip install blanc`\n\nNote that we pinned the `transformers` requirement to 2.4.0 due to Rust installation issues that are introduced in future versions. We used `transformers` 2.5.1 in our experiments, so you may want to try upgrading `transformers` if you can.\n\n## Python Usage\nBasic usage:\n```python\n>>> from blanc import BlancHelp, BlancTune\n>>> document = \"Jack walked all the way over to go to the big market and buy a few loaves of bread for himself.\"\n>>> summary = \"Jack bought bread from the market\"\n>>> blanc_help = BlancHelp()\n>>> blanc_tune = BlancTune()\n>>> blanc_help.eval_once(document, summary)\n0.14285714285714285\n>>> blanc_tune.eval_once(document, summary)\n0.14285714285714285\n```\n\nBy default, BLANC is run on the CPU. Using CUDA with batching is much faster:\n```python\nblanc_help = BlancHelp(device='cuda', inference_batch_size=128)\nblanc_tune = BlancTune(device='cuda', inference_batch_size=24, finetune_batch_size=24)\n```\nWith these batch sizes, BLANC-help takes around 1.4 sec per summary and BLANC-tune takes around 1.8 sec per summary on an NVIDIA V100. In addition to the parameters controlling device and batch sizes, BlancHelp and BlancTune take several other parameters controlling how the BLANC scores are calculated, and the default values for those parameters reproduce the results of the paper.\n\nIf you want to compute the BLANC scores of many documents and summaries at once, you can use `eval_pairs()` or `eval_summaries_for_docs()`. `eval_pairs()` is useful when you have many documents, each with a single summary:\n```python\n>>> documents = [\"Jack walked all the way over to go to the big market and buy a few loaves of bread for himself.\", \"As Jill started taking a walk in the park, she certainly noticed that the trees were extra green this year.\"]\n>>> summaries = [\"Jack bought bread from the market.\", \"Jill saw green trees in the park.\"]\n>>> blanc_help.eval_pairs(documents, summaries)\n[0.14285714285714285, 0.07142857142857142]\n```\n\n`eval_summaries_for_docs()` is useful when you have many documents, each with many summaries:\n```python\n>>> blanc_tune.eval_summaries_for_docs(documents, doc_summaries)\n[[0.14285714285714285, 0.14285714285714285], [0.14285714285714285, 0.07142857142857142]]\n```\n\n## CLI Usage\nA CLI for computing BLANC scores is provided for convenience.\n```\n$ blanc help --doc \"Jack walked all the way over to go to the big market and buy a few loaves of bread for himself.\" --summary \"Jack bought bread from the market.\"\n0.14285714285714285\n```\n\nInput data can also be provided in JSON format, with sample JSON input provided in `data/`\n```\n$ blanc help --single_json data/single.json\n0.14285714285714285\n$ blanc tune --pairs_json data/pairs.json\n[0.2857142857142857, 0.21428571428571427]\n$ blanc tune --doc_summaries_json data/doc-summaries.json\n[[0.2857142857142857, 0.14285714285714285], [0.14285714285714285, 0.14285714285714285]]\n```\n\nThe `single_json` input format expects a single JSON blob with keys `document` and `summary`. The `pairs_json` input format expects a list of JSON blobs, each with a `document` and a `summary`. The `doc_summaries_json` input format expects a list of JSON blobs, each with keys `document` and `summaries`, where `summaries` is a list of strings. These keys are customizable with the `doc_key`, `summary_key`, and `summaries_key` arguments. By default, the output is printed to STDOUT, but it can be written to a JSON file provided with the `output_json` argument.\n\nFull documentation is available with `blanc --help`:\n```\nrequired arguments:\n  {help,tune}           BLANC-help or BLANC-tune\n\ninput arguments:\n  --doc DOC             single input document (default: None)\n  --summary SUMMARY     single input summary (default: None)\n  --single_json FILENAME\n                        filename for single document summary pair (default:\n                        None)\n  --pairs_json FILENAME\n                        filename for list of document summary pairs (default:\n                        None)\n  --doc_summaries_json FILENAME\n                        filename for list of documents, each with a list of\n                        summaries (default: None)\n  --doc_key KEY         json key for the input document (default: doc)\n  --summary_key KEY     json key for the input summary (single_json or\n                        pairs_json input) (default: summary)\n  --summaries_key KEY   json key for the input summaries (doc_summaries_json\n                        input) (default: summaries)\n\narguments for BLANC-help and BLANC-tune:\n  --model_name NAME     BERT model type (default: bert-base-uncased)\n  --measure {improve,relative}\n                        measure improve or relative, as defined in the paper\n                        (default: relative)\n  --gap GAP             distance between words to mask during inference\n                        (default: 6)\n  --min_token_length_normal LEN\n                        minimum number of chars in normal tokens to mask,\n                        where a normal token is a whole word (default: 4)\n  --min_token_length_lead LEN\n                        minimum number of chars in lead token to mask, where a\n                        lead token begins a word (default: 2)\n  --min_token_length_followup LEN\n                        minimum number of chars in followup token to mask,\n                        where a followup token continues a word (default: 100)\n  --device DEVICE       cpu or cuda device (default: cpu)\n  --random_seed SEED    random seed for python and torch (default: 1)\n  --inference_batch_size SIZE\n                        batch size to use during inference (default: 1)\n  --inference_mask_evenly MASK_EVENLY\n                        when True, mask every `gap` tokens that are longer\n                        than `min_token_length`during finetuning, when False\n                        randomly mask tokens with probability 0.15 (default:\n                        True)\n\nBLANC-help arguments:\n  --filler_token TOKEN  token to use as filler in lieu of summary (default: .)\n  --help_sep SEP        token to use to separate the summary or filler from\n                        the sentence, or '' for no separator (default: )\n\nBLANC-tune arguments:\n  --finetune_batch_size SIZE\n                        batch size to use when finetuning on summary (default:\n                        1)\n  --finetune_epochs EPOCHS\n                        number of epochs to train for when finetuning on\n                        summary (default: 10)\n  --finetune_mask_evenly MASK_EVENLY\n                        when True, mask every `gap` tokens that are longer\n                        than `min_token_length`during finetuning, when False\n                        randomly mask tokens with probability 0.15 (default:\n                        False)\n  --finetune_chunk_size SIZE\n                        number of summary tokens to use at a time when\n                        finetuning (default: 64)\n  --finetune_chunk_stride STRIDE\n                        number of tokens between summary chunks for finetuning\n                        (default: 32)\n  --learning_rate LR    learning rate when finetuning on summary (default:\n                        5e-05)\n  --warmup_steps STEPS  warmup steps when finetuning on summary (default: 0)\n  ```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/PrimerAI/blanc", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "blanc", "package_url": "https://pypi.org/project/blanc/", "platform": "", "project_url": "https://pypi.org/project/blanc/", "project_urls": {"Homepage": "https://github.com/PrimerAI/blanc"}, "release_url": "https://pypi.org/project/blanc/0.1.0/", "requires_dist": ["nltk (<4.0,>=3.1)", "numpy (<2.0,>=1.0)", "torch (<2.0,>=1.0)", "tqdm (<5.0,>=4.27.0)", "transformers (==2.4.0)"], "requires_python": ">=3.6", "summary": "Human-free quality estimation of document summaries", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>BLANC</h1>\n<p>This is the reference implementation of BLANC-help and BLANC-tune as defined in <a href=\"https://arxiv.org/abs/2002.09836\" rel=\"nofollow\">Fill in the BLANC: Human-free quality estimation of document summaries</a>.</p>\n<p>BLANC is a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document's text. We present evidence that BLANC scores have at least as good correlation with human evaluations as do the ROUGE family of summary quality measurements. And unlike ROUGE, the BLANC method does not require human-written reference summaries, allowing for fully human-free summary quality estimation.</p>\n<h2>Setup</h2>\n<ol>\n<li>Install Python 3.6 or higher</li>\n<li>Install with <code>pip install blanc</code></li>\n</ol>\n<p>Note that we pinned the <code>transformers</code> requirement to 2.4.0 due to Rust installation issues that are introduced in future versions. We used <code>transformers</code> 2.5.1 in our experiments, so you may want to try upgrading <code>transformers</code> if you can.</p>\n<h2>Python Usage</h2>\n<p>Basic usage:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">blanc</span> <span class=\"kn\">import</span> <span class=\"n\">BlancHelp</span><span class=\"p\">,</span> <span class=\"n\">BlancTune</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">document</span> <span class=\"o\">=</span> <span class=\"s2\">\"Jack walked all the way over to go to the big market and buy a few loaves of bread for himself.\"</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">summary</span> <span class=\"o\">=</span> <span class=\"s2\">\"Jack bought bread from the market\"</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">blanc_help</span> <span class=\"o\">=</span> <span class=\"n\">BlancHelp</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">blanc_tune</span> <span class=\"o\">=</span> <span class=\"n\">BlancTune</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">blanc_help</span><span class=\"o\">.</span><span class=\"n\">eval_once</span><span class=\"p\">(</span><span class=\"n\">document</span><span class=\"p\">,</span> <span class=\"n\">summary</span><span class=\"p\">)</span>\n<span class=\"mf\">0.14285714285714285</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">blanc_tune</span><span class=\"o\">.</span><span class=\"n\">eval_once</span><span class=\"p\">(</span><span class=\"n\">document</span><span class=\"p\">,</span> <span class=\"n\">summary</span><span class=\"p\">)</span>\n<span class=\"mf\">0.14285714285714285</span>\n</pre>\n<p>By default, BLANC is run on the CPU. Using CUDA with batching is much faster:</p>\n<pre><span class=\"n\">blanc_help</span> <span class=\"o\">=</span> <span class=\"n\">BlancHelp</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">'cuda'</span><span class=\"p\">,</span> <span class=\"n\">inference_batch_size</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">)</span>\n<span class=\"n\">blanc_tune</span> <span class=\"o\">=</span> <span class=\"n\">BlancTune</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">'cuda'</span><span class=\"p\">,</span> <span class=\"n\">inference_batch_size</span><span class=\"o\">=</span><span class=\"mi\">24</span><span class=\"p\">,</span> <span class=\"n\">finetune_batch_size</span><span class=\"o\">=</span><span class=\"mi\">24</span><span class=\"p\">)</span>\n</pre>\n<p>With these batch sizes, BLANC-help takes around 1.4 sec per summary and BLANC-tune takes around 1.8 sec per summary on an NVIDIA V100. In addition to the parameters controlling device and batch sizes, BlancHelp and BlancTune take several other parameters controlling how the BLANC scores are calculated, and the default values for those parameters reproduce the results of the paper.</p>\n<p>If you want to compute the BLANC scores of many documents and summaries at once, you can use <code>eval_pairs()</code> or <code>eval_summaries_for_docs()</code>. <code>eval_pairs()</code> is useful when you have many documents, each with a single summary:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"Jack walked all the way over to go to the big market and buy a few loaves of bread for himself.\"</span><span class=\"p\">,</span> <span class=\"s2\">\"As Jill started taking a walk in the park, she certainly noticed that the trees were extra green this year.\"</span><span class=\"p\">]</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">summaries</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"Jack bought bread from the market.\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Jill saw green trees in the park.\"</span><span class=\"p\">]</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">blanc_help</span><span class=\"o\">.</span><span class=\"n\">eval_pairs</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">,</span> <span class=\"n\">summaries</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"mf\">0.14285714285714285</span><span class=\"p\">,</span> <span class=\"mf\">0.07142857142857142</span><span class=\"p\">]</span>\n</pre>\n<p><code>eval_summaries_for_docs()</code> is useful when you have many documents, each with many summaries:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">blanc_tune</span><span class=\"o\">.</span><span class=\"n\">eval_summaries_for_docs</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">,</span> <span class=\"n\">doc_summaries</span><span class=\"p\">)</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.14285714285714285</span><span class=\"p\">,</span> <span class=\"mf\">0.14285714285714285</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.14285714285714285</span><span class=\"p\">,</span> <span class=\"mf\">0.07142857142857142</span><span class=\"p\">]]</span>\n</pre>\n<h2>CLI Usage</h2>\n<p>A CLI for computing BLANC scores is provided for convenience.</p>\n<pre><code>$ blanc help --doc \"Jack walked all the way over to go to the big market and buy a few loaves of bread for himself.\" --summary \"Jack bought bread from the market.\"\n0.14285714285714285\n</code></pre>\n<p>Input data can also be provided in JSON format, with sample JSON input provided in <code>data/</code></p>\n<pre><code>$ blanc help --single_json data/single.json\n0.14285714285714285\n$ blanc tune --pairs_json data/pairs.json\n[0.2857142857142857, 0.21428571428571427]\n$ blanc tune --doc_summaries_json data/doc-summaries.json\n[[0.2857142857142857, 0.14285714285714285], [0.14285714285714285, 0.14285714285714285]]\n</code></pre>\n<p>The <code>single_json</code> input format expects a single JSON blob with keys <code>document</code> and <code>summary</code>. The <code>pairs_json</code> input format expects a list of JSON blobs, each with a <code>document</code> and a <code>summary</code>. The <code>doc_summaries_json</code> input format expects a list of JSON blobs, each with keys <code>document</code> and <code>summaries</code>, where <code>summaries</code> is a list of strings. These keys are customizable with the <code>doc_key</code>, <code>summary_key</code>, and <code>summaries_key</code> arguments. By default, the output is printed to STDOUT, but it can be written to a JSON file provided with the <code>output_json</code> argument.</p>\n<p>Full documentation is available with <code>blanc --help</code>:</p>\n<pre><code>required arguments:\n  {help,tune}           BLANC-help or BLANC-tune\n\ninput arguments:\n  --doc DOC             single input document (default: None)\n  --summary SUMMARY     single input summary (default: None)\n  --single_json FILENAME\n                        filename for single document summary pair (default:\n                        None)\n  --pairs_json FILENAME\n                        filename for list of document summary pairs (default:\n                        None)\n  --doc_summaries_json FILENAME\n                        filename for list of documents, each with a list of\n                        summaries (default: None)\n  --doc_key KEY         json key for the input document (default: doc)\n  --summary_key KEY     json key for the input summary (single_json or\n                        pairs_json input) (default: summary)\n  --summaries_key KEY   json key for the input summaries (doc_summaries_json\n                        input) (default: summaries)\n\narguments for BLANC-help and BLANC-tune:\n  --model_name NAME     BERT model type (default: bert-base-uncased)\n  --measure {improve,relative}\n                        measure improve or relative, as defined in the paper\n                        (default: relative)\n  --gap GAP             distance between words to mask during inference\n                        (default: 6)\n  --min_token_length_normal LEN\n                        minimum number of chars in normal tokens to mask,\n                        where a normal token is a whole word (default: 4)\n  --min_token_length_lead LEN\n                        minimum number of chars in lead token to mask, where a\n                        lead token begins a word (default: 2)\n  --min_token_length_followup LEN\n                        minimum number of chars in followup token to mask,\n                        where a followup token continues a word (default: 100)\n  --device DEVICE       cpu or cuda device (default: cpu)\n  --random_seed SEED    random seed for python and torch (default: 1)\n  --inference_batch_size SIZE\n                        batch size to use during inference (default: 1)\n  --inference_mask_evenly MASK_EVENLY\n                        when True, mask every `gap` tokens that are longer\n                        than `min_token_length`during finetuning, when False\n                        randomly mask tokens with probability 0.15 (default:\n                        True)\n\nBLANC-help arguments:\n  --filler_token TOKEN  token to use as filler in lieu of summary (default: .)\n  --help_sep SEP        token to use to separate the summary or filler from\n                        the sentence, or '' for no separator (default: )\n\nBLANC-tune arguments:\n  --finetune_batch_size SIZE\n                        batch size to use when finetuning on summary (default:\n                        1)\n  --finetune_epochs EPOCHS\n                        number of epochs to train for when finetuning on\n                        summary (default: 10)\n  --finetune_mask_evenly MASK_EVENLY\n                        when True, mask every `gap` tokens that are longer\n                        than `min_token_length`during finetuning, when False\n                        randomly mask tokens with probability 0.15 (default:\n                        False)\n  --finetune_chunk_size SIZE\n                        number of summary tokens to use at a time when\n                        finetuning (default: 64)\n  --finetune_chunk_stride STRIDE\n                        number of tokens between summary chunks for finetuning\n                        (default: 32)\n  --learning_rate LR    learning rate when finetuning on summary (default:\n                        5e-05)\n  --warmup_steps STEPS  warmup steps when finetuning on summary (default: 0)\n</code></pre>\n\n          </div>"}, "last_serial": 6877314, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "db2d0e0dea5c515f6dcea588ed83fd84", "sha256": "2dfc9a2122c13a77f7574c9e8a0866288db6acbe4b8c161d20f44ee2159942d6"}, "downloads": -1, "filename": "blanc-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "db2d0e0dea5c515f6dcea588ed83fd84", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 16554, "upload_time": "2020-03-24T23:41:14", "upload_time_iso_8601": "2020-03-24T23:41:14.603761Z", "url": "https://files.pythonhosted.org/packages/0e/28/1eea9b23842da2cefcb64d6461e388fd9a9e2b66a54a3d49b761ba5eb72f/blanc-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9f6fccdedc5e3fedd00538755312aed4", "sha256": "09a7d2dbcab0b23416df90748b2f6fb82dc468949de39d2b01e04e4bdd8bf3f4"}, "downloads": -1, "filename": "blanc-0.1.0.tar.gz", "has_sig": false, "md5_digest": "9f6fccdedc5e3fedd00538755312aed4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14241, "upload_time": "2020-03-24T23:41:16", "upload_time_iso_8601": "2020-03-24T23:41:16.713995Z", "url": "https://files.pythonhosted.org/packages/fa/22/a46203c7f37d8dab73c34670c115cbc0a7c0482e9ff2c0fe3f86d1bc5bae/blanc-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "db2d0e0dea5c515f6dcea588ed83fd84", "sha256": "2dfc9a2122c13a77f7574c9e8a0866288db6acbe4b8c161d20f44ee2159942d6"}, "downloads": -1, "filename": "blanc-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "db2d0e0dea5c515f6dcea588ed83fd84", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 16554, "upload_time": "2020-03-24T23:41:14", "upload_time_iso_8601": "2020-03-24T23:41:14.603761Z", "url": "https://files.pythonhosted.org/packages/0e/28/1eea9b23842da2cefcb64d6461e388fd9a9e2b66a54a3d49b761ba5eb72f/blanc-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9f6fccdedc5e3fedd00538755312aed4", "sha256": "09a7d2dbcab0b23416df90748b2f6fb82dc468949de39d2b01e04e4bdd8bf3f4"}, "downloads": -1, "filename": "blanc-0.1.0.tar.gz", "has_sig": false, "md5_digest": "9f6fccdedc5e3fedd00538755312aed4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14241, "upload_time": "2020-03-24T23:41:16", "upload_time_iso_8601": "2020-03-24T23:41:16.713995Z", "url": "https://files.pythonhosted.org/packages/fa/22/a46203c7f37d8dab73c34670c115cbc0a7c0482e9ff2c0fe3f86d1bc5bae/blanc-0.1.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:09 2020"}