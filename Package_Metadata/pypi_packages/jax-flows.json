{"info": {"author": "Chris Waites", "author_email": "cwaites10@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "<img align=\"right\" width=\"300\" src=\"assets/flows.gif\">\n\n# Normalizing Flows in JAX\n\nImplementations of normalizing flows (RealNVP, GLOW, MAF) in the <a href=\"https://github.com/google/jax/\">JAX</a> deep learning framework.</p>\n\n<a href=\"https://circleci.com/gh/ChrisWaites/jax-flows\">\n    <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/ChrisWaites/jax-flows/master\">\n</a>\n<a href=\"https://github.com/ChrisWaites/jax-flows/blob/master/LICENSE\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/ChrisWaites/jax-flows.svg?color=blue\">\n</a>\n<a href=\"https://ChrisWaites.co/jax-flows/index.html\">\n    <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/ChrisWaites.co/jax-flows/index.html.svg?down_color=red&down_message=offline&up_message=online\">\n</a>\n<a href=\"https://github.com/ChrisWaites/jax-flows/releases\">\n    <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/ChrisWaites/jax-flows.svg\">\n</a>\n\n## What are normalizing flows?\n\nNormalizing flow models are _generative models_. That is, they infer the probability distribution of a given dataset. With that distribution we can do a number of interesting things, namely query the likelihood of given points as well as sample new realistic points.\n\n<!---\nHow is are these things achieved? Well, we learn a function <img src=\"https://render.githubusercontent.com/render/math?math=f_{\\theta}\"> characterized by a parameter vector <img src=\"https://render.githubusercontent.com/render/math?math=\\theta\"> with an inverse <img src=\"https://render.githubusercontent.com/render/math?math=f^{-1}_{\\theta}\">. If X is our approximated distribution and Z is some known distribution we choose (say, the multivariate normal distribution), we're simply going to define X as f_\\theta(Z).\n-->\n\n## How are things structured?\n\n### Transformations\n\nA `transformation` is a parameterized invertible function.\n\n```python\ninit_fun = flows.MADE()\n\nparams, direct_fun, inverse_fun = init_fun(rng, input_shape)\n\n# Transform some inputs\ntransformed_inputs, log_det_direct = direct_fun(params, inputs)\n\n# Reconstruct original inputs\nreconstructed_inputs, log_det_inverse = inverse_fun(params, inputs)\n\nassert np.array_equal(inputs, reconstructed_inputs)\n```\n\nWe can construct a larger meta-transformation by composing a sequence of sub-transformations using `flows.serial`. The resulting transformation adheres to the exact same interface and is indistinguishable from any other regular transformation.\n\n```python\ninit_fun = flows.serial(\n  flows.MADE(),\n  flows.BatchNorm(),\n  flows.Reverse()\n)\n\nparams, direct_fun, inverse_fun = init_fun(rng, input_shape)\n```\n\n### Distributions\n\nA `distribution` has a similarly simple interface. It is characterized by a set of parameters, a function for querying the log of the pdf at a given point, and a sampling function.\n\n```python\ninit_fun = Normal()\n\nparams, log_pdf, sample = init_fun(rng, input_shape)\n\nlog_pdfs = log_pdf(params, inputs)\n\nsamples = sample(rng, params, num_samples)\n```\n\n### Normalizing Flow Models\n\nUnder this definition, a normalizing flow model is just a `distribution`. But to retrieve one, we have to give it a transformation and another prior distribution.\n\n```python\ntransformation = flows.serial(\n  flows.MADE(),\n  flows.BatchNorm(),\n  flows.Reverse(),\n  flows.MADE(),\n  flows.BatchNorm(),\n  flows.Reverse(),\n)\n\nprior = Normal()\n\ninit_fun = flows.Flow(transformation, prior)\n\nparams, log_pdf, sample = init_fun(rng, input_shape)\n```\n\n### How do I train a model?\n\nTo train our model, we would typically define an appropriate loss function and parameter update step.\n\n```python\ndef loss(params, inputs):\n  return -log_pdf(params, inputs).mean()\n\n@jit\ndef step(i, opt_state, inputs):\n  params = get_params(opt_state)\n  return opt_update(i, grad(loss)(params, inputs), opt_state)\n```\n\nGiven these, we can go forward and execute a standard JAX training loop.\n\n```python\nbatch_size = 32\n\nitercount = itertools.count()\nfor epoch in range(num_epochs):\n  npr.shuffle(X)\n  for batch_index in range(0, len(X), batch_size):\n    opt_state = step(next(itercount), opt_state, X[batch_index:batch_index+batch_size])\n\noptimized_params = get_params(opt_state)\n```\n\nNow that we have our trained model parameters, we can query and sample as regular.\n\n```python\nlog_pdfs = log_pdf(optimized_params, inputs)\n\nsamples = sample(rng, optimized_params, num_samples)\n```\n\n_Magic!_\n\n## Interested in contributing?\n\nYay! Check out our contributing guidelines in `.github/CONTRIBUTING.md`.\n\n## Inspiration\n\nThis repository is largely modeled after the [`pytorch-flows`](https://github.com/ikostrikov/pytorch-flows) repository by [Ilya Kostrikov\n](https://github.com/ikostrikov).\n\nThe implementations are modeled after the work of the following papers.\n\n  > [Density estimation using Real NVP](https://arxiv.org/abs/1605.08803)\\\n  > Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio\\\n  > _arXiv:1605.08803_\n\n  > [Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/abs/1807.03039)\\\n  > Diederik P. Kingma, Prafulla Dhariwal\\\n  > _arXiv:1807.03039_\n\n  > [Flow++: Improving Flow-Based Generative Models\n  with Variational Dequantization and Architecture Design](https://openreview.net/forum?id=Hyg74h05tX)\\\n  > Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, Pieter Abbeel\\\n  > _OpenReview:Hyg74h05tX_\n\n  > [Masked Autoregressive Flow for Density Estimation](https://arxiv.org/abs/1705.07057)\\\n  > George Papamakarios, Theo Pavlakou, Iain Murray\\\n  > _arXiv:1705.07057_\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/ChrisWaites/jax-flows", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "jax-flows", "package_url": "https://pypi.org/project/jax-flows/", "platform": "", "project_url": "https://pypi.org/project/jax-flows/", "project_urls": {"Homepage": "http://github.com/ChrisWaites/jax-flows"}, "release_url": "https://pypi.org/project/jax-flows/0.0.0/", "requires_dist": ["black (==19.10b0)", "flake8 (==3.7.9)", "isort (==4.3.21)", "jax (==0.1.59)", "jaxlib (==0.1.39)", "jupyter (==1.0.0)", "jupyter-client (==5.3.4)", "jupyter-console (==6.1.0)", "jupyter-core (==4.6.1)", "matplotlib (==3.1.3)", "numpy (==1.18.1)", "pytest (==5.4.1)", "scikit-learn (==0.22.1)", "scipy (==1.4.1)", "seaborn (==0.10.0)", "tqdm (==4.43.0)", "twine ; extra == 'dev'", "pytest ; extra == 'dev'", "pytest-xdist ; extra == 'dev'", "black ; extra == 'dev'", "isort ; extra == 'dev'", "flake8 ; extra == 'dev'", "recommonmark ; extra == 'docs'", "sphinx ; extra == 'docs'", "sphinx-markdown-tables ; extra == 'docs'", "sphinx-rtd-theme ; extra == 'docs'", "black ; extra == 'quality'", "isort ; extra == 'quality'", "flake8 ; extra == 'quality'", "pytest ; extra == 'testing'", "pytest-xdist ; extra == 'testing'"], "requires_python": ">=3.6.0", "summary": "Normalizing Flows for JAX", "version": "0.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <img align=\"right\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/eee4d9bf0341215bca04b5eda554c8dbb5f09cfb/6173736574732f666c6f77732e676966\" width=\"300\">\n<h1>Normalizing Flows in JAX</h1>\n<p>Implementations of normalizing flows (RealNVP, GLOW, MAF) in the <a href=\"https://github.com/google/jax/\" rel=\"nofollow\">JAX</a> deep learning framework.</p><p></p>\n<a href=\"https://circleci.com/gh/ChrisWaites/jax-flows\" rel=\"nofollow\">\n    <img alt=\"Build\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c99e37c63abe5ee503554fddfab4a80170594133/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f6275696c642f6769746875622f43687269735761697465732f6a61782d666c6f77732f6d6173746572\">\n</a>\n<a href=\"https://github.com/ChrisWaites/jax-flows/blob/master/LICENSE\" rel=\"nofollow\">\n    <img alt=\"GitHub\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/eee08ecf7d761cd534316a66c714609ec77a3697/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f43687269735761697465732f6a61782d666c6f77732e7376673f636f6c6f723d626c7565\">\n</a>\n<a href=\"https://ChrisWaites.co/jax-flows/index.html\" rel=\"nofollow\">\n    <img alt=\"Documentation\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/64156e9ca83ce8206447d81e6a115ae9d56a064f/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652f687474702f43687269735761697465732e636f2f6a61782d666c6f77732f696e6465782e68746d6c2e7376673f646f776e5f636f6c6f723d72656426646f776e5f6d6573736167653d6f66666c696e652675705f6d6573736167653d6f6e6c696e65\">\n</a>\n<a href=\"https://github.com/ChrisWaites/jax-flows/releases\" rel=\"nofollow\">\n    <img alt=\"GitHub release\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/50fd27c0cfde625183db1356a61d2d3147a815fa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f43687269735761697465732f6a61782d666c6f77732e737667\">\n</a>\n<h2>What are normalizing flows?</h2>\n<p>Normalizing flow models are <em>generative models</em>. That is, they infer the probability distribution of a given dataset. With that distribution we can do a number of interesting things, namely query the likelihood of given points as well as sample new realistic points.</p>\n\n<h2>How are things structured?</h2>\n<h3>Transformations</h3>\n<p>A <code>transformation</code> is a parameterized invertible function.</p>\n<pre><span class=\"n\">init_fun</span> <span class=\"o\">=</span> <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">MADE</span><span class=\"p\">()</span>\n\n<span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">direct_fun</span><span class=\"p\">,</span> <span class=\"n\">inverse_fun</span> <span class=\"o\">=</span> <span class=\"n\">init_fun</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Transform some inputs</span>\n<span class=\"n\">transformed_inputs</span><span class=\"p\">,</span> <span class=\"n\">log_det_direct</span> <span class=\"o\">=</span> <span class=\"n\">direct_fun</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Reconstruct original inputs</span>\n<span class=\"n\">reconstructed_inputs</span><span class=\"p\">,</span> <span class=\"n\">log_det_inverse</span> <span class=\"o\">=</span> <span class=\"n\">inverse_fun</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n<span class=\"k\">assert</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array_equal</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">reconstructed_inputs</span><span class=\"p\">)</span>\n</pre>\n<p>We can construct a larger meta-transformation by composing a sequence of sub-transformations using <code>flows.serial</code>. The resulting transformation adheres to the exact same interface and is indistinguishable from any other regular transformation.</p>\n<pre><span class=\"n\">init_fun</span> <span class=\"o\">=</span> <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span>\n  <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">MADE</span><span class=\"p\">(),</span>\n  <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">BatchNorm</span><span class=\"p\">(),</span>\n  <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">Reverse</span><span class=\"p\">()</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">direct_fun</span><span class=\"p\">,</span> <span class=\"n\">inverse_fun</span> <span class=\"o\">=</span> <span class=\"n\">init_fun</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"p\">)</span>\n</pre>\n<h3>Distributions</h3>\n<p>A <code>distribution</code> has a similarly simple interface. It is characterized by a set of parameters, a function for querying the log of the pdf at a given point, and a sampling function.</p>\n<pre><span class=\"n\">init_fun</span> <span class=\"o\">=</span> <span class=\"n\">Normal</span><span class=\"p\">()</span>\n\n<span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">log_pdf</span><span class=\"p\">,</span> <span class=\"n\">sample</span> <span class=\"o\">=</span> <span class=\"n\">init_fun</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"p\">)</span>\n\n<span class=\"n\">log_pdfs</span> <span class=\"o\">=</span> <span class=\"n\">log_pdf</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n<span class=\"n\">samples</span> <span class=\"o\">=</span> <span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">num_samples</span><span class=\"p\">)</span>\n</pre>\n<h3>Normalizing Flow Models</h3>\n<p>Under this definition, a normalizing flow model is just a <code>distribution</code>. But to retrieve one, we have to give it a transformation and another prior distribution.</p>\n<pre><span class=\"n\">transformation</span> <span class=\"o\">=</span> <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span>\n  <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">MADE</span><span class=\"p\">(),</span>\n  <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">BatchNorm</span><span class=\"p\">(),</span>\n  <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">Reverse</span><span class=\"p\">(),</span>\n  <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">MADE</span><span class=\"p\">(),</span>\n  <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">BatchNorm</span><span class=\"p\">(),</span>\n  <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">Reverse</span><span class=\"p\">(),</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">prior</span> <span class=\"o\">=</span> <span class=\"n\">Normal</span><span class=\"p\">()</span>\n\n<span class=\"n\">init_fun</span> <span class=\"o\">=</span> <span class=\"n\">flows</span><span class=\"o\">.</span><span class=\"n\">Flow</span><span class=\"p\">(</span><span class=\"n\">transformation</span><span class=\"p\">,</span> <span class=\"n\">prior</span><span class=\"p\">)</span>\n\n<span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">log_pdf</span><span class=\"p\">,</span> <span class=\"n\">sample</span> <span class=\"o\">=</span> <span class=\"n\">init_fun</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"p\">)</span>\n</pre>\n<h3>How do I train a model?</h3>\n<p>To train our model, we would typically define an appropriate loss function and parameter update step.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">loss</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">):</span>\n  <span class=\"k\">return</span> <span class=\"o\">-</span><span class=\"n\">log_pdf</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n\n<span class=\"nd\">@jit</span>\n<span class=\"k\">def</span> <span class=\"nf\">step</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">opt_state</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">):</span>\n  <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">get_params</span><span class=\"p\">(</span><span class=\"n\">opt_state</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">opt_update</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">)(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">),</span> <span class=\"n\">opt_state</span><span class=\"p\">)</span>\n</pre>\n<p>Given these, we can go forward and execute a standard JAX training loop.</p>\n<pre><span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n\n<span class=\"n\">itercount</span> <span class=\"o\">=</span> <span class=\"n\">itertools</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_epochs</span><span class=\"p\">):</span>\n  <span class=\"n\">npr</span><span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n  <span class=\"k\">for</span> <span class=\"n\">batch_index</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">),</span> <span class=\"n\">batch_size</span><span class=\"p\">):</span>\n    <span class=\"n\">opt_state</span> <span class=\"o\">=</span> <span class=\"n\">step</span><span class=\"p\">(</span><span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">itercount</span><span class=\"p\">),</span> <span class=\"n\">opt_state</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">batch_index</span><span class=\"p\">:</span><span class=\"n\">batch_index</span><span class=\"o\">+</span><span class=\"n\">batch_size</span><span class=\"p\">])</span>\n\n<span class=\"n\">optimized_params</span> <span class=\"o\">=</span> <span class=\"n\">get_params</span><span class=\"p\">(</span><span class=\"n\">opt_state</span><span class=\"p\">)</span>\n</pre>\n<p>Now that we have our trained model parameters, we can query and sample as regular.</p>\n<pre><span class=\"n\">log_pdfs</span> <span class=\"o\">=</span> <span class=\"n\">log_pdf</span><span class=\"p\">(</span><span class=\"n\">optimized_params</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">)</span>\n\n<span class=\"n\">samples</span> <span class=\"o\">=</span> <span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">optimized_params</span><span class=\"p\">,</span> <span class=\"n\">num_samples</span><span class=\"p\">)</span>\n</pre>\n<p><em>Magic!</em></p>\n<h2>Interested in contributing?</h2>\n<p>Yay! Check out our contributing guidelines in <code>.github/CONTRIBUTING.md</code>.</p>\n<h2>Inspiration</h2>\n<p>This repository is largely modeled after the <a href=\"https://github.com/ikostrikov/pytorch-flows\" rel=\"nofollow\"><code>pytorch-flows</code></a> repository by <a href=\"https://github.com/ikostrikov\" rel=\"nofollow\">Ilya Kostrikov\n</a>.</p>\n<p>The implementations are modeled after the work of the following papers.</p>\n<blockquote>\n<p><a href=\"https://arxiv.org/abs/1605.08803\" rel=\"nofollow\">Density estimation using Real NVP</a><br>\nLaurent Dinh, Jascha Sohl-Dickstein, Samy Bengio<br>\n<em>arXiv:1605.08803</em></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://arxiv.org/abs/1807.03039\" rel=\"nofollow\">Glow: Generative Flow with Invertible 1x1 Convolutions</a><br>\nDiederik P. Kingma, Prafulla Dhariwal<br>\n<em>arXiv:1807.03039</em></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://openreview.net/forum?id=Hyg74h05tX\" rel=\"nofollow\">Flow++: Improving Flow-Based Generative Models\nwith Variational Dequantization and Architecture Design</a><br>\nJonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, Pieter Abbeel<br>\n<em>OpenReview:Hyg74h05tX</em></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://arxiv.org/abs/1705.07057\" rel=\"nofollow\">Masked Autoregressive Flow for Density Estimation</a><br>\nGeorge Papamakarios, Theo Pavlakou, Iain Murray<br>\n<em>arXiv:1705.07057</em></p>\n</blockquote>\n\n          </div>"}, "last_serial": 6886423, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "56fb4bbbf2bd2824e53b2ef6f06933e7", "sha256": "e1c272a488bc794078bbdd07b5b0ec7e1b8b3af81e3b7169d54a7cbcefb15025"}, "downloads": -1, "filename": "jax_flows-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "56fb4bbbf2bd2824e53b2ef6f06933e7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 8369, "upload_time": "2020-03-26T03:00:03", "upload_time_iso_8601": "2020-03-26T03:00:03.579362Z", "url": "https://files.pythonhosted.org/packages/58/53/4b8479223cdae3d998afb014f0b61b6dc695a4a0ad90c501b3edc873aafd/jax_flows-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8c52ca77a9efea7d46a80f334d7af784", "sha256": "4702adeebd56592f988c797e11ae4e0a46e51c22f9a7872f138808b1bfd9320d"}, "downloads": -1, "filename": "jax-flows-0.0.0.tar.gz", "has_sig": false, "md5_digest": "8c52ca77a9efea7d46a80f334d7af784", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 7278, "upload_time": "2020-03-26T03:00:05", "upload_time_iso_8601": "2020-03-26T03:00:05.850429Z", "url": "https://files.pythonhosted.org/packages/2a/e5/b7fbd922eb3de60f25c368c6abbf6efe479f842152411950e6e22401a908/jax-flows-0.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "56fb4bbbf2bd2824e53b2ef6f06933e7", "sha256": "e1c272a488bc794078bbdd07b5b0ec7e1b8b3af81e3b7169d54a7cbcefb15025"}, "downloads": -1, "filename": "jax_flows-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "56fb4bbbf2bd2824e53b2ef6f06933e7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 8369, "upload_time": "2020-03-26T03:00:03", "upload_time_iso_8601": "2020-03-26T03:00:03.579362Z", "url": "https://files.pythonhosted.org/packages/58/53/4b8479223cdae3d998afb014f0b61b6dc695a4a0ad90c501b3edc873aafd/jax_flows-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8c52ca77a9efea7d46a80f334d7af784", "sha256": "4702adeebd56592f988c797e11ae4e0a46e51c22f9a7872f138808b1bfd9320d"}, "downloads": -1, "filename": "jax-flows-0.0.0.tar.gz", "has_sig": false, "md5_digest": "8c52ca77a9efea7d46a80f334d7af784", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 7278, "upload_time": "2020-03-26T03:00:05", "upload_time_iso_8601": "2020-03-26T03:00:05.850429Z", "url": "https://files.pythonhosted.org/packages/2a/e5/b7fbd922eb3de60f25c368c6abbf6efe479f842152411950e6e22401a908/jax-flows-0.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:53:02 2020"}