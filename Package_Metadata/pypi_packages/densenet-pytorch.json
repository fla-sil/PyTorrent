{"info": {"author": "Liu Changyu", "author_email": "liuchangyu1111@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "\n# DenseNet-PyTorch\n\n`Note: Now supports the more efficient DenseNet-BC (DenseNet-Bottleneck-Compressed) networks. Using the DenseNet-BC-190-40 model, it obtaines state of the art performance on CIFAR-10 and CIFAR-100.`\n\n### Update (Feb 18, 2020)\n\nThe update is for ease of use and deployment.\n\n * [Example: Export to ONNX](#example-export-to-onnx)\n * [Example: Extract features](#example-feature-extraction)\n * [Example: Visual](#example-visual)\n\nIt is also now incredibly simple to load a pretrained model with a new number of classes for transfer learning:\n\n```python\nfrom densenet_pytorch import DenseNet \nmodel = DenseNet.from_pretrained('densenet121', num_classes=10)\n```\n\n### Update (January 15, 2020)\n\nThis update allows you to use NVIDIA's Apex tool for accelerated training. By default choice `hybrid training precision` + `dynamic loss amplified` version, if you need to learn more and details about `apex` tools, please visit https://github.com/NVIDIA/apex.\n\n### Update (January 6, 2020)\n\nThis update adds a modular neural network, making it more flexible in use. It can be deployed to many common dataset classification tasks. Of course, it can also be used in your products.\n\n### Overview\nThis repository contains an op-for-op PyTorch reimplementation of [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf).\n\nThe goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. This implementation is a work in progress -- new features are currently being implemented.  \n\nAt the moment, you can easily:  \n * Load pretrained DenseNet models \n * Use DenseNet models for classification or feature extraction \n\n_Upcoming features_: In the next few days, you will be able to:\n * Quickly finetune an DenseNet on your own dataset\n * Export DenseNet models for production\n\n### Table of contents\n1. [About DenseNet](#about-densenet)\n2. [Installation](#installation)\n3. [Usage](#usage)\n    * [Load pretrained models](#loading-pretrained-models)\n    * [Example: Classify](#example-classification)\n    * [Example: Extract features](#example-feature-extraction)\n    * [Example: Export to ONNX](#example-export-to-onnx)\n    * [Example: Visual](#example-visual)\n4. [Contributing](#contributing) \n\n### About DenseNet\n\nIf you're new to DenseNets, here is an explanation straight from the official PyTorch implementation: \n\nDense Convolutional Network (DenseNet), connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.\n\n### Installation\n\nInstall from pypi:\n```bash\npip install densenet_pytorch\n```\n\nInstall from source:\n```bash\ngit clone https://github.com/Lornatang/DenseNet-PyTorch\ncd DenseNet-PyTorch\npip install -e .\n``` \n\n### Usage\n\n#### Loading pretrained models\n\nLoad an densenet121 network:\n```python\nfrom densenet_pytorch import DenseNet\nmodel = DenseNet.from_name(\"densenet121\")\n```\n\nLoad a pretrained densenet11: \n```python\nfrom densenet_pytorch import DenseNet\nmodel = DenseNet.from_pretrained(\"densenet121\")\n```\n\nTheir 1-crop error rates on imagenet dataset with pretrained models are listed below.\n\n| Model structure | Top-1 error | Top-5 error |\n| --------------- | ----------- | ----------- |\n|  densenet121    | 25.35       | 7.83        |\n|  densenet169    | 24.00       | 7.00        |\n|  densenet201    | 22.80       | 6.43        |\n|  densenet161    | 22.35       | 6.20        |\n\n#### Example: Classification\n\nWe assume that in your current directory, there is a `img.jpg` file and a `labels_map.txt` file (ImageNet class names). These are both included in `examples/simple`. \n\nAll pre-trained models expect input images normalized in the same way,\ni.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\nThe images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\nand `std = [0.229, 0.224, 0.225]`.\n\nHere's a sample execution.\n\n```python\nimport json\n\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nfrom densenet_pytorch import DenseNet \n\n# Open image\ninput_image = Image.open(\"img.jpg\")\n\n# Preprocess image\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n\n# Load class names\nlabels_map = json.load(open(\"labels_map.txt\"))\nlabels_map = [labels_map[str(i)] for i in range(1000)]\n\n# Classify with DenseNet121\nmodel = DenseNet.from_pretrained(\"densenet121\")\nmodel.eval()\n\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to(\"cuda\")\n    model.to(\"cuda\")\n\nwith torch.no_grad():\n    logits = model(input_batch)\npreds = torch.topk(logits, k=5).indices.squeeze(0).tolist()\n\nprint(\"-----\")\nfor idx in preds:\n    label = labels_map[idx]\n    prob = torch.softmax(logits, dim=1)[0, idx].item()\n    print(f\"{label:<75} ({prob * 100:.2f}%)\")\n```\n\n#### Example: Feature Extraction \n\nYou can easily extract features with `model.extract_features`:\n```python\nimport torch\nfrom densenet_pytorch import DenseNet \nmodel = DenseNet.from_pretrained('densenet121')\n\n# ... image preprocessing as in the classification example ...\ninputs = torch.randn(1, 3, 224, 224)\nprint(inputs.shape) # torch.Size([1, 3, 224, 224])\n\nfeatures = model.extract_features(inputs)\nprint(features.shape) # torch.Size([1, 1024, 7, 7])\n```\n\n#### Example: Export to ONNX  \n\nExporting to ONNX for deploying to production is now simple: \n```python\nimport torch \nfrom densenet_pytorch import DenseNet \n\nmodel = DenseNet.from_pretrained('densenet121')\ndummy_input = torch.randn(16, 3, 224, 224)\n\ntorch.onnx.export(model, dummy_input, \"demo.onnx\", verbose=True)\n```\n\n#### Example: Visual\n\n```text\ncd $REPO$/framework\nsh start.sh\n```\n\nThen open the browser and type in the browser address [http://127.0.0.1:10003/](http://127.0.0.1:10003/).\n\nEnjoy it.\n\n#### ImageNet\n\nSee `examples/imagenet` for details about evaluating on ImageNet.\n\nFor more datasets result. Please see `research/README.md`.\n\n### Contributing\n\nIf you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues.   \n\nI look forward to seeing what the community does with these models! \n\n\n### Credit\n\n#### Densely Connected Convolutional Networks\n\n*Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger*\n\n##### Abstract\n\nRecent work has shown that convolutional networks can be substantially deeper, more accurate, \nand efficient to train if they contain shorter connections between layers close to the input\n and those close to the output. In this paper, we embrace this observation and introduce the \n Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a \n feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections\n  - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. \n  For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps \n  are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they \n  alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, \n  and substantially reduce the number of parameters. We evaluate our proposed architecture on four \n  highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). \n  DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring \n  less computation to achieve high performance. Code and pre-trained models are available at this [https URL](https://github.com/liuzhuang13/DenseNet) .\n\n[paper](http://arxiv.org/pdf/1608.06993v5) [code](https://github.com/liuzhuang13/DenseNet)\n\n```text\n@article{DenseNet,\ntitle:{Densely Connected Convolutional Networks},\nauthor:{Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger},\njournal={cvpr},\nyear={2016}\n}\n```\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Lornatang/DenseNet-PyTorch", "keywords": "", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "densenet-pytorch", "package_url": "https://pypi.org/project/densenet-pytorch/", "platform": "", "project_url": "https://pypi.org/project/densenet-pytorch/", "project_urls": {"Homepage": "https://github.com/Lornatang/DenseNet-PyTorch"}, "release_url": "https://pypi.org/project/densenet-pytorch/0.2.0/", "requires_dist": ["torch"], "requires_python": ">=3.6.0", "summary": "Restore the official code 100% and improve it to make it easier to research and facilitate production.", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>DenseNet-PyTorch</h1>\n<p><code>Note: Now supports the more efficient DenseNet-BC (DenseNet-Bottleneck-Compressed) networks. Using the DenseNet-BC-190-40 model, it obtaines state of the art performance on CIFAR-10 and CIFAR-100.</code></p>\n<h3>Update (Feb 18, 2020)</h3>\n<p>The update is for ease of use and deployment.</p>\n<ul>\n<li><a href=\"#example-export-to-onnx\" rel=\"nofollow\">Example: Export to ONNX</a></li>\n<li><a href=\"#example-feature-extraction\" rel=\"nofollow\">Example: Extract features</a></li>\n<li><a href=\"#example-visual\" rel=\"nofollow\">Example: Visual</a></li>\n</ul>\n<p>It is also now incredibly simple to load a pretrained model with a new number of classes for transfer learning:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">densenet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">DenseNet</span> \n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DenseNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'densenet121'</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<h3>Update (January 15, 2020)</h3>\n<p>This update allows you to use NVIDIA's Apex tool for accelerated training. By default choice <code>hybrid training precision</code> + <code>dynamic loss amplified</code> version, if you need to learn more and details about <code>apex</code> tools, please visit <a href=\"https://github.com/NVIDIA/apex\" rel=\"nofollow\">https://github.com/NVIDIA/apex</a>.</p>\n<h3>Update (January 6, 2020)</h3>\n<p>This update adds a modular neural network, making it more flexible in use. It can be deployed to many common dataset classification tasks. Of course, it can also be used in your products.</p>\n<h3>Overview</h3>\n<p>This repository contains an op-for-op PyTorch reimplementation of <a href=\"https://arxiv.org/pdf/1608.06993.pdf\" rel=\"nofollow\">Densely Connected Convolutional Networks</a>.</p>\n<p>The goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. This implementation is a work in progress -- new features are currently being implemented.</p>\n<p>At the moment, you can easily:</p>\n<ul>\n<li>Load pretrained DenseNet models</li>\n<li>Use DenseNet models for classification or feature extraction</li>\n</ul>\n<p><em>Upcoming features</em>: In the next few days, you will be able to:</p>\n<ul>\n<li>Quickly finetune an DenseNet on your own dataset</li>\n<li>Export DenseNet models for production</li>\n</ul>\n<h3>Table of contents</h3>\n<ol>\n<li><a href=\"#about-densenet\" rel=\"nofollow\">About DenseNet</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a>\n<ul>\n<li><a href=\"#loading-pretrained-models\" rel=\"nofollow\">Load pretrained models</a></li>\n<li><a href=\"#example-classification\" rel=\"nofollow\">Example: Classify</a></li>\n<li><a href=\"#example-feature-extraction\" rel=\"nofollow\">Example: Extract features</a></li>\n<li><a href=\"#example-export-to-onnx\" rel=\"nofollow\">Example: Export to ONNX</a></li>\n<li><a href=\"#example-visual\" rel=\"nofollow\">Example: Visual</a></li>\n</ul>\n</li>\n<li><a href=\"#contributing\" rel=\"nofollow\">Contributing</a></li>\n</ol>\n<h3>About DenseNet</h3>\n<p>If you're new to DenseNets, here is an explanation straight from the official PyTorch implementation:</p>\n<p>Dense Convolutional Network (DenseNet), connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.</p>\n<h3>Installation</h3>\n<p>Install from pypi:</p>\n<pre>pip install densenet_pytorch\n</pre>\n<p>Install from source:</p>\n<pre>git clone https://github.com/Lornatang/DenseNet-PyTorch\n<span class=\"nb\">cd</span> DenseNet-PyTorch\npip install -e .\n</pre>\n<h3>Usage</h3>\n<h4>Loading pretrained models</h4>\n<p>Load an densenet121 network:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">densenet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">DenseNet</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DenseNet</span><span class=\"o\">.</span><span class=\"n\">from_name</span><span class=\"p\">(</span><span class=\"s2\">\"densenet121\"</span><span class=\"p\">)</span>\n</pre>\n<p>Load a pretrained densenet11:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">densenet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">DenseNet</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DenseNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s2\">\"densenet121\"</span><span class=\"p\">)</span>\n</pre>\n<p>Their 1-crop error rates on imagenet dataset with pretrained models are listed below.</p>\n<table>\n<thead>\n<tr>\n<th>Model structure</th>\n<th>Top-1 error</th>\n<th>Top-5 error</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>densenet121</td>\n<td>25.35</td>\n<td>7.83</td>\n</tr>\n<tr>\n<td>densenet169</td>\n<td>24.00</td>\n<td>7.00</td>\n</tr>\n<tr>\n<td>densenet201</td>\n<td>22.80</td>\n<td>6.43</td>\n</tr>\n<tr>\n<td>densenet161</td>\n<td>22.35</td>\n<td>6.20</td>\n</tr></tbody></table>\n<h4>Example: Classification</h4>\n<p>We assume that in your current directory, there is a <code>img.jpg</code> file and a <code>labels_map.txt</code> file (ImageNet class names). These are both included in <code>examples/simple</code>.</p>\n<p>All pre-trained models expect input images normalized in the same way,\ni.e. mini-batches of 3-channel RGB images of shape <code>(3 x H x W)</code>, where <code>H</code> and <code>W</code> are expected to be at least <code>224</code>.\nThe images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code>\nand <code>std = [0.229, 0.224, 0.225]</code>.</p>\n<p>Here's a sample execution.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision.transforms</span> <span class=\"k\">as</span> <span class=\"nn\">transforms</span>\n<span class=\"kn\">from</span> <span class=\"nn\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">densenet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">DenseNet</span> \n\n<span class=\"c1\"># Open image</span>\n<span class=\"n\">input_image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s2\">\"img.jpg\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Preprocess image</span>\n<span class=\"n\">preprocess</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Resize</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">CenterCrop</span><span class=\"p\">(</span><span class=\"mi\">224</span><span class=\"p\">),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.485</span><span class=\"p\">,</span> <span class=\"mf\">0.456</span><span class=\"p\">,</span> <span class=\"mf\">0.406</span><span class=\"p\">],</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.229</span><span class=\"p\">,</span> <span class=\"mf\">0.224</span><span class=\"p\">,</span> <span class=\"mf\">0.225</span><span class=\"p\">]),</span>\n<span class=\"p\">])</span>\n<span class=\"n\">input_tensor</span> <span class=\"o\">=</span> <span class=\"n\">preprocess</span><span class=\"p\">(</span><span class=\"n\">input_image</span><span class=\"p\">)</span>\n<span class=\"n\">input_batch</span> <span class=\"o\">=</span> <span class=\"n\">input_tensor</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># create a mini-batch as expected by the model</span>\n\n<span class=\"c1\"># Load class names</span>\n<span class=\"n\">labels_map</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"labels_map.txt\"</span><span class=\"p\">))</span>\n<span class=\"n\">labels_map</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">labels_map</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)]</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">)]</span>\n\n<span class=\"c1\"># Classify with DenseNet121</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DenseNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s2\">\"densenet121\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># move the input and model to GPU for speed if available</span>\n<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">():</span>\n    <span class=\"n\">input_batch</span> <span class=\"o\">=</span> <span class=\"n\">input_batch</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_batch</span><span class=\"p\">)</span>\n<span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">topk</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">indices</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"-----\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">idx</span> <span class=\"ow\">in</span> <span class=\"n\">preds</span><span class=\"p\">:</span>\n    <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">labels_map</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span>\n    <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">idx</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">label</span><span class=\"si\">:</span><span class=\"s2\">&lt;75</span><span class=\"si\">}</span><span class=\"s2\"> (</span><span class=\"si\">{</span><span class=\"n\">prob</span> <span class=\"o\">*</span> <span class=\"mi\">100</span><span class=\"si\">:</span><span class=\"s2\">.2f</span><span class=\"si\">}</span><span class=\"s2\">%)\"</span><span class=\"p\">)</span>\n</pre>\n<h4>Example: Feature Extraction</h4>\n<p>You can easily extract features with <code>model.extract_features</code>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">densenet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">DenseNet</span> \n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DenseNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'densenet121'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ... image preprocessing as in the classification example ...</span>\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># torch.Size([1, 3, 224, 224])</span>\n\n<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">extract_features</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># torch.Size([1, 1024, 7, 7])</span>\n</pre>\n<h4>Example: Export to ONNX</h4>\n<p>Exporting to ONNX for deploying to production is now simple:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span> \n<span class=\"kn\">from</span> <span class=\"nn\">densenet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">DenseNet</span> \n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DenseNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'densenet121'</span><span class=\"p\">)</span>\n<span class=\"n\">dummy_input</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>\n\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">export</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">dummy_input</span><span class=\"p\">,</span> <span class=\"s2\">\"demo.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<h4>Example: Visual</h4>\n<pre>cd $REPO$/framework\nsh start.sh\n</pre>\n<p>Then open the browser and type in the browser address <a href=\"http://127.0.0.1:10003/\" rel=\"nofollow\">http://127.0.0.1:10003/</a>.</p>\n<p>Enjoy it.</p>\n<h4>ImageNet</h4>\n<p>See <code>examples/imagenet</code> for details about evaluating on ImageNet.</p>\n<p>For more datasets result. Please see <code>research/README.md</code>.</p>\n<h3>Contributing</h3>\n<p>If you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues.</p>\n<p>I look forward to seeing what the community does with these models!</p>\n<h3>Credit</h3>\n<h4>Densely Connected Convolutional Networks</h4>\n<p><em>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger</em></p>\n<h5>Abstract</h5>\n<p>Recent work has shown that convolutional networks can be substantially deeper, more accurate,\nand efficient to train if they contain shorter connections between layers close to the input\nand those close to the output. In this paper, we embrace this observation and introduce the\nDense Convolutional Network (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers have L connections</p>\n<ul>\n<li>one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections.\nFor each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps\nare used as inputs into all subsequent layers. DenseNets have several compelling advantages: they\nalleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse,\nand substantially reduce the number of parameters. We evaluate our proposed architecture on four\nhighly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring\nless computation to achieve high performance. Code and pre-trained models are available at this <a href=\"https://github.com/liuzhuang13/DenseNet\" rel=\"nofollow\">https URL</a> .</li>\n</ul>\n<p><a href=\"http://arxiv.org/pdf/1608.06993v5\" rel=\"nofollow\">paper</a> <a href=\"https://github.com/liuzhuang13/DenseNet\" rel=\"nofollow\">code</a></p>\n<pre>@article{DenseNet,\ntitle:{Densely Connected Convolutional Networks},\nauthor:{Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger},\njournal={cvpr},\nyear={2016}\n}\n</pre>\n\n          </div>"}, "last_serial": 6818879, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "3f4ad7ff63cedd500bb3ad6815fea8d3", "sha256": "c735129e3eb4c17ac21abc8fbfcd52f31dbf4db26cc692ca09affcf2a6bbe198"}, "downloads": -1, "filename": "densenet_pytorch-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "3f4ad7ff63cedd500bb3ad6815fea8d3", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5.0", "size": 12880, "upload_time": "2020-02-17T14:19:07", "upload_time_iso_8601": "2020-02-17T14:19:07.987798Z", "url": "https://files.pythonhosted.org/packages/fc/01/be33f8d6838b6d659d4cdc28a85bf918abbff6cacfb2256b5ec79d1cb4bb/densenet_pytorch-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b1354e81a9085174e59c28695ce97926", "sha256": "302193215a85ecd36ae093f24fb390854d4df52d56e91bb627a7c419a86dc97d"}, "downloads": -1, "filename": "densenet_pytorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "b1354e81a9085174e59c28695ce97926", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 8462, "upload_time": "2020-02-17T14:19:12", "upload_time_iso_8601": "2020-02-17T14:19:12.259425Z", "url": "https://files.pythonhosted.org/packages/45/52/1fcb8dfdc5652f5257bc0eb1478ab0e2976b13704ea1ae4827e7c44c18e4/densenet_pytorch-0.1.0.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "85928ad2f60e1067aba0905c82e4fbdd", "sha256": "41ce5fe9baaafc85adbd6cc29c9fd9b7fdc982b4295458330f61af1b28f1e417"}, "downloads": -1, "filename": "densenet_pytorch-0.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "85928ad2f60e1067aba0905c82e4fbdd", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5.0", "size": 14154, "upload_time": "2020-02-18T04:23:40", "upload_time_iso_8601": "2020-02-18T04:23:40.899106Z", "url": "https://files.pythonhosted.org/packages/27/3e/c9f46858bd82a324ea638d158949e542dea8e3567002887c5ae2a87e7443/densenet_pytorch-0.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bcddb66b7b46168febd76e2ee8d1a54b", "sha256": "b42d0271332f8add39a6cef9db2d73071374e9085ce9f2428222867e64483f9e"}, "downloads": -1, "filename": "densenet_pytorch-0.1.3.tar.gz", "has_sig": false, "md5_digest": "bcddb66b7b46168febd76e2ee8d1a54b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 10790, "upload_time": "2020-02-18T04:23:42", "upload_time_iso_8601": "2020-02-18T04:23:42.584285Z", "url": "https://files.pythonhosted.org/packages/6a/97/57b295c068f00bb30e6939aed9a0a084e378710bfaea7b055a07a1252fa8/densenet_pytorch-0.1.3.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "9abae599e042de9dd014906a8b720aec", "sha256": "2a214f0f03fd80a4738e039a37dc01b98646c38786fdc6d98a58978125eef60f"}, "downloads": -1, "filename": "densenet_pytorch-0.1.5-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "9abae599e042de9dd014906a8b720aec", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5.0", "size": 14174, "upload_time": "2020-02-18T10:15:52", "upload_time_iso_8601": "2020-02-18T10:15:52.223988Z", "url": "https://files.pythonhosted.org/packages/77/5d/3867f43b987617a104dc99d743d0f2315f9d356b2ab626d93c1486c440ba/densenet_pytorch-0.1.5-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ddaea2ab85a6a1837f421b0f44f86ab1", "sha256": "987e363470ae093a3c23c04c4a54d47fc12d248123de06c6123425d984471a68"}, "downloads": -1, "filename": "densenet_pytorch-0.1.5.tar.gz", "has_sig": false, "md5_digest": "ddaea2ab85a6a1837f421b0f44f86ab1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 10817, "upload_time": "2020-02-18T10:15:54", "upload_time_iso_8601": "2020-02-18T10:15:54.612700Z", "url": "https://files.pythonhosted.org/packages/4f/01/e74dd4b9537fc045f031ce6d582c7f65719f2f8c586e9b1bd3af997f2415/densenet_pytorch-0.1.5.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "b483dae1f83841b307885c17b1aa529c", "sha256": "9c96b17914173b9b07f7349cb239eb2aab3eb1cfeebe273ed5dbf11901fa52e8"}, "downloads": -1, "filename": "densenet_pytorch-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "b483dae1f83841b307885c17b1aa529c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 14553, "upload_time": "2020-03-16T06:05:01", "upload_time_iso_8601": "2020-03-16T06:05:01.113477Z", "url": "https://files.pythonhosted.org/packages/4f/50/5f94584e2b702693c17be720ce907975080015eb47d3df999eb9853c4980/densenet_pytorch-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "92c094ce9626a50891e763644c339499", "sha256": "f5dbbe1d6ec030452ff2d48495d095df93e0d5d7b3f9a8212613e4069abb488a"}, "downloads": -1, "filename": "densenet_pytorch-0.2.0.tar.gz", "has_sig": false, "md5_digest": "92c094ce9626a50891e763644c339499", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 11507, "upload_time": "2020-03-16T06:05:02", "upload_time_iso_8601": "2020-03-16T06:05:02.750971Z", "url": "https://files.pythonhosted.org/packages/19/84/f345e0d83f1065e6f623de5530fac3df431550e442baf0ce16b962d349dc/densenet_pytorch-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b483dae1f83841b307885c17b1aa529c", "sha256": "9c96b17914173b9b07f7349cb239eb2aab3eb1cfeebe273ed5dbf11901fa52e8"}, "downloads": -1, "filename": "densenet_pytorch-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "b483dae1f83841b307885c17b1aa529c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 14553, "upload_time": "2020-03-16T06:05:01", "upload_time_iso_8601": "2020-03-16T06:05:01.113477Z", "url": "https://files.pythonhosted.org/packages/4f/50/5f94584e2b702693c17be720ce907975080015eb47d3df999eb9853c4980/densenet_pytorch-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "92c094ce9626a50891e763644c339499", "sha256": "f5dbbe1d6ec030452ff2d48495d095df93e0d5d7b3f9a8212613e4069abb488a"}, "downloads": -1, "filename": "densenet_pytorch-0.2.0.tar.gz", "has_sig": false, "md5_digest": "92c094ce9626a50891e763644c339499", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 11507, "upload_time": "2020-03-16T06:05:02", "upload_time_iso_8601": "2020-03-16T06:05:02.750971Z", "url": "https://files.pythonhosted.org/packages/19/84/f345e0d83f1065e6f623de5530fac3df431550e442baf0ce16b962d349dc/densenet_pytorch-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:39:06 2020"}