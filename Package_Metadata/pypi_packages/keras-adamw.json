{"info": {"author": "OverLordGoldDragon", "author_email": "16495490+OverLordGoldDragon@users.noreply.github.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Information Technology", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Software Development", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Utilities"], "description": "# Keras AdamW\n\n[![Build Status](https://travis-ci.com/OverLordGoldDragon/keras-adamw.svg?token=dGKzzAxzJjaRLzddNsCd&branch=master)](https://travis-ci.com/OverLordGoldDragon/keras-adamw)\n[![Coverage Status](https://coveralls.io/repos/github/OverLordGoldDragon/keras-adamw/badge.svg?branch=master&service=github)](https://coveralls.io/github/OverLordGoldDragon/keras-adamw)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/1215c1605ad545cba419ee6e5cc870f5)](https://www.codacy.com?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=OverLordGoldDragon/keras-adamw&amp;utm_campaign=Badge_Grade)\n[![PyPI version](https://badge.fury.io/py/keras-adamw.svg)](https://badge.fury.io/py/keras-adamw)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n\n![](https://img.shields.io/badge/keras-tensorflow-blue.svg)\n![](https://img.shields.io/badge/keras-tf.keras-blue.svg)\n![](https://img.shields.io/badge/keras-tf.keras/eager-blue.svg)\n![](https://img.shields.io/badge/keras-tf.keras/2.0-blue.svg)\n\nKeras implementation of **AdamW**, **SGDW**, **NadamW**, and **Warm Restarts**, based on paper [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) - plus **Learning Rate Multipliers**\n\n<img src=\"https://user-images.githubusercontent.com/16495490/65381086-233f7d00-dcb7-11e9-8c83-d0aec7b3663a.png\" width=\"850\">\n\n## Features\n - **Weight decay fix**: decoupling L2 penalty from gradient. _Why use?_\n   - Weight decay via L2 penalty yields _worse generalization_, due to decay not working properly\n   - Weight decay via L2 penalty leads to a _hyperparameter coupling_ with `lr`, complicating search\n - **Warm restarts (WR)**: cosine annealing learning rate schedule. _Why use?_\n   - _Better generalization_ and _faster convergence_ was shown by authors for various data and model sizes\n - **LR multipliers**: _per-layer_ learning rate multipliers. _Why use?_\n   - _Pretraining_; if adding new layers to pretrained layers, using a global `lr` is prone to overfitting\n\n\n## Installation\n\n`pip install keras-adamw` or clone repository\n\n## Usage\n\nIf using tensorflow.keras imports, set `import os; os.environ[\"TF_KERAS\"]='1'`.\n\n### Weight decay\n\n`AdamW(model=model)`<br>\nThree methods to set `weight_decays = {<weight matrix name>:<weight decay value>,}`:\n\n```python\n# 1. Automatically\nJust pass in `model` (`AdamW(model=model)`), and decays will be automatically extracted.\nLoss-based penalties (l1, l2, l1_l2) will be zeroed by default, but can be kept via\n`zero_penalties=False` (NOT recommended, see Use guidelines).\n```\n```python\n# 2. Use keras_adamw.utils_common.py\nDense(.., kernel_regularizer=l2(0)) # set weight decays in layers as usual, but to ZERO\nwd_dict = get_weight_decays(model)\n# print(wd_dict) to see returned matrix names, note their order\n# specify values as (l1, l2) tuples, both for l1_l2 decay\nordered_values = [(0, 1e-3), (1e-4, 2e-4), ..]\nweight_decays = fill_dict_in_order(wd_dict, ordered_values)\n```\n```python\n# 3. Fill manually\nmodel.layers[1].kernel.name # get name of kernel weight matrix of layer indexed 1\nweight_decays.update({'conv1d_0/kernel:0': (1e-4, 0)}) # example\n```\n\n### Warm restarts\n`AdamW(.., use_cosine_annealing=True, total_iterations=200)` - refer to _Use guidelines_ below\n\n### LR multipliers\n`AdamW(.., lr_multipliers=lr_multipliers)` - to get, `{<layer name>:<multiplier value>,}`:\n\n 1. (a) Name every layer to be modified _(recommended)_, e.g. `Dense(.., name='dense_1')` - OR<br>\n (b) Get every layer name, note which to modify: `[print(idx,layer.name) for idx,layer in enumerate(model.layers)]`\n 2. (a) `lr_multipliers = {'conv1d_0':0.1} # target layer by full name` - OR<br>\n (b) `lr_multipliers = {'conv1d':0.1}   # target all layers w/ name substring 'conv1d'`\n\n ## Example\n```python\nimport numpy as np\nfrom keras.layers import Input, Dense, LSTM\nfrom keras.models import Model\nfrom keras.regularizers import l1, l2, l1_l2\nfrom keras_adamw import AdamW\n\nipt   = Input(shape=(120, 4))\nx     = LSTM(60, activation='relu', name='lstm_1',\n             kernel_regularizer=l1(1e-4), recurrent_regularizer=l2(2e-4))(ipt)\nout   = Dense(1, activation='sigmoid', kernel_regularizer=l1_l2(1e-4))(x)\nmodel = Model(ipt, out)\n```\n```python\nlr_multipliers = {'lstm_1': 0.5}\n\noptimizer = AdamW(lr=1e-4, model=model, lr_multipliers=lr_multipliers,\n                  use_cosine_annealing=True, total_iterations=24)\nmodel.compile(optimizer, loss='binary_crossentropy')\n```\n```python\nfor epoch in range(3):\n    for iteration in range(24):\n        x = np.random.rand(10, 120, 4) # dummy data\n        y = np.random.randint(0, 2, (10, 1)) # dummy labels\n        loss = model.train_on_batch(x, y)\n        print(\"Iter {} loss: {}\".format(iteration + 1, \"%.3f\" % loss))\n    print(\"EPOCH {} COMPLETED\\n\".format(epoch + 1))\n    K.set_value(model.optimizer.t_cur, 0) # WARM RESTART: reset cosine annealing argument\n```\n<img src=\"https://user-images.githubusercontent.com/16495490/65729113-2063d400-e08b-11e9-8b6a-3a2ea1c62fdd.png\" width=\"450\">\n\n(Full example + plot code: [example.py](https://github.com/OverLordGoldDragon/keras-adamw/blob/master/example.py))\n\n## Use guidelines\n### Weight decay\n - **Set L2 penalty to ZERO** if regularizing a weight via `weight_decays` - else the purpose of the 'fix' is largely defeated, and weights will be over-decayed --_My recommendation_\n - `lambda = lambda_norm * sqrt(batch_size/total_iterations)` --> _can be changed_; the intent is to scale \u03bb to _decouple_ it from other hyperparams - including (but _not limited to_), train duration & batch size. --_Authors_ (Appendix, pg.1) (A-1)\n - `total_iterations_wd` --> set to normalize over _all epochs_ (or other interval `!= total_iterations`) instead of per-WR when using WR; may _sometimes_ yield better results --_My note_\n\n### Warm restarts\n - Set `t_cur = 0` to restart schedule multiplier (see _Example_). Can be done at compilation or during training. Non-`0` is also valid, and will start `eta_t` at another point on the cosine curve. Details in A-2,3\n - Set `total_iterations` to the # of expected weight updates _for the given restart_ --_Authors_ (A-1,2)\n - `eta_min=0, eta_max=1` are tunable hyperparameters; e.g., an exponential schedule can be used for `eta_max`. If unsure, the defaults were shown to work well in the paper. --_Authors_\n - **[Save/load](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) optimizer state**; WR relies on using the optimizer's update history for effective transitions --_Authors_ (A-2)\n```python\n# 'total_iterations' general purpose example\ndef get_total_iterations(restart_idx, num_epochs, iterations_per_epoch):\n    return num_epochs[restart_idx] * iterations_per_epoch[restart_idx]\nget_total_iterations(0, num_epochs=[1,3,5,8], iterations_per_epoch=[240,120,60,30])\n```\n### Learning rate multipliers\n - Best used for pretrained layers - e.g. greedy layer-wise pretraining, or pretraining a feature extractor to a classifier network. Can be a better alternative to freezing layer weights. --_My recommendation_\n - It's often best not to pretrain layers fully (till convergence, or even best obtainable validation score) - as it may inhibit their ability to adapt to newly-added layers.  --_My recommendation_\n - The more the layers are pretrained, the lower their fraction of new layers' `lr` should be. --_My recommendation_\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/OverLordGoldDragon/keras-adamw", "keywords": "tensorflow keras optimizers adamw adamwr nadam sgd learning-rate-multipliers warm-restarts", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "keras-adamw", "package_url": "https://pypi.org/project/keras-adamw/", "platform": "", "project_url": "https://pypi.org/project/keras-adamw/", "project_urls": {"Homepage": "https://github.com/OverLordGoldDragon/keras-adamw"}, "release_url": "https://pypi.org/project/keras-adamw/1.23/", "requires_dist": ["numpy", "tensorflow"], "requires_python": "", "summary": "Keras implementation of AdamW, SGDW, NadamW, Warm Restarts, and Learning Rate multipliers", "version": "1.23", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Keras AdamW</h1>\n<p><a href=\"https://travis-ci.com/OverLordGoldDragon/keras-adamw\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9ded365d6413cdc6d5bdaeda87fed2789659c77d/68747470733a2f2f7472617669732d63692e636f6d2f4f7665724c6f7264476f6c64447261676f6e2f6b657261732d6164616d772e7376673f746f6b656e3d64474b7a7a41787a4a6a61524c7a64644e734364266272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/OverLordGoldDragon/keras-adamw\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d934a2ec29ca3a172fbfc9afb56e7cee9db157a6/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4f7665724c6f7264476f6c64447261676f6e2f6b657261732d6164616d772f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562\"></a>\n<a href=\"https://www.codacy.com?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=OverLordGoldDragon/keras-adamw&amp;utm_campaign=Badge_Grade\" rel=\"nofollow\"><img alt=\"Codacy Badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a887a88ba09b2a23578729d085d7fc652800bc09/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3132313563313630356164353435636261343139656536653563633837306635\"></a>\n<a href=\"https://badge.fury.io/py/keras-adamw\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e1661943c6c2a0a9e05a8beef6ab5bb9451cdc45/68747470733a2f2f62616467652e667572792e696f2f70792f6b657261732d6164616d772e737667\"></a>\n<a href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"><img alt=\"License: MIT\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5aab1d039acf22567ba072834df6bce204ac48ad/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e2e737667\"></a></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e393eea6bde0401d67d8c1037f20197db5f67415/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6b657261732d74656e736f72666c6f772d626c75652e737667\">\n<img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/10bcc5d5c79439ea8a8eed66fab216ba966761df/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6b657261732d74662e6b657261732d626c75652e737667\">\n<img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/512a2743e8eb70e905cd9c53156b1207e6478fb0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6b657261732d74662e6b657261732f65616765722d626c75652e737667\">\n<img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c8c1a68578f6a20362355750942edb5bd69d0add/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6b657261732d74662e6b657261732f322e302d626c75652e737667\"></p>\n<p>Keras implementation of <strong>AdamW</strong>, <strong>SGDW</strong>, <strong>NadamW</strong>, and <strong>Warm Restarts</strong>, based on paper <a href=\"https://arxiv.org/abs/1711.05101\" rel=\"nofollow\">Decoupled Weight Decay Regularization</a> - plus <strong>Learning Rate Multipliers</strong></p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c7208a89507ba909f53184a4041915ee2908f819/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f31363439353439302f36353338313038362d32333366376430302d646362372d313165392d386338332d6430616563376233363633612e706e67\" width=\"850\">\n<h2>Features</h2>\n<ul>\n<li><strong>Weight decay fix</strong>: decoupling L2 penalty from gradient. <em>Why use?</em>\n<ul>\n<li>Weight decay via L2 penalty yields <em>worse generalization</em>, due to decay not working properly</li>\n<li>Weight decay via L2 penalty leads to a <em>hyperparameter coupling</em> with <code>lr</code>, complicating search</li>\n</ul>\n</li>\n<li><strong>Warm restarts (WR)</strong>: cosine annealing learning rate schedule. <em>Why use?</em>\n<ul>\n<li><em>Better generalization</em> and <em>faster convergence</em> was shown by authors for various data and model sizes</li>\n</ul>\n</li>\n<li><strong>LR multipliers</strong>: <em>per-layer</em> learning rate multipliers. <em>Why use?</em>\n<ul>\n<li><em>Pretraining</em>; if adding new layers to pretrained layers, using a global <code>lr</code> is prone to overfitting</li>\n</ul>\n</li>\n</ul>\n<h2>Installation</h2>\n<p><code>pip install keras-adamw</code> or clone repository</p>\n<h2>Usage</h2>\n<p>If using tensorflow.keras imports, set <code>import os; os.environ[\"TF_KERAS\"]='1'</code>.</p>\n<h3>Weight decay</h3>\n<p><code>AdamW(model=model)</code><br>\nThree methods to set <code>weight_decays = {&lt;weight matrix name&gt;:&lt;weight decay value&gt;,}</code>:</p>\n<pre><span class=\"c1\"># 1. Automatically</span>\n<span class=\"n\">Just</span> <span class=\"k\">pass</span> <span class=\"ow\">in</span> <span class=\"err\">`</span><span class=\"n\">model</span><span class=\"err\">`</span> <span class=\"p\">(</span><span class=\"err\">`</span><span class=\"n\">AdamW</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">)</span><span class=\"err\">`</span><span class=\"p\">),</span> <span class=\"ow\">and</span> <span class=\"n\">decays</span> <span class=\"n\">will</span> <span class=\"n\">be</span> <span class=\"n\">automatically</span> <span class=\"n\">extracted</span><span class=\"o\">.</span>\n<span class=\"n\">Loss</span><span class=\"o\">-</span><span class=\"n\">based</span> <span class=\"n\">penalties</span> <span class=\"p\">(</span><span class=\"n\">l1</span><span class=\"p\">,</span> <span class=\"n\">l2</span><span class=\"p\">,</span> <span class=\"n\">l1_l2</span><span class=\"p\">)</span> <span class=\"n\">will</span> <span class=\"n\">be</span> <span class=\"n\">zeroed</span> <span class=\"n\">by</span> <span class=\"n\">default</span><span class=\"p\">,</span> <span class=\"n\">but</span> <span class=\"n\">can</span> <span class=\"n\">be</span> <span class=\"n\">kept</span> <span class=\"n\">via</span>\n<span class=\"err\">`</span><span class=\"n\">zero_penalties</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"err\">`</span> <span class=\"p\">(</span><span class=\"n\">NOT</span> <span class=\"n\">recommended</span><span class=\"p\">,</span> <span class=\"n\">see</span> <span class=\"n\">Use</span> <span class=\"n\">guidelines</span><span class=\"p\">)</span><span class=\"o\">.</span>\n</pre>\n<pre><span class=\"c1\"># 2. Use keras_adamw.utils_common.py</span>\n<span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"o\">..</span><span class=\"p\">,</span> <span class=\"n\">kernel_regularizer</span><span class=\"o\">=</span><span class=\"n\">l2</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))</span> <span class=\"c1\"># set weight decays in layers as usual, but to ZERO</span>\n<span class=\"n\">wd_dict</span> <span class=\"o\">=</span> <span class=\"n\">get_weight_decays</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"c1\"># print(wd_dict) to see returned matrix names, note their order</span>\n<span class=\"c1\"># specify values as (l1, l2) tuples, both for l1_l2 decay</span>\n<span class=\"n\">ordered_values</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mf\">1e-3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"mf\">2e-4</span><span class=\"p\">),</span> <span class=\"o\">..</span><span class=\"p\">]</span>\n<span class=\"n\">weight_decays</span> <span class=\"o\">=</span> <span class=\"n\">fill_dict_in_order</span><span class=\"p\">(</span><span class=\"n\">wd_dict</span><span class=\"p\">,</span> <span class=\"n\">ordered_values</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"c1\"># 3. Fill manually</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">kernel</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"c1\"># get name of kernel weight matrix of layer indexed 1</span>\n<span class=\"n\">weight_decays</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">({</span><span class=\"s1\">'conv1d_0/kernel:0'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)})</span> <span class=\"c1\"># example</span>\n</pre>\n<h3>Warm restarts</h3>\n<p><code>AdamW(.., use_cosine_annealing=True, total_iterations=200)</code> - refer to <em>Use guidelines</em> below</p>\n<h3>LR multipliers</h3>\n<p><code>AdamW(.., lr_multipliers=lr_multipliers)</code> - to get, <code>{&lt;layer name&gt;:&lt;multiplier value&gt;,}</code>:</p>\n<ol>\n<li>(a) Name every layer to be modified <em>(recommended)</em>, e.g. <code>Dense(.., name='dense_1')</code> - OR<br>\n(b) Get every layer name, note which to modify: <code>[print(idx,layer.name) for idx,layer in enumerate(model.layers)]</code></li>\n<li>(a) <code>lr_multipliers = {'conv1d_0':0.1} # target layer by full name</code> - OR<br>\n(b) <code>lr_multipliers = {'conv1d':0.1} # target all layers w/ name substring 'conv1d'</code></li>\n</ol>\n<h2>Example</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Input</span><span class=\"p\">,</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">LSTM</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Model</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.regularizers</span> <span class=\"kn\">import</span> <span class=\"n\">l1</span><span class=\"p\">,</span> <span class=\"n\">l2</span><span class=\"p\">,</span> <span class=\"n\">l1_l2</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras_adamw</span> <span class=\"kn\">import</span> <span class=\"n\">AdamW</span>\n\n<span class=\"n\">ipt</span>   <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">120</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">))</span>\n<span class=\"n\">x</span>     <span class=\"o\">=</span> <span class=\"n\">LSTM</span><span class=\"p\">(</span><span class=\"mi\">60</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'lstm_1'</span><span class=\"p\">,</span>\n             <span class=\"n\">kernel_regularizer</span><span class=\"o\">=</span><span class=\"n\">l1</span><span class=\"p\">(</span><span class=\"mf\">1e-4</span><span class=\"p\">),</span> <span class=\"n\">recurrent_regularizer</span><span class=\"o\">=</span><span class=\"n\">l2</span><span class=\"p\">(</span><span class=\"mf\">2e-4</span><span class=\"p\">))(</span><span class=\"n\">ipt</span><span class=\"p\">)</span>\n<span class=\"n\">out</span>   <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'sigmoid'</span><span class=\"p\">,</span> <span class=\"n\">kernel_regularizer</span><span class=\"o\">=</span><span class=\"n\">l1_l2</span><span class=\"p\">(</span><span class=\"mf\">1e-4</span><span class=\"p\">))(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">ipt</span><span class=\"p\">,</span> <span class=\"n\">out</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"n\">lr_multipliers</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">'lstm_1'</span><span class=\"p\">:</span> <span class=\"mf\">0.5</span><span class=\"p\">}</span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">AdamW</span><span class=\"p\">(</span><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">lr_multipliers</span><span class=\"o\">=</span><span class=\"n\">lr_multipliers</span><span class=\"p\">,</span>\n                  <span class=\"n\">use_cosine_annealing</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">total_iterations</span><span class=\"o\">=</span><span class=\"mi\">24</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s1\">'binary_crossentropy'</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">iteration</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">24</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">120</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span> <span class=\"c1\"># dummy data</span>\n        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span> <span class=\"c1\"># dummy labels</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_on_batch</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Iter </span><span class=\"si\">{}</span><span class=\"s2\"> loss: </span><span class=\"si\">{}</span><span class=\"s2\">\"</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">iteration</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s2\">\"</span><span class=\"si\">%.3f</span><span class=\"s2\">\"</span> <span class=\"o\">%</span> <span class=\"n\">loss</span><span class=\"p\">))</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"EPOCH </span><span class=\"si\">{}</span><span class=\"s2\"> COMPLETED</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">epoch</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">set_value</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">t_cur</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"c1\"># WARM RESTART: reset cosine annealing argument</span>\n</pre>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0004afb303808256aca55261af47a5a6051613e4/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f31363439353439302f36353732393131332d32303633643430302d653038622d313165392d386236612d3361326561316336326664642e706e67\" width=\"450\">\n<p>(Full example + plot code: <a href=\"https://github.com/OverLordGoldDragon/keras-adamw/blob/master/example.py\" rel=\"nofollow\">example.py</a>)</p>\n<h2>Use guidelines</h2>\n<h3>Weight decay</h3>\n<ul>\n<li><strong>Set L2 penalty to ZERO</strong> if regularizing a weight via <code>weight_decays</code> - else the purpose of the 'fix' is largely defeated, and weights will be over-decayed --<em>My recommendation</em></li>\n<li><code>lambda = lambda_norm * sqrt(batch_size/total_iterations)</code> --&gt; <em>can be changed</em>; the intent is to scale \u03bb to <em>decouple</em> it from other hyperparams - including (but <em>not limited to</em>), train duration &amp; batch size. --<em>Authors</em> (Appendix, pg.1) (A-1)</li>\n<li><code>total_iterations_wd</code> --&gt; set to normalize over <em>all epochs</em> (or other interval <code>!= total_iterations</code>) instead of per-WR when using WR; may <em>sometimes</em> yield better results --<em>My note</em></li>\n</ul>\n<h3>Warm restarts</h3>\n<ul>\n<li>Set <code>t_cur = 0</code> to restart schedule multiplier (see <em>Example</em>). Can be done at compilation or during training. Non-<code>0</code> is also valid, and will start <code>eta_t</code> at another point on the cosine curve. Details in A-2,3</li>\n<li>Set <code>total_iterations</code> to the # of expected weight updates <em>for the given restart</em> --<em>Authors</em> (A-1,2)</li>\n<li><code>eta_min=0, eta_max=1</code> are tunable hyperparameters; e.g., an exponential schedule can be used for <code>eta_max</code>. If unsure, the defaults were shown to work well in the paper. --<em>Authors</em></li>\n<li><strong><a href=\"https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\" rel=\"nofollow\">Save/load</a> optimizer state</strong>; WR relies on using the optimizer's update history for effective transitions --<em>Authors</em> (A-2)</li>\n</ul>\n<pre><span class=\"c1\"># 'total_iterations' general purpose example</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_total_iterations</span><span class=\"p\">(</span><span class=\"n\">restart_idx</span><span class=\"p\">,</span> <span class=\"n\">num_epochs</span><span class=\"p\">,</span> <span class=\"n\">iterations_per_epoch</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">num_epochs</span><span class=\"p\">[</span><span class=\"n\">restart_idx</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">iterations_per_epoch</span><span class=\"p\">[</span><span class=\"n\">restart_idx</span><span class=\"p\">]</span>\n<span class=\"n\">get_total_iterations</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_epochs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"n\">iterations_per_epoch</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">240</span><span class=\"p\">,</span><span class=\"mi\">120</span><span class=\"p\">,</span><span class=\"mi\">60</span><span class=\"p\">,</span><span class=\"mi\">30</span><span class=\"p\">])</span>\n</pre>\n<h3>Learning rate multipliers</h3>\n<ul>\n<li>Best used for pretrained layers - e.g. greedy layer-wise pretraining, or pretraining a feature extractor to a classifier network. Can be a better alternative to freezing layer weights. --<em>My recommendation</em></li>\n<li>It's often best not to pretrain layers fully (till convergence, or even best obtainable validation score) - as it may inhibit their ability to adapt to newly-added layers.  --<em>My recommendation</em></li>\n<li>The more the layers are pretrained, the lower their fraction of new layers' <code>lr</code> should be. --<em>My recommendation</em></li>\n</ul>\n\n          </div>"}, "last_serial": 7189047, "releases": {"1.2": [{"comment_text": "", "digests": {"md5": "64bb4f8a9058629623f5fcd828597d2f", "sha256": "b19b27339ea0c241692b9600edd5eda185862f0e103087b72bcaffcbddbf4bf6"}, "downloads": -1, "filename": "keras_adamw-1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "64bb4f8a9058629623f5fcd828597d2f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 50226, "upload_time": "2020-05-04T21:14:50", "upload_time_iso_8601": "2020-05-04T21:14:50.018938Z", "url": "https://files.pythonhosted.org/packages/71/5a/73e6160b1d4e21e43b5d24d88aa950e715ead10ee8dece993790ed945170/keras_adamw-1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9f1a45030041c9061be6150b53701233", "sha256": "446659f3d84236ca2a5a48f0348e6e2fae04bb1996ce35c774c23fc150a8f10b"}, "downloads": -1, "filename": "keras-adamw-1.2.tar.gz", "has_sig": false, "md5_digest": "9f1a45030041c9061be6150b53701233", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29658, "upload_time": "2020-05-04T21:14:52", "upload_time_iso_8601": "2020-05-04T21:14:52.003811Z", "url": "https://files.pythonhosted.org/packages/c2/3e/f531c8b1d166b3ef7dfd095322c817714ebf2e5b556f8d5abe42ca896e04/keras-adamw-1.2.tar.gz", "yanked": false}], "1.21": [{"comment_text": "", "digests": {"md5": "bef731a145910e6f72c8d61f994773d9", "sha256": "8ed3df5694dd3210c5e248acf29e6d9c32d73b77c89c357f7be79372aef641b9"}, "downloads": -1, "filename": "keras_adamw-1.21-py3-none-any.whl", "has_sig": false, "md5_digest": "bef731a145910e6f72c8d61f994773d9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 50793, "upload_time": "2020-05-06T18:50:33", "upload_time_iso_8601": "2020-05-06T18:50:33.157181Z", "url": "https://files.pythonhosted.org/packages/15/cb/c68930c877d03e3a5cbcdbfd941e6ca72def6074b99a27378bf50221ad2a/keras_adamw-1.21-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2eaa3ac428a117113155ef0fdd15fefa", "sha256": "84334b36adb10cd22f7cd5e8b7d0137b536ead09420c0ed32a91f99e8bb95d0c"}, "downloads": -1, "filename": "keras-adamw-1.21.tar.gz", "has_sig": false, "md5_digest": "2eaa3ac428a117113155ef0fdd15fefa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29913, "upload_time": "2020-05-06T18:50:34", "upload_time_iso_8601": "2020-05-06T18:50:34.096088Z", "url": "https://files.pythonhosted.org/packages/a9/aa/f6e90b70160fc72928880dece53dd6ca814a998437b8b0950c1137d5bce5/keras-adamw-1.21.tar.gz", "yanked": false}], "1.23": [{"comment_text": "", "digests": {"md5": "73fe0873bb729c12c166f4276ea0f924", "sha256": "d89d2574a4db7a9871fcc2e24e773a2574d2584eceed0fce4bf61efb7dbaf931"}, "downloads": -1, "filename": "keras_adamw-1.23-py3-none-any.whl", "has_sig": false, "md5_digest": "73fe0873bb729c12c166f4276ea0f924", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 50941, "upload_time": "2020-05-07T14:30:03", "upload_time_iso_8601": "2020-05-07T14:30:03.622791Z", "url": "https://files.pythonhosted.org/packages/22/1f/92c68a3af6833cc34d4cb0daa86ef234739703604fa213271549cd756ddf/keras_adamw-1.23-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "878e338abe1666b0fef03c4addd775f2", "sha256": "7c91a8ba4fcfbcefc7a0f2d95ea6ffa6ea2db05c35d619519bde5f9d654978b3"}, "downloads": -1, "filename": "keras-adamw-1.23.tar.gz", "has_sig": false, "md5_digest": "878e338abe1666b0fef03c4addd775f2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29983, "upload_time": "2020-05-07T14:30:04", "upload_time_iso_8601": "2020-05-07T14:30:04.684759Z", "url": "https://files.pythonhosted.org/packages/f1/39/fcbb10a3486b12544003851967e24081aa360ed8642b5fb8b27cda95a120/keras-adamw-1.23.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "73fe0873bb729c12c166f4276ea0f924", "sha256": "d89d2574a4db7a9871fcc2e24e773a2574d2584eceed0fce4bf61efb7dbaf931"}, "downloads": -1, "filename": "keras_adamw-1.23-py3-none-any.whl", "has_sig": false, "md5_digest": "73fe0873bb729c12c166f4276ea0f924", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 50941, "upload_time": "2020-05-07T14:30:03", "upload_time_iso_8601": "2020-05-07T14:30:03.622791Z", "url": "https://files.pythonhosted.org/packages/22/1f/92c68a3af6833cc34d4cb0daa86ef234739703604fa213271549cd756ddf/keras_adamw-1.23-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "878e338abe1666b0fef03c4addd775f2", "sha256": "7c91a8ba4fcfbcefc7a0f2d95ea6ffa6ea2db05c35d619519bde5f9d654978b3"}, "downloads": -1, "filename": "keras-adamw-1.23.tar.gz", "has_sig": false, "md5_digest": "878e338abe1666b0fef03c4addd775f2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29983, "upload_time": "2020-05-07T14:30:04", "upload_time_iso_8601": "2020-05-07T14:30:04.684759Z", "url": "https://files.pythonhosted.org/packages/f1/39/fcbb10a3486b12544003851967e24081aa360ed8642b5fb8b27cda95a120/keras-adamw-1.23.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:14 2020"}