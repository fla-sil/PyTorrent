{"info": {"author": "Anatoly Belikov", "author_email": "awbelikov@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# README #\n\nCaffe-like machine learning framework in python\n\n\n## layers types\n\n### ImageData\nlayer to read images. Possible sources: txt file with path labels on each line, cifar archive.\n\n**image_data_param**:\n\nsource: string - path to cifar archive or txt file\n\nbatch_size: int - how many images to process in each iteration\n\nshuffle: bool - images will be shuffled when sampling for a batch\n\nnew_height: int - new image height(can be same as original)\n\nnew_width: int - new image width(can be same as original)\n\nnew_labels: int - expected number of labels(txt file may contain multiple labels for each path)\n\nexample:\n\n     layer {\n       top: \"data\"\n       top: \"label\"\n       name: \"data\"\n       type: \"ImageData\"\n       image_data_param {\n         source: \"../cifar/cifar-10-python.tar.gz\"\n         source: \"data.txt.3\"  # use cifar or txt file\n         batch_size: 65\n         shuffle: true\n         new_height: 32\n         new_width: 32\n         n_labels: 10\n       }\n       transform_param {\n         mean_value: 126 # r\n         mean_value: 123 # g\n         mean_value: 114 # b\n         mirror: true\n         scale: 0.02728125\n         standard_params {\n             var_average: 5000\n             mean_average: 5000\n             mean_per_channel: false\n             var_per_channel: false\n         }\n       }\n       include: { phase: TRAIN }\n     }\n\n**transform_param**\n\nParameters for data preprocessing\n\n**standard_params**\n\nParameters for preprocessor for data standardization.\nTo achieve zero mean and unit variance the preprocessor will subtract iterative \nmean from each sample and divide the result by standard deviation.\n\n    standard_params {\n        var_average: 1\n        mean_average: 1\n        mean_per_channel: false\n        var_per_channel: true\n    }\n\nvar_average: int [default = 0] - use last var_average samples to compute variance and std  \ndisabled if var_average == 0  \nmean_average: int [default = 0]  - use last var_average samples to compute mean  \ndisabled if mean_average == 0  \nmean_per_channel: bool [default = false] - subtract from each channel mean for that channel  \nvar_per_channel: [default = false] - divide each channel by separate std value  \n\n### Convolution\n\nConvolution of 2-3 d images(matrices)\n\n**convolution_param**\n\nnum_output: int   number of filters(output feature maps)\n\nkernel_size:  int    size of receptive field of filters. Receptive field is kernel_size * kernel_size\n\nstride: int - filter will be applied after stride pixels\n\nweight_filler: see weight filler\n\nexample:\n\n    layer {\n      name: \"conv1\"\n      type: \"Convolution\"\n      bottom: \"data\"\n      top: \"conv1\"\n      convolution_param {\n        num_output: 40\n        kernel_size: 3\n        stride: 2\n        weight_filler {\n          type: \"xavier\"\n          variance_norm: AVERAGE\n        }\n      }\n    }\n\n\n### Pooling\n\nSubsampling layer for max or average pooling\n\n**pooling_param**\n\npool: MAX or AVE\n\nkernel_size: int \nsubsampling window size\n\nstride: int\nperform pooling each *stride* pixels\n\nexample:\n\n    layer {\n      name: \"pool1\"\n      type: \"Pooling\"\n      bottom: \"conv1\"\n      top: \"pool1\"\n      pooling_param {\n        pool: MAX\n        kernel_size: 3\n        stride: 2\n      }\n    }\n\n### Accuracy\n\nLayer for computing accuracy\nAccuracy of a classifier is defined as (true positive + true negative)/total  \nIn multilabel classification example counts as correctly classified iff **all** outputs\n are correct.  \nExample:   \n\n    layer {\n      name: \"accuracy\"\n      type: \"Accuracy\"\n      bottom: \"pool10\"\n      bottom: \"label\"\n      top: \"accuracy\"\n    }\n\n## weight_filler\n\nWeigth filler parameters are common for all layers with weights\n\ntype: string  \n\"xavier\", \"gaussian\", \"uniform\"\n\nmean: float  \nmean value for gaussian initialization\n\nstd: float  \nstandard deviation for gaussian initialization\n\nmin: float  \nlower bound for uniform initialization\n\nmax: float  \nupper bound for uniform initialization\n\n## Activation functions\n\n### SeLU\n\nSelf-regularized linear unit:\n\nexample:  \n    {\n      name: \"relu_conv1\"\n      type: \"SeLU\"\n      bottom: \"conv1\"\n      top: \"conv1\"\n    }\n\n## Loss layers\n\n### SigmoidCrossEntropyLoss\n\nLayer that applies sigmoid elementwise, followed by cross-entropy log loss\n-mean(sum(y * log(p(y))  + (1 - y) * log(1 - p(y))))\n\nwhere p(y) - sigmoid transformation of layer's input, that is vector of independent probabilities for each class.\n\n    example:  \n        layer { \n          name: \"loss\"\n          type: \"SigmoidCrossEntropyLoss\"\n          bottom: \"pool1\"\n          bottom: \"label\"\n          top: \"loss\"\n          include {\n              phase: TRAIN\n          }\n        }\n\n## Maxout layer\n\nApply max operator for each *size* channels  \n\nsize: int [default = 0] - take max over each *size* channels  \nlambda: int [default = 0.0] - apply probabilistic max if lambda != 0  \n\n    layer {\n      name: \"maxout_1\"\n      type: \"Maxout\"\n      maxout_param {\n        lambda: 1\n        size: 2\n      }\n      bottom: \"conv1\"\n      top: \"conv1\"\n    }", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://bitbucket.org/noSkill/fireml", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "fireml", "package_url": "https://pypi.org/project/fireml/", "platform": "", "project_url": "https://pypi.org/project/fireml/", "project_urls": {"Homepage": "https://bitbucket.org/noSkill/fireml"}, "release_url": "https://pypi.org/project/fireml/0.1.1/", "requires_dist": null, "requires_python": "", "summary": "fireml machine learning framework", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>README</h1>\n<p>Caffe-like machine learning framework in python</p>\n<h2>layers types</h2>\n<h3>ImageData</h3>\n<p>layer to read images. Possible sources: txt file with path labels on each line, cifar archive.</p>\n<p><strong>image_data_param</strong>:</p>\n<p>source: string - path to cifar archive or txt file</p>\n<p>batch_size: int - how many images to process in each iteration</p>\n<p>shuffle: bool - images will be shuffled when sampling for a batch</p>\n<p>new_height: int - new image height(can be same as original)</p>\n<p>new_width: int - new image width(can be same as original)</p>\n<p>new_labels: int - expected number of labels(txt file may contain multiple labels for each path)</p>\n<p>example:</p>\n<pre><code> layer {\n   top: \"data\"\n   top: \"label\"\n   name: \"data\"\n   type: \"ImageData\"\n   image_data_param {\n     source: \"../cifar/cifar-10-python.tar.gz\"\n     source: \"data.txt.3\"  # use cifar or txt file\n     batch_size: 65\n     shuffle: true\n     new_height: 32\n     new_width: 32\n     n_labels: 10\n   }\n   transform_param {\n     mean_value: 126 # r\n     mean_value: 123 # g\n     mean_value: 114 # b\n     mirror: true\n     scale: 0.02728125\n     standard_params {\n         var_average: 5000\n         mean_average: 5000\n         mean_per_channel: false\n         var_per_channel: false\n     }\n   }\n   include: { phase: TRAIN }\n }\n</code></pre>\n<p><strong>transform_param</strong></p>\n<p>Parameters for data preprocessing</p>\n<p><strong>standard_params</strong></p>\n<p>Parameters for preprocessor for data standardization.\nTo achieve zero mean and unit variance the preprocessor will subtract iterative\nmean from each sample and divide the result by standard deviation.</p>\n<pre><code>standard_params {\n    var_average: 1\n    mean_average: 1\n    mean_per_channel: false\n    var_per_channel: true\n}\n</code></pre>\n<p>var_average: int [default = 0] - use last var_average samples to compute variance and std<br>\ndisabled if var_average == 0<br>\nmean_average: int [default = 0]  - use last var_average samples to compute mean<br>\ndisabled if mean_average == 0<br>\nmean_per_channel: bool [default = false] - subtract from each channel mean for that channel<br>\nvar_per_channel: [default = false] - divide each channel by separate std value</p>\n<h3>Convolution</h3>\n<p>Convolution of 2-3 d images(matrices)</p>\n<p><strong>convolution_param</strong></p>\n<p>num_output: int   number of filters(output feature maps)</p>\n<p>kernel_size:  int    size of receptive field of filters. Receptive field is kernel_size * kernel_size</p>\n<p>stride: int - filter will be applied after stride pixels</p>\n<p>weight_filler: see weight filler</p>\n<p>example:</p>\n<pre><code>layer {\n  name: \"conv1\"\n  type: \"Convolution\"\n  bottom: \"data\"\n  top: \"conv1\"\n  convolution_param {\n    num_output: 40\n    kernel_size: 3\n    stride: 2\n    weight_filler {\n      type: \"xavier\"\n      variance_norm: AVERAGE\n    }\n  }\n}\n</code></pre>\n<h3>Pooling</h3>\n<p>Subsampling layer for max or average pooling</p>\n<p><strong>pooling_param</strong></p>\n<p>pool: MAX or AVE</p>\n<p>kernel_size: int\nsubsampling window size</p>\n<p>stride: int\nperform pooling each <em>stride</em> pixels</p>\n<p>example:</p>\n<pre><code>layer {\n  name: \"pool1\"\n  type: \"Pooling\"\n  bottom: \"conv1\"\n  top: \"pool1\"\n  pooling_param {\n    pool: MAX\n    kernel_size: 3\n    stride: 2\n  }\n}\n</code></pre>\n<h3>Accuracy</h3>\n<p>Layer for computing accuracy\nAccuracy of a classifier is defined as (true positive + true negative)/total<br>\nIn multilabel classification example counts as correctly classified iff <strong>all</strong> outputs\nare correct.<br>\nExample:</p>\n<pre><code>layer {\n  name: \"accuracy\"\n  type: \"Accuracy\"\n  bottom: \"pool10\"\n  bottom: \"label\"\n  top: \"accuracy\"\n}\n</code></pre>\n<h2>weight_filler</h2>\n<p>Weigth filler parameters are common for all layers with weights</p>\n<p>type: string<br>\n\"xavier\", \"gaussian\", \"uniform\"</p>\n<p>mean: float<br>\nmean value for gaussian initialization</p>\n<p>std: float<br>\nstandard deviation for gaussian initialization</p>\n<p>min: float<br>\nlower bound for uniform initialization</p>\n<p>max: float<br>\nupper bound for uniform initialization</p>\n<h2>Activation functions</h2>\n<h3>SeLU</h3>\n<p>Self-regularized linear unit:</p>\n<p>example:<br>\n{\nname: \"relu_conv1\"\ntype: \"SeLU\"\nbottom: \"conv1\"\ntop: \"conv1\"\n}</p>\n<h2>Loss layers</h2>\n<h3>SigmoidCrossEntropyLoss</h3>\n<p>Layer that applies sigmoid elementwise, followed by cross-entropy log loss\n-mean(sum(y * log(p(y))  + (1 - y) * log(1 - p(y))))</p>\n<p>where p(y) - sigmoid transformation of layer's input, that is vector of independent probabilities for each class.</p>\n<pre><code>example:  \n    layer { \n      name: \"loss\"\n      type: \"SigmoidCrossEntropyLoss\"\n      bottom: \"pool1\"\n      bottom: \"label\"\n      top: \"loss\"\n      include {\n          phase: TRAIN\n      }\n    }\n</code></pre>\n<h2>Maxout layer</h2>\n<p>Apply max operator for each <em>size</em> channels</p>\n<p>size: int [default = 0] - take max over each <em>size</em> channels<br>\nlambda: int [default = 0.0] - apply probabilistic max if lambda != 0</p>\n<pre><code>layer {\n  name: \"maxout_1\"\n  type: \"Maxout\"\n  maxout_param {\n    lambda: 1\n    size: 2\n  }\n  bottom: \"conv1\"\n  top: \"conv1\"\n}\n</code></pre>\n\n          </div>"}, "last_serial": 4380761, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "59c141c27a63b196ded35149c0f963ae", "sha256": "730dee4422d0d12b73c265acca96aedbe090f30410e4303dd24493cbb79be7b5"}, "downloads": -1, "filename": "fireml-0.1.2.tar.gz", "has_sig": false, "md5_digest": "59c141c27a63b196ded35149c0f963ae", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 93720, "upload_time": "2018-10-16T09:41:56", "upload_time_iso_8601": "2018-10-16T09:41:56.226507Z", "url": "https://files.pythonhosted.org/packages/b5/09/7ec331593cb0bfc242d657695890f217eea7176837f04b25acb1645c3251/fireml-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "59c141c27a63b196ded35149c0f963ae", "sha256": "730dee4422d0d12b73c265acca96aedbe090f30410e4303dd24493cbb79be7b5"}, "downloads": -1, "filename": "fireml-0.1.2.tar.gz", "has_sig": false, "md5_digest": "59c141c27a63b196ded35149c0f963ae", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 93720, "upload_time": "2018-10-16T09:41:56", "upload_time_iso_8601": "2018-10-16T09:41:56.226507Z", "url": "https://files.pythonhosted.org/packages/b5/09/7ec331593cb0bfc242d657695890f217eea7176837f04b25acb1645c3251/fireml-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:42:13 2020"}