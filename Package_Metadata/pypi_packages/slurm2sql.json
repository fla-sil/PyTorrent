{"info": {"author": "Richard Darst", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: System Administrators", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Topic :: Database", "Topic :: System :: Clustering", "Topic :: System :: Distributed Computing"], "description": "Convert Slurm accounting database to sqlite3 file\n=================================================\n\nThis contains one utility, ``slurm2sql``, which uses the `Slurm\n<https://slurm.schedmd.com/overview>`__ workload manager's ``sacct``,\nto export all statistics from jobs and load them to a well-formed\nsqlite3 file.  This file can then be queried for analytics much more\neasily than the raw database or your own exports.\n\n\n\nInstallation\n------------\n\nThere is only a single file with no dependencies.  Python greater than\n2.7 is required.\n\n\n\nUsage\n-----\n\nSample usage::\n\n  slurm2sql.py [output_db] -- [sacct selection options]\n\n\nFor example, to get all data from July and August (``-S``) for all\nusers (``-a``)::\n\n  slurm2sql.py sincejuly.sqlite3 -- -S 2019-07-1 -a\n\n\nTo get the data from the last *N* days.  This will, day by day, get\neach of these history and cumulatively update the database.  This\nupdates a database by default, so that it can be used every day in\norder to efficiently keep a running database.  The ``-u`` option means\n\"don't delete existing database\" (jobs with the same JobID get\nupdated, not duplicated)::\n\n  slurm2sql.py --history-days=N -u sincejuly.sqlite3 -- -a\n\nThe ``--history-start=YYYY-MM-DD`` option can do a similar thing\nstarting from a certain day, and ``--history=DD-HH:MM:SS`` starts\ncollecting from a given interval of time ago (the time format is as in\nSlurm).\n\n\nIt can also be used from Python as what is essentially a glorified\nparser::\n\n  db = sqlite3.connect(':memory:')\n  slurm2sql.slurm2sql(db, ['-S', '2019-08-26'])\n\n  # For example, you can then convert to a dataframe:\n  import pandas as pd\n  df = pd.read_sql('SELECT * FROM slurm', db)\n\n\nDatabase format\n---------------\n\nThere is one table with name ``slurm``.\n\nThere is one row for each item returned by ``sacct``.\n\nIn general, there is one column for each item returned by ``sacct``,\nbut some of them are converted into a more useful form.  Some columns\nare added by re-processing other columns.  In general, just use the\nsource.  See ``COLUMNS`` in ``slurm2sql.py`` for details.  Extra\ncolumns can easily be added.\n\nThere are two types of converter functions: easy ones, which map one\nslurm column directly to a database column via a function, and line\nfunctions, which take the whole row and can do arbitrary remixing of\nthe data.\n\nColumns\n~~~~~~~\n\nAll column values are converted to standard units: *bytes* (not MB,\nKB, etc), *seconds*, *fraction 0.0-1.0* for things like\npercentages.\n\nBelow are some notable columns which do not exist in sacct.  It's good\nto verify that any of our custom columns make sense before trusting\nthem.  For other columns, check ``man sacct``.\n\n* ``Time``: approximation of last active time of a job.  The first of\n  these that exists: ``End``, ``Start``, ``Submitted``.  This is\n  intended to be used when you need to classify a job by when it ran,\n  but you don't care to be that specific.  (Only the Time column is\n  indexed by default, not the other times)\n\n* ``Submit``, ``Start``, ``End``: like the sacct equivalents,\n  but unixtime.  Assume that the sacct timestamps are in localtime of\n  the machine doing the conversion.  (``slurm2sql.unixtime`` converts\n  slurm-format timestamp to unixtime)\n\n* Job IDs.  Slurm Job ID is by default of format\n  ``JobID.JobStep`` or ``ArrayJobID_ArrayTaskID.JobStep``.\n  Furthermore, each array job has a \"Raw JobID\" (different for each\n  job, and is an actual JobID) in addition to the \"ArrayJobID\" which\n  is the same for all jobs in an array.  We split all of these\n  different IDs into the following fields:\n\n  * ``JobID``: Only the integer Job ID, without the trailing array\n    tasks or job IDs.  For array jobs, this is the \"Raw JobID\" as\n    described above, use ``ArrayJobID`` to filter jobs that are the\n    same.  Integer\n\n  * ``ArrayJobID``: The common array ID for all jobs in an array -\n    only.  For non-array jobs, same as JobID.  Integer or null.\n\n  * ``ArrayTaskID``: As used above.  Integer on null.\n\n  * ``JobStep``: Job step - only.  If you SQL filter for ``StepID is\n    null`` you get only the main allocations.  String.\n\n  * ``JobIDSlurm``: The raw output from sacct JobID field, including\n    ``.`` and ``_``.  String.\n\n* ``ReqMem``: The raw slurm value in a format like \"5Gn\".  Instead of\n  parsing this, you probably want to use one of the other values below.\n\n* ``ReqMemNode``, ``ReqMemCPU``: Requested memory per node or CPU,\n  either taken from ReqMem (if it matches) or computed (you might want\n  to check our logic if you rely on this).  In Slurm, you\n  can request memory either per-node or per-core, and this calculates\n  the other one for you.\n\n* ``ReqMemType``: ``c`` if the user requested mem-per-core originally,\n  ``n`` if mem-per-node.  Extracted from ``ReqMem``.\n\n* ``ReqMemRaw``: The numeric value of the ``ReqMem``, whether it is\n  ``c`` or ``n``.\n\n* ``ReqGPU``: Number of GPUs requested.  Extracted from ``ReqGRES``.\n\n* GPU information.  At Aalto we store GPU usage information in the\n  ``Comment`` field in JSON of the form ``{\"gpu_util\": NN.NN,\n  \"gpu_max_mem\": NN, \"ngpu\": N}``.  This extracts information from that.\n\n  * ``GPUMem``: Max amount of memory used from any GPU.  Note: all GPU\n    stats require a separate Aalto-developed script.\n\n  * ``GPUEff``: Percent usage of the GPUs (0.0-1.0).\n\n  * ``NGPU``: Number of GPUs.  Should be the same as ``ReqGPU``, but\n    who knows.\n\n* ``MemEff``: Memory efficiency (0.0-1.0).  Like in ``seff``.  We\n  compute it ourselves, so it could be wrong.  Test before trusting!\n  There can still be corner cases, job steps may be off, etc.  This\n  also relies on memory reporting being correct, which may not be the\n  case...\n\n* ``CPUEff``: CPU efficiency (0.0-1.0).  All the same caveats as above\n  apply: test before trusting.\n\nQuick reference of the other most important columns from the\naccounting database:\n\n* ``Elapsed``: Wall clock time\n\n* ``CPUTime``: Reserved CPU time (Elapsed * number of CPUs).  CPUEff \u2248\n  TotalCPU/CPUTime = TotalCPU/(NCPUs x Elapsed)\n\n* ``TotalCPU``: SystemCPU + TotalCPU, seconds of productive work.\n\n\n\n\nDevelopment and maintenance\n---------------------------\n\nThis could be considered functional alpha right or almost beta now.\n\nRelease process::\n\n  python setup.py sdist bdist_wheel\n  twine upload [--repository-url https://test.pypi.org/legacy/] dist/*0.9.0*\n\n\nOriginally developed at Aalto University, Finland.\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/NordicHPC/slurm2sql", "keywords": "slurm sqlite3", "license": "", "maintainer": "", "maintainer_email": "", "name": "slurm2sql", "package_url": "https://pypi.org/project/slurm2sql/", "platform": "", "project_url": "https://pypi.org/project/slurm2sql/", "project_urls": {"Homepage": "https://github.com/NordicHPC/slurm2sql"}, "release_url": "https://pypi.org/project/slurm2sql/0.9.0/", "requires_dist": null, "requires_python": ">= 2.7, >=3.4", "summary": "Import Slurm accounting database from sacct to sqlite3 database", "version": "0.9.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This contains one utility, <tt>slurm2sql</tt>, which uses the <a href=\"https://slurm.schedmd.com/overview\" rel=\"nofollow\">Slurm</a> workload manager\u2019s <tt>sacct</tt>,\nto export all statistics from jobs and load them to a well-formed\nsqlite3 file.  This file can then be queried for analytics much more\neasily than the raw database or your own exports.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>There is only a single file with no dependencies.  Python greater than\n2.7 is required.</p>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>Sample usage:</p>\n<pre>slurm2sql.py [output_db] -- [sacct selection options]\n</pre>\n<p>For example, to get all data from July and August (<tt><span class=\"pre\">-S</span></tt>) for all\nusers (<tt><span class=\"pre\">-a</span></tt>):</p>\n<pre>slurm2sql.py sincejuly.sqlite3 -- -S 2019-07-1 -a\n</pre>\n<p>To get the data from the last <em>N</em> days.  This will, day by day, get\neach of these history and cumulatively update the database.  This\nupdates a database by default, so that it can be used every day in\norder to efficiently keep a running database.  The <tt><span class=\"pre\">-u</span></tt> option means\n\u201cdon\u2019t delete existing database\u201d (jobs with the same JobID get\nupdated, not duplicated):</p>\n<pre>slurm2sql.py --history-days=N -u sincejuly.sqlite3 -- -a\n</pre>\n<p>The <tt><span class=\"pre\">--history-start=YYYY-MM-DD</span></tt> option can do a similar thing\nstarting from a certain day, and <tt><span class=\"pre\">--history=DD-HH:MM:SS</span></tt> starts\ncollecting from a given interval of time ago (the time format is as in\nSlurm).</p>\n<p>It can also be used from Python as what is essentially a glorified\nparser:</p>\n<pre>db = sqlite3.connect(':memory:')\nslurm2sql.slurm2sql(db, ['-S', '2019-08-26'])\n\n# For example, you can then convert to a dataframe:\nimport pandas as pd\ndf = pd.read_sql('SELECT * FROM slurm', db)\n</pre>\n</div>\n<div id=\"database-format\">\n<h2>Database format</h2>\n<p>There is one table with name <tt>slurm</tt>.</p>\n<p>There is one row for each item returned by <tt>sacct</tt>.</p>\n<p>In general, there is one column for each item returned by <tt>sacct</tt>,\nbut some of them are converted into a more useful form.  Some columns\nare added by re-processing other columns.  In general, just use the\nsource.  See <tt>COLUMNS</tt> in <tt>slurm2sql.py</tt> for details.  Extra\ncolumns can easily be added.</p>\n<p>There are two types of converter functions: easy ones, which map one\nslurm column directly to a database column via a function, and line\nfunctions, which take the whole row and can do arbitrary remixing of\nthe data.</p>\n<div id=\"columns\">\n<h3>Columns</h3>\n<p>All column values are converted to standard units: <em>bytes</em> (not MB,\nKB, etc), <em>seconds</em>, <em>fraction 0.0-1.0</em> for things like\npercentages.</p>\n<p>Below are some notable columns which do not exist in sacct.  It\u2019s good\nto verify that any of our custom columns make sense before trusting\nthem.  For other columns, check <tt>man sacct</tt>.</p>\n<ul>\n<li><tt>Time</tt>: approximation of last active time of a job.  The first of\nthese that exists: <tt>End</tt>, <tt>Start</tt>, <tt>Submitted</tt>.  This is\nintended to be used when you need to classify a job by when it ran,\nbut you don\u2019t care to be that specific.  (Only the Time column is\nindexed by default, not the other times)</li>\n<li><tt>Submit</tt>, <tt>Start</tt>, <tt>End</tt>: like the sacct equivalents,\nbut unixtime.  Assume that the sacct timestamps are in localtime of\nthe machine doing the conversion.  (<tt>slurm2sql.unixtime</tt> converts\nslurm-format timestamp to unixtime)</li>\n<li>Job IDs.  Slurm Job ID is by default of format\n<tt>JobID.JobStep</tt> or <tt>ArrayJobID_ArrayTaskID.JobStep</tt>.\nFurthermore, each array job has a \u201cRaw JobID\u201d (different for each\njob, and is an actual JobID) in addition to the \u201cArrayJobID\u201d which\nis the same for all jobs in an array.  We split all of these\ndifferent IDs into the following fields:<ul>\n<li><tt>JobID</tt>: Only the integer Job ID, without the trailing array\ntasks or job IDs.  For array jobs, this is the \u201cRaw JobID\u201d as\ndescribed above, use <tt>ArrayJobID</tt> to filter jobs that are the\nsame.  Integer</li>\n<li><tt>ArrayJobID</tt>: The common array ID for all jobs in an array -\nonly.  For non-array jobs, same as JobID.  Integer or null.</li>\n<li><tt>ArrayTaskID</tt>: As used above.  Integer on null.</li>\n<li><tt>JobStep</tt>: Job step - only.  If you SQL filter for <tt>StepID is\nnull</tt> you get only the main allocations.  String.</li>\n<li><tt>JobIDSlurm</tt>: The raw output from sacct JobID field, including\n<tt>.</tt> and <tt>_</tt>.  String.</li>\n</ul>\n</li>\n<li><tt>ReqMem</tt>: The raw slurm value in a format like \u201c5Gn\u201d.  Instead of\nparsing this, you probably want to use one of the other values below.</li>\n<li><tt>ReqMemNode</tt>, <tt>ReqMemCPU</tt>: Requested memory per node or CPU,\neither taken from ReqMem (if it matches) or computed (you might want\nto check our logic if you rely on this).  In Slurm, you\ncan request memory either per-node or per-core, and this calculates\nthe other one for you.</li>\n<li><tt>ReqMemType</tt>: <tt>c</tt> if the user requested mem-per-core originally,\n<tt>n</tt> if mem-per-node.  Extracted from <tt>ReqMem</tt>.</li>\n<li><tt>ReqMemRaw</tt>: The numeric value of the <tt>ReqMem</tt>, whether it is\n<tt>c</tt> or <tt>n</tt>.</li>\n<li><tt>ReqGPU</tt>: Number of GPUs requested.  Extracted from <tt>ReqGRES</tt>.</li>\n<li>GPU information.  At Aalto we store GPU usage information in the\n<tt>Comment</tt> field in JSON of the form <tt>{\"gpu_util\": NN.NN,\n\"gpu_max_mem\": NN, \"ngpu\": N}</tt>.  This extracts information from that.<ul>\n<li><tt>GPUMem</tt>: Max amount of memory used from any GPU.  Note: all GPU\nstats require a separate Aalto-developed script.</li>\n<li><tt>GPUEff</tt>: Percent usage of the GPUs (0.0-1.0).</li>\n<li><tt>NGPU</tt>: Number of GPUs.  Should be the same as <tt>ReqGPU</tt>, but\nwho knows.</li>\n</ul>\n</li>\n<li><tt>MemEff</tt>: Memory efficiency (0.0-1.0).  Like in <tt>seff</tt>.  We\ncompute it ourselves, so it could be wrong.  Test before trusting!\nThere can still be corner cases, job steps may be off, etc.  This\nalso relies on memory reporting being correct, which may not be the\ncase\u2026</li>\n<li><tt>CPUEff</tt>: CPU efficiency (0.0-1.0).  All the same caveats as above\napply: test before trusting.</li>\n</ul>\n<p>Quick reference of the other most important columns from the\naccounting database:</p>\n<ul>\n<li><tt>Elapsed</tt>: Wall clock time</li>\n<li><tt>CPUTime</tt>: Reserved CPU time (Elapsed * number of CPUs).  CPUEff \u2248\nTotalCPU/CPUTime = TotalCPU/(NCPUs x Elapsed)</li>\n<li><tt>TotalCPU</tt>: SystemCPU + TotalCPU, seconds of productive work.</li>\n</ul>\n</div>\n</div>\n<div id=\"development-and-maintenance\">\n<h2>Development and maintenance</h2>\n<p>This could be considered functional alpha right or almost beta now.</p>\n<p>Release process:</p>\n<pre>python setup.py sdist bdist_wheel\ntwine upload [--repository-url https://test.pypi.org/legacy/] dist/*0.9.0*\n</pre>\n<p>Originally developed at Aalto University, Finland.</p>\n</div>\n\n          </div>"}, "last_serial": 6715662, "releases": {"0.9.0": [{"comment_text": "", "digests": {"md5": "0b3f55fa545c2f276cb9c67f680ca1e0", "sha256": "b0aba660c612cc833d5d444ff8c7a27c778370554c08b690758f6cc511c0666b"}, "downloads": -1, "filename": "slurm2sql-0.9.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0b3f55fa545c2f276cb9c67f680ca1e0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">= 2.7, >=3.4", "size": 12486, "upload_time": "2020-02-28T06:55:57", "upload_time_iso_8601": "2020-02-28T06:55:57.605941Z", "url": "https://files.pythonhosted.org/packages/3a/85/b2c1c436ff82009b22a2d16d1e2580429b56d74b1641998924daa9f2090f/slurm2sql-0.9.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8a6b9e2030b335c46afe75bf49823ed4", "sha256": "b3c9269c9421c6ba4c03c3e589da7b8c2e1ce70f9e6ce0dd78b534c6ed431251"}, "downloads": -1, "filename": "slurm2sql-0.9.0.tar.gz", "has_sig": false, "md5_digest": "8a6b9e2030b335c46afe75bf49823ed4", "packagetype": "sdist", "python_version": "source", "requires_python": ">= 2.7, >=3.4", "size": 14521, "upload_time": "2020-02-28T06:56:00", "upload_time_iso_8601": "2020-02-28T06:56:00.539001Z", "url": "https://files.pythonhosted.org/packages/82/06/9c812582e4ea59355827734b42c08bfc4467ee58c05469207ec37d431a45/slurm2sql-0.9.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0b3f55fa545c2f276cb9c67f680ca1e0", "sha256": "b0aba660c612cc833d5d444ff8c7a27c778370554c08b690758f6cc511c0666b"}, "downloads": -1, "filename": "slurm2sql-0.9.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0b3f55fa545c2f276cb9c67f680ca1e0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">= 2.7, >=3.4", "size": 12486, "upload_time": "2020-02-28T06:55:57", "upload_time_iso_8601": "2020-02-28T06:55:57.605941Z", "url": "https://files.pythonhosted.org/packages/3a/85/b2c1c436ff82009b22a2d16d1e2580429b56d74b1641998924daa9f2090f/slurm2sql-0.9.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8a6b9e2030b335c46afe75bf49823ed4", "sha256": "b3c9269c9421c6ba4c03c3e589da7b8c2e1ce70f9e6ce0dd78b534c6ed431251"}, "downloads": -1, "filename": "slurm2sql-0.9.0.tar.gz", "has_sig": false, "md5_digest": "8a6b9e2030b335c46afe75bf49823ed4", "packagetype": "sdist", "python_version": "source", "requires_python": ">= 2.7, >=3.4", "size": 14521, "upload_time": "2020-02-28T06:56:00", "upload_time_iso_8601": "2020-02-28T06:56:00.539001Z", "url": "https://files.pythonhosted.org/packages/82/06/9c812582e4ea59355827734b42c08bfc4467ee58c05469207ec37d431a45/slurm2sql-0.9.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:08:12 2020"}