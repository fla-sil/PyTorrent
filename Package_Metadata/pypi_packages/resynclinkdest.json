{"info": {"author": "Vincent Pelletier", "author_email": "plr.vincent@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: System Administrators", "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy", "Topic :: System :: Archiving :: Backup"], "description": ".. contents::\n\nRe-hardlink directories created using ``rsync --link-dest``.\n\n``rsync --link-dest``\n=====================\n\nThis is a nice way to make backups which are trivial to restore (cp -a) and use\nlittle disk space. Low-tech enough that I trust it when I often have new files\nrather than modified large files.\n\nThe problem\n===========\n\nSometimes, because of a user mistake or a bug, you get a new full backup\nhappening. As a result, a lot of disk space is suddenly allocated for a backup\nwhere little changed. This program is a shot at curing this situation after it\nhappened.\n\nThe algorithm\n=============\n\nLet's take the following simple backup content::\n\n  /some/place/2017-01-01/home/foo/bar\n  /some/place/2017-01-01/home/foo/boo\n  /some/place/2017-01-02/home/foo/bar\n  /some/place/2017-01-02/home/foo/baz\n  /some/place/2017-01-03/home/foo/bar\n  /some/place/2017-01-03/home/foo/baz\n  /some/place/2017-01-03/home/foo/boo\n\nIt can be seen as a sparse 2d grid where cells in each row are strongly\nrelated (they are more likely to be the same file than accross rows):\n\n+----------------+-------+----------------+----------------+---------------+\n| Relative path\u2193 | Head\u2192 | ``2017-01-01`` | ``2017-01-02`` | ``2017-01-03``|\n+================+=======+================+================+===============+\n|``/home/foo/bar``       | inode 1                         | inode 3       |\n+------------------------+----------------+----------------+---------------+\n|``/home/foo/baz``       | (n/a)          | inode 2                        |\n+------------------------+----------------+----------------+---------------+\n|``/home/foo/boo``       | inode 4        | (n/a)          | inode 5       |\n+------------------------+----------------+----------------+---------------+\n\nThis tool takes the ``heads`` as arguments (ex: one folder per day).\nIt resurses through the first head, and, for each non-directory path found,\niterates over the heads on its right, building mapping between inodes and the\nheads each inode is in:\n\n.. code:: python\n\n  # relative_path = \"/home/foo/bar\"\n  {\n    1: [\"2017-01-01\", \"2017-01-02\"],\n    3: [\"2017-03-03\"],\n  }\n\nThen, each inode is compared to the others (by picking an arbitrary file in the\nlist), and each inode identical to a given one gets unlinked and relinked to\nthat inode.\n\nOptionally (enabled by default), the code then remembers the (relative) path it\njust analysed, so that it does not analyse it again when iterating over next\nhead. This is very efficient, but makes memory usage proportional to the number\nof unique relative paths and their length. In my experience, with pypy, this is\nunder 200MB for over 300k unique relative paths.\n\nThen it goes to the next relative path in current head. Then it moves on to the\nnext head to process files which did not yet exist in current head.\n\nAs a result, execution time is dominated by the number of unique relative paths\ninstead of the total number of files. For the example above, it will read these\nfiles, in this order, this number of times::\n\n  /some/place/2017-01-01/home/foo/bar\n  /some/place/2017-01-03/home/foo/bar\n  /some/place/2017-01-01/home/foo/boo\n  /some/place/2017-01-03/home/foo/boo\n  /some/place/2017-01-02/home/foo/baz\n\nAssuming inodes 1 and 2 are identical, and 4 and 5 are identical, optimal\n``rsync --link-dest`` (so without users & code issues) would have produced:\n\n+----------------+-------+----------------+----------------+---------------+\n| Relative path\u2193 | Head\u2192 | ``2017-01-01`` | ``2017-01-02`` | ``2017-01-03``|\n+================+=======+================+================+===============+\n|``/home/foo/bar``       | inode 1                                         |\n+------------------------+----------------+----------------+---------------+\n|``/home/foo/baz``       | (n/a)          | inode 2                        |\n+------------------------+----------------+----------------+---------------+\n|``/home/foo/boo``       | inode 4        | (n/a)          | inode 5       |\n+------------------------+----------------+----------------+---------------+\n\nAnd this script will produce:\n\n+----------------+-------+----------------+----------------+---------------+\n| Relative path\u2193 | Head\u2192 | ``2017-01-01`` | ``2017-01-02`` | ``2017-01-03``|\n+================+=======+================+================+===============+\n|``/home/foo/bar``       | inode 1                                         |\n+------------------------+----------------+----------------+---------------+\n|``/home/foo/baz``       | (n/a)          | inode 2                        |\n+------------------------+----------------+----------------+---------------+\n|``/home/foo/boo``       | inode 4        | (n/a)          | inode 4       |\n+------------------------+----------------+----------------+---------------+\n\nAn advantage of this method compared to other hard-linkers is that it freezes\nspace regularly while running by unlinking all homonym duplicates at once.\n\nA disadvantage of this method compared to other hard-linkers is that it will not\nlink together identical files if their name differs. But neither would\n``rsync --link-dest``, so it would not be our backup too of choice.", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/vpelletier/resynclinkdest", "keywords": "rsync --link-dest hardlink", "license": "GPL 3+", "maintainer": "", "maintainer_email": "", "name": "resynclinkdest", "package_url": "https://pypi.org/project/resynclinkdest/", "platform": "any", "project_url": "https://pypi.org/project/resynclinkdest/", "project_urls": {"Homepage": "http://github.com/vpelletier/resynclinkdest"}, "release_url": "https://pypi.org/project/resynclinkdest/1.0.1/", "requires_dist": null, "requires_python": "", "summary": "Re-hardlink directories created using ``rsync --link-dest``.", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"contents\">\n<p>Contents</p>\n<ul>\n<li><a href=\"#rsync-link-dest\" id=\"id1\" rel=\"nofollow\"><tt>rsync <span class=\"pre\">--link-dest</span></tt></a></li>\n<li><a href=\"#the-problem\" id=\"id2\" rel=\"nofollow\">The problem</a></li>\n<li><a href=\"#the-algorithm\" id=\"id3\" rel=\"nofollow\">The algorithm</a></li>\n</ul>\n</div>\n<p>Re-hardlink directories created using <tt>rsync <span class=\"pre\">--link-dest</span></tt>.</p>\n<div id=\"rsync-link-dest\">\n<h2><a href=\"#id1\" rel=\"nofollow\"><tt>rsync <span class=\"pre\">--link-dest</span></tt></a></h2>\n<p>This is a nice way to make backups which are trivial to restore (cp -a) and use\nlittle disk space. Low-tech enough that I trust it when I often have new files\nrather than modified large files.</p>\n</div>\n<div id=\"the-problem\">\n<h2><a href=\"#id2\" rel=\"nofollow\">The problem</a></h2>\n<p>Sometimes, because of a user mistake or a bug, you get a new full backup\nhappening. As a result, a lot of disk space is suddenly allocated for a backup\nwhere little changed. This program is a shot at curing this situation after it\nhappened.</p>\n</div>\n<div id=\"the-algorithm\">\n<h2><a href=\"#id3\" rel=\"nofollow\">The algorithm</a></h2>\n<p>Let\u2019s take the following simple backup content:</p>\n<pre>/some/place/2017-01-01/home/foo/bar\n/some/place/2017-01-01/home/foo/boo\n/some/place/2017-01-02/home/foo/bar\n/some/place/2017-01-02/home/foo/baz\n/some/place/2017-01-03/home/foo/bar\n/some/place/2017-01-03/home/foo/baz\n/some/place/2017-01-03/home/foo/boo\n</pre>\n<p>It can be seen as a sparse 2d grid where cells in each row are strongly\nrelated (they are more likely to be the same file than accross rows):</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>Relative path\u2193</th>\n<th>Head\u2192</th>\n<th><tt><span class=\"pre\">2017-01-01</span></tt></th>\n<th><tt><span class=\"pre\">2017-01-02</span></tt></th>\n<th><tt><span class=\"pre\">2017-01-03</span></tt></th>\n</tr>\n</thead>\n<tbody>\n<tr><td><tt>/home/foo/bar</tt></td>\n<td>inode 1</td>\n<td>inode 3</td>\n</tr>\n<tr><td><tt>/home/foo/baz</tt></td>\n<td>(n/a)</td>\n<td>inode 2</td>\n</tr>\n<tr><td><tt>/home/foo/boo</tt></td>\n<td>inode 4</td>\n<td>(n/a)</td>\n<td>inode 5</td>\n</tr>\n</tbody>\n</table>\n<p>This tool takes the <tt>heads</tt> as arguments (ex: one folder per day).\nIt resurses through the first head, and, for each non-directory path found,\niterates over the heads on its right, building mapping between inodes and the\nheads each inode is in:</p>\n<pre><span class=\"c1\"># relative_path = \"/home/foo/bar\"</span>\n<span class=\"p\">{</span>\n  <span class=\"mi\">1</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"2017-01-01\"</span><span class=\"p\">,</span> <span class=\"s2\">\"2017-01-02\"</span><span class=\"p\">],</span>\n  <span class=\"mi\">3</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"2017-03-03\"</span><span class=\"p\">],</span>\n<span class=\"p\">}</span>\n</pre>\n<p>Then, each inode is compared to the others (by picking an arbitrary file in the\nlist), and each inode identical to a given one gets unlinked and relinked to\nthat inode.</p>\n<p>Optionally (enabled by default), the code then remembers the (relative) path it\njust analysed, so that it does not analyse it again when iterating over next\nhead. This is very efficient, but makes memory usage proportional to the number\nof unique relative paths and their length. In my experience, with pypy, this is\nunder 200MB for over 300k unique relative paths.</p>\n<p>Then it goes to the next relative path in current head. Then it moves on to the\nnext head to process files which did not yet exist in current head.</p>\n<p>As a result, execution time is dominated by the number of unique relative paths\ninstead of the total number of files. For the example above, it will read these\nfiles, in this order, this number of times:</p>\n<pre>/some/place/2017-01-01/home/foo/bar\n/some/place/2017-01-03/home/foo/bar\n/some/place/2017-01-01/home/foo/boo\n/some/place/2017-01-03/home/foo/boo\n/some/place/2017-01-02/home/foo/baz\n</pre>\n<p>Assuming inodes 1 and 2 are identical, and 4 and 5 are identical, optimal\n<tt>rsync <span class=\"pre\">--link-dest</span></tt> (so without users &amp; code issues) would have produced:</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>Relative path\u2193</th>\n<th>Head\u2192</th>\n<th><tt><span class=\"pre\">2017-01-01</span></tt></th>\n<th><tt><span class=\"pre\">2017-01-02</span></tt></th>\n<th><tt><span class=\"pre\">2017-01-03</span></tt></th>\n</tr>\n</thead>\n<tbody>\n<tr><td><tt>/home/foo/bar</tt></td>\n<td>inode 1</td>\n</tr>\n<tr><td><tt>/home/foo/baz</tt></td>\n<td>(n/a)</td>\n<td>inode 2</td>\n</tr>\n<tr><td><tt>/home/foo/boo</tt></td>\n<td>inode 4</td>\n<td>(n/a)</td>\n<td>inode 5</td>\n</tr>\n</tbody>\n</table>\n<p>And this script will produce:</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>Relative path\u2193</th>\n<th>Head\u2192</th>\n<th><tt><span class=\"pre\">2017-01-01</span></tt></th>\n<th><tt><span class=\"pre\">2017-01-02</span></tt></th>\n<th><tt><span class=\"pre\">2017-01-03</span></tt></th>\n</tr>\n</thead>\n<tbody>\n<tr><td><tt>/home/foo/bar</tt></td>\n<td>inode 1</td>\n</tr>\n<tr><td><tt>/home/foo/baz</tt></td>\n<td>(n/a)</td>\n<td>inode 2</td>\n</tr>\n<tr><td><tt>/home/foo/boo</tt></td>\n<td>inode 4</td>\n<td>(n/a)</td>\n<td>inode 4</td>\n</tr>\n</tbody>\n</table>\n<p>An advantage of this method compared to other hard-linkers is that it freezes\nspace regularly while running by unlinking all homonym duplicates at once.</p>\n<p>A disadvantage of this method compared to other hard-linkers is that it will not\nlink together identical files if their name differs. But neither would\n<tt>rsync <span class=\"pre\">--link-dest</span></tt>, so it would not be our backup too of choice.</p>\n</div>\n\n          </div>"}, "last_serial": 3100710, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "c132dcd846f75fcb8c6a1fb8bd053716", "sha256": "4ada7215f058bcba383b9631fb7abd9d5e3020041f5bc2426d5e6316f128a16b"}, "downloads": -1, "filename": "resynclinkdest-1.0.tar.gz", "has_sig": false, "md5_digest": "c132dcd846f75fcb8c6a1fb8bd053716", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3534, "upload_time": "2017-08-15T10:55:50", "upload_time_iso_8601": "2017-08-15T10:55:50.541414Z", "url": "https://files.pythonhosted.org/packages/37/99/ff7113f387a7261d52b03cb489688b52dd01b6fb62d8c5dea984f08251e3/resynclinkdest-1.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "cb285df8186f643250e66ae73e3aa2e8", "sha256": "fb48b3ab55115b9c70fafbef0506b6f660084796962e98afd82d95519aa71a7f"}, "downloads": -1, "filename": "resynclinkdest-1.0.1.tar.gz", "has_sig": false, "md5_digest": "cb285df8186f643250e66ae73e3aa2e8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5506, "upload_time": "2017-08-16T13:27:27", "upload_time_iso_8601": "2017-08-16T13:27:27.894330Z", "url": "https://files.pythonhosted.org/packages/31/7f/b2f1c2ada4e69cd8c80e50190c4966d01f7918d704d763f734017f138572/resynclinkdest-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cb285df8186f643250e66ae73e3aa2e8", "sha256": "fb48b3ab55115b9c70fafbef0506b6f660084796962e98afd82d95519aa71a7f"}, "downloads": -1, "filename": "resynclinkdest-1.0.1.tar.gz", "has_sig": false, "md5_digest": "cb285df8186f643250e66ae73e3aa2e8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5506, "upload_time": "2017-08-16T13:27:27", "upload_time_iso_8601": "2017-08-16T13:27:27.894330Z", "url": "https://files.pythonhosted.org/packages/31/7f/b2f1c2ada4e69cd8c80e50190c4966d01f7918d704d763f734017f138572/resynclinkdest-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:03:27 2020"}