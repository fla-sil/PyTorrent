{"info": {"author": "Victor Alejandro Gil Sepulveda", "author_email": "victor.gil.sepulveda@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "pyProCT\n=======\n\npyProCT is an open source cluster analysis software especially adapted\nfor jobs related with structural proteomics. Its approach allows users\nto define a clustering goal (clustering hypothesis) based on their\ndomain knowledge. This hypothesis will guide the software in order to\nfind the best algorithm and parameters (including the number of\nclusters) to obtain the result that better fulfills their expectatives.\nIn this way users do not need to use cluster analysis algorithms as a\nblack box, which will (hopefully) improve their results. pyProCT not\nonly generates a resulting clustering, it also implements some use cases\nlike the extraction of representatives or trajectory redundance\nelimination.\n\n`Table of Contents <http://doctoc.herokuapp.com/>`_\n\n-  `pyProCT <#user-content-pyproct>`_\n\n   -  `Installation <#user-content-installation>`_\n   -  `Using pyProCT as a standalone\n      program <#user-content-using-pyproct-as-a-standalone-program>`_\n\n      -  `Global <#user-content-global>`_\n      -  `Data <#user-content-data>`_\n      -  `Clustering <#user-content-clustering>`_\n\n         -  `generation <#user-content-generation>`_\n         -  `algorithms <#user-content-algorithms>`_\n         -  `evaluation <#user-content-evaluation>`_\n\n      -  `Postprocessing <#user-content-postprocessing>`_\n      -  `Checking the script <#user-content-checking-the-script>`_\n      -  `Learn more <#user-content-learn-more>`_\n\n   -  `Using pyProCT as part of other\n      programs <#user-content-using-pyproct-as-part-of-other-programs>`_\n\n      -  `Using it as a separate program from other Python\n         script <#user-content-using-it-as-a-separate-program-from-other-python-script>`_\n\n   -  `Parallel execution <#user-content-parallel-execution>`_\n\n-  `Documentation <#user-content-documentation>`_\n-  `TODO <#user-content-todo>`_\n\nInstallation\n------------\n\npyProCT is quite easy to install using *pip*. Just write:\n\n``Shell > sudo pip install pyProCT`` And *pip* will take care of all the\ndependencies (shown below).\n\n It is recommended to install Numpy and Scipy before starting the\ninstallation using your OS software manager. You can try to download and\ninstall them\n`manually <http://docs.scipy.org/doc/numpy/user/install.html>`_ if you\ndare.\n\n mpi4py is pyProCT's last dependency. It can give problems when\ninstalling it in OS such as SUSE. If the installation of this last\npackage is not succesful, pyProCT can still work in Serial and Parallel\n(using *multiprocessing*) modes.\n\nUsing pyProCT as a standalone program\n-------------------------------------\n\nThe preferred way to use pyProCT is through a JSON \"script\" that\ndescribes the clustering task. It can be executed using the following\nline in your shell:\n\n::\n\n    > python -m pyproct.main script.json\n\nThe JSON script has 4 main parts, each one dealing with a different\naspect of the clustering pipeline. This sections are: \\* *\"global\"*:\nHandles workspace and scheduler parameterization. \\* *\"data\"*: Handles\ndistance matrix parameterization. \\* *\"clustering\"*: Handles algorithms\nand evaluation parameterization. \\* *\"preprocessing\"*: Handles what to\ndo with the clustering we have calculated.\n\n::\n\n    {\n        \"global\":{},\n        \"data\":{},\n        \"clustering\":{},\n        \"postprocessing\":{}\n    }\n\nGlobal\n~~~~~~\n\n``JSON {     \"control\": {         \"scheduler_type\": \"Process/Parallel\",         \"number_of_processes\": 4     },     \"workspace\": {          \"tmp\": \"tmp\",          \"matrix\": \"matrix\",          \"clusterings\": \"clusterings\",          \"results\": \"results\",          \"base\": \"/home/john/ClusteringProject\"     } }``\nThis is an example of *\"global\"* section. It describes the work\nenvironment (workspace) and the type of scheduler that will be built.\nDefining the subfolders of the wokspace is not mandatory, however it may\nbe convenient in some scenarios (for instance, in serial multiple\nclustering projects, sharing the *tmp* folder would lower the disk usage\nas at each step it will be overwritten).\n\nThis is a valid global section using a serial scheduler and default\nnames for workspace inner folders:\n``JSON {     \"control\": {         \"scheduler_type\": \"Serial\"     },     \"workspace\": {          \"base\": \"/home/john/ClusteringProject\"     } }``\npyProCT allows the use of 3 different schedulers that help to improve\nthe overall performance of the software by parallelizing some parts of\nthe code. The available schedulers are \"Serial\", \"Process/Parallel\"\n(uses Python's\n`multiprocessing <https://docs.python.org/2/library/multiprocessing.html>`_)\nand \"MPI/Parallel\" (uses MPI through the module\n`mpi4py <http://mpi4py.scipy.org/>`_).\n\nData\n~~~~\n\nThe *\"data\"* section defines how pyProCT must build the distance matrix\nthat will be used by the compression algorithms. Currently pyProCT\noffers up to three options to build that matrix: \"load\", \"rmsd\" and\n\"distance\" - *\"rmsd\"*: Calculates a all vs all rmsd matrix using any of\nthe\n`pyRMSD <https://github.com/victor-gil-sepulveda/pyRMSD#collective-operations>`_\ncalculators available. It can calculate the RMSD of the fitted region\n(defined by `Prody <http://prody.csb.pitt.edu/>`_ compatible selection\nstring in *fit\\_selection*) or one can use one selection to superimpose\nand another to calculate the rmsd (*calc\\_selection*). There are two\nextra *parameters* that must be considered when building an RMSD matrix.\n- *\"type\"*: This property can have two values: *\"COORDINATES\"* or\n*\"DIHEDRALS\"*. If *DIHEDRALS* is chosen, each element *(i,j)* of the\ndistance matrix will be the RMSD of the arrays containing the phi-psi\ndihedral angle series of conformation *i* and *j*. - *\"chain\\_map\"*: If\nset to *true* pyProCT will try to reorder the chains of the biomolecule\nin order to minimize the global RMSD value. This means that it will\ncorrectly calculate the RMSD even if chain coordinates were permuted in\nsome way. The price to pay is an increase of the calculation time\n(directly proportional to the number of chains or the number of chains\nhaving the same length). - *\"distance\"*: After superimposing the\nselected region it calculates the all vs all distances of the\ngeometrical center of the region of interest (*body\\_selection*). -\n*\"load\"*: Loads a precalculated matrix.\n\nJSON chunk needed to generate an RMSD matrix from two trajectories:\n``JSON {     \"type\": \"pdb_ensemble\",     \"files\": [         \"A.pdb\",         \"B.pdb\"     ],     \"matrix\": {         \"method\": \"rmsd\",         \"parameters\": {             \"calculator_type\": \"QCP_OMP_CALCULATOR\",             \"fit_selection\": \"backbone\",         },         \"image\": {             \"filename\": \"matrix_plot\"         },         \"filename\":\"matrix\"     } }``\nJSON chunk to generate a dihedral angles RMSD matrix from one\ntrajectories:\n``JSON {     \"type\": \"pdb_ensemble\",     \"files\": [         \"A.pdb\"     ],     \"matrix\": {         \"method\": \"rmsd\",         \"parameters\": {             \"type\":\"DIHEDRAL\"         },         \"image\": {             \"filename\": \"matrix_plot\"         },         \"filename\":\"matrix\"     } }``\n\nThe matrix can be stored if the *filename* property is defined. The\nmatrix can also be stored as an image if the *image* property is\ndefined.\n\npyProCT can currently load *pdb* and *dcd* files. The details to load\nthe files must be written into the array under the \"files\" keyword.\nThere are many ways of telling pyProCT the files that have to be load\nand can be combined in any way you like:\n\n1. Using a list of file paths. If the file extension is \".txt\" or\n   \".list\" it will be treated as a pdb list file. Each line of such\n   files will be a pdb path or a pdb path and a selection string,\n   separated by comma.\n\n``A.pdb, name CA B.pdb C.pdb, name CA ...`` 2. Using a list of file\nobjects: ``JSON {     \"file\": ... ,     \"base_selection\": ... }`` Where\n*base\\_selection* is a `Prody <http://prody.csb.pitt.edu/>`_ compatible\nselection string. Loading files this way can help in cases where not all\nfiles have structure with the same number of atoms: *base\\_selection*\nshould define the common region between them (if a 1 to 1 map does not\nexist, the RMSD calculation will be wrong).\n\n3. Only for *dcd* files:\n   ``JSON {     \"file\": ...,     \"atoms_file\": ...,     \"base_selection\": ... }``\n   Where *atoms\\_file* is a *pdb* file with at least one frame that\n   holds the atomic information needed by the *dcd* file.\n\n**Note*: data.type is currently unsused*\n\nClustering\n~~~~~~~~~~\n\nThe *clustering* section specifies how the clustering exploration will\nbe done. It is divided in 3 other subsections:\n\n``JSON {     \"generation\": {         \"method\": \"generate\"     },     \"algorithms\": {         ...     },     \"evaluation\": {         ...     } }``\n#### Generation Defines how to generate the clustering (*\"load\"* or\n*\"generate\"*). if *\"load\"* is chosen, this section will also contain the\nclustering that may be used in the *\"clusters\"* property. Ex.:\n\n::\n\n    {\n        \"clustering\": {\n            \"generation\": {\n                \"method\" : \"load\",\n                \"clusters\": [\n                        {\n                            \"prototype \" : 16,\n                            \"id\": \"cluster_00\",\n                            \"elements\" : \"9, 14:20\"\n                        },\n                        {\n                            \"prototype\": 7,\n                            \"id\": \"cluster_01\",\n                            \"elements\": \"0:8, 10:14, 21\"\n                        }\n                ]\n            }\n    }\n\nAlgorithms\n^^^^^^^^^^\n\nIf clustering.generation.method equals \"generate\", this section defines\nthe algorithms that will be used as well as their parameters (if\nnecessary). The currently available algorithms are : *\"kmedoids\"*,\n*\"hierarchical\"*, *\"dbscan\"*, *\"gromos\"*, *\"spectral\"* and *\"random\"*.\nEach algorithm can store its list of parameters, however the preferred\nway to work with pyProCT is to let it automatically generate them.\nAlmost all algorithms accept the property *max*, that defines the\nmaximum amount of parameter collections that will be generated for that\nalgorithm. Ex.\n\n::\n\n    {\n        \"kmedoids\": {\n            \"seeding_type\": \"RANDOM\",\n            \"max\": 50,\n            \"tries\": 5\n        },\n        \"hierarchical\": {\n\n        },\n        \"dbscan\": {\n            \"max\": 50\n        },\n        \"gromos\": {\n            \"max\": 50\n        },\n        \"spectral\": {\n            \"max\": 50,\n            \"force_sparse\":true,\n        }\n    }\n\nAlgorithm parameters can be explicitly written, however it is not\nrecommended:\n\n::\n\n    {\n        \"kmedoids\": {\n            \"seeding_type\": \"RANDOM\",\n            \"max\": 50,\n            \"tries\": 5,\n            \"parameters\":[{\"k\":4},{\"k\":5},{\"k\":6}]\n        }\n    }\n\nEvaluation\n^^^^^^^^^^\n\nThis section holds the *Clustering Hypothesis*, the core of pyProCT.\nHere the user can define how the expected clustering will be. First the\nuser must set the expected number of clusters range. Also, an estimation\nof the dataset noise and the cluster minimum size (the minimum number of\nelements a cluster must have to not be considered noise) will complete\nthe quantitative definition of the target result.\n\nEx.\n``JSON {     \"maximum_noise\": 15,     \"minimum_cluster_size\": 50,     \"maximum_clusters\": 200,     \"minimum_clusters\": 6,     \"query_types\": [ ... ],     \"evaluation_criteria\": {         ...     } }``\n\nThe second part of the *Clustering Hypothesis* tries to characterize the\nclustering internal traits in a more qualitative way. Concepts like\ncluster \"Compactness\" or \"Separation\" can be used here to define the\nexpected clustering. To this end users must write their expectations in\nform of *criteria*. This criteria are, in general, linear combinations\nof Internal Clustering Validation Indices (ICVs). The best clustering\nwill be the one that gets the best score in any of these *criteria*. See\n`this\ndocument <https://dl.dropboxusercontent.com/u/58918851/icv_info.pdf>`_\nto get more insight about the different implemented criteria and their\nmeaning.\n\nAdditionally users may choose to ask pyProCT about the results of this\nICVs and other evaluation functions(e.g. the average cluster size) by\nadding them to the *queries* array.\n\n::\n\n    {\n            ...\n        \"query_types\": [\n            \"NumClusters\",\n            \"NoiseLevel\",\n            \"MeanClusterSize\"\n        ],\n        \"evaluation_criteria\": {\n            \"criteria_0\": {\n                \"Silhouette\": {\n                    \"action\": \">\",\n                    \"weight\": 1\n                }\n            }\n        }\n    }\n\nPostprocessing\n~~~~~~~~~~~~~~\n\nGetting a good quality clustering is not enough, we would like to use\nthem to extract useful information. pyProCT implements some use cases\nthat may help users to extract this information.\n\n\\`\\`\\`JSON { \"rmsf\":{},\n\n::\n\n    \"centers_and_trace\":{},\n\n    \"representatives\":{\n        \"keep_remarks\": [true/false],\n        \"keep_frame_number\": [true/false]\n    },\n\n    \"pdb_clusters\":{\n        \"keep_remarks\": [true/false],\n        \"keep_frame_number\": [true/false]\n    },\n\n    \"compression\":{\n        \"final_number_of_frames\": INT,\n        \"file\": STRING,\n        \"type\":[\u2018RANDOM\u2019,\u2019KMEDOIDS\u2019]\n    },\n\n    \"cluster_stats\":{\n        \"file\": STRING\n    },\n\n\n    \"conformational_space_comparison\":{},\n\n    \"kullback_liebler\":{}\n\n} \\`\\`\\` - *\"rmsf\"* : Calculates the global and per-cluster (and\nper-residue) root mean square fluctuation (to be visualized using the\n`GUI <https://github.com/victor-gil-sepulveda/pyProCT-GUI>`_).\n\n-  *\"centers\\_and\\_trace\"* : Calculates all geometrical centers of the\n   calculation selection of the system (to be visualized using the\n   `GUI <https://github.com/victor-gil-sepulveda/pyProCT-GUI>`_).\n\n-  *\"representatives\"* : Extracts all the representatives of the\n   clusters in the same pdb.\n    Parameters:\n\n   -  *\"keep\\_remarks\"*: If true every stored model will be written\n      along with its original remarks header. Default: false.\n   -  *\"keep\\_frame\\_number\"*: If true, the model number of any stored\n      conformation will be the original pdb one. Default: false.\n\n-  *\"pdb\\_clusters\"* : Extracts all clusters in separate pdbs.\n    Parameters:\n\n   -  *\"keep\\_remarks\"*: If true every stored model will be written\n      along with its original remarks header. Default: false.\n   -  *\"keep\\_frame\\_number\"*: If true, the model number of any stored\n      conformation will be the original pdb one. Default: false.\n\n-  *\"compression\"* : Reduces the redundancy of the trajectory using the\n   resulting clustering.\n    Parameters:\n\n   -  *\"file\"*: The name of the output file without extension. Default\n      \"compressed\"(.pdb)\n   -  *\"final\\_number\\_of\\_frames\"*: The expected (minimum) number of\n      frames of the compressed file.\n   -  *\"type\"*: The method used to get samples from each cluster.\n      Options:\n\n      -  \"RANDOM\": Gets a random sample of the elements of each cluster.\n      -  \"KMEDOIDS\": Applies the k-medoids algorithm to the elements of\n         a cluster and stores the representatives.\n         Default: \"KMEDOIDS\".\n\n-  *\"cluster\\_stats\"*: Generates a human readable file with the\n   distances between cluster centers and their diameters.\n    Parameters:\n\n   -  *\"file\"*: The name of the output file without extension (will be\n      sotred into the results folder). Default:\n      \"per\\_cluster\\_stats\"(.csv).\n\n-  *\"conformational\\_space\\_comparison\"* : Work in progress.\n\n-  *\"kullback\\_liebler\"* : Work in progress.\n\nScript validation\n~~~~~~~~~~~~~~~~~\n\nAs the control script is indeed holding a JSON object, any JSON\nvalidator can be used to discover the errors in case of script loading\nproblems. A good example of such validators is\n`JSONLint <http://jsonlint.com/>`_. pyProCT scripts accept javascript\ncomments ( // and /\\* \\*/)\n\n Using pyProCT as part of other programs\n----------------------------------------\n\n-  Using algorithms\n-  Clustering from label lists\n-  Using ICVs with custom clusterings\n-  Performing the whole protocol\n   ``Python Driver(Observer()).run(parameters)``\n\nThe necessary documentation to use pyProCT classes is written inside the\ncode. It has been extracted\n`here <pyproct/docs/_build/html/index.html>`_ and\n`here <pyproct/docs/doxyxml/html/index.html>`_. We are currently trying\nto improve this documentation with better explanations and examples.\n\nSee `this\nfile <https://github.com/victor-gil-sepulveda/pyProCT/blob/master/validation/bidimensional/validation_main.py>`_.\n\nUsing it as a separate program from other Python script\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-   Loading results\n-   Generating scripts programatically\n\nSee `this project <https://github.com/victor-gil-sepulveda/PhD-GPCR>`_\nfor some examples.\n\nParallel execution\n------------------\n\nTo execute pyProCT in parallel you just need to issue this line:\n\n::\n\n    > mpirun -np NumberOfProcesses -m pyproct.main --mpi script.json\n\n When running pyProCT using MPI you will need to use the *MPI/Parallel*\nScheduler or it will just execute several independent serial runs.\n\n Remember that you need to use the same libraries and versions to build\nmpi4py and mpirun, otherwise you won't be able to execute it.\n\nDocumentation\n=============\n\nWe are still experimenting to see which documentation generator fits\nbetter with us. Currently we have two versions of the documentations:\none using `Sphinx <http://sphinx-doc.org/>`_ and the other using\n`Doxygen <http://www.stack.nl/~dimitri/doxygen/>`_+`doxpy <http://code.foosel.org/doxypy>`_.\nSee them `here <pyproct/docs/_build/html/index.html>`_ and\n`here <pyproct/docs/doxyxml/html/index.html>`_. We will possibly publish\nit in a cloud solution like\n`readthedocs.org <https://readthedocs.org/>`_\n\nLearn more\n~~~~~~~~~~\n\nA more detailed explanation of the script contents can be found\n`here <https://dl.dropboxusercontent.com/u/58918851/script_info.pdf>`_,\nand a discussion about the different implemented ICVs can be found\n`here <https://dl.dropboxusercontent.com/u/58918851/icv_info.pdf>`_.\n\nPlease, do not hesitate to send a mail to victor.gil.sepulveda@gmail.com\nwith your questions, criticisms and whatever you think it is not working\nor can be done better. Any contribution can help to improve this\nsoftware!\n\nTODO\n====\n\n-  To improve this documentation (better explanations, more examples and\n   downloadable scripts).\n\n-  Refactoring and general improvements:\n\n   -  Total refactoring (Clustering and Clusters are inmutable, hold a\n      reference to the matrix -> prototypes are always updated)\n   -  Rename script stuff\n   -  Rename functions and vars\n   -  Minimizing dependencies with scipy\n   -  Minimizing dependencies with prody (create my own reader)\n   -  Adding its own Hierarchical clustering code (educational\n      motivations)\n   -  Improve spectral algoritm (add more tests - comparisons with other\n      implementations, adding new types)\n   -  Improve MPI load balance (i.e. parameter generation must be\n      processed in parallel)\n   -  Improve test coverage\n   -  The script must accept numbers and percentages\n   -  Use JSON schema to validate the script. Try to delegate the full\n      responsibility of validating to pyProCT (instead of the GUI) [x] -\n      Users must be able to comment their scripts (with '//' for\n      instance).\n   -  When loading a dcd file, we only want to load atomic data of the\n      the associated pdb.\n   -  Change \"compression\" by \"redundancy\\_elimination\"\n   -  Allow to load all files (or glob) from a folder.\n\n-  Symetry handling:\n\n   -  Symmetry handling for fitting coordinates.\n   -  Improve symmetry handling for calculation coordinates (e.g.\n      ligands). [x] - Simple chain mapping feature.\n\n-  New algorithms:\n\n   -  Modularity-based (Newman J. 2003)\n   -  Passing messages (Frey and Dueck 2007)\n   -  Flow simmulation (Stijin van Dongen)\n   -  Fuzzy Clustering\n   -  `Jarvis-Patrick\n      Algorithm <http://www.improvedoutcomes.com/docs/WebSiteDocs/Clustering/Jarvis-Patrick_Clustering_Overview.htm>`_\n   -  Others (adaptative spectral clustering flavours)\n\n-  New quality functions.\n\n   -  Balancedness: The sizes of the clusters must be balanced.\n   -  J quality function: Cai Xiaoyan Proceedings of the 27th Chinese\n      Control Conference\n   -  Metastability function (Q) in Chodera et al. J. Chem. Phys. 126\n      155101 2007 .\n   -  Improve separation quality functions.\n   -  New standard separation ICVs (require inmutable prototypes)\n\n          Separation, the clusters themselves should be widely spaced.\n          There are three common approaches measuring the distance\n          between two different clusters:\n\n          -  Single linkage: It measures the distance between the\n             closest members of the clusters.\n          -  Complete linkage: It measures the distance between the most\n             distant members.\n          -  Comparison of centroids: It measures the distance between\n             the centers of the clusters.\n\n-  New features:\n\n   -  Refine noise in DBSCAN\n   -  Refine a preselected cluster (e.g \"noise\"), or \"heterogeneous\".\n\n-  New postprocessing options:\n\n   -  Refinement\n   -  Kinetic analysis", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/victor-gil-sepulveda/pyProCT", "keywords": "", "license": "LICENSE.txt", "maintainer": "", "maintainer_email": "", "name": "pyProCT", "package_url": "https://pypi.org/project/pyProCT/", "platform": "", "project_url": "https://pypi.org/project/pyProCT/", "project_urls": {"Homepage": "https://github.com/victor-gil-sepulveda/pyProCT"}, "release_url": "https://pypi.org/project/pyProCT/1.7.3/", "requires_dist": null, "requires_python": "", "summary": "pyProCT is an open source cluster analysis software especially adapted for jobs related with structural proteomics", "version": "1.7.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            pyProCT<br>=======<br><br>pyProCT is an open source cluster analysis software especially adapted<br>for jobs related with structural proteomics. Its approach allows users<br>to define a clustering goal (clustering hypothesis) based on their<br>domain knowledge. This hypothesis will guide the software in order to<br>find the best algorithm and parameters (including the number of<br>clusters) to obtain the result that better fulfills their expectatives.<br>In this way users do not need to use cluster analysis algorithms as a<br>black box, which will (hopefully) improve their results. pyProCT not<br>only generates a resulting clustering, it also implements some use cases<br>like the extraction of representatives or trajectory redundance<br>elimination.<br><br>`Table of Contents &lt;http://doctoc.herokuapp.com/&gt;`_<br><br>-  `pyProCT &lt;#user-content-pyproct&gt;`_<br><br>   -  `Installation &lt;#user-content-installation&gt;`_<br>   -  `Using pyProCT as a standalone<br>      program &lt;#user-content-using-pyproct-as-a-standalone-program&gt;`_<br><br>      -  `Global &lt;#user-content-global&gt;`_<br>      -  `Data &lt;#user-content-data&gt;`_<br>      -  `Clustering &lt;#user-content-clustering&gt;`_<br><br>         -  `generation &lt;#user-content-generation&gt;`_<br>         -  `algorithms &lt;#user-content-algorithms&gt;`_<br>         -  `evaluation &lt;#user-content-evaluation&gt;`_<br><br>      -  `Postprocessing &lt;#user-content-postprocessing&gt;`_<br>      -  `Checking the script &lt;#user-content-checking-the-script&gt;`_<br>      -  `Learn more &lt;#user-content-learn-more&gt;`_<br><br>   -  `Using pyProCT as part of other<br>      programs &lt;#user-content-using-pyproct-as-part-of-other-programs&gt;`_<br><br>      -  `Using it as a separate program from other Python<br>         script &lt;#user-content-using-it-as-a-separate-program-from-other-python-script&gt;`_<br><br>   -  `Parallel execution &lt;#user-content-parallel-execution&gt;`_<br><br>-  `Documentation &lt;#user-content-documentation&gt;`_<br>-  `TODO &lt;#user-content-todo&gt;`_<br><br>Installation<br>------------<br><br>pyProCT is quite easy to install using *pip*. Just write:<br><br>``Shell &gt; sudo pip install pyProCT`` And *pip* will take care of all the<br>dependencies (shown below).<br><br> It is recommended to install Numpy and Scipy before starting the<br>installation using your OS software manager. You can try to download and<br>install them<br>`manually &lt;http://docs.scipy.org/doc/numpy/user/install.html&gt;`_ if you<br>dare.<br><br> mpi4py is pyProCT's last dependency. It can give problems when<br>installing it in OS such as SUSE. If the installation of this last<br>package is not succesful, pyProCT can still work in Serial and Parallel<br>(using *multiprocessing*) modes.<br><br>Using pyProCT as a standalone program<br>-------------------------------------<br><br>The preferred way to use pyProCT is through a JSON \"script\" that<br>describes the clustering task. It can be executed using the following<br>line in your shell:<br><br>::<br><br>    &gt; python -m pyproct.main script.json<br><br>The JSON script has 4 main parts, each one dealing with a different<br>aspect of the clustering pipeline. This sections are: \\* *\"global\"*:<br>Handles workspace and scheduler parameterization. \\* *\"data\"*: Handles<br>distance matrix parameterization. \\* *\"clustering\"*: Handles algorithms<br>and evaluation parameterization. \\* *\"preprocessing\"*: Handles what to<br>do with the clustering we have calculated.<br><br>::<br><br>    {<br>        \"global\":{},<br>        \"data\":{},<br>        \"clustering\":{},<br>        \"postprocessing\":{}<br>    }<br><br>Global<br>~~~~~~<br><br>``JSON {     \"control\": {         \"scheduler_type\": \"Process/Parallel\",         \"number_of_processes\": 4     },     \"workspace\": {          \"tmp\": \"tmp\",          \"matrix\": \"matrix\",          \"clusterings\": \"clusterings\",          \"results\": \"results\",          \"base\": \"/home/john/ClusteringProject\"     } }``<br>This is an example of *\"global\"* section. It describes the work<br>environment (workspace) and the type of scheduler that will be built.<br>Defining the subfolders of the wokspace is not mandatory, however it may<br>be convenient in some scenarios (for instance, in serial multiple<br>clustering projects, sharing the *tmp* folder would lower the disk usage<br>as at each step it will be overwritten).<br><br>This is a valid global section using a serial scheduler and default<br>names for workspace inner folders:<br>``JSON {     \"control\": {         \"scheduler_type\": \"Serial\"     },     \"workspace\": {          \"base\": \"/home/john/ClusteringProject\"     } }``<br>pyProCT allows the use of 3 different schedulers that help to improve<br>the overall performance of the software by parallelizing some parts of<br>the code. The available schedulers are \"Serial\", \"Process/Parallel\"<br>(uses Python's<br>`multiprocessing &lt;https://docs.python.org/2/library/multiprocessing.html&gt;`_)<br>and \"MPI/Parallel\" (uses MPI through the module<br>`mpi4py &lt;http://mpi4py.scipy.org/&gt;`_).<br><br>Data<br>~~~~<br><br>The *\"data\"* section defines how pyProCT must build the distance matrix<br>that will be used by the compression algorithms. Currently pyProCT<br>offers up to three options to build that matrix: \"load\", \"rmsd\" and<br>\"distance\" - *\"rmsd\"*: Calculates a all vs all rmsd matrix using any of<br>the<br>`pyRMSD &lt;https://github.com/victor-gil-sepulveda/pyRMSD#collective-operations&gt;`_<br>calculators available. It can calculate the RMSD of the fitted region<br>(defined by `Prody &lt;http://prody.csb.pitt.edu/&gt;`_ compatible selection<br>string in *fit\\_selection*) or one can use one selection to superimpose<br>and another to calculate the rmsd (*calc\\_selection*). There are two<br>extra *parameters* that must be considered when building an RMSD matrix.<br>- *\"type\"*: This property can have two values: *\"COORDINATES\"* or<br>*\"DIHEDRALS\"*. If *DIHEDRALS* is chosen, each element *(i,j)* of the<br>distance matrix will be the RMSD of the arrays containing the phi-psi<br>dihedral angle series of conformation *i* and *j*. - *\"chain\\_map\"*: If<br>set to *true* pyProCT will try to reorder the chains of the biomolecule<br>in order to minimize the global RMSD value. This means that it will<br>correctly calculate the RMSD even if chain coordinates were permuted in<br>some way. The price to pay is an increase of the calculation time<br>(directly proportional to the number of chains or the number of chains<br>having the same length). - *\"distance\"*: After superimposing the<br>selected region it calculates the all vs all distances of the<br>geometrical center of the region of interest (*body\\_selection*). -<br>*\"load\"*: Loads a precalculated matrix.<br><br>JSON chunk needed to generate an RMSD matrix from two trajectories:<br>``JSON {     \"type\": \"pdb_ensemble\",     \"files\": [         \"A.pdb\",         \"B.pdb\"     ],     \"matrix\": {         \"method\": \"rmsd\",         \"parameters\": {             \"calculator_type\": \"QCP_OMP_CALCULATOR\",             \"fit_selection\": \"backbone\",         },         \"image\": {             \"filename\": \"matrix_plot\"         },         \"filename\":\"matrix\"     } }``<br>JSON chunk to generate a dihedral angles RMSD matrix from one<br>trajectories:<br>``JSON {     \"type\": \"pdb_ensemble\",     \"files\": [         \"A.pdb\"     ],     \"matrix\": {         \"method\": \"rmsd\",         \"parameters\": {             \"type\":\"DIHEDRAL\"         },         \"image\": {             \"filename\": \"matrix_plot\"         },         \"filename\":\"matrix\"     } }``<br><br>The matrix can be stored if the *filename* property is defined. The<br>matrix can also be stored as an image if the *image* property is<br>defined.<br><br>pyProCT can currently load *pdb* and *dcd* files. The details to load<br>the files must be written into the array under the \"files\" keyword.<br>There are many ways of telling pyProCT the files that have to be load<br>and can be combined in any way you like:<br><br>1. Using a list of file paths. If the file extension is \".txt\" or<br>   \".list\" it will be treated as a pdb list file. Each line of such<br>   files will be a pdb path or a pdb path and a selection string,<br>   separated by comma.<br><br>``A.pdb, name CA B.pdb C.pdb, name CA ...`` 2. Using a list of file<br>objects: ``JSON {     \"file\": ... ,     \"base_selection\": ... }`` Where<br>*base\\_selection* is a `Prody &lt;http://prody.csb.pitt.edu/&gt;`_ compatible<br>selection string. Loading files this way can help in cases where not all<br>files have structure with the same number of atoms: *base\\_selection*<br>should define the common region between them (if a 1 to 1 map does not<br>exist, the RMSD calculation will be wrong).<br><br>3. Only for *dcd* files:<br>   ``JSON {     \"file\": ...,     \"atoms_file\": ...,     \"base_selection\": ... }``<br>   Where *atoms\\_file* is a *pdb* file with at least one frame that<br>   holds the atomic information needed by the *dcd* file.<br><br>**Note*: data.type is currently unsused*<br><br>Clustering<br>~~~~~~~~~~<br><br>The *clustering* section specifies how the clustering exploration will<br>be done. It is divided in 3 other subsections:<br><br>``JSON {     \"generation\": {         \"method\": \"generate\"     },     \"algorithms\": {         ...     },     \"evaluation\": {         ...     } }``<br>#### Generation Defines how to generate the clustering (*\"load\"* or<br>*\"generate\"*). if *\"load\"* is chosen, this section will also contain the<br>clustering that may be used in the *\"clusters\"* property. Ex.:<br><br>::<br><br>    {<br>        \"clustering\": {<br>            \"generation\": {<br>                \"method\" : \"load\",<br>                \"clusters\": [<br>                        {<br>                            \"prototype \" : 16,<br>                            \"id\": \"cluster_00\",<br>                            \"elements\" : \"9, 14:20\"<br>                        },<br>                        {<br>                            \"prototype\": 7,<br>                            \"id\": \"cluster_01\",<br>                            \"elements\": \"0:8, 10:14, 21\"<br>                        }<br>                ]<br>            }<br>    }<br><br>Algorithms<br>^^^^^^^^^^<br><br>If clustering.generation.method equals \"generate\", this section defines<br>the algorithms that will be used as well as their parameters (if<br>necessary). The currently available algorithms are : *\"kmedoids\"*,<br>*\"hierarchical\"*, *\"dbscan\"*, *\"gromos\"*, *\"spectral\"* and *\"random\"*.<br>Each algorithm can store its list of parameters, however the preferred<br>way to work with pyProCT is to let it automatically generate them.<br>Almost all algorithms accept the property *max*, that defines the<br>maximum amount of parameter collections that will be generated for that<br>algorithm. Ex.<br><br>::<br><br>    {<br>        \"kmedoids\": {<br>            \"seeding_type\": \"RANDOM\",<br>            \"max\": 50,<br>            \"tries\": 5<br>        },<br>        \"hierarchical\": {<br><br>        },<br>        \"dbscan\": {<br>            \"max\": 50<br>        },<br>        \"gromos\": {<br>            \"max\": 50<br>        },<br>        \"spectral\": {<br>            \"max\": 50,<br>            \"force_sparse\":true,<br>        }<br>    }<br><br>Algorithm parameters can be explicitly written, however it is not<br>recommended:<br><br>::<br><br>    {<br>        \"kmedoids\": {<br>            \"seeding_type\": \"RANDOM\",<br>            \"max\": 50,<br>            \"tries\": 5,<br>            \"parameters\":[{\"k\":4},{\"k\":5},{\"k\":6}]<br>        }<br>    }<br><br>Evaluation<br>^^^^^^^^^^<br><br>This section holds the *Clustering Hypothesis*, the core of pyProCT.<br>Here the user can define how the expected clustering will be. First the<br>user must set the expected number of clusters range. Also, an estimation<br>of the dataset noise and the cluster minimum size (the minimum number of<br>elements a cluster must have to not be considered noise) will complete<br>the quantitative definition of the target result.<br><br>Ex.<br>``JSON {     \"maximum_noise\": 15,     \"minimum_cluster_size\": 50,     \"maximum_clusters\": 200,     \"minimum_clusters\": 6,     \"query_types\": [ ... ],     \"evaluation_criteria\": {         ...     } }``<br><br>The second part of the *Clustering Hypothesis* tries to characterize the<br>clustering internal traits in a more qualitative way. Concepts like<br>cluster \"Compactness\" or \"Separation\" can be used here to define the<br>expected clustering. To this end users must write their expectations in<br>form of *criteria*. This criteria are, in general, linear combinations<br>of Internal Clustering Validation Indices (ICVs). The best clustering<br>will be the one that gets the best score in any of these *criteria*. See<br>`this<br>document &lt;https://dl.dropboxusercontent.com/u/58918851/icv_info.pdf&gt;`_<br>to get more insight about the different implemented criteria and their<br>meaning.<br><br>Additionally users may choose to ask pyProCT about the results of this<br>ICVs and other evaluation functions(e.g. the average cluster size) by<br>adding them to the *queries* array.<br><br>::<br><br>    {<br>            ...<br>        \"query_types\": [<br>            \"NumClusters\",<br>            \"NoiseLevel\",<br>            \"MeanClusterSize\"<br>        ],<br>        \"evaluation_criteria\": {<br>            \"criteria_0\": {<br>                \"Silhouette\": {<br>                    \"action\": \"&gt;\",<br>                    \"weight\": 1<br>                }<br>            }<br>        }<br>    }<br><br>Postprocessing<br>~~~~~~~~~~~~~~<br><br>Getting a good quality clustering is not enough, we would like to use<br>them to extract useful information. pyProCT implements some use cases<br>that may help users to extract this information.<br><br>\\`\\`\\`JSON { \"rmsf\":{},<br><br>::<br><br>    \"centers_and_trace\":{},<br><br>    \"representatives\":{<br>        \"keep_remarks\": [true/false],<br>        \"keep_frame_number\": [true/false]<br>    },<br><br>    \"pdb_clusters\":{<br>        \"keep_remarks\": [true/false],<br>        \"keep_frame_number\": [true/false]<br>    },<br><br>    \"compression\":{<br>        \"final_number_of_frames\": INT,<br>        \"file\": STRING,<br>        \"type\":[\u2018RANDOM\u2019,\u2019KMEDOIDS\u2019]<br>    },<br><br>    \"cluster_stats\":{<br>        \"file\": STRING<br>    },<br><br><br>    \"conformational_space_comparison\":{},<br><br>    \"kullback_liebler\":{}<br><br>} \\`\\`\\` - *\"rmsf\"* : Calculates the global and per-cluster (and<br>per-residue) root mean square fluctuation (to be visualized using the<br>`GUI &lt;https://github.com/victor-gil-sepulveda/pyProCT-GUI&gt;`_).<br><br>-  *\"centers\\_and\\_trace\"* : Calculates all geometrical centers of the<br>   calculation selection of the system (to be visualized using the<br>   `GUI &lt;https://github.com/victor-gil-sepulveda/pyProCT-GUI&gt;`_).<br><br>-  *\"representatives\"* : Extracts all the representatives of the<br>   clusters in the same pdb.<br>    Parameters:<br><br>   -  *\"keep\\_remarks\"*: If true every stored model will be written<br>      along with its original remarks header. Default: false.<br>   -  *\"keep\\_frame\\_number\"*: If true, the model number of any stored<br>      conformation will be the original pdb one. Default: false.<br><br>-  *\"pdb\\_clusters\"* : Extracts all clusters in separate pdbs.<br>    Parameters:<br><br>   -  *\"keep\\_remarks\"*: If true every stored model will be written<br>      along with its original remarks header. Default: false.<br>   -  *\"keep\\_frame\\_number\"*: If true, the model number of any stored<br>      conformation will be the original pdb one. Default: false.<br><br>-  *\"compression\"* : Reduces the redundancy of the trajectory using the<br>   resulting clustering.<br>    Parameters:<br><br>   -  *\"file\"*: The name of the output file without extension. Default<br>      \"compressed\"(.pdb)<br>   -  *\"final\\_number\\_of\\_frames\"*: The expected (minimum) number of<br>      frames of the compressed file.<br>   -  *\"type\"*: The method used to get samples from each cluster.<br>      Options:<br><br>      -  \"RANDOM\": Gets a random sample of the elements of each cluster.<br>      -  \"KMEDOIDS\": Applies the k-medoids algorithm to the elements of<br>         a cluster and stores the representatives.<br>         Default: \"KMEDOIDS\".<br><br>-  *\"cluster\\_stats\"*: Generates a human readable file with the<br>   distances between cluster centers and their diameters.<br>    Parameters:<br><br>   -  *\"file\"*: The name of the output file without extension (will be<br>      sotred into the results folder). Default:<br>      \"per\\_cluster\\_stats\"(.csv).<br><br>-  *\"conformational\\_space\\_comparison\"* : Work in progress.<br><br>-  *\"kullback\\_liebler\"* : Work in progress.<br><br>Script validation<br>~~~~~~~~~~~~~~~~~<br><br>As the control script is indeed holding a JSON object, any JSON<br>validator can be used to discover the errors in case of script loading<br>problems. A good example of such validators is<br>`JSONLint &lt;http://jsonlint.com/&gt;`_. pyProCT scripts accept javascript<br>comments ( // and /\\* \\*/)<br><br> Using pyProCT as part of other programs<br>----------------------------------------<br><br>-  Using algorithms<br>-  Clustering from label lists<br>-  Using ICVs with custom clusterings<br>-  Performing the whole protocol<br>   ``Python Driver(Observer()).run(parameters)``<br><br>The necessary documentation to use pyProCT classes is written inside the<br>code. It has been extracted<br>`here &lt;pyproct/docs/_build/html/index.html&gt;`_ and<br>`here &lt;pyproct/docs/doxyxml/html/index.html&gt;`_. We are currently trying<br>to improve this documentation with better explanations and examples.<br><br>See `this<br>file &lt;https://github.com/victor-gil-sepulveda/pyProCT/blob/master/validation/bidimensional/validation_main.py&gt;`_.<br><br>Using it as a separate program from other Python script<br>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br><br>-   Loading results<br>-   Generating scripts programatically<br><br>See `this project &lt;https://github.com/victor-gil-sepulveda/PhD-GPCR&gt;`_<br>for some examples.<br><br>Parallel execution<br>------------------<br><br>To execute pyProCT in parallel you just need to issue this line:<br><br>::<br><br>    &gt; mpirun -np NumberOfProcesses -m pyproct.main --mpi script.json<br><br> When running pyProCT using MPI you will need to use the *MPI/Parallel*<br>Scheduler or it will just execute several independent serial runs.<br><br> Remember that you need to use the same libraries and versions to build<br>mpi4py and mpirun, otherwise you won't be able to execute it.<br><br>Documentation<br>=============<br><br>We are still experimenting to see which documentation generator fits<br>better with us. Currently we have two versions of the documentations:<br>one using `Sphinx &lt;http://sphinx-doc.org/&gt;`_ and the other using<br>`Doxygen &lt;http://www.stack.nl/~dimitri/doxygen/&gt;`_+`doxpy &lt;http://code.foosel.org/doxypy&gt;`_.<br>See them `here &lt;pyproct/docs/_build/html/index.html&gt;`_ and<br>`here &lt;pyproct/docs/doxyxml/html/index.html&gt;`_. We will possibly publish<br>it in a cloud solution like<br>`readthedocs.org &lt;https://readthedocs.org/&gt;`_<br><br>Learn more<br>~~~~~~~~~~<br><br>A more detailed explanation of the script contents can be found<br>`here &lt;https://dl.dropboxusercontent.com/u/58918851/script_info.pdf&gt;`_,<br>and a discussion about the different implemented ICVs can be found<br>`here &lt;https://dl.dropboxusercontent.com/u/58918851/icv_info.pdf&gt;`_.<br><br>Please, do not hesitate to send a mail to victor.gil.sepulveda@gmail.com<br>with your questions, criticisms and whatever you think it is not working<br>or can be done better. Any contribution can help to improve this<br>software!<br><br>TODO<br>====<br><br>-  To improve this documentation (better explanations, more examples and<br>   downloadable scripts).<br><br>-  Refactoring and general improvements:<br><br>   -  Total refactoring (Clustering and Clusters are inmutable, hold a<br>      reference to the matrix -&gt; prototypes are always updated)<br>   -  Rename script stuff<br>   -  Rename functions and vars<br>   -  Minimizing dependencies with scipy<br>   -  Minimizing dependencies with prody (create my own reader)<br>   -  Adding its own Hierarchical clustering code (educational<br>      motivations)<br>   -  Improve spectral algoritm (add more tests - comparisons with other<br>      implementations, adding new types)<br>   -  Improve MPI load balance (i.e. parameter generation must be<br>      processed in parallel)<br>   -  Improve test coverage<br>   -  The script must accept numbers and percentages<br>   -  Use JSON schema to validate the script. Try to delegate the full<br>      responsibility of validating to pyProCT (instead of the GUI) [x] -<br>      Users must be able to comment their scripts (with '//' for<br>      instance).<br>   -  When loading a dcd file, we only want to load atomic data of the<br>      the associated pdb.<br>   -  Change \"compression\" by \"redundancy\\_elimination\"<br>   -  Allow to load all files (or glob) from a folder.<br><br>-  Symetry handling:<br><br>   -  Symmetry handling for fitting coordinates.<br>   -  Improve symmetry handling for calculation coordinates (e.g.<br>      ligands). [x] - Simple chain mapping feature.<br><br>-  New algorithms:<br><br>   -  Modularity-based (Newman J. 2003)<br>   -  Passing messages (Frey and Dueck 2007)<br>   -  Flow simmulation (Stijin van Dongen)<br>   -  Fuzzy Clustering<br>   -  `Jarvis-Patrick<br>      Algorithm &lt;http://www.improvedoutcomes.com/docs/WebSiteDocs/Clustering/Jarvis-Patrick_Clustering_Overview.htm&gt;`_<br>   -  Others (adaptative spectral clustering flavours)<br><br>-  New quality functions.<br><br>   -  Balancedness: The sizes of the clusters must be balanced.<br>   -  J quality function: Cai Xiaoyan Proceedings of the 27th Chinese<br>      Control Conference<br>   -  Metastability function (Q) in Chodera et al. J. Chem. Phys. 126<br>      155101 2007 .<br>   -  Improve separation quality functions.<br>   -  New standard separation ICVs (require inmutable prototypes)<br><br>          Separation, the clusters themselves should be widely spaced.<br>          There are three common approaches measuring the distance<br>          between two different clusters:<br><br>          -  Single linkage: It measures the distance between the<br>             closest members of the clusters.<br>          -  Complete linkage: It measures the distance between the most<br>             distant members.<br>          -  Comparison of centroids: It measures the distance between<br>             the centers of the clusters.<br><br>-  New features:<br><br>   -  Refine noise in DBSCAN<br>   -  Refine a preselected cluster (e.g \"noise\"), or \"heterogeneous\".<br><br>-  New postprocessing options:<br><br>   -  Refinement<br>   -  Kinetic analysis\n          </div>"}, "last_serial": 3122840, "releases": {"1.4.3": [{"comment_text": "", "digests": {"md5": "6c8cad0ca13802bfa46de35a2333fd09", "sha256": "2554d1ce27e6cb5b056d33af63500463e1517fb1dc6e06aab0ba41f1afa2674b"}, "downloads": -1, "filename": "pyProCT-1.4.3.tar.gz", "has_sig": false, "md5_digest": "6c8cad0ca13802bfa46de35a2333fd09", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 316790, "upload_time": "2014-08-11T13:29:43", "upload_time_iso_8601": "2014-08-11T13:29:43.025978Z", "url": "https://files.pythonhosted.org/packages/52/1c/b4088ebb3c8ef5dae5a9fbf2780c32949f850f13210d441f0af834c10dc4/pyProCT-1.4.3.tar.gz", "yanked": false}], "1.5.1": [{"comment_text": "", "digests": {"md5": "c5727c96f3acde61e2caa3fd328c55b2", "sha256": "127f30d231224c2a4179ade5402f1b29ea0ae7a57751bd5668e9b4b4b5057631"}, "downloads": -1, "filename": "pyProCT-1.5.1.tar.gz", "has_sig": false, "md5_digest": "c5727c96f3acde61e2caa3fd328c55b2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 353844, "upload_time": "2014-10-13T15:23:31", "upload_time_iso_8601": "2014-10-13T15:23:31.912151Z", "url": "https://files.pythonhosted.org/packages/4d/01/69b5080ed36e50232cc14cf2f249efe052aea5b7ad07a9de32ff7d1f0637/pyProCT-1.5.1.tar.gz", "yanked": false}], "1.6.0": [{"comment_text": "", "digests": {"md5": "58bacca2ca4cb8e72109f9a4fb5908f4", "sha256": "40b7f4b3e2f490cceaa42894de8dfbac17e36582e5dd6e11ecbd1682f7b99eaf"}, "downloads": -1, "filename": "pyProCT-1.6.0.tar.gz", "has_sig": false, "md5_digest": "58bacca2ca4cb8e72109f9a4fb5908f4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 355422, "upload_time": "2015-01-08T11:21:07", "upload_time_iso_8601": "2015-01-08T11:21:07.706193Z", "url": "https://files.pythonhosted.org/packages/9b/3b/66b340743ed770d82f2bfc801d0cc962e6a21e7198d6f4b36f08556d5d7f/pyProCT-1.6.0.tar.gz", "yanked": false}], "1.7.0": [{"comment_text": "", "digests": {"md5": "1380c8f4a0996526c22e4e7ec7f951b7", "sha256": "8538ac78a014cb46bbe8b6df7b7810db729a0ebb2f2888f21f836af571ea2170"}, "downloads": -1, "filename": "pyProCT-1.7.0.tar.gz", "has_sig": false, "md5_digest": "1380c8f4a0996526c22e4e7ec7f951b7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 352777, "upload_time": "2015-02-12T10:30:15", "upload_time_iso_8601": "2015-02-12T10:30:15.080737Z", "url": "https://files.pythonhosted.org/packages/2b/fc/a8bdf108133f848f24205b9ac98797fa553cbab59ab4dbd9392dfbcac130/pyProCT-1.7.0.tar.gz", "yanked": false}], "1.7.1": [{"comment_text": "", "digests": {"md5": "ee0b5c09ad9f2e60236911a03fdcf268", "sha256": "5b2787f96cf0cc681cc84c3f4f2fb2be45780f9561c1d3c2a18612e7e6c67128"}, "downloads": -1, "filename": "pyProCT-1.7.1.tar.gz", "has_sig": false, "md5_digest": "ee0b5c09ad9f2e60236911a03fdcf268", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 352885, "upload_time": "2015-02-12T11:47:47", "upload_time_iso_8601": "2015-02-12T11:47:47.134305Z", "url": "https://files.pythonhosted.org/packages/8c/64/691788ea3373605021028c9a320de36c42b371e2dd13dacf490d9bfd0ce8/pyProCT-1.7.1.tar.gz", "yanked": false}], "1.7.2": [{"comment_text": "", "digests": {"md5": "c80e56297c992ba005a4c1fa4c1ec95e", "sha256": "806ca0fec13967b6bcc55305079a528ad4288a6a9528d5eb79dffc1869bd05d2"}, "downloads": -1, "filename": "pyProCT-1.7.2.tar.gz", "has_sig": false, "md5_digest": "c80e56297c992ba005a4c1fa4c1ec95e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 361992, "upload_time": "2015-02-27T16:11:43", "upload_time_iso_8601": "2015-02-27T16:11:43.091821Z", "url": "https://files.pythonhosted.org/packages/33/b3/e22e8fbacd034685cba0a1b33660d529d11a97e55a1894cbbe44afe2386c/pyProCT-1.7.2.tar.gz", "yanked": false}], "1.7.3": [{"comment_text": "", "digests": {"md5": "d27056533b02338257c7da47da8cc3ff", "sha256": "dbd5a6e834256977f1f0a8861765e6aa366166212b045c8afde89196cb5e908b"}, "downloads": -1, "filename": "pyProCT-1.7.3.tar.gz", "has_sig": false, "md5_digest": "d27056533b02338257c7da47da8cc3ff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 364042, "upload_time": "2017-08-25T11:12:24", "upload_time_iso_8601": "2017-08-25T11:12:24.460861Z", "url": "https://files.pythonhosted.org/packages/86/61/ac027e48e1feef08f5cf6e2752f1c67c4a6130de550c515dc62a03eceeb7/pyProCT-1.7.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d27056533b02338257c7da47da8cc3ff", "sha256": "dbd5a6e834256977f1f0a8861765e6aa366166212b045c8afde89196cb5e908b"}, "downloads": -1, "filename": "pyProCT-1.7.3.tar.gz", "has_sig": false, "md5_digest": "d27056533b02338257c7da47da8cc3ff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 364042, "upload_time": "2017-08-25T11:12:24", "upload_time_iso_8601": "2017-08-25T11:12:24.460861Z", "url": "https://files.pythonhosted.org/packages/86/61/ac027e48e1feef08f5cf6e2752f1c67c4a6130de550c515dc62a03eceeb7/pyProCT-1.7.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:59:07 2020"}