{"info": {"author": "Leif Johnson", "author_email": "leif@lmjohns3.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Topic :: Scientific/Engineering"], "description": ".. image:: https://travis-ci.org/lmjohns3/downhill.svg\n.. image:: https://coveralls.io/repos/lmjohns3/downhill/badge.svg\n   :target: https://coveralls.io/r/lmjohns3/downhill\n.. image:: http://depsy.org/api/package/pypi/downhill/badge.svg\n   :target: http://depsy.org/package/python/downhill\n\n============\n``DOWNHILL``\n============\n\nThe ``downhill`` package provides algorithms for minimizing scalar loss\nfunctions that are defined using Theano_.\n\nSeveral optimization algorithms are included:\n\n- ADADELTA_\n- ADAGRAD_\n- Adam_\n- `Equilibrated SGD`_\n- `Nesterov's Accelerated Gradient`_\n- RMSProp_\n- `Resilient Backpropagation`_\n- `Stochastic Gradient Descent`_\n\nAll algorithms permit the use of regular or Nesterov-style momentum as well.\n\n.. _Theano: http://deeplearning.net/software/theano/\n\n.. _Stochastic Gradient Descent: http://downhill.readthedocs.org/en/stable/generated/downhill.first_order.SGD.html\n.. _Nesterov's Accelerated Gradient: http://downhill.readthedocs.org/en/stable/generated/downhill.first_order.NAG.html\n.. _Resilient Backpropagation: http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.RProp.html\n.. _ADAGRAD: http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.ADAGRAD.html\n.. _RMSProp: http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.RMSProp.html\n.. _ADADELTA: http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.ADADELTA.html\n.. _Adam: http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.Adam.html\n.. _Equilibrated SGD: http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.ESGD.html\n\nQuick Start: Matrix Factorization\n=================================\n\nLet's say you have 100 samples of 1000-dimensional data, and you want to\nrepresent your data as 100 coefficients in a 10-dimensional basis. This is\npretty straightforward to model using Theano: you can use a matrix\nmultiplication as the data model, a squared-error term for optimization, and a\nsparse regularizer to encourage small coefficient values.\n\nOnce you have constructed an expression for the loss, you can optimize it with a\nsingle call to ``downhill.minimize``:\n\n.. code:: python\n\n  import downhill\n  import numpy as np\n  import theano\n  import theano.tensor as TT\n\n  FLOAT = 'df'[theano.config.floatX == 'float32']\n\n  def rand(a, b):\n      return np.random.randn(a, b).astype(FLOAT)\n\n  A, B, K = 20, 5, 3\n\n  # Set up a matrix factorization problem to optimize.\n  u = theano.shared(rand(A, K), name='u')\n  v = theano.shared(rand(K, B), name='v')\n  z = TT.matrix()\n  err = TT.sqr(z - TT.dot(u, v))\n  loss = err.mean() + abs(u).mean() + (v * v).mean()\n\n  # Minimize the regularized loss with respect to a data matrix.\n  y = np.dot(rand(A, K), rand(K, B)) + rand(A, B)\n\n  # Monitor during optimization.\n  monitors = (('err', err.mean()),\n              ('|u|<0.1', (abs(u) < 0.1).mean()),\n              ('|v|<0.1', (abs(v) < 0.1).mean()))\n\n  downhill.minimize(\n      loss=loss,\n      train=[y],\n      patience=0,\n      batch_size=A,                 # Process y as a single batch.\n      max_gradient_norm=1,          # Prevent gradient explosion!\n      learning_rate=0.1,\n      monitors=monitors,\n      monitor_gradients=True)\n\n  # Print out the optimized coefficients u and basis v.\n  print('u =', u.get_value())\n  print('v =', v.get_value())\n\nIf you prefer to maintain more control over your model during optimization,\ndownhill provides an iterative optimization interface:\n\n.. code:: python\n\n  opt = downhill.build(algo='rmsprop',\n                       loss=loss,\n                       monitors=monitors,\n                       monitor_gradients=True)\n\n  for metrics, _ in opt.iterate(train=[[y]],\n                                patience=0,\n                                batch_size=A,\n                                max_gradient_norm=1,\n                                learning_rate=0.1):\n      print(metrics)\n\nIf that's still not enough, you can just plain ask downhill for the updates to\nyour model variables and do everything else yourself:\n\n.. code:: python\n\n  updates = downhill.build('rmsprop', loss).get_updates(\n      batch_size=A, max_gradient_norm=1, learning_rate=0.1)\n  func = theano.function([z], loss, updates=list(updates))\n  for _ in range(100):\n      print(func(y))  # Evaluate func and apply variable updates.\n\nMore Information\n================\n\nSource: http://github.com/lmjohns3/downhill\n\nDocumentation: http://downhill.readthedocs.org\n\nMailing list: https://groups.google.com/forum/#!forum/downhill-users\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/lmjohns3/downhill", "keywords": "adadelta adam esgd gradient-descent nesterov optimization rmsprop sgd theano", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "downhill", "package_url": "https://pypi.org/project/downhill/", "platform": "", "project_url": "https://pypi.org/project/downhill/", "project_urls": {"Homepage": "http://github.com/lmjohns3/downhill"}, "release_url": "https://pypi.org/project/downhill/0.4.0/", "requires_dist": null, "requires_python": "", "summary": "Stochastic optimization routines for Theano", "version": "0.4.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <img alt=\"https://travis-ci.org/lmjohns3/downhill.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6b61c7216ad216129544201232b58459ae204717/68747470733a2f2f7472617669732d63692e6f72672f6c6d6a6f686e73332f646f776e68696c6c2e737667\">\n<a href=\"https://coveralls.io/r/lmjohns3/downhill\" rel=\"nofollow\"><img alt=\"https://coveralls.io/repos/lmjohns3/downhill/badge.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6b4e0804990aa36a9284a9527c1f71f76f84a6b7/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6c6d6a6f686e73332f646f776e68696c6c2f62616467652e737667\"></a>\n<a href=\"http://depsy.org/package/python/downhill\" rel=\"nofollow\"><img alt=\"http://depsy.org/api/package/pypi/downhill/badge.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0db3ab3038cb3592bd2d37bc117199da55059c91/687474703a2f2f64657073792e6f72672f6170692f7061636b6167652f707970692f646f776e68696c6c2f62616467652e737667\"></a>\n<div id=\"downhill\">\n<h2><tt>DOWNHILL</tt></h2>\n<p>The <tt>downhill</tt> package provides algorithms for minimizing scalar loss\nfunctions that are defined using <a href=\"http://deeplearning.net/software/theano/\" rel=\"nofollow\">Theano</a>.</p>\n<p>Several optimization algorithms are included:</p>\n<ul>\n<li><a href=\"http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.ADADELTA.html\" rel=\"nofollow\">ADADELTA</a></li>\n<li><a href=\"http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.ADAGRAD.html\" rel=\"nofollow\">ADAGRAD</a></li>\n<li><a href=\"http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.Adam.html\" rel=\"nofollow\">Adam</a></li>\n<li><a href=\"http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.ESGD.html\" rel=\"nofollow\">Equilibrated SGD</a></li>\n<li><a href=\"http://downhill.readthedocs.org/en/stable/generated/downhill.first_order.NAG.html\" rel=\"nofollow\">Nesterov\u2019s Accelerated Gradient</a></li>\n<li><a href=\"http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.RMSProp.html\" rel=\"nofollow\">RMSProp</a></li>\n<li><a href=\"http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.RProp.html\" rel=\"nofollow\">Resilient Backpropagation</a></li>\n<li><a href=\"http://downhill.readthedocs.org/en/stable/generated/downhill.first_order.SGD.html\" rel=\"nofollow\">Stochastic Gradient Descent</a></li>\n</ul>\n<p>All algorithms permit the use of regular or Nesterov-style momentum as well.</p>\n<div id=\"quick-start-matrix-factorization\">\n<h3>Quick Start: Matrix Factorization</h3>\n<p>Let\u2019s say you have 100 samples of 1000-dimensional data, and you want to\nrepresent your data as 100 coefficients in a 10-dimensional basis. This is\npretty straightforward to model using Theano: you can use a matrix\nmultiplication as the data model, a squared-error term for optimization, and a\nsparse regularizer to encourage small coefficient values.</p>\n<p>Once you have constructed an expression for the loss, you can optimize it with a\nsingle call to <tt>downhill.minimize</tt>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">downhill</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">theano</span>\n<span class=\"kn\">import</span> <span class=\"nn\">theano.tensor</span> <span class=\"k\">as</span> <span class=\"nn\">TT</span>\n\n<span class=\"n\">FLOAT</span> <span class=\"o\">=</span> <span class=\"s1\">'df'</span><span class=\"p\">[</span><span class=\"n\">theano</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">floatX</span> <span class=\"o\">==</span> <span class=\"s1\">'float32'</span><span class=\"p\">]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">rand</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">FLOAT</span><span class=\"p\">)</span>\n\n<span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">3</span>\n\n<span class=\"c1\"># Set up a matrix factorization problem to optimize.</span>\n<span class=\"n\">u</span> <span class=\"o\">=</span> <span class=\"n\">theano</span><span class=\"o\">.</span><span class=\"n\">shared</span><span class=\"p\">(</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'u'</span><span class=\"p\">)</span>\n<span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">theano</span><span class=\"o\">.</span><span class=\"n\">shared</span><span class=\"p\">(</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'v'</span><span class=\"p\">)</span>\n<span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">TT</span><span class=\"o\">.</span><span class=\"n\">matrix</span><span class=\"p\">()</span>\n<span class=\"n\">err</span> <span class=\"o\">=</span> <span class=\"n\">TT</span><span class=\"o\">.</span><span class=\"n\">sqr</span><span class=\"p\">(</span><span class=\"n\">z</span> <span class=\"o\">-</span> <span class=\"n\">TT</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">u</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">))</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">err</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">u</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">v</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Minimize the regularized loss with respect to a data matrix.</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Monitor during optimization.</span>\n<span class=\"n\">monitors</span> <span class=\"o\">=</span> <span class=\"p\">((</span><span class=\"s1\">'err'</span><span class=\"p\">,</span> <span class=\"n\">err</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()),</span>\n            <span class=\"p\">(</span><span class=\"s1\">'|u|&lt;0.1'</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">u</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"mf\">0.1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()),</span>\n            <span class=\"p\">(</span><span class=\"s1\">'|v|&lt;0.1'</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"mf\">0.1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()))</span>\n\n<span class=\"n\">downhill</span><span class=\"o\">.</span><span class=\"n\">minimize</span><span class=\"p\">(</span>\n    <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">loss</span><span class=\"p\">,</span>\n    <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">y</span><span class=\"p\">],</span>\n    <span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">A</span><span class=\"p\">,</span>                 <span class=\"c1\"># Process y as a single batch.</span>\n    <span class=\"n\">max_gradient_norm</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>          <span class=\"c1\"># Prevent gradient explosion!</span>\n    <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>\n    <span class=\"n\">monitors</span><span class=\"o\">=</span><span class=\"n\">monitors</span><span class=\"p\">,</span>\n    <span class=\"n\">monitor_gradients</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Print out the optimized coefficients u and basis v.</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'u ='</span><span class=\"p\">,</span> <span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">get_value</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'v ='</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">get_value</span><span class=\"p\">())</span>\n</pre>\n<p>If you prefer to maintain more control over your model during optimization,\ndownhill provides an iterative optimization interface:</p>\n<pre><span class=\"n\">opt</span> <span class=\"o\">=</span> <span class=\"n\">downhill</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"n\">algo</span><span class=\"o\">=</span><span class=\"s1\">'rmsprop'</span><span class=\"p\">,</span>\n                     <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">loss</span><span class=\"p\">,</span>\n                     <span class=\"n\">monitors</span><span class=\"o\">=</span><span class=\"n\">monitors</span><span class=\"p\">,</span>\n                     <span class=\"n\">monitor_gradients</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">metrics</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">iterate</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"o\">=</span><span class=\"p\">[[</span><span class=\"n\">y</span><span class=\"p\">]],</span>\n                              <span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n                              <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">A</span><span class=\"p\">,</span>\n                              <span class=\"n\">max_gradient_norm</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n                              <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">metrics</span><span class=\"p\">)</span>\n</pre>\n<p>If that\u2019s still not enough, you can just plain ask downhill for the updates to\nyour model variables and do everything else yourself:</p>\n<pre><span class=\"n\">updates</span> <span class=\"o\">=</span> <span class=\"n\">downhill</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"s1\">'rmsprop'</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">get_updates</span><span class=\"p\">(</span>\n    <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">max_gradient_norm</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n<span class=\"n\">func</span> <span class=\"o\">=</span> <span class=\"n\">theano</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">([</span><span class=\"n\">z</span><span class=\"p\">],</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">updates</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">updates</span><span class=\"p\">))</span>\n<span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">))</span>  <span class=\"c1\"># Evaluate func and apply variable updates.</span>\n</pre>\n</div>\n<div id=\"more-information\">\n<h3>More Information</h3>\n<p>Source: <a href=\"http://github.com/lmjohns3/downhill\" rel=\"nofollow\">http://github.com/lmjohns3/downhill</a></p>\n<p>Documentation: <a href=\"http://downhill.readthedocs.org\" rel=\"nofollow\">http://downhill.readthedocs.org</a></p>\n<p>Mailing list: <a href=\"https://groups.google.com/forum/#!forum/downhill-users\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/downhill-users</a></p>\n</div>\n</div>\n\n          </div>"}, "last_serial": 2570284, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "aaf551cbfa9f8b3ad99d0589fbdbd81a", "sha256": "8242ebf14a32bc83972903233dfe338dab4e87774d6aeb5ec3b282bf05f08c82"}, "downloads": -1, "filename": "downhill-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "aaf551cbfa9f8b3ad99d0589fbdbd81a", "packagetype": "bdist_wheel", "python_version": "3.4", "requires_python": null, "size": 21629, "upload_time": "2015-05-29T20:58:07", "upload_time_iso_8601": "2015-05-29T20:58:07.776212Z", "url": "https://files.pythonhosted.org/packages/a0/0d/c76649675ce4a459f47b194f733c3ec784a78498722ccd3967b2f19287cf/downhill-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c0e10e7feed8e8751f1a6cb756f112ee", "sha256": "baf726aff9db294961dafd4e6e62dedeb355257746360ce9b650a3602034047c"}, "downloads": -1, "filename": "downhill-0.1.0.tar.gz", "has_sig": false, "md5_digest": "c0e10e7feed8e8751f1a6cb756f112ee", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17024, "upload_time": "2015-05-29T20:58:04", "upload_time_iso_8601": "2015-05-29T20:58:04.140435Z", "url": "https://files.pythonhosted.org/packages/62/d2/d3dae577eb08abce4a2923bd7ea1f1c21f6a2f60bc7e409dbe379e1b39eb/downhill-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "93677cdb8b2efbc4cd5225774e48cce2", "sha256": "77538ae4856c152e441dedb63c501f088983b537e89fefca6e580c60363232b5"}, "downloads": -1, "filename": "downhill-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "93677cdb8b2efbc4cd5225774e48cce2", "packagetype": "bdist_wheel", "python_version": "3.4", "requires_python": null, "size": 22231, "upload_time": "2015-06-25T01:48:16", "upload_time_iso_8601": "2015-06-25T01:48:16.744671Z", "url": "https://files.pythonhosted.org/packages/4b/ca/7ad8e5319125f6e60ec7f2ff9177a6d4a82d195b3b083fb2b0ee15e9e504/downhill-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "183636f2b500dcea5b37534add2ce496", "sha256": "098a61d5e5148de21ef487d420599228596a42e03f2b13652a73516d50490965"}, "downloads": -1, "filename": "downhill-0.2.0.tar.gz", "has_sig": false, "md5_digest": "183636f2b500dcea5b37534add2ce496", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17179, "upload_time": "2015-06-25T01:48:12", "upload_time_iso_8601": "2015-06-25T01:48:12.801915Z", "url": "https://files.pythonhosted.org/packages/fa/fc/5cf477ccbcf970f331addc2f37ee6c253d4e691a383930fcba7d0db8df43/downhill-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "f5f6c2bd6d367b1717e60a8b97795ffc", "sha256": "e3f899a91690eba5fd965e819a41b98b557cbc4121e29aa5a8a46c39fecf0bd6"}, "downloads": -1, "filename": "downhill-0.2.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "f5f6c2bd6d367b1717e60a8b97795ffc", "packagetype": "bdist_wheel", "python_version": "3.4", "requires_python": null, "size": 24134, "upload_time": "2015-07-09T17:47:05", "upload_time_iso_8601": "2015-07-09T17:47:05.420130Z", "url": "https://files.pythonhosted.org/packages/1f/7f/da71777b77d7e6a8d71042b5034057d07cc88708b3b151f3e729887a0b77/downhill-0.2.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "82d3e655761a3b1eff2679fd92a96dcf", "sha256": "d11314d5db829f46af439dcbe17742e2de16d033dcbeb5e3c1923859f7dddb45"}, "downloads": -1, "filename": "downhill-0.2.1.tar.gz", "has_sig": false, "md5_digest": "82d3e655761a3b1eff2679fd92a96dcf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18070, "upload_time": "2015-07-09T17:47:01", "upload_time_iso_8601": "2015-07-09T17:47:01.654758Z", "url": "https://files.pythonhosted.org/packages/cd/57/91064fb9e1013b041d63cda247ee31b9306c2a11b99f560bf434a4ffca83/downhill-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "17c6c51200537e296e7342a75425ed87", "sha256": "64661203bcd3e1b3f2b43e5669879e228a23ca06fe170dc09a01e94b61dd6ca5"}, "downloads": -1, "filename": "downhill-0.2.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "17c6c51200537e296e7342a75425ed87", "packagetype": "bdist_wheel", "python_version": "3.4", "requires_python": null, "size": 24254, "upload_time": "2015-07-09T17:56:10", "upload_time_iso_8601": "2015-07-09T17:56:10.264925Z", "url": "https://files.pythonhosted.org/packages/3d/82/62d0184e597433bfe18b36fed34de12219c228d8541b08e559501a2b73e5/downhill-0.2.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "122eadb75619b8f0c4e95ceadc913bb9", "sha256": "f608190c62baae3f2349beaf449a241ac8a88bb9287176531c1cf5ece93e4075"}, "downloads": -1, "filename": "downhill-0.2.2.tar.gz", "has_sig": false, "md5_digest": "122eadb75619b8f0c4e95ceadc913bb9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18172, "upload_time": "2015-07-09T17:56:06", "upload_time_iso_8601": "2015-07-09T17:56:06.410303Z", "url": "https://files.pythonhosted.org/packages/e4/83/32019b870de00eade371168fc60bd9af25cead2e2d4f6ec29befb726af97/downhill-0.2.2.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "a1e179e46439f1e8ea119953e29a7ed4", "sha256": "2c06d59e3d6693be2899b2bc28c3156d9d4bbb531e34115b1b275c49baa688c7"}, "downloads": -1, "filename": "downhill-0.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "a1e179e46439f1e8ea119953e29a7ed4", "packagetype": "bdist_wheel", "python_version": "3.4", "requires_python": null, "size": 24785, "upload_time": "2015-09-25T17:18:10", "upload_time_iso_8601": "2015-09-25T17:18:10.026495Z", "url": "https://files.pythonhosted.org/packages/9e/fa/02de822b20fe09bd7f14bd95e32bd597bb9a772111c469f43b7948fddc3b/downhill-0.3.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "56eb70e1cb53c9a4669a363c8a859070", "sha256": "210b7e5f928f881b3ca93d4f256a21f30661fdb8a6a0846a0e0b1e62b233ed9a"}, "downloads": -1, "filename": "downhill-0.3.0.tar.gz", "has_sig": false, "md5_digest": "56eb70e1cb53c9a4669a363c8a859070", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18669, "upload_time": "2015-09-25T17:17:59", "upload_time_iso_8601": "2015-09-25T17:17:59.209844Z", "url": "https://files.pythonhosted.org/packages/01/be/1bb392d7b84d8b80019a57427e45b18f1b1abb621ac686d1bbf8e9e99a61/downhill-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "a5fdd46c9460ab45cb5a0abdb8fbc3d5", "sha256": "ab8b3a8c66982e7e13fa856f5762daab66468c58fa584119cd50208b95f1b7d7"}, "downloads": -1, "filename": "downhill-0.3.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "a5fdd46c9460ab45cb5a0abdb8fbc3d5", "packagetype": "bdist_wheel", "python_version": "3.4", "requires_python": null, "size": 22933, "upload_time": "2015-09-27T19:37:47", "upload_time_iso_8601": "2015-09-27T19:37:47.839112Z", "url": "https://files.pythonhosted.org/packages/c9/66/13d51f53dd3a2ffb732a21eb7dfc60594926eb72bded2eca35bf3d5d367c/downhill-0.3.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "05a1ff0ef9f99a1929059764b11247fc", "sha256": "d5fa00e1fccea3cb7bf329e823524b4f1ee21b375768ddb4b8788a4b829751df"}, "downloads": -1, "filename": "downhill-0.3.1.tar.gz", "has_sig": false, "md5_digest": "05a1ff0ef9f99a1929059764b11247fc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17938, "upload_time": "2015-09-27T19:37:43", "upload_time_iso_8601": "2015-09-27T19:37:43.876790Z", "url": "https://files.pythonhosted.org/packages/ca/f5/4d2209e99744b43c8dd2fe4bbe04a7452ec2808232b8364a0f9277b55233/downhill-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "d1f6195d1c5fb98a8b8f37bdce5c6c4a", "sha256": "1d32ef7a5687b2765ddd00258fafd771ce43c4882a308f1258c8de8ea8ca2d1a"}, "downloads": -1, "filename": "downhill-0.3.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "d1f6195d1c5fb98a8b8f37bdce5c6c4a", "packagetype": "bdist_wheel", "python_version": "3.4", "requires_python": null, "size": 24850, "upload_time": "2015-12-08T17:29:46", "upload_time_iso_8601": "2015-12-08T17:29:46.850676Z", "url": "https://files.pythonhosted.org/packages/e0/e8/be02d03cc1484bb1034d307c66fcd415f606e79623475046a94b076873a7/downhill-0.3.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "31f29e82ca3af46d986b9c292558b1c3", "sha256": "32b7a7dc10073f4f937ecbca0dc3582fae6dd5dfa147ae663b4fcc6cac982d24"}, "downloads": -1, "filename": "downhill-0.3.2.tar.gz", "has_sig": false, "md5_digest": "31f29e82ca3af46d986b9c292558b1c3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18800, "upload_time": "2015-12-08T17:29:42", "upload_time_iso_8601": "2015-12-08T17:29:42.401905Z", "url": "https://files.pythonhosted.org/packages/3c/a2/b4f904fc008a2f6bff3f5d8dd7e18c334b97afebf2acaf1902bc22be4347/downhill-0.3.2.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "cb6f063d242a085de20b3ff38cab25e9", "sha256": "29e3dbf4db13021734c5bbef0eef230a17c49dfd4155a41016b712f909868f1b"}, "downloads": -1, "filename": "downhill-0.4.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "cb6f063d242a085de20b3ff38cab25e9", "packagetype": "bdist_wheel", "python_version": "3.5", "requires_python": null, "size": 24621, "upload_time": "2017-01-12T17:54:01", "upload_time_iso_8601": "2017-01-12T17:54:01.104212Z", "url": "https://files.pythonhosted.org/packages/32/07/fb2b465371d80d5686328640f31ad403193fe91d527cca538ff1834880b1/downhill-0.4.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "be37d1834489f2d2f04780fa1fa09a25", "sha256": "074ad91deb06c05108c67d982ef71ffffb6ede2c77201abc69e332649f823b42"}, "downloads": -1, "filename": "downhill-0.4.0.tar.gz", "has_sig": false, "md5_digest": "be37d1834489f2d2f04780fa1fa09a25", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20035, "upload_time": "2017-01-12T17:53:58", "upload_time_iso_8601": "2017-01-12T17:53:58.752660Z", "url": "https://files.pythonhosted.org/packages/d5/0d/7f07a67ee0f4890d8a924ee6e12a6eb8a445b4b55fc40fff48d8d9857dfa/downhill-0.4.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cb6f063d242a085de20b3ff38cab25e9", "sha256": "29e3dbf4db13021734c5bbef0eef230a17c49dfd4155a41016b712f909868f1b"}, "downloads": -1, "filename": "downhill-0.4.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "cb6f063d242a085de20b3ff38cab25e9", "packagetype": "bdist_wheel", "python_version": "3.5", "requires_python": null, "size": 24621, "upload_time": "2017-01-12T17:54:01", "upload_time_iso_8601": "2017-01-12T17:54:01.104212Z", "url": "https://files.pythonhosted.org/packages/32/07/fb2b465371d80d5686328640f31ad403193fe91d527cca538ff1834880b1/downhill-0.4.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "be37d1834489f2d2f04780fa1fa09a25", "sha256": "074ad91deb06c05108c67d982ef71ffffb6ede2c77201abc69e332649f823b42"}, "downloads": -1, "filename": "downhill-0.4.0.tar.gz", "has_sig": false, "md5_digest": "be37d1834489f2d2f04780fa1fa09a25", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20035, "upload_time": "2017-01-12T17:53:58", "upload_time_iso_8601": "2017-01-12T17:53:58.752660Z", "url": "https://files.pythonhosted.org/packages/d5/0d/7f07a67ee0f4890d8a924ee6e12a6eb8a445b4b55fc40fff48d8d9857dfa/downhill-0.4.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:07 2020"}