{"info": {"author": "Mikhail Korobov", "author_email": "kmike84@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "===========\nscrapy-poet\n===========\n\n.. image:: https://img.shields.io/pypi/v/scrapy-poet.svg\n   :target: https://pypi.python.org/pypi/scrapy-poet\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/scrapy-poet.svg\n   :target: https://pypi.python.org/pypi/scrapy-poet\n   :alt: Supported Python Versions\n\n.. image:: https://travis-ci.com/scrapinghub/scrapy-po.svg?branch=master\n   :target: https://travis-ci.com/scrapinghub/scrapy-po\n   :alt: Build Status\n\n.. image:: https://codecov.io/github/scrapinghub/scrapy-poet/coverage.svg?branch=master\n   :target: https://codecov.io/gh/scrapinghub/scrapy-poet\n   :alt: Coverage report\n\n.. warning::\n    Current status is \"experimental\".\n\n``scrapy-poet`` implements Page Object pattern for Scrapy.\n\nLicense is BSD 3-clause.\n\nInstallation\n============\n\n::\n\n    pip install scrapy-poet\n\nscrapy-poet requires Python >= 3.6 and Scrapy 2.1.0+.\n\nUsage\n=====\n\nFirst, enable middleware in your settings.py::\n\n    DOWNLOADER_MIDDLEWARES = {\n       'scrapy_poet.InjectionMiddleware': 543,\n    }\n\nAfter that you can write spiders which use page object pattern to separate\nextraction code from a spider:\n\n.. code-block:: python\n\n    import scrapy\n    from web_poet.pages import WebPage\n\n\n    class BookPage(WebPage):\n        def to_item(self):\n            return {\n                'url': self.url,\n                'name': self.css(\"title::text\").get(),\n            }\n\n\n    class BooksSpider(scrapy.Spider):\n        name = 'books'\n        start_urls = ['http://books.toscrape.com/']\n\n        def parse(self, response):\n            for url in response.css('.image_container a::attr(href)').getall():\n                yield response.follow(url, self.parse_book)\n\n        def parse_book(self, response, book_page: BookPage):\n            yield book_page.to_item()\n\nTODO: document motivation, the rest of the features, provide\nmore usage examples, explain shortcuts, etc.\nFor now, please check spiders in \"example\" folder:\nhttps://github.com/scrapinghub/scrapy-poet/tree/master/example/example/spiders\n\nContributing\n============\n\n* Source code: https://github.com/scrapinghub/scrapy-poet\n* Issue tracker: https://github.com/scrapinghub/scrapy-poet/issues\n\nUse tox_ to run tests with different Python versions::\n\n    tox\n\nThe command above also runs type checks; we use mypy.\n\n.. _tox: https://tox.readthedocs.io\n\n\nChanges\n=======\n\n0.0.2 (2020-04-28)\n------------------\n\nThe repository is renamed to ``scrapy-poet``, and split into two:\n\n* ``web-poet`` (https://github.com/scrapinghub/web-poet) contains\n  definitions and code useful for writing Page Objects for web\n  data extraction - it is not tied to Scrapy;\n* ``scrapy-poet`` (this package) provides Scrapy integration for such\n  Page Objects.\n\nAPI of the library changed in a backwards incompatible way;\nsee README and examples.\n\nNew features:\n\n* ``DummyResponse`` annotation allows to skip downloading of scrapy Response.\n* ``callback_for`` works for Scrapy disk queues if it is used to create\n  a spider method (but not in its inline form)\n* Page objects may require page objects as dependencies; dependencies are\n  resolved recursively and built as needed.\n* InjectionMiddleware supports ``async def`` and asyncio providers.\n\n\n0.0.1 (2019-08-28)\n------------------\n\nInitial release.", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/scrapinghub/scrapy-poet", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "scrapy-poet", "package_url": "https://pypi.org/project/scrapy-poet/", "platform": "", "project_url": "https://pypi.org/project/scrapy-poet/", "project_urls": {"Homepage": "https://github.com/scrapinghub/scrapy-poet"}, "release_url": "https://pypi.org/project/scrapy-poet/0.0.2/", "requires_dist": null, "requires_python": "", "summary": "Page Object pattern for Scrapy", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://pypi.python.org/pypi/scrapy-poet\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3da329f0eda67ce7240e79b27a9bbbf94156d3ac/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363726170792d706f65742e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/scrapy-poet\" rel=\"nofollow\"><img alt=\"Supported Python Versions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/24f0a7ac98e469fc50a4df3e98d5c8e474bde5ef/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363726170792d706f65742e737667\"></a>\n<a href=\"https://travis-ci.com/scrapinghub/scrapy-po\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/13dd6e36351c5f752d45bb6668a1f2bd72996917/68747470733a2f2f7472617669732d63692e636f6d2f7363726170696e676875622f7363726170792d706f2e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://codecov.io/gh/scrapinghub/scrapy-poet\" rel=\"nofollow\"><img alt=\"Coverage report\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f0624c37ebec7f57b2201d953c1a6db85c4e9fc9/68747470733a2f2f636f6465636f762e696f2f6769746875622f7363726170696e676875622f7363726170792d706f65742f636f7665726167652e7376673f6272616e63683d6d6173746572\"></a>\n<div>\n<p>Warning</p>\n<p>Current status is \u201cexperimental\u201d.</p>\n</div>\n<p><tt><span class=\"pre\">scrapy-poet</span></tt> implements Page Object pattern for Scrapy.</p>\n<p>License is BSD 3-clause.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<pre>pip install scrapy-poet\n</pre>\n<p>scrapy-poet requires Python &gt;= 3.6 and Scrapy 2.1.0+.</p>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>First, enable middleware in your settings.py:</p>\n<pre>DOWNLOADER_MIDDLEWARES = {\n   'scrapy_poet.InjectionMiddleware': 543,\n}\n</pre>\n<p>After that you can write spiders which use page object pattern to separate\nextraction code from a spider:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">scrapy</span>\n<span class=\"kn\">from</span> <span class=\"nn\">web_poet.pages</span> <span class=\"kn\">import</span> <span class=\"n\">WebPage</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">BookPage</span><span class=\"p\">(</span><span class=\"n\">WebPage</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">to_item</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"p\">{</span>\n            <span class=\"s1\">'url'</span><span class=\"p\">:</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">url</span><span class=\"p\">,</span>\n            <span class=\"s1\">'name'</span><span class=\"p\">:</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">css</span><span class=\"p\">(</span><span class=\"s2\">\"title::text\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(),</span>\n        <span class=\"p\">}</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">BooksSpider</span><span class=\"p\">(</span><span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Spider</span><span class=\"p\">):</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s1\">'books'</span>\n    <span class=\"n\">start_urls</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'http://books.toscrape.com/'</span><span class=\"p\">]</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">):</span>\n        <span class=\"k\">for</span> <span class=\"n\">url</span> <span class=\"ow\">in</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">css</span><span class=\"p\">(</span><span class=\"s1\">'.image_container a::attr(href)'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">getall</span><span class=\"p\">():</span>\n            <span class=\"k\">yield</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">follow</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">parse_book</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">parse_book</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">,</span> <span class=\"n\">book_page</span><span class=\"p\">:</span> <span class=\"n\">BookPage</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"n\">book_page</span><span class=\"o\">.</span><span class=\"n\">to_item</span><span class=\"p\">()</span>\n</pre>\n<p>TODO: document motivation, the rest of the features, provide\nmore usage examples, explain shortcuts, etc.\nFor now, please check spiders in \u201cexample\u201d folder:\n<a href=\"https://github.com/scrapinghub/scrapy-poet/tree/master/example/example/spiders\" rel=\"nofollow\">https://github.com/scrapinghub/scrapy-poet/tree/master/example/example/spiders</a></p>\n</div>\n<div id=\"contributing\">\n<h2>Contributing</h2>\n<ul>\n<li>Source code: <a href=\"https://github.com/scrapinghub/scrapy-poet\" rel=\"nofollow\">https://github.com/scrapinghub/scrapy-poet</a></li>\n<li>Issue tracker: <a href=\"https://github.com/scrapinghub/scrapy-poet/issues\" rel=\"nofollow\">https://github.com/scrapinghub/scrapy-poet/issues</a></li>\n</ul>\n<p>Use <a href=\"https://tox.readthedocs.io\" rel=\"nofollow\">tox</a> to run tests with different Python versions:</p>\n<pre>tox\n</pre>\n<p>The command above also runs type checks; we use mypy.</p>\n</div>\n<div id=\"changes\">\n<h2>Changes</h2>\n<div id=\"id1\">\n<h3>0.0.2 (2020-04-28)</h3>\n<p>The repository is renamed to <tt><span class=\"pre\">scrapy-poet</span></tt>, and split into two:</p>\n<ul>\n<li><tt><span class=\"pre\">web-poet</span></tt> (<a href=\"https://github.com/scrapinghub/web-poet\" rel=\"nofollow\">https://github.com/scrapinghub/web-poet</a>) contains\ndefinitions and code useful for writing Page Objects for web\ndata extraction - it is not tied to Scrapy;</li>\n<li><tt><span class=\"pre\">scrapy-poet</span></tt> (this package) provides Scrapy integration for such\nPage Objects.</li>\n</ul>\n<p>API of the library changed in a backwards incompatible way;\nsee README and examples.</p>\n<p>New features:</p>\n<ul>\n<li><tt>DummyResponse</tt> annotation allows to skip downloading of scrapy Response.</li>\n<li><tt>callback_for</tt> works for Scrapy disk queues if it is used to create\na spider method (but not in its inline form)</li>\n<li>Page objects may require page objects as dependencies; dependencies are\nresolved recursively and built as needed.</li>\n<li>InjectionMiddleware supports <tt>async def</tt> and asyncio providers.</li>\n</ul>\n</div>\n<div id=\"id2\">\n<h3>0.0.1 (2019-08-28)</h3>\n<p>Initial release.</p>\n</div>\n</div>\n\n          </div>"}, "last_serial": 7115280, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "506e9c55ddecd5dc75171a093be538c0", "sha256": "b751e9a796a867a7a42ae2cfec685c6054cbc99cebdea0b0efe6313932a77f48"}, "downloads": -1, "filename": "scrapy-poet-0.0.2.tar.gz", "has_sig": false, "md5_digest": "506e9c55ddecd5dc75171a093be538c0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51392, "upload_time": "2020-04-27T21:12:29", "upload_time_iso_8601": "2020-04-27T21:12:29.494347Z", "url": "https://files.pythonhosted.org/packages/a8/dc/de7640a90539b3c4b4a08912cbb3f147ff106f395abebcecbb1c3b99b692/scrapy-poet-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "506e9c55ddecd5dc75171a093be538c0", "sha256": "b751e9a796a867a7a42ae2cfec685c6054cbc99cebdea0b0efe6313932a77f48"}, "downloads": -1, "filename": "scrapy-poet-0.0.2.tar.gz", "has_sig": false, "md5_digest": "506e9c55ddecd5dc75171a093be538c0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51392, "upload_time": "2020-04-27T21:12:29", "upload_time_iso_8601": "2020-04-27T21:12:29.494347Z", "url": "https://files.pythonhosted.org/packages/a8/dc/de7640a90539b3c4b4a08912cbb3f147ff106f395abebcecbb1c3b99b692/scrapy-poet-0.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:44 2020"}