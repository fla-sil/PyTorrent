{"info": {"author": "Bernhard Raml", "author_email": "pypi-reinforcment@googlegroups.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development :: Testing"], "description": "[![Build Status](https://travis-ci.org/SwamyDev/gym-quickcheck.svg?branch=master)](https://travis-ci.org/SwamyDev/gym-quickcheck) [![Coverage Status](https://coveralls.io/repos/github/SwamyDev/gym-quickcheck/badge.svg?branch=master)](https://coveralls.io/github/SwamyDev/gym-quickcheck?branch=master) [![PyPI version](https://badge.fury.io/py/gym-quickcheck.svg)](https://badge.fury.io/py/gym-quickcheck)\n\n# gym-quickcheck\nMany bugs and implementation errors can already be spotted by running the agent in relatively simple environments. This gym extension provides environments which run fast even on low spec VMs and can be used in Continuous Integration tests. This project aims to help improve code quality and stability of Reinforcement Learning algorithms by providing additional means for automated testing.\n\n## Installation\nYou can install the package using pip:\n```bash\npip install gym-quickcheck\n```\n\n## Quick Start\n\n### Random Walk\nA random agent navigating the random walk environment, rendering a textual representation to the standard output:\n\n[embedmd]:# (examples/random_walk.py python)\n```python\nimport gym\n\nenv = gym.make('gym_quickcheck:random-walk-v0')\ndone = False\nobservation = env.reset()\nwhile not done:\n    observation, reward, done, info = env.step(env.action_space.sample())\n    env.render()\n    print(f\"Observation: {observation}, Reward: {reward}\")\n```\n\nRunning the example should produce an output similar to this:\n```\n...\n(Left)\n#######\nObservation: [0. 0. 0. 0. 0. 1. 0.], Reward: -1\n(Right)\n#######\nObservation: [0. 0. 0. 0. 0. 0. 1.], Reward: 1\n```\n\n### Alternation\nA random agent navigating the alteration environment, rendering a textual representation to the standard output:\n\n[embedmd]:# (examples/alternation.py python)\n```python\nimport gym\n\nenv = gym.make('gym_quickcheck:alternation-v0')\ndone = False\nobservation = env.reset()\nwhile not done:\n    observation, reward, done, info = env.step(env.action_space.sample())\n    env.render()\n    print(f\"Observation: {observation}, Reward: {reward}\")\n```\n\nRunning the example should produce an output similar to this:\n```\n...\n(Right)\n##\nObservation: [0 1], Reward: -0.9959229664071392\n(Left)\n##\nObservation: [1 0], Reward: 0.8693727604523271\n```\n\n### N-Knob\nA random agent trying random values for the correct knob settings, rendering a textual representation to the standard output:\n\n[embedmd]:# (examples/n_knob.py python)\n```python\nimport gym\n\nenv = gym.make('gym_quickcheck:n-knob-v0')\ndone = False\nobservation = env.reset()\nwhile not done:\n    observation, reward, done, info = env.step(env.action_space.sample())\n    env.render()\n    print(f\"Observation: {observation}, Reward: {reward}\")\n```\n\nRunning the example should produce an output similar to this:\n```\n...\nObservation: [-1. -1. -1. -1. -1. -1. -1.], Reward: -1\n(0.315/-0.791) (0.111/0.905) (-0.198/0.278) (-0.008/-0.918) (-0.848/0.477) (-0.447/0.510) (0.642/0.665)\nObservation: [ 1. -1. -1.  1. -1. -1. -1.], Reward: -1\n(0.315/0.648) (0.111/-0.968) (-0.198/0.666) (-0.008/0.404) (-0.848/0.652) (-0.447/-0.453) (0.642/-0.497)\nObservation: [-1.  1. -1. -1. -1.  0.  1.], Reward: -1\n```\n\n## Random Walk\nThis random walk environment is similar to the one described in [Reinforcement Learning An Introduction](http://incompleteideas.net/book/the-book-2nd.html). It differs in having max episode length instead of terminating at both ends, and in penalizing each step except the goal.\n\n![random walk graph](assets/random-walk.png)\n\nThe agent receives a reward of 1 when it reaches the goal, which is the rightmost cell and -1 on reaching any other cell. The environment either terminates upon reaching the goal or after a maximum amount of steps. First, this ensures that the environment has an upper bound of episodes it takes to complete, making testing faster. Second, because the maximum negative reward has a lower bound that is reached quickly, reasonable baseline estimates should improve learning significantly. With baselines having such a noticeable effect, it makes this environment well suited for testing algorithms which make use of baseline estimates. \n\n## Alternation\nThe alteration environment is straightforward, as it just requires the agent to alternate between its two possible states to achieve the maximum reward.\n\n![alteration graph](assets/alteration.png)\n\nThe agent receives a normally distributed reward of 1 when switching from one state to the other, and a normally distributes penalty of -1 when staying in its current state. The environment terminates after a fixed amount of steps. This environment's rewards nicely scale linearly with performance. Meaning if the agent alternates one sequence more, it gets precisely one more reward. It makes it easier for agents not to get stuck at local minima. Hence most agents should be able to learn the optimal policy quickly. However, a random agent only achieves, on average, a total reward around zero. It makes this environment well suited for sanity checking algorithms making sure that they learn at all. By providing such a simple setup, it is also easier to comprehend any obvious problems an algorithm might have.  \n\n## N-Knob\nThe knob environment initially chooses random floating-point values as the correct \"knob\" settings. The goal for the agent is to recover these settings. To accomplish this the environment gives hints to the direction of the correct value as observations. For instance, if the correct knob value is -0.3 and the agent sets the action value to 0.5 the observation would return -1 indicating to the agent that its value is too high. The environment is very simple to solve efficiently, however, a purely random agent can't solve it within the given time frame of 200 steps. This makes it a good testing environment to be used to check the learning behaviour of algorithms. \n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/SwamyDev/gym-quickcheck", "keywords": "OpeanAI gym testing continuous integration", "license": "", "maintainer": "", "maintainer_email": "", "name": "gym-quickcheck", "package_url": "https://pypi.org/project/gym-quickcheck/", "platform": "", "project_url": "https://pypi.org/project/gym-quickcheck/", "project_urls": {"Bug Reports": "https://github.com/SwamyDev/gym-quickcheck/issues", "Homepage": "https://github.com/SwamyDev/gym-quickcheck", "Source": "https://github.com/SwamyDev/gym-quickcheck"}, "release_url": "https://pypi.org/project/gym-quickcheck/1.2.1/", "requires_dist": ["gym", "pytest ; extra == 'test'"], "requires_python": "", "summary": "Gym environments that allow for coarse but fast testing of AI agents.", "version": "1.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://travis-ci.org/SwamyDev/gym-quickcheck\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5adbeded29f863439dd3467a2e0ca9b2eefe9088/68747470733a2f2f7472617669732d63692e6f72672f5377616d794465762f67796d2d717569636b636865636b2e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://coveralls.io/github/SwamyDev/gym-quickcheck?branch=master\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c2ff784cc9c6f2dd9736e167f610d72804d04aac/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f5377616d794465762f67796d2d717569636b636865636b2f62616467652e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://badge.fury.io/py/gym-quickcheck\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/642f3ec5ee72e3150c1f77bad8f2fd36387bea56/68747470733a2f2f62616467652e667572792e696f2f70792f67796d2d717569636b636865636b2e737667\"></a></p>\n<h1>gym-quickcheck</h1>\n<p>Many bugs and implementation errors can already be spotted by running the agent in relatively simple environments. This gym extension provides environments which run fast even on low spec VMs and can be used in Continuous Integration tests. This project aims to help improve code quality and stability of Reinforcement Learning algorithms by providing additional means for automated testing.</p>\n<h2>Installation</h2>\n<p>You can install the package using pip:</p>\n<pre>pip install gym-quickcheck\n</pre>\n<h2>Quick Start</h2>\n<h3>Random Walk</h3>\n<p>A random agent navigating the random walk environment, rendering a textual representation to the standard output:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">'gym_quickcheck:random-walk-v0'</span><span class=\"p\">)</span>\n<span class=\"n\">done</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n<span class=\"n\">observation</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n<span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">done</span><span class=\"p\">:</span>\n    <span class=\"n\">observation</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">info</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">action_space</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">())</span>\n    <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">render</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"Observation: </span><span class=\"si\">{</span><span class=\"n\">observation</span><span class=\"si\">}</span><span class=\"s2\">, Reward: </span><span class=\"si\">{</span><span class=\"n\">reward</span><span class=\"si\">}</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n</pre>\n<p>Running the example should produce an output similar to this:</p>\n<pre><code>...\n(Left)\n#######\nObservation: [0. 0. 0. 0. 0. 1. 0.], Reward: -1\n(Right)\n#######\nObservation: [0. 0. 0. 0. 0. 0. 1.], Reward: 1\n</code></pre>\n<h3>Alternation</h3>\n<p>A random agent navigating the alteration environment, rendering a textual representation to the standard output:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">'gym_quickcheck:alternation-v0'</span><span class=\"p\">)</span>\n<span class=\"n\">done</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n<span class=\"n\">observation</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n<span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">done</span><span class=\"p\">:</span>\n    <span class=\"n\">observation</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">info</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">action_space</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">())</span>\n    <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">render</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"Observation: </span><span class=\"si\">{</span><span class=\"n\">observation</span><span class=\"si\">}</span><span class=\"s2\">, Reward: </span><span class=\"si\">{</span><span class=\"n\">reward</span><span class=\"si\">}</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n</pre>\n<p>Running the example should produce an output similar to this:</p>\n<pre><code>...\n(Right)\n##\nObservation: [0 1], Reward: -0.9959229664071392\n(Left)\n##\nObservation: [1 0], Reward: 0.8693727604523271\n</code></pre>\n<h3>N-Knob</h3>\n<p>A random agent trying random values for the correct knob settings, rendering a textual representation to the standard output:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">'gym_quickcheck:n-knob-v0'</span><span class=\"p\">)</span>\n<span class=\"n\">done</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n<span class=\"n\">observation</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n<span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">done</span><span class=\"p\">:</span>\n    <span class=\"n\">observation</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">info</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">action_space</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">())</span>\n    <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">render</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"Observation: </span><span class=\"si\">{</span><span class=\"n\">observation</span><span class=\"si\">}</span><span class=\"s2\">, Reward: </span><span class=\"si\">{</span><span class=\"n\">reward</span><span class=\"si\">}</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n</pre>\n<p>Running the example should produce an output similar to this:</p>\n<pre><code>...\nObservation: [-1. -1. -1. -1. -1. -1. -1.], Reward: -1\n(0.315/-0.791) (0.111/0.905) (-0.198/0.278) (-0.008/-0.918) (-0.848/0.477) (-0.447/0.510) (0.642/0.665)\nObservation: [ 1. -1. -1.  1. -1. -1. -1.], Reward: -1\n(0.315/0.648) (0.111/-0.968) (-0.198/0.666) (-0.008/0.404) (-0.848/0.652) (-0.447/-0.453) (0.642/-0.497)\nObservation: [-1.  1. -1. -1. -1.  0.  1.], Reward: -1\n</code></pre>\n<h2>Random Walk</h2>\n<p>This random walk environment is similar to the one described in <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" rel=\"nofollow\">Reinforcement Learning An Introduction</a>. It differs in having max episode length instead of terminating at both ends, and in penalizing each step except the goal.</p>\n<p><img alt=\"random walk graph\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b79706d22ff711c8b9da3995c2b4e44057cb68e2/6173736574732f72616e646f6d2d77616c6b2e706e67\"></p>\n<p>The agent receives a reward of 1 when it reaches the goal, which is the rightmost cell and -1 on reaching any other cell. The environment either terminates upon reaching the goal or after a maximum amount of steps. First, this ensures that the environment has an upper bound of episodes it takes to complete, making testing faster. Second, because the maximum negative reward has a lower bound that is reached quickly, reasonable baseline estimates should improve learning significantly. With baselines having such a noticeable effect, it makes this environment well suited for testing algorithms which make use of baseline estimates.</p>\n<h2>Alternation</h2>\n<p>The alteration environment is straightforward, as it just requires the agent to alternate between its two possible states to achieve the maximum reward.</p>\n<p><img alt=\"alteration graph\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/de8fcb504bea0b781b2440b51fb71b88a5f2328a/6173736574732f616c7465726174696f6e2e706e67\"></p>\n<p>The agent receives a normally distributed reward of 1 when switching from one state to the other, and a normally distributes penalty of -1 when staying in its current state. The environment terminates after a fixed amount of steps. This environment's rewards nicely scale linearly with performance. Meaning if the agent alternates one sequence more, it gets precisely one more reward. It makes it easier for agents not to get stuck at local minima. Hence most agents should be able to learn the optimal policy quickly. However, a random agent only achieves, on average, a total reward around zero. It makes this environment well suited for sanity checking algorithms making sure that they learn at all. By providing such a simple setup, it is also easier to comprehend any obvious problems an algorithm might have.</p>\n<h2>N-Knob</h2>\n<p>The knob environment initially chooses random floating-point values as the correct \"knob\" settings. The goal for the agent is to recover these settings. To accomplish this the environment gives hints to the direction of the correct value as observations. For instance, if the correct knob value is -0.3 and the agent sets the action value to 0.5 the observation would return -1 indicating to the agent that its value is too high. The environment is very simple to solve efficiently, however, a purely random agent can't solve it within the given time frame of 200 steps. This makes it a good testing environment to be used to check the learning behaviour of algorithms.</p>\n\n          </div>"}, "last_serial": 6677086, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "3e5c4f1db2bf49e8659d0957dbb3cded", "sha256": "4fa21cc5dffbefc9322f117a49d2303e37d305921692a8f02100e3d1f9f640a9"}, "downloads": -1, "filename": "gym_quickcheck-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3e5c4f1db2bf49e8659d0957dbb3cded", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 3277, "upload_time": "2019-10-06T19:30:34", "upload_time_iso_8601": "2019-10-06T19:30:34.719158Z", "url": "https://files.pythonhosted.org/packages/44/d9/4f33b5f7a8738b83f8be711d2e4de6901b8a77099e94471cc385beab652e/gym_quickcheck-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ddb1000be857b0323fe85888d3dbfb58", "sha256": "c3e0acbb4ce00d83632bb92f9d845683594f4027a8d031e895e91fae9918bd42"}, "downloads": -1, "filename": "gym_quickcheck-1.0.0.tar.gz", "has_sig": false, "md5_digest": "ddb1000be857b0323fe85888d3dbfb58", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2696, "upload_time": "2019-10-06T19:30:37", "upload_time_iso_8601": "2019-10-06T19:30:37.325532Z", "url": "https://files.pythonhosted.org/packages/b8/d0/dae5b6a889ce0acfde2c8cc35df4317f9f2a71650b7cfecd375a6d91a253/gym_quickcheck-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "331abc9bdd6a6a990b82cf667ae93540", "sha256": "16af1acb29479409e85c608ff6bff6a168ac0af06071a0ea9a36e1ecb94e6017"}, "downloads": -1, "filename": "gym_quickcheck-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "331abc9bdd6a6a990b82cf667ae93540", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 3353, "upload_time": "2019-10-06T19:59:47", "upload_time_iso_8601": "2019-10-06T19:59:47.498384Z", "url": "https://files.pythonhosted.org/packages/9f/53/54db1ac97278868ecd1db5256140c39db433b5ca4f5e572dccb9b41fd55f/gym_quickcheck-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ac41a95bcf4c75b351460129ea27e040", "sha256": "9dbc36a34fa3c5d944d0b6983367db7da90f2ea0aa1e570bb67f0a0002c324dc"}, "downloads": -1, "filename": "gym_quickcheck-1.0.1.tar.gz", "has_sig": false, "md5_digest": "ac41a95bcf4c75b351460129ea27e040", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2809, "upload_time": "2019-10-06T19:59:48", "upload_time_iso_8601": "2019-10-06T19:59:48.897185Z", "url": "https://files.pythonhosted.org/packages/64/e3/c2fd80a556d0999201f9739fb430788280f6d6ca2cdb9ff7c05b1be856e4/gym_quickcheck-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "baaee5758df3c128f5ed9681ab51fdbd", "sha256": "19fb90c1c0631f9c6301b49337b63c3a1237ca801219015a588b02f305b5979b"}, "downloads": -1, "filename": "gym_quickcheck-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "baaee5758df3c128f5ed9681ab51fdbd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5175, "upload_time": "2019-10-06T20:19:07", "upload_time_iso_8601": "2019-10-06T20:19:07.247176Z", "url": "https://files.pythonhosted.org/packages/89/cb/6283bdc7fe79c710df53464099090d561125f0f812dda22f46bccdaf802f/gym_quickcheck-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bc4bbf548d90c25106e0f240dbe31baf", "sha256": "9f752af6adf29d504cef7779dde7ac220449d32ae1c1aebf79d34fdc952b877f"}, "downloads": -1, "filename": "gym_quickcheck-1.0.2.tar.gz", "has_sig": false, "md5_digest": "bc4bbf548d90c25106e0f240dbe31baf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3864, "upload_time": "2019-10-06T20:19:08", "upload_time_iso_8601": "2019-10-06T20:19:08.631541Z", "url": "https://files.pythonhosted.org/packages/5c/56/b0f8b1432e02501426badb58f6c85a61b5b413101c47b5ba40fba96fd53f/gym_quickcheck-1.0.2.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "5b07b70e5bebbc886c432e8187b50da3", "sha256": "5481e656280142418c6813f5693997dd719246c0857f91fd1234a9147fb3f5b7"}, "downloads": -1, "filename": "gym_quickcheck-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5b07b70e5bebbc886c432e8187b50da3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7613, "upload_time": "2019-10-27T13:00:41", "upload_time_iso_8601": "2019-10-27T13:00:41.588392Z", "url": "https://files.pythonhosted.org/packages/27/31/0ae3a12ec3f4b9ba19ad4fa8a60261ba00400e9e9b1aaf73a5943cb7397d/gym_quickcheck-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d834ecdbf0a5fb696903b078d2807081", "sha256": "24e478f47f33e8ed5ceaf954b1d655bf034f003251d1a153cb767e5dc64e1869"}, "downloads": -1, "filename": "gym_quickcheck-1.1.0.tar.gz", "has_sig": false, "md5_digest": "d834ecdbf0a5fb696903b078d2807081", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5681, "upload_time": "2019-10-27T13:00:42", "upload_time_iso_8601": "2019-10-27T13:00:42.960652Z", "url": "https://files.pythonhosted.org/packages/a3/94/84862af9723e4eb58d0e8bc0fc1f0b07f59e5ae516861b17bc32f8691b05/gym_quickcheck-1.1.0.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "41c6a2b5bbc8689b37c07b402c1ebdc5", "sha256": "1d25067fe4731538d00249e8b35518962aa445475cc7c7c462e3e5c376523cd4"}, "downloads": -1, "filename": "gym_quickcheck-1.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "41c6a2b5bbc8689b37c07b402c1ebdc5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9107, "upload_time": "2020-02-18T21:06:30", "upload_time_iso_8601": "2020-02-18T21:06:30.250379Z", "url": "https://files.pythonhosted.org/packages/da/85/a6ccc0c17c871b3622aa63f7e6ff598f019f1ad235ec763ba277b9d72f3b/gym_quickcheck-1.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "71996424633066ada8057b83db91fb3c", "sha256": "e4a4a0b938ba74809be0f2d3019c0094e79f99434d9ad5c8b4a40cdadfd131ed"}, "downloads": -1, "filename": "gym_quickcheck-1.2.0.tar.gz", "has_sig": false, "md5_digest": "71996424633066ada8057b83db91fb3c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6728, "upload_time": "2020-02-18T21:06:31", "upload_time_iso_8601": "2020-02-18T21:06:31.071273Z", "url": "https://files.pythonhosted.org/packages/02/c4/7e80d4cc7a3eaa3711ebb65740eb37e05bd84919a907a0fbe98d9738e763/gym_quickcheck-1.2.0.tar.gz", "yanked": false}], "1.2.1": [{"comment_text": "", "digests": {"md5": "04b1f92ca364c3b57bb6d243b015615a", "sha256": "bff1ec216667d0ce123642ed9feb6105efd864cf6fde928ae4506cd8420a4781"}, "downloads": -1, "filename": "gym_quickcheck-1.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "04b1f92ca364c3b57bb6d243b015615a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9118, "upload_time": "2020-02-21T19:00:05", "upload_time_iso_8601": "2020-02-21T19:00:05.246741Z", "url": "https://files.pythonhosted.org/packages/d5/25/b815725f46231bb39a98c29077a9d131296ca924718d3fb27efda3f7f8bb/gym_quickcheck-1.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9588bc75577349be7f3170a9002bb156", "sha256": "c593b43f1ed707d248a33dc1aad9e68954b075dd448c27453e9746c04f07a45a"}, "downloads": -1, "filename": "gym_quickcheck-1.2.1.tar.gz", "has_sig": false, "md5_digest": "9588bc75577349be7f3170a9002bb156", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6745, "upload_time": "2020-02-21T19:00:06", "upload_time_iso_8601": "2020-02-21T19:00:06.219671Z", "url": "https://files.pythonhosted.org/packages/f3/66/9131c260c8ca73dc5075bf6ae63307f049a6658f3b2cd54b976c94d2eadf/gym_quickcheck-1.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "04b1f92ca364c3b57bb6d243b015615a", "sha256": "bff1ec216667d0ce123642ed9feb6105efd864cf6fde928ae4506cd8420a4781"}, "downloads": -1, "filename": "gym_quickcheck-1.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "04b1f92ca364c3b57bb6d243b015615a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9118, "upload_time": "2020-02-21T19:00:05", "upload_time_iso_8601": "2020-02-21T19:00:05.246741Z", "url": "https://files.pythonhosted.org/packages/d5/25/b815725f46231bb39a98c29077a9d131296ca924718d3fb27efda3f7f8bb/gym_quickcheck-1.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9588bc75577349be7f3170a9002bb156", "sha256": "c593b43f1ed707d248a33dc1aad9e68954b075dd448c27453e9746c04f07a45a"}, "downloads": -1, "filename": "gym_quickcheck-1.2.1.tar.gz", "has_sig": false, "md5_digest": "9588bc75577349be7f3170a9002bb156", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6745, "upload_time": "2020-02-21T19:00:06", "upload_time_iso_8601": "2020-02-21T19:00:06.219671Z", "url": "https://files.pythonhosted.org/packages/f3/66/9131c260c8ca73dc5075bf6ae63307f049a6658f3b2cd54b976c94d2eadf/gym_quickcheck-1.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:53:00 2020"}