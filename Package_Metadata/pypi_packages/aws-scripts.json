{"info": {"author": "Marcos Martinez", "author_email": "frommelmak@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Topic :: Utilities"], "description": "[![PyPI](https://img.shields.io/pypi/v/aws-scripts.svg)](https://pypi.org/project/aws-scripts/)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://opensource.org/licenses/MIT)\n\naws-scripts\n===========\n\nHere you will find some useful AWS scripts I use from time to time.\n\nAll the scripts relies on [Boto](http://aws.amazon.com/sdkforpython/), a Python package that provides interfaces to Amazon Web Services.\n\nSo, to use these scripts, you need to install Boto and provide your AWS credentinals:\n\nTo install aws-scripts and all the required Python packages just type:\n\n```\npip install aws-scripts \n```\n\nIf dependencies are already satisfied, nothing will be installed.\n\nTo provide your AWS credentials use the boto/boto3 config file `~/.aws/credentials`:\n\n``` ini\n[default]\naws_access_key_id = <XXXXXXXXXXXXXXXXXXX>\naws_secret_access_key = <xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx>\nregion=xx-xxxx-x\n```\n\n> Note that you can use the environment variable: ```AWS_DEFAULT_REGION=xx-xxxx-x``` to override the default region on the config file.\n> In the ec2-instances.py script you can also use the ```--region``` option for the same purpose \n\nec2-instances.py\n----------------\n\nLists the EC2 instances including the Name Tag, IP, type, zone, vpc, ID and the status.\n\nYou can filter the result by name, type and/or status. Or you can provide a list of instance IDs instead.\n\nFinally you can execute remote commands on all the instances returned by the filter or the list.\n\nThe '-h' option shows you how to use the available options.\n\n```\nusage: ec2-instances.py [-h] [-n NAME] [-t TYPE] [-s STATUS]\n                        [-l ID_LIST [ID_LIST ...]] [-e EXECUTE] [-r REGION]\n                        [-u USER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n NAME, --name NAME  Filter result by name.\n  -t TYPE, --type TYPE  Filer result by type.\n  -s STATUS, --status STATUS\n                        Filter result by status.\n  -l ID_LIST [ID_LIST ...], --id_list ID_LIST [ID_LIST ...]\n                        Do not filter the result. Provide a InstanceIds list instead.\n  -i IGNORE, --ignore IGNORE\n                        Do not show hosts lines containing the \"IGNORE\"\n                        pattern in the tag Name\n  -e EXECUTE, --execute EXECUTE\n                        Execute a command on instances\n  -r REGION, --region REGION\n                        Specify an alternate region to override the one\n                        defined in the .aws/credentials file\n  -u USER, --user USER  User to run commands if -e option is used. Ubuntu user\n                        is used by default\n```\n\nec2-reserved.py\n----------------\n\nLists details of all your Instance Reservations, including a summary of the active reservations by type and size.\n\nThe summary also shows your reserved active capacity after apply the normalization factor. This is useful to compare the reserved capacity with the deployed in production.\n\nYou can also use the option `--create-google-calendar-events` to add the expiration date of the active reservations in your Google Calendar Account.\n\n```\nusage: ec2-reserved.py [-h]\n                        [-s {payment-pending,active,payment-failed,retired}]\n                        [--create-google-calendar-events] [-t TYPE]\n\nShow reserved EC2 instances\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s {payment-pending,active,payment-failed,retired}, --state {payment-pending,active,payment-failed,retired}\n                        Filer result by reservation state.\n  --create-google-calendar-events\n                        Create events in your Google Calendar, using the\n                        expiration dates of your active reservations\n  -t TYPE, --type TYPE  Filer result by instance type.\n```\n\nTo use the Google calendar feature you just have to [enable the calendar API in your Google Account](https://console.developers.google.com) and create a calendar called aws in the [Google Calendar](http://calendar.google.com/). Then create the *OAuth client ID* credentials. Download the credentials file and save it as `client_secret.json` in the aws-scripts folder repo. When you run the script using the `--create-google-calendar-events` option for the first time, a web browser will be opened asking your to login with the Google account you want to use.\n\nThen, whenever you buy new reservations on Amazon Web Services, you can add the new reservations in your calendar by just running the script.\n\nec2-ebs.py\n----------\nLists the EC2 EBS volumes including the Name Tag, size, device, ID, attached instance ID, Attached instance Tag Name, type, IOPS, zone and status.\n\nYou can filter the result by tyoe, status and Tag name.\n\nThe '-h' option shows you how to use the available options.\n\n```\nusage: ec2-ebs.py [-h] [-n NAME] [-t {gp2,io1,st1,sc1,standard}] [-s {creating,available,in-use,deleting,deleted,error}]\n\nList all the Elastic Block Storage volumes\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n NAME, --name NAME  Filter result by name.\n  -t {gp2,io1,st1,sc1,standard}, --type {gp2,io1,st1,sc1,standard}\n                        Filer result by type.\n  -s {creating,available,in-use,deleting,deleted,error}, --status {creating,available,in-use,deleting,deleted,error}\n                        Filter result by status.\n```\n\nec2-elb.py\n----------\n\nLists all your Elastic Load Balancers and his related instances.\n\n```\nusage: ec2-elb.py [-h]\n\nFor every Elastic Load Balancer list the attached instances\n\noptional arguments:\n  -h, --help  show this help message and exit\n```\n\nec2-snap-mgmt.py\n----------------\n\nWith this script you can see the relationships between your snapshots and your EBS volumes and AMIs. This allows you to choose the snapshots you don't need to keep in the AWS S3 service.\n\nBy default the script shows all the volumes and AMIs related to each snapshost.\n\nYou you can also show all the snapshots related with each volume. This option is specially usefull when you only want to keep a certain number of snapshots per volume.\n\nFinally, you can show all the snapshots related with each AMI.\n\nThe '-h' option shows you how to use the available options.\n\n```\nusage: ec2-snap-mgmt.py [-h] [-v {orphan,volumes}] owner_id\n\npositional arguments:\n  owner_id              12-digit AWS Account Number\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v {orphan,volumes,images}, --view {orphan,volumes,images}\n                        Available views: orphan and volumes. Orphan is the\n                        default one.\n```\n\nThe script doesn't delete anything actually, just shows you the relationship in a tree view.\n\ns3-mongodump.py\n---------------\n\nThis is a tool to make mongodb backups on Amazon s3.\n\nIt uses mongodump to perform a binary backup of your local or remote mongodb instance. The dumped files are compressed in a tarball file and uploaded to a Amazon S3 bucket.\nYou can specify the number of copies to retain in the bucket. The oldest ones will be automatically removed.\n\n```\nusage: s3-mongodump.py [-h] [-u USER] [-p PASSWORD] [-H HOST] [-d DATABASE]\n                       [-o OUT] [-n NUMBER] -b BUCKET [-P PREFIX]\n\nA tool to make mongodb backups on Amazon s3\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -u USER, --user USER  Mongodb user (optional)\n  -p PASSWORD, --password PASSWORD\n                        Mongodb password (optional)\n  -H HOST, --host HOST  Mongodb host: <hostname>:<port>.\n  -d DATABASE, --database DATABASE\n                        The database to backup (all if not provided)\n  -o OUT, --out OUT     The output directory for dumped files\n  -n NUMBER, --number NUMBER\n                        Number of copies to retain in the S3 bucket\n  -b BUCKET, --bucket BUCKET\n                        Amazon s3 bucket.\n  -P PREFIX, --prefix PREFIX\n                        For grouped objects aka s3 folders, provide the prefix\n                        key\n```\n\nroute53-set-hostname.py\n-----------------------\n\nThis script allows you to automatically set predictable DNS records for instances launched using AWS Auto Scaling. \n\nIt is intended to be executed from the ec2 instance at launch time.\nThe script looks for an available name matching the provided pattern in the DNS zone. Then, it adds this name as a CNAME record in the DNS zone pointing to the EC2 instance public name.\n\n```\nusage: route53-set-hostname.py [-h] --HostedZoneId HOSTEDZONEID --HostStr\n                               HOSTSTR [--rangeSize RANGESIZE] [--dryrun]\n\nAWS Route53 hostname managment for Autoscaled EC2 Instances\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --HostedZoneId HOSTEDZONEID\n                        The ID of the hosted zone where the new resource\n                        record will be added.\n  --HostStr HOSTSTR     The host string used to build the new name\n  --rangeSize RANGESIZE\n                        The maximun number to be assigned. The first available\n                        will be used\n  --dryrun              Shows what is going to be done but doesn't change\n                        anything actually\n```\n\nExample:\n\n``` bash\nuser@host:~$ ./route53-set-hostname.py --HostedZoneId XXXXXXXXXXXXXX --HostStr websrv --rangeSize 10\n15:41:58 06/09/16: creating CNAME websrv03.example.com. -> ec2-XX-XX-XXX-XX.compute-1.amazonaws.com......INSYNC\n```\n\nroute53-del-hostname.py\n-----------------------\n\nThis script is executed from the ec2 instance at shutdown.\nThe script delete his host record zone from the passed DNS zone identifier.\n\n```\nusage: route53-del-hostname.py [-h] --HostedZoneId HOSTEDZONEID [--dryrun]\n\nAWS Route53 hostname managment for Autoscaled EC2 Instances\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --HostedZoneId HOSTEDZONEID\n                        The ID of the hosted zone where the new resource\n                        record will be added.\n  --dryrun              Shows what is going to be done but doesn't change\n                        anything actually\n```\n\ns3-download-file.py\n-------------------\n\nThis script just download the requested S3 object.\n\n```\nusage: s3-download-file.py [-h] -b BUCKET -o OBJECTKEY -f FILEPATH\n\nDonwload file from AWS S3\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -b BUCKET, --bucket BUCKET\n                        The bucket name.\n  -o OBJECTKEY, --objectkey OBJECTKEY\n                        The host string used to build the new name\n  -f FILEPATH, --filepath FILEPATH\n                        The filepath of the file to be saved\n```\n\nlifecycle-hook-worker.py\n------------------------\n\nAs its own name says, this worker is designed to use auto scaling [lifecycle hooks](http://docs.aws.amazon.com/autoscaling/latest/userguide/lifecycle-hooks.html).\n\nThe process looks for incoming messages into the SQS queue associated with the autoscaling group. Then, when a message comes for the instance, it is consumed and the associated custom action is triggered. Finally, using the lifecycle action token, the worker completes the autoscaling action going on with the launch or ending the instance action.\n\n```\nusage: lifecycle-hook-worker.py [-h] -q QUEUE -s {LAUNCHING,TERMINATING} -g\n                                GROUP -H HOOKNAME -e EXECUTE [-w WAIT]\n\nSQS Lifecycle hook consumer and trigger\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -q QUEUE, --queue QUEUE\n                        Queue resource.\n  -s {LAUNCHING,TERMINATING}, --state {LAUNCHING,TERMINATING}\n                        Indicates if the consumer is waiting for LAUNCHING or\n                        TERMINATING state\n  -g GROUP, --group GROUP\n                        Auto Scaling Group Name\n  -H HOOKNAME, --hookName HOOKNAME\n                        Life Cycle Hook Name\n  -e EXECUTE, --execute EXECUTE\n                        The filepath of the triggered script\n  -w WAIT, --wait WAIT  Time between query loops in seconds (default: 60)\n```\n\nrds-instances.py\n----------------\n\nShows the main info regarding all the RDS instances such as: endpoint, engine, version, status etc.\n\n```\nusage: rds-instances.py [-h]\n\nList all the RDS instances\n\noptional arguments:\n  -h, --help  show this help message and exit\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/frommelmak/aws-scripts", "keywords": "aws amazon-web-services ec2-instance google-calendar-synchronization amazon", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "aws-scripts", "package_url": "https://pypi.org/project/aws-scripts/", "platform": "", "project_url": "https://pypi.org/project/aws-scripts/", "project_urls": {"Homepage": "http://github.com/frommelmak/aws-scripts"}, "release_url": "https://pypi.org/project/aws-scripts/0.0.19/", "requires_dist": null, "requires_python": ">=2.7", "summary": "Some useful AWS scripts I use from time to time", "version": "0.0.19", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://pypi.org/project/aws-scripts/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4bd9744a6a5f63388e2f8f14c7cb65b76b5a54a5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6177732d736372697074732e737667\"></a>\n<a href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"><img alt=\"license\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/36686084675cebbeff3809cb9d8291b8e6ebd672/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e737667\"></a></p>\n<h1>aws-scripts</h1>\n<p>Here you will find some useful AWS scripts I use from time to time.</p>\n<p>All the scripts relies on <a href=\"http://aws.amazon.com/sdkforpython/\" rel=\"nofollow\">Boto</a>, a Python package that provides interfaces to Amazon Web Services.</p>\n<p>So, to use these scripts, you need to install Boto and provide your AWS credentinals:</p>\n<p>To install aws-scripts and all the required Python packages just type:</p>\n<pre><code>pip install aws-scripts \n</code></pre>\n<p>If dependencies are already satisfied, nothing will be installed.</p>\n<p>To provide your AWS credentials use the boto/boto3 config file <code>~/.aws/credentials</code>:</p>\n<pre><span class=\"k\">[default]</span>\n<span class=\"na\">aws_access_key_id</span> <span class=\"o\">=</span> <span class=\"s\">&lt;XXXXXXXXXXXXXXXXXXX&gt;</span>\n<span class=\"na\">aws_secret_access_key</span> <span class=\"o\">=</span> <span class=\"s\">&lt;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&gt;</span>\n<span class=\"na\">region</span><span class=\"o\">=</span><span class=\"s\">xx-xxxx-x</span>\n</pre>\n<blockquote>\n<p>Note that you can use the environment variable: <code>AWS_DEFAULT_REGION=xx-xxxx-x</code> to override the default region on the config file.\nIn the ec2-instances.py script you can also use the <code>--region</code> option for the same purpose</p>\n</blockquote>\n<h2>ec2-instances.py</h2>\n<p>Lists the EC2 instances including the Name Tag, IP, type, zone, vpc, ID and the status.</p>\n<p>You can filter the result by name, type and/or status. Or you can provide a list of instance IDs instead.</p>\n<p>Finally you can execute remote commands on all the instances returned by the filter or the list.</p>\n<p>The '-h' option shows you how to use the available options.</p>\n<pre><code>usage: ec2-instances.py [-h] [-n NAME] [-t TYPE] [-s STATUS]\n                        [-l ID_LIST [ID_LIST ...]] [-e EXECUTE] [-r REGION]\n                        [-u USER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n NAME, --name NAME  Filter result by name.\n  -t TYPE, --type TYPE  Filer result by type.\n  -s STATUS, --status STATUS\n                        Filter result by status.\n  -l ID_LIST [ID_LIST ...], --id_list ID_LIST [ID_LIST ...]\n                        Do not filter the result. Provide a InstanceIds list instead.\n  -i IGNORE, --ignore IGNORE\n                        Do not show hosts lines containing the \"IGNORE\"\n                        pattern in the tag Name\n  -e EXECUTE, --execute EXECUTE\n                        Execute a command on instances\n  -r REGION, --region REGION\n                        Specify an alternate region to override the one\n                        defined in the .aws/credentials file\n  -u USER, --user USER  User to run commands if -e option is used. Ubuntu user\n                        is used by default\n</code></pre>\n<h2>ec2-reserved.py</h2>\n<p>Lists details of all your Instance Reservations, including a summary of the active reservations by type and size.</p>\n<p>The summary also shows your reserved active capacity after apply the normalization factor. This is useful to compare the reserved capacity with the deployed in production.</p>\n<p>You can also use the option <code>--create-google-calendar-events</code> to add the expiration date of the active reservations in your Google Calendar Account.</p>\n<pre><code>usage: ec2-reserved.py [-h]\n                        [-s {payment-pending,active,payment-failed,retired}]\n                        [--create-google-calendar-events] [-t TYPE]\n\nShow reserved EC2 instances\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s {payment-pending,active,payment-failed,retired}, --state {payment-pending,active,payment-failed,retired}\n                        Filer result by reservation state.\n  --create-google-calendar-events\n                        Create events in your Google Calendar, using the\n                        expiration dates of your active reservations\n  -t TYPE, --type TYPE  Filer result by instance type.\n</code></pre>\n<p>To use the Google calendar feature you just have to <a href=\"https://console.developers.google.com\" rel=\"nofollow\">enable the calendar API in your Google Account</a> and create a calendar called aws in the <a href=\"http://calendar.google.com/\" rel=\"nofollow\">Google Calendar</a>. Then create the <em>OAuth client ID</em> credentials. Download the credentials file and save it as <code>client_secret.json</code> in the aws-scripts folder repo. When you run the script using the <code>--create-google-calendar-events</code> option for the first time, a web browser will be opened asking your to login with the Google account you want to use.</p>\n<p>Then, whenever you buy new reservations on Amazon Web Services, you can add the new reservations in your calendar by just running the script.</p>\n<h2>ec2-ebs.py</h2>\n<p>Lists the EC2 EBS volumes including the Name Tag, size, device, ID, attached instance ID, Attached instance Tag Name, type, IOPS, zone and status.</p>\n<p>You can filter the result by tyoe, status and Tag name.</p>\n<p>The '-h' option shows you how to use the available options.</p>\n<pre><code>usage: ec2-ebs.py [-h] [-n NAME] [-t {gp2,io1,st1,sc1,standard}] [-s {creating,available,in-use,deleting,deleted,error}]\n\nList all the Elastic Block Storage volumes\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n NAME, --name NAME  Filter result by name.\n  -t {gp2,io1,st1,sc1,standard}, --type {gp2,io1,st1,sc1,standard}\n                        Filer result by type.\n  -s {creating,available,in-use,deleting,deleted,error}, --status {creating,available,in-use,deleting,deleted,error}\n                        Filter result by status.\n</code></pre>\n<h2>ec2-elb.py</h2>\n<p>Lists all your Elastic Load Balancers and his related instances.</p>\n<pre><code>usage: ec2-elb.py [-h]\n\nFor every Elastic Load Balancer list the attached instances\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>\n<h2>ec2-snap-mgmt.py</h2>\n<p>With this script you can see the relationships between your snapshots and your EBS volumes and AMIs. This allows you to choose the snapshots you don't need to keep in the AWS S3 service.</p>\n<p>By default the script shows all the volumes and AMIs related to each snapshost.</p>\n<p>You you can also show all the snapshots related with each volume. This option is specially usefull when you only want to keep a certain number of snapshots per volume.</p>\n<p>Finally, you can show all the snapshots related with each AMI.</p>\n<p>The '-h' option shows you how to use the available options.</p>\n<pre><code>usage: ec2-snap-mgmt.py [-h] [-v {orphan,volumes}] owner_id\n\npositional arguments:\n  owner_id              12-digit AWS Account Number\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v {orphan,volumes,images}, --view {orphan,volumes,images}\n                        Available views: orphan and volumes. Orphan is the\n                        default one.\n</code></pre>\n<p>The script doesn't delete anything actually, just shows you the relationship in a tree view.</p>\n<h2>s3-mongodump.py</h2>\n<p>This is a tool to make mongodb backups on Amazon s3.</p>\n<p>It uses mongodump to perform a binary backup of your local or remote mongodb instance. The dumped files are compressed in a tarball file and uploaded to a Amazon S3 bucket.\nYou can specify the number of copies to retain in the bucket. The oldest ones will be automatically removed.</p>\n<pre><code>usage: s3-mongodump.py [-h] [-u USER] [-p PASSWORD] [-H HOST] [-d DATABASE]\n                       [-o OUT] [-n NUMBER] -b BUCKET [-P PREFIX]\n\nA tool to make mongodb backups on Amazon s3\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -u USER, --user USER  Mongodb user (optional)\n  -p PASSWORD, --password PASSWORD\n                        Mongodb password (optional)\n  -H HOST, --host HOST  Mongodb host: &lt;hostname&gt;:&lt;port&gt;.\n  -d DATABASE, --database DATABASE\n                        The database to backup (all if not provided)\n  -o OUT, --out OUT     The output directory for dumped files\n  -n NUMBER, --number NUMBER\n                        Number of copies to retain in the S3 bucket\n  -b BUCKET, --bucket BUCKET\n                        Amazon s3 bucket.\n  -P PREFIX, --prefix PREFIX\n                        For grouped objects aka s3 folders, provide the prefix\n                        key\n</code></pre>\n<h2>route53-set-hostname.py</h2>\n<p>This script allows you to automatically set predictable DNS records for instances launched using AWS Auto Scaling.</p>\n<p>It is intended to be executed from the ec2 instance at launch time.\nThe script looks for an available name matching the provided pattern in the DNS zone. Then, it adds this name as a CNAME record in the DNS zone pointing to the EC2 instance public name.</p>\n<pre><code>usage: route53-set-hostname.py [-h] --HostedZoneId HOSTEDZONEID --HostStr\n                               HOSTSTR [--rangeSize RANGESIZE] [--dryrun]\n\nAWS Route53 hostname managment for Autoscaled EC2 Instances\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --HostedZoneId HOSTEDZONEID\n                        The ID of the hosted zone where the new resource\n                        record will be added.\n  --HostStr HOSTSTR     The host string used to build the new name\n  --rangeSize RANGESIZE\n                        The maximun number to be assigned. The first available\n                        will be used\n  --dryrun              Shows what is going to be done but doesn't change\n                        anything actually\n</code></pre>\n<p>Example:</p>\n<pre>user@host:~$ ./route53-set-hostname.py --HostedZoneId XXXXXXXXXXXXXX --HostStr websrv --rangeSize <span class=\"m\">10</span>\n<span class=\"m\">15</span>:41:58 <span class=\"m\">06</span>/09/16: creating CNAME websrv03.example.com. -&gt; ec2-XX-XX-XXX-XX.compute-1.amazonaws.com......INSYNC\n</pre>\n<h2>route53-del-hostname.py</h2>\n<p>This script is executed from the ec2 instance at shutdown.\nThe script delete his host record zone from the passed DNS zone identifier.</p>\n<pre><code>usage: route53-del-hostname.py [-h] --HostedZoneId HOSTEDZONEID [--dryrun]\n\nAWS Route53 hostname managment for Autoscaled EC2 Instances\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --HostedZoneId HOSTEDZONEID\n                        The ID of the hosted zone where the new resource\n                        record will be added.\n  --dryrun              Shows what is going to be done but doesn't change\n                        anything actually\n</code></pre>\n<h2>s3-download-file.py</h2>\n<p>This script just download the requested S3 object.</p>\n<pre><code>usage: s3-download-file.py [-h] -b BUCKET -o OBJECTKEY -f FILEPATH\n\nDonwload file from AWS S3\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -b BUCKET, --bucket BUCKET\n                        The bucket name.\n  -o OBJECTKEY, --objectkey OBJECTKEY\n                        The host string used to build the new name\n  -f FILEPATH, --filepath FILEPATH\n                        The filepath of the file to be saved\n</code></pre>\n<h2>lifecycle-hook-worker.py</h2>\n<p>As its own name says, this worker is designed to use auto scaling <a href=\"http://docs.aws.amazon.com/autoscaling/latest/userguide/lifecycle-hooks.html\" rel=\"nofollow\">lifecycle hooks</a>.</p>\n<p>The process looks for incoming messages into the SQS queue associated with the autoscaling group. Then, when a message comes for the instance, it is consumed and the associated custom action is triggered. Finally, using the lifecycle action token, the worker completes the autoscaling action going on with the launch or ending the instance action.</p>\n<pre><code>usage: lifecycle-hook-worker.py [-h] -q QUEUE -s {LAUNCHING,TERMINATING} -g\n                                GROUP -H HOOKNAME -e EXECUTE [-w WAIT]\n\nSQS Lifecycle hook consumer and trigger\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -q QUEUE, --queue QUEUE\n                        Queue resource.\n  -s {LAUNCHING,TERMINATING}, --state {LAUNCHING,TERMINATING}\n                        Indicates if the consumer is waiting for LAUNCHING or\n                        TERMINATING state\n  -g GROUP, --group GROUP\n                        Auto Scaling Group Name\n  -H HOOKNAME, --hookName HOOKNAME\n                        Life Cycle Hook Name\n  -e EXECUTE, --execute EXECUTE\n                        The filepath of the triggered script\n  -w WAIT, --wait WAIT  Time between query loops in seconds (default: 60)\n</code></pre>\n<h2>rds-instances.py</h2>\n<p>Shows the main info regarding all the RDS instances such as: endpoint, engine, version, status etc.</p>\n<pre><code>usage: rds-instances.py [-h]\n\nList all the RDS instances\n\noptional arguments:\n  -h, --help  show this help message and exit\n</code></pre>\n\n          </div>"}, "last_serial": 6976391, "releases": {"0.0.11": [{"comment_text": "", "digests": {"md5": "1b0954b12031e9adc30ccbf80949f8bc", "sha256": "63a49570a16600fb92154e981f6a089468e194d5270d3c1312f948a3ee5b6e9f"}, "downloads": -1, "filename": "aws_scripts-0.0.11-py2-none-any.whl", "has_sig": false, "md5_digest": "1b0954b12031e9adc30ccbf80949f8bc", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": ">=2.7, <3", "size": 32393, "upload_time": "2018-08-13T11:22:49", "upload_time_iso_8601": "2018-08-13T11:22:49.711729Z", "url": "https://files.pythonhosted.org/packages/fc/5c/3a2cb1003eff2e37d2067d70421a88578480350b96aa15576ff61be4d9f9/aws_scripts-0.0.11-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "32bc822a8660e252735e39acd2f0b590", "sha256": "d0ef097abb4b1a6df607a87d3a9796a57b26069af8da15e48b5b09835274f003"}, "downloads": -1, "filename": "aws-scripts-0.0.11.tar.gz", "has_sig": false, "md5_digest": "32bc822a8660e252735e39acd2f0b590", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, <3", "size": 14281, "upload_time": "2018-08-13T11:22:51", "upload_time_iso_8601": "2018-08-13T11:22:51.140852Z", "url": "https://files.pythonhosted.org/packages/1e/f9/898bc79b61c5af626523fd731bde97b9dfe40b2cd224486d2a03d634fbfc/aws-scripts-0.0.11.tar.gz", "yanked": false}], "0.0.12": [{"comment_text": "", "digests": {"md5": "4bf76adfb5c3a3927abae2d1c6bf6c7e", "sha256": "750a8e4a40649295abed6b51d9f77458d955eb2d161f543a3d7c6b05c9a8ed93"}, "downloads": -1, "filename": "aws-scripts-0.0.12.tar.gz", "has_sig": false, "md5_digest": "4bf76adfb5c3a3927abae2d1c6bf6c7e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, <3", "size": 14327, "upload_time": "2019-01-11T16:53:15", "upload_time_iso_8601": "2019-01-11T16:53:15.288596Z", "url": "https://files.pythonhosted.org/packages/df/2f/f13247609a33774b6fcd217aded42bb8043e65a24662f5c1059324689591/aws-scripts-0.0.12.tar.gz", "yanked": false}], "0.0.13": [{"comment_text": "", "digests": {"md5": "9d3b7124596ed3b9822edc98bdc6d8d5", "sha256": "7e0d12f2feb69bc5ad75fcd82c7f04b72e89edc2c084360cef3d3fd58f3c44eb"}, "downloads": -1, "filename": "aws_scripts-0.0.13-py2-none-any.whl", "has_sig": false, "md5_digest": "9d3b7124596ed3b9822edc98bdc6d8d5", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": ">=2.7, <3", "size": 32664, "upload_time": "2019-03-15T09:22:23", "upload_time_iso_8601": "2019-03-15T09:22:23.590873Z", "url": "https://files.pythonhosted.org/packages/8f/58/24860f38c710d4ae9724a088c8820edde83eefe92bc93b28263e3640f6c9/aws_scripts-0.0.13-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7b1e9095b746daa3701fb3d8f6997c34", "sha256": "18ba511874216ad5cef9fdf2eb2ef2422151eb5192166f3b9d0b8aa4e0fad84d"}, "downloads": -1, "filename": "aws-scripts-0.0.13.tar.gz", "has_sig": false, "md5_digest": "7b1e9095b746daa3701fb3d8f6997c34", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, <3", "size": 14435, "upload_time": "2019-03-15T09:22:25", "upload_time_iso_8601": "2019-03-15T09:22:25.801518Z", "url": "https://files.pythonhosted.org/packages/fa/59/6fa06a2d02dff0963fab9838a90d69984db87e2ab07e2f0a45db557150f8/aws-scripts-0.0.13.tar.gz", "yanked": false}], "0.0.14": [{"comment_text": "", "digests": {"md5": "9e32167c8cb723b72984b12bb9aeb0f0", "sha256": "f990bb3114a49b055bc8491daca7efd65d92b91483e6ccaceec498db933cc4d9"}, "downloads": -1, "filename": "aws_scripts-0.0.14-py2-none-any.whl", "has_sig": false, "md5_digest": "9e32167c8cb723b72984b12bb9aeb0f0", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": ">=2.7, <3", "size": 33153, "upload_time": "2020-01-13T11:41:30", "upload_time_iso_8601": "2020-01-13T11:41:30.853375Z", "url": "https://files.pythonhosted.org/packages/e6/8b/6bc3040dd471f047e88fada10888bc540cdf51d451274a10d2f68abd1932/aws_scripts-0.0.14-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9d81e6bc5afdb6f09dd5f04e9daf3b66", "sha256": "20800ff760aabf578646414ab14e489c361f92bf955cfbeac8aac6644587ddae"}, "downloads": -1, "filename": "aws-scripts-0.0.14.tar.gz", "has_sig": false, "md5_digest": "9d81e6bc5afdb6f09dd5f04e9daf3b66", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, <3", "size": 14668, "upload_time": "2020-01-13T11:41:34", "upload_time_iso_8601": "2020-01-13T11:41:34.437230Z", "url": "https://files.pythonhosted.org/packages/20/46/b4e05e186d13c98e65e52c813b7a11c8bab6a2a271a1a3dc7fa327f740c1/aws-scripts-0.0.14.tar.gz", "yanked": false}], "0.0.15": [{"comment_text": "", "digests": {"md5": "cebc5c0639823d38c357b1869032daff", "sha256": "f9655c30c733b48878527b681b79db457dc73a6dbf2d7a2e2897b83a11b2b568"}, "downloads": -1, "filename": "aws_scripts-0.0.15-py2-none-any.whl", "has_sig": false, "md5_digest": "cebc5c0639823d38c357b1869032daff", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": ">=2.7, <3", "size": 33148, "upload_time": "2020-01-13T12:27:34", "upload_time_iso_8601": "2020-01-13T12:27:34.972087Z", "url": "https://files.pythonhosted.org/packages/ce/13/e5d1e9562300c1215c05592ae6f0f3a9a71ad0b358665e2cc20558cba048/aws_scripts-0.0.15-py2-none-any.whl", "yanked": false}], "0.0.16": [{"comment_text": "", "digests": {"md5": "2489251c72948942f82bf8f4373d5bc9", "sha256": "9b2b8deef86ed03fbf2dfa21081a0340357a880fa7e5a08db3894dfd76dc1adc"}, "downloads": -1, "filename": "aws-scripts-0.0.16.tar.gz", "has_sig": false, "md5_digest": "2489251c72948942f82bf8f4373d5bc9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 14802, "upload_time": "2020-01-13T16:18:47", "upload_time_iso_8601": "2020-01-13T16:18:47.229260Z", "url": "https://files.pythonhosted.org/packages/4b/3e/ffc87f5fb76ad88fdecff85fdbd6416fa7f7ab1641a28eb3d95095703d20/aws-scripts-0.0.16.tar.gz", "yanked": false}], "0.0.17": [{"comment_text": "", "digests": {"md5": "bdcc97a006865467ec6aa24d8d7a236c", "sha256": "ad85e364761019fbd329dc20d1535bd93029a5a209f17082941285925892fe14"}, "downloads": -1, "filename": "aws-scripts-0.0.17.tar.gz", "has_sig": false, "md5_digest": "bdcc97a006865467ec6aa24d8d7a236c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 19276, "upload_time": "2020-04-01T05:16:36", "upload_time_iso_8601": "2020-04-01T05:16:36.544810Z", "url": "https://files.pythonhosted.org/packages/69/c1/cece733598bfc65b151ff1904f80bda0068112c9fad140af8bdff7f4a7ad/aws-scripts-0.0.17.tar.gz", "yanked": false}], "0.0.18": [{"comment_text": "", "digests": {"md5": "fb41ab349cd8653a43cb32a66a9dc219", "sha256": "b37b27add0258c455eaa6f47e3b9c715531b531950739f704b5a284dceb94592"}, "downloads": -1, "filename": "aws-scripts-0.0.18.tar.gz", "has_sig": false, "md5_digest": "fb41ab349cd8653a43cb32a66a9dc219", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 20222, "upload_time": "2020-04-08T06:35:04", "upload_time_iso_8601": "2020-04-08T06:35:04.389940Z", "url": "https://files.pythonhosted.org/packages/09/f7/965b1c9665a31d342571d5a90c80d5deeee1abde5ff1abccafd3d0410110/aws-scripts-0.0.18.tar.gz", "yanked": false}], "0.0.19": [{"comment_text": "", "digests": {"md5": "98a03d36c038389e559ab3b8a534d1e1", "sha256": "90fd7ac5403005ede0bf1c13915ddc3d5827e1f7410e48d751fd459004b71341"}, "downloads": -1, "filename": "aws-scripts-0.0.19.tar.gz", "has_sig": false, "md5_digest": "98a03d36c038389e559ab3b8a534d1e1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 20206, "upload_time": "2020-04-08T09:44:43", "upload_time_iso_8601": "2020-04-08T09:44:43.103954Z", "url": "https://files.pythonhosted.org/packages/7b/26/4bd1e80cbac375ee2f7758798a591d51eebae2e4ff381d6d20232f95cce1/aws-scripts-0.0.19.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "98a03d36c038389e559ab3b8a534d1e1", "sha256": "90fd7ac5403005ede0bf1c13915ddc3d5827e1f7410e48d751fd459004b71341"}, "downloads": -1, "filename": "aws-scripts-0.0.19.tar.gz", "has_sig": false, "md5_digest": "98a03d36c038389e559ab3b8a534d1e1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 20206, "upload_time": "2020-04-08T09:44:43", "upload_time_iso_8601": "2020-04-08T09:44:43.103954Z", "url": "https://files.pythonhosted.org/packages/7b/26/4bd1e80cbac375ee2f7758798a591d51eebae2e4ff381d6d20232f95cce1/aws-scripts-0.0.19.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:15:35 2020"}