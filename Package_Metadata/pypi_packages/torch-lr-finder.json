{"info": {"author": "David Silva", "author_email": "davidtvs10@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development"], "description": "# PyTorch learning rate finder\n\nA PyTorch implementation of the learning rate range test detailed in [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186) by Leslie N. Smith and the tweaked version used by [fastai](https://github.com/fastai/fastai).\n\nThe learning rate range test is a test that provides valuable information about the optimal learning rate. During a pre-training run, the learning rate is increased linearly or exponentially between two boundaries. The low initial learning rate allows the network to start converging and as the learning rate is increased it will eventually be too large and the network will diverge.\n\nTypically, a good static learning rate can be found half-way on the descending loss curve. In the plot below that would be `lr = 0.002`.\n\nFor cyclical learning rates (also detailed in Leslie Smith's paper) where the learning rate is cycled between two boundaries `(start_lr, end_lr)`, the author advises the point at which the loss starts descending and the point at which the loss stops descending or becomes ragged for `start_lr` and `end_lr` respectively.  In the plot below, `start_lr = 0.0002` and `end_lr=0.2`.\n\n![Learning rate range test](images/lr_finder_cifar10.png)\n\n## Installation\n\nPython 2.7 and above:\n\n```bash\npip install torch-lr-finder\n```\n\nInstall with the support of mixed precision training (requires Python 3, see also [this section](#Mixed-precision-training)):\n\n```bash\npip install torch-lr-finder -v --global-option=\"amp\"\n```\n\n## Implementation details and usage\n\n### Tweaked version from fastai\n\nIncreases the learning rate in an exponential manner and computes the training loss for each learning rate. `lr_finder.plot()` plots the training loss versus logarithmic learning rate.\n\n```python\nfrom torch_lr_finder import LRFinder\n\nmodel = ...\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\nlr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\nlr_finder.range_test(trainloader, end_lr=100, num_iter=100)\nlr_finder.plot() # to inspect the loss-learning rate graph\nlr_finder.reset() # to reset the model and optimizer to their initial state\n```\n\n### Leslie Smith's approach\n\nIncreases the learning rate linearly and computes the evaluation loss for each learning rate. `lr_finder.plot()` plots the evaluation loss versus learning rate.\nThis approach typically produces more precise curves because the evaluation loss is more susceptible to divergence but it takes significantly longer to perform the test, especially if the evaluation dataset is large.\n\n```python\nfrom torch_lr_finder import LRFinder\n\nmodel = ...\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=1e-2)\nlr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\nlr_finder.range_test(trainloader, val_loader=val_loader, end_lr=1, num_iter=100, step_mode=\"linear\")\nlr_finder.plot(log_lr=False)\nlr_finder.reset()\n```\n\n### Notes\n\n- Examples for CIFAR10 and MNIST can be found in the examples folder.\n- The optimizer passed to `LRFinder` should not have an `LRScheduler` attached to it.\n- `LRFinder.range_test()` will change the model weights and the optimizer parameters. Both can be restored to their initial state with `LRFinder.reset()`.\n- The learning rate and loss history can be accessed through `lr_finder.history`. This will return a dictionary with `lr` and `loss` keys.\n- When using `step_mode=\"linear\"` the learning rate range should be within the same order of magnitude.\n\n## Additional support for training\n\n### Gradient accumulation\n\nYou can set the `accumulation_steps` parameter in `LRFinder.range_test()` with a proper value to perform gradient accumulation:\n\n```python\nfrom torch.utils.data import DataLoader\nfrom torch_lr_finder import LRFinder\n\ndesired_batch_size, real_batch_size = 32, 4\naccumulation_steps = desired_batch_size // real_batch_size\n\ndataset = ...\n\n# Beware of the `batch_size` used by `DataLoader`\ntrainloader = DataLoader(dataset, batch_size=real_bs, shuffle=True)\n\nmodel = ...\ncriterion = ...\noptimizer = ...\n\nlr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\nlr_finder.range_test(trainloader, end_lr=10, num_iter=100, step_mode=\"exp\", accumulation_steps=accumulation_steps)\nlr_finder.plot()\nlr_finder.reset()\n```\n\n### Mixed precision training\n\nCurrently, we use [`apex`](https://github.com/NVIDIA/apex) as the dependency for mixed precision training.\nTo enable mixed precision training, you just need to call `amp.initialize()` before running `LRFinder`. e.g.\n\n```python\nfrom torch_lr_finder import LRFinder\nfrom apex import amp\n\n# Add this line before running `LRFinder`\nmodel, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n\nlr_finder = LRFinder(model, optimizer, criterion, device='cuda')\nlr_finder.range_test(trainloader, end_lr=10, num_iter=100, step_mode='exp')\nlr_finder.plot()\nlr_finder.reset()\n```\n\nNote that the benefit of mixed precision training requires a nvidia GPU with tensor cores (see also: [NVIDIA/apex #297](https://github.com/NVIDIA/apex/issues/297))\n\nBesides, you can try to set `torch.backends.cudnn.benchmark = True` to improve the training speed. (but it won't work for some cases, you should use it at your own risk)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/davidtvs/pytorch-lr-finder", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "torch-lr-finder", "package_url": "https://pypi.org/project/torch-lr-finder/", "platform": "", "project_url": "https://pypi.org/project/torch-lr-finder/", "project_urls": {"Homepage": "https://github.com/davidtvs/pytorch-lr-finder"}, "release_url": "https://pypi.org/project/torch-lr-finder/0.1.5/", "requires_dist": ["matplotlib", "numpy", "torch (>=0.4.1)", "tqdm"], "requires_python": ">=2.7", "summary": "Pytorch implementation of the learning rate range test", "version": "0.1.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>PyTorch learning rate finder</h1>\n<p>A PyTorch implementation of the learning rate range test detailed in <a href=\"https://arxiv.org/abs/1506.01186\" rel=\"nofollow\">Cyclical Learning Rates for Training Neural Networks</a> by Leslie N. Smith and the tweaked version used by <a href=\"https://github.com/fastai/fastai\" rel=\"nofollow\">fastai</a>.</p>\n<p>The learning rate range test is a test that provides valuable information about the optimal learning rate. During a pre-training run, the learning rate is increased linearly or exponentially between two boundaries. The low initial learning rate allows the network to start converging and as the learning rate is increased it will eventually be too large and the network will diverge.</p>\n<p>Typically, a good static learning rate can be found half-way on the descending loss curve. In the plot below that would be <code>lr = 0.002</code>.</p>\n<p>For cyclical learning rates (also detailed in Leslie Smith's paper) where the learning rate is cycled between two boundaries <code>(start_lr, end_lr)</code>, the author advises the point at which the loss starts descending and the point at which the loss stops descending or becomes ragged for <code>start_lr</code> and <code>end_lr</code> respectively.  In the plot below, <code>start_lr = 0.0002</code> and <code>end_lr=0.2</code>.</p>\n<p><img alt=\"Learning rate range test\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/03b59d30349132dc3246da1d9dad1485911d7adc/696d616765732f6c725f66696e6465725f636966617231302e706e67\"></p>\n<h2>Installation</h2>\n<p>Python 2.7 and above:</p>\n<pre>pip install torch-lr-finder\n</pre>\n<p>Install with the support of mixed precision training (requires Python 3, see also <a href=\"#Mixed-precision-training\" rel=\"nofollow\">this section</a>):</p>\n<pre>pip install torch-lr-finder -v --global-option<span class=\"o\">=</span><span class=\"s2\">\"amp\"</span>\n</pre>\n<h2>Implementation details and usage</h2>\n<h3>Tweaked version from fastai</h3>\n<p>Increases the learning rate in an exponential manner and computes the training loss for each learning rate. <code>lr_finder.plot()</code> plots the training loss versus logarithmic learning rate.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torch_lr_finder</span> <span class=\"kn\">import</span> <span class=\"n\">LRFinder</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n<span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span><span class=\"p\">()</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-7</span><span class=\"p\">,</span> <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span> <span class=\"o\">=</span> <span class=\"n\">LRFinder</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">criterion</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">range_test</span><span class=\"p\">(</span><span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">end_lr</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">()</span> <span class=\"c1\"># to inspect the loss-learning rate graph</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span> <span class=\"c1\"># to reset the model and optimizer to their initial state</span>\n</pre>\n<h3>Leslie Smith's approach</h3>\n<p>Increases the learning rate linearly and computes the evaluation loss for each learning rate. <code>lr_finder.plot()</code> plots the evaluation loss versus learning rate.\nThis approach typically produces more precise curves because the evaluation loss is more susceptible to divergence but it takes significantly longer to perform the test, especially if the evaluation dataset is large.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torch_lr_finder</span> <span class=\"kn\">import</span> <span class=\"n\">LRFinder</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n<span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span><span class=\"p\">()</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span> <span class=\"o\">=</span> <span class=\"n\">LRFinder</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">criterion</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">range_test</span><span class=\"p\">(</span><span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">val_loader</span><span class=\"o\">=</span><span class=\"n\">val_loader</span><span class=\"p\">,</span> <span class=\"n\">end_lr</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">step_mode</span><span class=\"o\">=</span><span class=\"s2\">\"linear\"</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">log_lr</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n</pre>\n<h3>Notes</h3>\n<ul>\n<li>Examples for CIFAR10 and MNIST can be found in the examples folder.</li>\n<li>The optimizer passed to <code>LRFinder</code> should not have an <code>LRScheduler</code> attached to it.</li>\n<li><code>LRFinder.range_test()</code> will change the model weights and the optimizer parameters. Both can be restored to their initial state with <code>LRFinder.reset()</code>.</li>\n<li>The learning rate and loss history can be accessed through <code>lr_finder.history</code>. This will return a dictionary with <code>lr</code> and <code>loss</code> keys.</li>\n<li>When using <code>step_mode=\"linear\"</code> the learning rate range should be within the same order of magnitude.</li>\n</ul>\n<h2>Additional support for training</h2>\n<h3>Gradient accumulation</h3>\n<p>You can set the <code>accumulation_steps</code> parameter in <code>LRFinder.range_test()</code> with a proper value to perform gradient accumulation:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch_lr_finder</span> <span class=\"kn\">import</span> <span class=\"n\">LRFinder</span>\n\n<span class=\"n\">desired_batch_size</span><span class=\"p\">,</span> <span class=\"n\">real_batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">4</span>\n<span class=\"n\">accumulation_steps</span> <span class=\"o\">=</span> <span class=\"n\">desired_batch_size</span> <span class=\"o\">//</span> <span class=\"n\">real_batch_size</span>\n\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n\n<span class=\"c1\"># Beware of the `batch_size` used by `DataLoader`</span>\n<span class=\"n\">trainloader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">real_bs</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n<span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n\n<span class=\"n\">lr_finder</span> <span class=\"o\">=</span> <span class=\"n\">LRFinder</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">criterion</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">range_test</span><span class=\"p\">(</span><span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">end_lr</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">step_mode</span><span class=\"o\">=</span><span class=\"s2\">\"exp\"</span><span class=\"p\">,</span> <span class=\"n\">accumulation_steps</span><span class=\"o\">=</span><span class=\"n\">accumulation_steps</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">()</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n</pre>\n<h3>Mixed precision training</h3>\n<p>Currently, we use <a href=\"https://github.com/NVIDIA/apex\" rel=\"nofollow\"><code>apex</code></a> as the dependency for mixed precision training.\nTo enable mixed precision training, you just need to call <code>amp.initialize()</code> before running <code>LRFinder</code>. e.g.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torch_lr_finder</span> <span class=\"kn\">import</span> <span class=\"n\">LRFinder</span>\n<span class=\"kn\">from</span> <span class=\"nn\">apex</span> <span class=\"kn\">import</span> <span class=\"n\">amp</span>\n\n<span class=\"c1\"># Add this line before running `LRFinder`</span>\n<span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">amp</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">opt_level</span><span class=\"o\">=</span><span class=\"s1\">'O1'</span><span class=\"p\">)</span>\n\n<span class=\"n\">lr_finder</span> <span class=\"o\">=</span> <span class=\"n\">LRFinder</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">criterion</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">'cuda'</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">range_test</span><span class=\"p\">(</span><span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">end_lr</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">step_mode</span><span class=\"o\">=</span><span class=\"s1\">'exp'</span><span class=\"p\">)</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">()</span>\n<span class=\"n\">lr_finder</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n</pre>\n<p>Note that the benefit of mixed precision training requires a nvidia GPU with tensor cores (see also: <a href=\"https://github.com/NVIDIA/apex/issues/297\" rel=\"nofollow\">NVIDIA/apex #297</a>)</p>\n<p>Besides, you can try to set <code>torch.backends.cudnn.benchmark = True</code> to improve the training speed. (but it won't work for some cases, you should use it at your own risk)</p>\n\n          </div>"}, "last_serial": 7088011, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "134a574f9dab12ebc005ddff441694ec", "sha256": "7f690a2e093b10c0e1935e013572c4be7225820c5be40af56139a6c28626db9e"}, "downloads": -1, "filename": "torch_lr_finder-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "134a574f9dab12ebc005ddff441694ec", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7", "size": 7397, "upload_time": "2019-09-14T20:48:59", "upload_time_iso_8601": "2019-09-14T20:48:59.386785Z", "url": "https://files.pythonhosted.org/packages/62/7c/397078ef7ca83880a35037285fd9d1f0e70ab5414db3a6f81e868c7230e4/torch_lr_finder-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f5f590e69484e6fb07314582669ea875", "sha256": "3d1f91f0232f069325b456348b97d7936921f77393cef10e8253b3cb7cc2932d"}, "downloads": -1, "filename": "torch-lr-finder-0.0.1.tar.gz", "has_sig": false, "md5_digest": "f5f590e69484e6fb07314582669ea875", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 5973, "upload_time": "2019-09-14T20:49:02", "upload_time_iso_8601": "2019-09-14T20:49:02.021438Z", "url": "https://files.pythonhosted.org/packages/5d/cd/91f910b0b05cb72ad66db3b8a82b86f8e1ab4241398eef8dc4dfd033a7eb/torch-lr-finder-0.0.1.tar.gz", "yanked": false}], "0.1": [{"comment_text": "", "digests": {"md5": "67d038f0aa54fc2d895cf81a834c7e59", "sha256": "1f218b9704e6cbac26d754c72e088124411c7a495665517f91f05c050d55443e"}, "downloads": -1, "filename": "torch_lr_finder-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "67d038f0aa54fc2d895cf81a834c7e59", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7", "size": 9677, "upload_time": "2020-01-12T16:21:41", "upload_time_iso_8601": "2020-01-12T16:21:41.860725Z", "url": "https://files.pythonhosted.org/packages/c5/a8/1a003a5b3ff76d3ee55e814028f2fd4d79c555956c36eaebd663a9cf0935/torch_lr_finder-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7badfa271e6e35a21f5200f5fc648f7f", "sha256": "cfee0b94701cd51eb69856a2def48423f5c26cbe74514880a1f5eb09d90aa841"}, "downloads": -1, "filename": "torch-lr-finder-0.1.tar.gz", "has_sig": false, "md5_digest": "7badfa271e6e35a21f5200f5fc648f7f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 8635, "upload_time": "2020-01-12T16:21:43", "upload_time_iso_8601": "2020-01-12T16:21:43.325750Z", "url": "https://files.pythonhosted.org/packages/0b/7b/a8ccefca947877cb0112dc3b1ceea38caa24b9f75d6601769075ab6d624b/torch-lr-finder-0.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "4cfdf9c6643e984ef27f733de67e2dfc", "sha256": "c2af8c2cd539d29738c8903b5f604c11367de8415a90fafdd3cdc179c5d13bcc"}, "downloads": -1, "filename": "torch_lr_finder-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "4cfdf9c6643e984ef27f733de67e2dfc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7", "size": 9754, "upload_time": "2020-02-16T17:18:39", "upload_time_iso_8601": "2020-02-16T17:18:39.518438Z", "url": "https://files.pythonhosted.org/packages/8d/5a/28e71a45ff80efb2ed0759ceef0d34a713f4376119746246c12671c0c807/torch_lr_finder-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6ea25bfbcd9448d3e6a86e3528d8971b", "sha256": "79b97995cab86b392230497313857891041861d6d63d1b608fb285e01f8bfafa"}, "downloads": -1, "filename": "torch-lr-finder-0.1.2.tar.gz", "has_sig": false, "md5_digest": "6ea25bfbcd9448d3e6a86e3528d8971b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 9246, "upload_time": "2020-02-16T17:18:41", "upload_time_iso_8601": "2020-02-16T17:18:41.156401Z", "url": "https://files.pythonhosted.org/packages/6f/05/8384b9c02748b40d17a08c18cea7d090872423990934b1a95d0db0ea3481/torch-lr-finder-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "94aa7c7330f196896772b56a8e53ad64", "sha256": "d9ed12a5ad5a37c8df1d6e88818ef06a9a9585187b3ea2abf58ace7670474df8"}, "downloads": -1, "filename": "torch_lr_finder-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "94aa7c7330f196896772b56a8e53ad64", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7", "size": 9943, "upload_time": "2020-03-25T07:29:27", "upload_time_iso_8601": "2020-03-25T07:29:27.309829Z", "url": "https://files.pythonhosted.org/packages/cd/ff/38ec8729a7a0a4d8045f100705a5dc1ae259d169e6f5f67c6e21c3f9d5cf/torch_lr_finder-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "abbf51e543859e6629b8bc2a520255a5", "sha256": "7ad4f78300c5a6754890765db51bcc2f0335b05208010c5e97a0f174ef0036d9"}, "downloads": -1, "filename": "torch-lr-finder-0.1.3.tar.gz", "has_sig": false, "md5_digest": "abbf51e543859e6629b8bc2a520255a5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 9435, "upload_time": "2020-03-25T07:29:29", "upload_time_iso_8601": "2020-03-25T07:29:29.290780Z", "url": "https://files.pythonhosted.org/packages/92/d5/893fc09e1a9fc72bcef256806edb911db8e3762dc3cb90fd0f817d45f8c6/torch-lr-finder-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "a7ea7f8bce4c9885a6941a7887af558e", "sha256": "fb9ec3599b913202540ff3b960c691c1a07abeab0c2b7e60b314704bc886193e"}, "downloads": -1, "filename": "torch_lr_finder-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "a7ea7f8bce4c9885a6941a7887af558e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7", "size": 9945, "upload_time": "2020-04-08T17:39:15", "upload_time_iso_8601": "2020-04-08T17:39:15.998339Z", "url": "https://files.pythonhosted.org/packages/68/06/7301400132b63f96c3b4f5fbb5033130b9d07b85c9e0e247d4ac03451d80/torch_lr_finder-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "90ebe84210dc7533d1b7119f7ecd23dd", "sha256": "8329faef69f5638fc641813f13a2fdec3156d754e1fa484bb4ddf6cd28f2feef"}, "downloads": -1, "filename": "torch-lr-finder-0.1.4.tar.gz", "has_sig": false, "md5_digest": "90ebe84210dc7533d1b7119f7ecd23dd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 9783, "upload_time": "2020-04-08T17:39:17", "upload_time_iso_8601": "2020-04-08T17:39:17.180644Z", "url": "https://files.pythonhosted.org/packages/c3/27/976e1a3bbf9721619a06304b8db7ab9c40207ab29e7e0ba84bbe3ff28ad0/torch-lr-finder-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "40d43196b2db9a1372b377a668178290", "sha256": "97bc4199b74bea703e0d5963b0ad0a68eeedc0e3496777bd3273c6eb580da52a"}, "downloads": -1, "filename": "torch_lr_finder-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "40d43196b2db9a1372b377a668178290", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7", "size": 10050, "upload_time": "2020-04-23T21:36:39", "upload_time_iso_8601": "2020-04-23T21:36:39.432049Z", "url": "https://files.pythonhosted.org/packages/f9/42/baaf4556393f8d23e81e522e1e3ed1ab407a1aa79350ecbce8df065d9e12/torch_lr_finder-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f1261e4f4a01258a2395495ae5dbdbbb", "sha256": "3ef483580661f8926cbff4189ae4be16b9142aadb6a7dddb4d1e00e5b2e34369"}, "downloads": -1, "filename": "torch-lr-finder-0.1.5.tar.gz", "has_sig": false, "md5_digest": "f1261e4f4a01258a2395495ae5dbdbbb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 9902, "upload_time": "2020-04-23T21:36:41", "upload_time_iso_8601": "2020-04-23T21:36:41.612653Z", "url": "https://files.pythonhosted.org/packages/ff/64/176f98c19dc8bc35f785a28b88b6f5bb86e0abdb6388c51ec2cd193a0f97/torch-lr-finder-0.1.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "40d43196b2db9a1372b377a668178290", "sha256": "97bc4199b74bea703e0d5963b0ad0a68eeedc0e3496777bd3273c6eb580da52a"}, "downloads": -1, "filename": "torch_lr_finder-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "40d43196b2db9a1372b377a668178290", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7", "size": 10050, "upload_time": "2020-04-23T21:36:39", "upload_time_iso_8601": "2020-04-23T21:36:39.432049Z", "url": "https://files.pythonhosted.org/packages/f9/42/baaf4556393f8d23e81e522e1e3ed1ab407a1aa79350ecbce8df065d9e12/torch_lr_finder-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f1261e4f4a01258a2395495ae5dbdbbb", "sha256": "3ef483580661f8926cbff4189ae4be16b9142aadb6a7dddb4d1e00e5b2e34369"}, "downloads": -1, "filename": "torch-lr-finder-0.1.5.tar.gz", "has_sig": false, "md5_digest": "f1261e4f4a01258a2395495ae5dbdbbb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 9902, "upload_time": "2020-04-23T21:36:41", "upload_time_iso_8601": "2020-04-23T21:36:41.612653Z", "url": "https://files.pythonhosted.org/packages/ff/64/176f98c19dc8bc35f785a28b88b6f5bb86e0abdb6388c51ec2cd193a0f97/torch-lr-finder-0.1.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:17 2020"}