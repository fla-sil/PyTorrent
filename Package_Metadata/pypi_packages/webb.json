{"info": {"author": "Hardik Vasa", "author_email": "hnvasa@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Web Environment", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Topic :: Internet :: WWW/HTTP", "Topic :: Software Development :: Libraries :: Application Frameworks", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "## Webb - A Complete Web Scrapper and Crawler Library\r\nAn all-in-one Python library to scrap, parse and crawl web pages\r\n\r\n### Gist\r\nThis is a light-weight, dynamic and highly-flexible Python library. It can be used to **crawl, download, index, parse, scrap and analyze web pages** in a systematic manner or any of the individual functionality. It is also used to **clean web pages, normalize web pages, store web data, extract server-side information** and **import/export relevant components** from the web. Some of the other features also include **downloading images from a web page, downloading google images and spidering wikipedia articles**.\r\n\r\n### Usage and Instructions\r\nFor usage and instructions please visit the [Official Documentation](https://github.com/hardikvasa/webb/blob/master/docs/Documentation.md)\r\n\r\nFor issues and discussion visit the [Issue Tracker](https://github.com/hardikvasa/webb/issues)\r\n\r\nFor sample codes and examples, please visit [Examples Codes](https://github.com/hardikvasa/webb/tree/master/examples)\r\n\r\n\r\n### Compatability\r\nThis library is compatible with both Python 2 (2.x) as well as Python 3 (3.x) versions. It is a download-import-and-run program with no or little changes as required by users.\r\n\r\n### Dependencies\r\nThere are **no dependencies** to this project. Hurray! It functions entirely of the standard 'built-in' library support. It does not need any external support or installations. Just download and run!!!\r\n\r\n### Status\r\nThis is a stand-alone python script which is ready-to-run, but still under development. Many more features will be added to it shortly.\r\n\r\n### Disclaimer\r\nThe crawler function lets you download  and crawl tons of web pages. Please do not download and crawl any pages of a domain without reading the 'robot.txt' file of that specific domain. \r\n\r\nIt is inappropriate to violate the robot.txt file and is strictly not recommended. This may even lead to the domain completely blocking your crawler and thus blacklisting it. It is also not appropriate to crawl pages at high rate as it may put a lot of pressure on the requesting server.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hardikvasa/webb", "keywords": "scraper crawler spider webb", "license": "Apache License, Version 2.0", "maintainer": null, "maintainer_email": null, "name": "webb", "package_url": "https://pypi.org/project/webb/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/webb/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/hardikvasa/webb"}, "release_url": "https://pypi.org/project/webb/0.9.2.5/", "requires_dist": null, "requires_python": null, "summary": "An all-in-one Web Crawler, Web Parser and Web Scrapping library!", "version": "0.9.2.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>## Webb - A Complete Web Scrapper and Crawler Library\nAn all-in-one Python library to scrap, parse and crawl web pages</p>\n<p>### Gist\nThis is a light-weight, dynamic and highly-flexible Python library. It can be used to <strong>crawl, download, index, parse, scrap and analyze web pages</strong> in a systematic manner or any of the individual functionality. It is also used to <strong>clean web pages, normalize web pages, store web data, extract server-side information</strong> and <strong>import/export relevant components</strong> from the web. Some of the other features also include <strong>downloading images from a web page, downloading google images and spidering wikipedia articles</strong>.</p>\n<p>### Usage and Instructions\nFor usage and instructions please visit the [Official Documentation](<a href=\"https://github.com/hardikvasa/webb/blob/master/docs/Documentation.md\" rel=\"nofollow\">https://github.com/hardikvasa/webb/blob/master/docs/Documentation.md</a>)</p>\n<p>For issues and discussion visit the [Issue Tracker](<a href=\"https://github.com/hardikvasa/webb/issues\" rel=\"nofollow\">https://github.com/hardikvasa/webb/issues</a>)</p>\n<p>For sample codes and examples, please visit [Examples Codes](<a href=\"https://github.com/hardikvasa/webb/tree/master/examples\" rel=\"nofollow\">https://github.com/hardikvasa/webb/tree/master/examples</a>)</p>\n<p>### Compatability\nThis library is compatible with both Python 2 (2.x) as well as Python 3 (3.x) versions. It is a download-import-and-run program with no or little changes as required by users.</p>\n<p>### Dependencies\nThere are <strong>no dependencies</strong> to this project. Hurray! It functions entirely of the standard \u2018built-in\u2019 library support. It does not need any external support or installations. Just download and run!!!</p>\n<p>### Status\nThis is a stand-alone python script which is ready-to-run, but still under development. Many more features will be added to it shortly.</p>\n<p>### Disclaimer\nThe crawler function lets you download  and crawl tons of web pages. Please do not download and crawl any pages of a domain without reading the \u2018robot.txt\u2019 file of that specific domain.</p>\n<p>It is inappropriate to violate the robot.txt file and is strictly not recommended. This may even lead to the domain completely blocking your crawler and thus blacklisting it. It is also not appropriate to crawl pages at high rate as it may put a lot of pressure on the requesting server.</p>\n\n          </div>"}, "last_serial": 1674797, "releases": {"0.9.1.2": [{"comment_text": "", "digests": {"md5": "7c2feb92b544147744cd00a615d46ebb", "sha256": "a617d417c84a1c46b2d43c4605de955640a93087749c729ea2d1e01f1356d3e0"}, "downloads": -1, "filename": "webb-0.9.1.2.zip", "has_sig": false, "md5_digest": "7c2feb92b544147744cd00a615d46ebb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12936, "upload_time": "2015-08-06T18:44:51", "upload_time_iso_8601": "2015-08-06T18:44:51.037355Z", "url": "https://files.pythonhosted.org/packages/a7/46/84b3b737d83eb1bdce907f521cc1427902db74eca4b503ccf11058098c18/webb-0.9.1.2.zip", "yanked": false}], "0.9.2.5": [{"comment_text": "", "digests": {"md5": "d4708bbf318c86ca70a46f09a878d306", "sha256": "5b4b64abc1ba0843618ff6cb7b16777cfb307743abba50483f876e49243bf780"}, "downloads": -1, "filename": "webb-0.9.2.5.zip", "has_sig": false, "md5_digest": "d4708bbf318c86ca70a46f09a878d306", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14384, "upload_time": "2015-08-12T16:41:51", "upload_time_iso_8601": "2015-08-12T16:41:51.330924Z", "url": "https://files.pythonhosted.org/packages/6d/1b/207dc4f94fe3ad68122dd67f107c9954bc12888663bda2041c29b9164176/webb-0.9.2.5.zip", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d4708bbf318c86ca70a46f09a878d306", "sha256": "5b4b64abc1ba0843618ff6cb7b16777cfb307743abba50483f876e49243bf780"}, "downloads": -1, "filename": "webb-0.9.2.5.zip", "has_sig": false, "md5_digest": "d4708bbf318c86ca70a46f09a878d306", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14384, "upload_time": "2015-08-12T16:41:51", "upload_time_iso_8601": "2015-08-12T16:41:51.330924Z", "url": "https://files.pythonhosted.org/packages/6d/1b/207dc4f94fe3ad68122dd67f107c9954bc12888663bda2041c29b9164176/webb-0.9.2.5.zip", "yanked": false}], "timestamp": "Fri May  8 03:31:25 2020"}