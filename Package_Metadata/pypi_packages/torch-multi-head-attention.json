{"info": {"author": "CyberZHG", "author_email": "CyberZHG@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.6"], "description": "# PyTorch Multi-Head Attention\n\n[![Travis](https://travis-ci.org/CyberZHG/torch-multi-head-attention.svg)](https://travis-ci.org/CyberZHG/torch-multi-head-attention)\n[![Coverage](https://coveralls.io/repos/github/CyberZHG/torch-multi-head-attention/badge.svg?branch=master)](https://coveralls.io/github/CyberZHG/torch-multi-head-attention)\n\n## Install\n\n```bash\npip install torch-multi-head-attention\n```\n\n## Usage\n\n```python\nfrom torch_multi_head_attention import MultiHeadAttention\n\nMultiHeadAttention(in_features=768, head_num=12)\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/CyberZHG/torch-multi-head-attention", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "torch-multi-head-attention", "package_url": "https://pypi.org/project/torch-multi-head-attention/", "platform": "", "project_url": "https://pypi.org/project/torch-multi-head-attention/", "project_urls": {"Homepage": "https://github.com/CyberZHG/torch-multi-head-attention"}, "release_url": "https://pypi.org/project/torch-multi-head-attention/0.15.1/", "requires_dist": null, "requires_python": "", "summary": "Multi-head attention implemented in PyTorch", "version": "0.15.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>PyTorch Multi-Head Attention</h1>\n<p><a href=\"https://travis-ci.org/CyberZHG/torch-multi-head-attention\" rel=\"nofollow\"><img alt=\"Travis\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f6b5d7ef8b0e2786573f6b17dfecdf8410d19a1d/68747470733a2f2f7472617669732d63692e6f72672f43796265725a48472f746f7263682d6d756c74692d686561642d617474656e74696f6e2e737667\"></a>\n<a href=\"https://coveralls.io/github/CyberZHG/torch-multi-head-attention\" rel=\"nofollow\"><img alt=\"Coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bf1404866c6a1b213e9902b07c9e98c375a85c31/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f43796265725a48472f746f7263682d6d756c74692d686561642d617474656e74696f6e2f62616467652e7376673f6272616e63683d6d6173746572\"></a></p>\n<h2>Install</h2>\n<pre>pip install torch-multi-head-attention\n</pre>\n<h2>Usage</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torch_multi_head_attention</span> <span class=\"kn\">import</span> <span class=\"n\">MultiHeadAttention</span>\n\n<span class=\"n\">MultiHeadAttention</span><span class=\"p\">(</span><span class=\"n\">in_features</span><span class=\"o\">=</span><span class=\"mi\">768</span><span class=\"p\">,</span> <span class=\"n\">head_num</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">)</span>\n</pre>\n\n          </div>"}, "last_serial": 4860010, "releases": {"0.15.0": [{"comment_text": "", "digests": {"md5": "8c15db37e0ca2bdc088d89da7c0f30ba", "sha256": "7e51ea6a54b4ee16134c00ea8930e444757f326951ba9cde709410e7c11c9fe1"}, "downloads": -1, "filename": "torch-multi-head-attention-0.15.0.tar.gz", "has_sig": false, "md5_digest": "8c15db37e0ca2bdc088d89da7c0f30ba", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3762, "upload_time": "2019-02-21T09:58:05", "upload_time_iso_8601": "2019-02-21T09:58:05.196854Z", "url": "https://files.pythonhosted.org/packages/fe/c6/6bf35cf1292e4a7756c534fa4373f539a946904435338e5b1d0a6ae444a6/torch-multi-head-attention-0.15.0.tar.gz", "yanked": false}], "0.15.1": [{"comment_text": "", "digests": {"md5": "e24c9e56e808eee69921d26768f8bcca", "sha256": "e181602fe1ef6da8322cb6bc1ffb41f52d3658c54e3937040e8f186754bb3056"}, "downloads": -1, "filename": "torch-multi-head-attention-0.15.1.tar.gz", "has_sig": false, "md5_digest": "e24c9e56e808eee69921d26768f8bcca", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3592, "upload_time": "2019-02-24T04:44:07", "upload_time_iso_8601": "2019-02-24T04:44:07.255356Z", "url": "https://files.pythonhosted.org/packages/8d/73/b0734654ec4c950270d32c3d4ffb7460e63df229021a52386bf86356e815/torch-multi-head-attention-0.15.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e24c9e56e808eee69921d26768f8bcca", "sha256": "e181602fe1ef6da8322cb6bc1ffb41f52d3658c54e3937040e8f186754bb3056"}, "downloads": -1, "filename": "torch-multi-head-attention-0.15.1.tar.gz", "has_sig": false, "md5_digest": "e24c9e56e808eee69921d26768f8bcca", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3592, "upload_time": "2019-02-24T04:44:07", "upload_time_iso_8601": "2019-02-24T04:44:07.255356Z", "url": "https://files.pythonhosted.org/packages/8d/73/b0734654ec4c950270d32c3d4ffb7460e63df229021a52386bf86356e815/torch-multi-head-attention-0.15.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:15 2020"}