{"info": {"author": "Abhijit Balaji", "author_email": "balaabhijit5@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Model-Server\n\nA Pure `python-3` based flexible gRPC server for hosting Deep Learning, Machine Learning models trained on any framework!\n\n## Installation\n\n### Method 1:\nInstalling from [python pip](https://pypi.org/project/model-server)\n\n`pip3 install model-server`\n\n### Method 2:\nCreating wheel from github\n\n1. clone the repository\n2. Run `bash create_pip_wheel_and_upload.sh`. This will prompt for userid and password. You can `ctrl-c` this\n3. Then install the created wheel\n\n### Method 3:\n\nNo installation. Using the source code directly.\n\nIf this is the case, you need to compile the protobufs. Run `bash compile_protobufs.sh`. Then add the project root to your `$PYTHONPATH`.\n\n### Note:\nMethod 2 and 3 requires `libprotoc>=3.6.0`\n\n\n## Why Model-Server?\n\nTaking deep learning models to production at scale is not a very straight forward process. If you are using Tensorflow then you have [Tensorflow Serving](https://www.tensorflow.org/tfx/guide/serving). But, if you are using other frameworks like [PyTorch](https://pytorch.org/), [MXNet](https://mxnet.apache.org/), [scikit-learn](https://scikit-learn.org/stable/) etc. Taking your model to production is not very straight forward (Flask, Django and other ReST frameworks). Ideally you should be able to extend Tensorflow Serving to support models from other frameworks also but, this is extremly cumbersome! Thus, to bridge this gap we have [Model-Server](https://abhijit-2592.github.io/model-server/): A high performance framework neutral serving solution! **The idea is:** if you are able to train your model in `python` you should be  able to deploy at scale using pure `python`\n\n## Salient Features\n\n [Model-Server](https://abhijit-2592.github.io/model-server/) is heavily inspired from [Tensorflow Serving](https://www.tensorflow.org/tfx/guide/serving)\n\n* **Out of box client side batching support**\n* **Pure python implementation**: You don't need to fiddle around with C++ to have a scalable deployment solution\n* **Framework neutral**:  Using [PyTorch](https://pytorch.org/), [MXNet](https://mxnet.apache.org/) etc? Don't worry! The solution is platform neutral. If you can use a framework to train in `python-3`, [Model-Server](https://abhijit-2592.github.io/model-server/) can deploy it at scale\n* **Single server for multi-framework and multi-models**: You can host multiple models using the same framework or a mixture of multiple [PyTorch](https://pytorch.org/), [MXNet](https://mxnet.apache.org/), [scikit-learn](https://scikit-learn.org/stable/) [Tensorflow](https://www.tensorflow.org/) etc models!\n\n## Getting started\n\nThe core of Model Server is a `Servable`. A servable is nothing but a `python class` containing your model's prediction definition which will be served by the `Model-Server`. All servables must inherit from `model_server.Servable` for the  `Model-Server` to serve it.\n\nTo deploy your model to production with `Model Server`, you just have to write a single `python-3` file containing a `class` which inherits from `model_server.Servable` and has the following two methods:\n\n```python\npredict(self, input_array_dict)\nget_model_info(self, list_of_model_info_dict)\n```\n\nNow run the floowing to start the server in `5001` port\n```bash\npython -m model_server.runserver path_to_custom_servable_file.py\n```\n\nFor more info on  command line arguments:\n```bash\npython -m model_server.runserver --help\n```\n\n\n### A simple example\n\ncreate a file called `simple_servable.py` with the following contents:\n```python\n\nimport numpy as np\nfrom model_server import Servable\n\n\nclass my_custom_servable(Servable):\n    def __init__(self, args):\n        # args contains values from ArgumentParser\n        # Thus you can pass any kwargs via command line and you get them here\n        pass\n\n    def predict(self, input_array_dict):\n        \"\"\"This method is responsible for the gRPC call GetPredictions().\n        All custom servables must define this method.\n\n        Arguments:\n            input_array_dict (dict): The PredictionRequest proto decoded as a python dictionary.\n\n        # example\n        input_array_dict = {\n                           \"input_tensor_name1\": numpy array,\n                           \"input_tensor_name2\": numpy array\n                            }\n\n        Returns:\n            A python dictionary with key (typically output name) and value as numpy array of predictions\n\n        # example\n        output = {\n                   \"output_tensor_name1\": numpy array,\n                   \"output_tensor_name2\": numpy array\n                  }\n        \"\"\"\n        print(input_array_dict)\n        return ({\"output_array1\": np.array([100, 200]).astype(np.float32),\n                 \"output_array2\": np.array([\"foo\".encode(),\"bar\".encode()]).astype(object),  # you can get and pass strings encoded as bytes also\n                 })\n\n    def get_model_info(self, list_of_model_info_dict):\n        \"\"\"This method which is responsible for the call GetModelInfo()\n\n        Arguments:\n            list_of_model_info_dict (list/tuple): A list containing model_info_dicts\n\n        Note:\n            model_info_dict contains the following keys:\n\n            {\n                \"name\": \"model name as string\"\n                \"version\": \"version as string\"\n                \"status\": \"status string\"\n                \"misc\": \"string with miscellaneous info\"\n            }\n\n        Returns:\n            list_of_model_info_dict (dict): containing the model and server info. This is similar to the function input\n        \"\"\"\n        return [{\"name\": \"first_model\", \"version\": 1, \"status\": \"up\"},\n                {\"name\": \"second_model\", \"version\": 2, \"status\": \"up\", \"misc\": \"Other miscellaneous details\"}]\n```\n\nNow run:\n\n```bash\npython -m model_server.runserver path/to/simple_servable.py\n```\nTo start the gRPC server!\n\nNow let's define the client!\n\n```python\nimport grpc\nimport numpy as np\n\nfrom model_server import server_pb2, server_pb2_grpc\nfrom model_server.utils import create_tensor_proto\nfrom model_server.utils import create_predict_request\nfrom model_server.utils import create_array_from_proto\nfrom model_server.utils import create_model_info_proto\n\nchannel = grpc.insecure_channel('localhost:5001')  # default port\n# create a stub (client)\nstub = server_pb2_grpc.ModelServerStub(channel)\ninput_array_dict = {\"input1\":create_tensor_proto(np.array([1,2]).astype(np.uint8)),\n                    \"input2\":create_tensor_proto(np.array([[10.0,11.0], [12.0,13.0]]).astype(np.float32)),\n                    \"input3\":create_tensor_proto(np.array([\"Hi\".encode(), \"Hello\".encode(), \"test\".encode()]).astype(object))\n                   }\n# create the prediction request\npredict_request= create_predict_request(input_array_dict, name=\"simple_call\")\n# make the call\nresponse = stub.GetPredictions(predict_request)\n\n# decode the response\nprint(create_array_from_proto(response.outputs[\"output_array1\"]))\n\n# prints: array([100., 200.], dtype=float32)\n\n# Getting the model status\n\nmodel_info_proto = create_model_info_proto([])  # you can pass an empty list also\nresponse = stub.GetModelInfo(model_info_proto)\n\n```\n\nLook at [examples](https://github.com/Abhijit-2592/model-server/tree/master/examples) folder for further examples\n\n\n## Work in Progress\n\n- Support server side batching and async calls.\n- Provide a gRPC endpoint for [Active Learning](https://en.wikipedia.org/wiki/Active_learning_(machine_learning)) so that you can plug in `Model Server` with your labeling tool and train on fly!\n- Provide a ReST wrapper\n\nFeel free to file issues, provide suggestions and pull requests\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Abhijit-2592/model-server", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "model-server", "package_url": "https://pypi.org/project/model-server/", "platform": "", "project_url": "https://pypi.org/project/model-server/", "project_urls": {"Homepage": "https://github.com/Abhijit-2592/model-server"}, "release_url": "https://pypi.org/project/model-server/1.0/", "requires_dist": ["grpcio (==1.21.1)", "numpy (>=1.14.5)", "pytest"], "requires_python": "", "summary": "A Pure `python-3` based flexible gRPC server for hosting Deep Learning, Machine Learning models trained on any framework!", "version": "1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Model-Server</h1>\n<p>A Pure <code>python-3</code> based flexible gRPC server for hosting Deep Learning, Machine Learning models trained on any framework!</p>\n<h2>Installation</h2>\n<h3>Method 1:</h3>\n<p>Installing from <a href=\"https://pypi.org/project/model-server\" rel=\"nofollow\">python pip</a></p>\n<p><code>pip3 install model-server</code></p>\n<h3>Method 2:</h3>\n<p>Creating wheel from github</p>\n<ol>\n<li>clone the repository</li>\n<li>Run <code>bash create_pip_wheel_and_upload.sh</code>. This will prompt for userid and password. You can <code>ctrl-c</code> this</li>\n<li>Then install the created wheel</li>\n</ol>\n<h3>Method 3:</h3>\n<p>No installation. Using the source code directly.</p>\n<p>If this is the case, you need to compile the protobufs. Run <code>bash compile_protobufs.sh</code>. Then add the project root to your <code>$PYTHONPATH</code>.</p>\n<h3>Note:</h3>\n<p>Method 2 and 3 requires <code>libprotoc&gt;=3.6.0</code></p>\n<h2>Why Model-Server?</h2>\n<p>Taking deep learning models to production at scale is not a very straight forward process. If you are using Tensorflow then you have <a href=\"https://www.tensorflow.org/tfx/guide/serving\" rel=\"nofollow\">Tensorflow Serving</a>. But, if you are using other frameworks like <a href=\"https://pytorch.org/\" rel=\"nofollow\">PyTorch</a>, <a href=\"https://mxnet.apache.org/\" rel=\"nofollow\">MXNet</a>, <a href=\"https://scikit-learn.org/stable/\" rel=\"nofollow\">scikit-learn</a> etc. Taking your model to production is not very straight forward (Flask, Django and other ReST frameworks). Ideally you should be able to extend Tensorflow Serving to support models from other frameworks also but, this is extremly cumbersome! Thus, to bridge this gap we have <a href=\"https://abhijit-2592.github.io/model-server/\" rel=\"nofollow\">Model-Server</a>: A high performance framework neutral serving solution! <strong>The idea is:</strong> if you are able to train your model in <code>python</code> you should be  able to deploy at scale using pure <code>python</code></p>\n<h2>Salient Features</h2>\n<p><a href=\"https://abhijit-2592.github.io/model-server/\" rel=\"nofollow\">Model-Server</a> is heavily inspired from <a href=\"https://www.tensorflow.org/tfx/guide/serving\" rel=\"nofollow\">Tensorflow Serving</a></p>\n<ul>\n<li><strong>Out of box client side batching support</strong></li>\n<li><strong>Pure python implementation</strong>: You don't need to fiddle around with C++ to have a scalable deployment solution</li>\n<li><strong>Framework neutral</strong>:  Using <a href=\"https://pytorch.org/\" rel=\"nofollow\">PyTorch</a>, <a href=\"https://mxnet.apache.org/\" rel=\"nofollow\">MXNet</a> etc? Don't worry! The solution is platform neutral. If you can use a framework to train in <code>python-3</code>, <a href=\"https://abhijit-2592.github.io/model-server/\" rel=\"nofollow\">Model-Server</a> can deploy it at scale</li>\n<li><strong>Single server for multi-framework and multi-models</strong>: You can host multiple models using the same framework or a mixture of multiple <a href=\"https://pytorch.org/\" rel=\"nofollow\">PyTorch</a>, <a href=\"https://mxnet.apache.org/\" rel=\"nofollow\">MXNet</a>, <a href=\"https://scikit-learn.org/stable/\" rel=\"nofollow\">scikit-learn</a> <a href=\"https://www.tensorflow.org/\" rel=\"nofollow\">Tensorflow</a> etc models!</li>\n</ul>\n<h2>Getting started</h2>\n<p>The core of Model Server is a <code>Servable</code>. A servable is nothing but a <code>python class</code> containing your model's prediction definition which will be served by the <code>Model-Server</code>. All servables must inherit from <code>model_server.Servable</code> for the  <code>Model-Server</code> to serve it.</p>\n<p>To deploy your model to production with <code>Model Server</code>, you just have to write a single <code>python-3</code> file containing a <code>class</code> which inherits from <code>model_server.Servable</code> and has the following two methods:</p>\n<pre><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">input_array_dict</span><span class=\"p\">)</span>\n<span class=\"n\">get_model_info</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">list_of_model_info_dict</span><span class=\"p\">)</span>\n</pre>\n<p>Now run the floowing to start the server in <code>5001</code> port</p>\n<pre>python -m model_server.runserver path_to_custom_servable_file.py\n</pre>\n<p>For more info on  command line arguments:</p>\n<pre>python -m model_server.runserver --help\n</pre>\n<h3>A simple example</h3>\n<p>create a file called <code>simple_servable.py</code> with the following contents:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">model_server</span> <span class=\"kn\">import</span> <span class=\"n\">Servable</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">my_custom_servable</span><span class=\"p\">(</span><span class=\"n\">Servable</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">):</span>\n        <span class=\"c1\"># args contains values from ArgumentParser</span>\n        <span class=\"c1\"># Thus you can pass any kwargs via command line and you get them here</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">input_array_dict</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"This method is responsible for the gRPC call GetPredictions().</span>\n<span class=\"sd\">        All custom servables must define this method.</span>\n\n<span class=\"sd\">        Arguments:</span>\n<span class=\"sd\">            input_array_dict (dict): The PredictionRequest proto decoded as a python dictionary.</span>\n\n<span class=\"sd\">        # example</span>\n<span class=\"sd\">        input_array_dict = {</span>\n<span class=\"sd\">                           \"input_tensor_name1\": numpy array,</span>\n<span class=\"sd\">                           \"input_tensor_name2\": numpy array</span>\n<span class=\"sd\">                            }</span>\n\n<span class=\"sd\">        Returns:</span>\n<span class=\"sd\">            A python dictionary with key (typically output name) and value as numpy array of predictions</span>\n\n<span class=\"sd\">        # example</span>\n<span class=\"sd\">        output = {</span>\n<span class=\"sd\">                   \"output_tensor_name1\": numpy array,</span>\n<span class=\"sd\">                   \"output_tensor_name2\": numpy array</span>\n<span class=\"sd\">                  }</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">input_array_dict</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"p\">({</span><span class=\"s2\">\"output_array1\"</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">),</span>\n                 <span class=\"s2\">\"output_array2\"</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"s2\">\"foo\"</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(),</span><span class=\"s2\">\"bar\"</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">()])</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">),</span>  <span class=\"c1\"># you can get and pass strings encoded as bytes also</span>\n                 <span class=\"p\">})</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">get_model_info</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">list_of_model_info_dict</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"This method which is responsible for the call GetModelInfo()</span>\n\n<span class=\"sd\">        Arguments:</span>\n<span class=\"sd\">            list_of_model_info_dict (list/tuple): A list containing model_info_dicts</span>\n\n<span class=\"sd\">        Note:</span>\n<span class=\"sd\">            model_info_dict contains the following keys:</span>\n\n<span class=\"sd\">            {</span>\n<span class=\"sd\">                \"name\": \"model name as string\"</span>\n<span class=\"sd\">                \"version\": \"version as string\"</span>\n<span class=\"sd\">                \"status\": \"status string\"</span>\n<span class=\"sd\">                \"misc\": \"string with miscellaneous info\"</span>\n<span class=\"sd\">            }</span>\n\n<span class=\"sd\">        Returns:</span>\n<span class=\"sd\">            list_of_model_info_dict (dict): containing the model and server info. This is similar to the function input</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">return</span> <span class=\"p\">[{</span><span class=\"s2\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"first_model\"</span><span class=\"p\">,</span> <span class=\"s2\">\"version\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s2\">\"status\"</span><span class=\"p\">:</span> <span class=\"s2\">\"up\"</span><span class=\"p\">},</span>\n                <span class=\"p\">{</span><span class=\"s2\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"second_model\"</span><span class=\"p\">,</span> <span class=\"s2\">\"version\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"status\"</span><span class=\"p\">:</span> <span class=\"s2\">\"up\"</span><span class=\"p\">,</span> <span class=\"s2\">\"misc\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Other miscellaneous details\"</span><span class=\"p\">}]</span>\n</pre>\n<p>Now run:</p>\n<pre>python -m model_server.runserver path/to/simple_servable.py\n</pre>\n<p>To start the gRPC server!</p>\n<p>Now let's define the client!</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">grpc</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">model_server</span> <span class=\"kn\">import</span> <span class=\"n\">server_pb2</span><span class=\"p\">,</span> <span class=\"n\">server_pb2_grpc</span>\n<span class=\"kn\">from</span> <span class=\"nn\">model_server.utils</span> <span class=\"kn\">import</span> <span class=\"n\">create_tensor_proto</span>\n<span class=\"kn\">from</span> <span class=\"nn\">model_server.utils</span> <span class=\"kn\">import</span> <span class=\"n\">create_predict_request</span>\n<span class=\"kn\">from</span> <span class=\"nn\">model_server.utils</span> <span class=\"kn\">import</span> <span class=\"n\">create_array_from_proto</span>\n<span class=\"kn\">from</span> <span class=\"nn\">model_server.utils</span> <span class=\"kn\">import</span> <span class=\"n\">create_model_info_proto</span>\n\n<span class=\"n\">channel</span> <span class=\"o\">=</span> <span class=\"n\">grpc</span><span class=\"o\">.</span><span class=\"n\">insecure_channel</span><span class=\"p\">(</span><span class=\"s1\">'localhost:5001'</span><span class=\"p\">)</span>  <span class=\"c1\"># default port</span>\n<span class=\"c1\"># create a stub (client)</span>\n<span class=\"n\">stub</span> <span class=\"o\">=</span> <span class=\"n\">server_pb2_grpc</span><span class=\"o\">.</span><span class=\"n\">ModelServerStub</span><span class=\"p\">(</span><span class=\"n\">channel</span><span class=\"p\">)</span>\n<span class=\"n\">input_array_dict</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input1\"</span><span class=\"p\">:</span><span class=\"n\">create_tensor_proto</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)),</span>\n                    <span class=\"s2\">\"input2\"</span><span class=\"p\">:</span><span class=\"n\">create_tensor_proto</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">10.0</span><span class=\"p\">,</span><span class=\"mf\">11.0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">12.0</span><span class=\"p\">,</span><span class=\"mf\">13.0</span><span class=\"p\">]])</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)),</span>\n                    <span class=\"s2\">\"input3\"</span><span class=\"p\">:</span><span class=\"n\">create_tensor_proto</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"s2\">\"Hi\"</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(),</span> <span class=\"s2\">\"Hello\"</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(),</span> <span class=\"s2\">\"test\"</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">()])</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">))</span>\n                   <span class=\"p\">}</span>\n<span class=\"c1\"># create the prediction request</span>\n<span class=\"n\">predict_request</span><span class=\"o\">=</span> <span class=\"n\">create_predict_request</span><span class=\"p\">(</span><span class=\"n\">input_array_dict</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"simple_call\"</span><span class=\"p\">)</span>\n<span class=\"c1\"># make the call</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">stub</span><span class=\"o\">.</span><span class=\"n\">GetPredictions</span><span class=\"p\">(</span><span class=\"n\">predict_request</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># decode the response</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">create_array_from_proto</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"s2\">\"output_array1\"</span><span class=\"p\">]))</span>\n\n<span class=\"c1\"># prints: array([100., 200.], dtype=float32)</span>\n\n<span class=\"c1\"># Getting the model status</span>\n\n<span class=\"n\">model_info_proto</span> <span class=\"o\">=</span> <span class=\"n\">create_model_info_proto</span><span class=\"p\">([])</span>  <span class=\"c1\"># you can pass an empty list also</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">stub</span><span class=\"o\">.</span><span class=\"n\">GetModelInfo</span><span class=\"p\">(</span><span class=\"n\">model_info_proto</span><span class=\"p\">)</span>\n</pre>\n<p>Look at <a href=\"https://github.com/Abhijit-2592/model-server/tree/master/examples\" rel=\"nofollow\">examples</a> folder for further examples</p>\n<h2>Work in Progress</h2>\n<ul>\n<li>Support server side batching and async calls.</li>\n<li>Provide a gRPC endpoint for <a href=\"https://en.wikipedia.org/wiki/Active_learning_(machine_learning)\" rel=\"nofollow\">Active Learning</a> so that you can plug in <code>Model Server</code> with your labeling tool and train on fly!</li>\n<li>Provide a ReST wrapper</li>\n</ul>\n<p>Feel free to file issues, provide suggestions and pull requests</p>\n\n          </div>"}, "last_serial": 5434603, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "fdb2e234ca8410083f780c98cf97bb0e", "sha256": "9b4a7a4058e558cebfe1389a5512d49543d150ab8f56f911642fb08df532c126"}, "downloads": -1, "filename": "model_server-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "fdb2e234ca8410083f780c98cf97bb0e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17345, "upload_time": "2019-06-22T12:47:45", "upload_time_iso_8601": "2019-06-22T12:47:45.106432Z", "url": "https://files.pythonhosted.org/packages/dc/d6/9a25302e182e59399286c6b4792aa5b343ccd5d16abddf95ff673a2c7015/model_server-1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6136f50723605d353eb593f66864e572", "sha256": "bd281149df4e6911b6e0d2f1404468a0731d78adc516487e86b58d949217cfec"}, "downloads": -1, "filename": "model-server-1.0.tar.gz", "has_sig": false, "md5_digest": "6136f50723605d353eb593f66864e572", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11412, "upload_time": "2019-06-22T12:47:47", "upload_time_iso_8601": "2019-06-22T12:47:47.865541Z", "url": "https://files.pythonhosted.org/packages/2f/f1/40e84565063f3778d4ff5f8330e679b2e75d9125e34a4b1cbed6b4d1c1bf/model-server-1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fdb2e234ca8410083f780c98cf97bb0e", "sha256": "9b4a7a4058e558cebfe1389a5512d49543d150ab8f56f911642fb08df532c126"}, "downloads": -1, "filename": "model_server-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "fdb2e234ca8410083f780c98cf97bb0e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17345, "upload_time": "2019-06-22T12:47:45", "upload_time_iso_8601": "2019-06-22T12:47:45.106432Z", "url": "https://files.pythonhosted.org/packages/dc/d6/9a25302e182e59399286c6b4792aa5b343ccd5d16abddf95ff673a2c7015/model_server-1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6136f50723605d353eb593f66864e572", "sha256": "bd281149df4e6911b6e0d2f1404468a0731d78adc516487e86b58d949217cfec"}, "downloads": -1, "filename": "model-server-1.0.tar.gz", "has_sig": false, "md5_digest": "6136f50723605d353eb593f66864e572", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11412, "upload_time": "2019-06-22T12:47:47", "upload_time_iso_8601": "2019-06-22T12:47:47.865541Z", "url": "https://files.pythonhosted.org/packages/2f/f1/40e84565063f3778d4ff5f8330e679b2e75d9125e34a4b1cbed6b4d1c1bf/model-server-1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:52:52 2020"}