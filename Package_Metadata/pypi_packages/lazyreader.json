{"info": {"author": "Alex Chan", "author_email": "alex@alexwlchan.net", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: Implementation :: CPython"], "description": "lazyreader\n==========\n\nlazyreader is a Python module for doing lazy reading of file objects.\n\nThe Python standard library lets you read a file a line-at-a-time, saving you from loading the entire file into memory.\nFor example:\n\n.. code-block:: python\n\n   with open('large_file.txt') as f:\n       for line in f:\n           print(line)\n\nlazyreader lets you do the same thing, but with an arbitrary delimiter, and for any object that presents a ``.read()`` method.\nFor example:\n\n.. code-block:: python\n\n   from lazyreader import lazyread\n\n   with open('large_file.txt') as f:\n       for doc in lazyread(f, delimiter=';'):\n           print(doc)\n\nThis is a snippet of code I spun out from the `Wellcome Digital Platform <https://github.com/wellcometrust/platform-api>`_.\nWe have large XML and JSON files stored in S3 -- sometimes multiple GBs -- but each file is really a series of \"documents\", separated by known delimiters.\nDownloading and parsing the entire file would be prohibitively expensive, but lazyreader allows us to hold just a single document in memory at a time.\n\nInstallation\n************\n\nlazyreader is available from PyPI:\n\n.. code-block:: console\n\n   $ pip install lazyreader\n\nExamples\n********\n\nIf we have a file stored locally, we can open it and split based on any choice of delimiter.\nFor example, if we had a text file in which record were separated by commas:\n\n.. code-block:: python\n\n   with open('lots_of_records.txt') as f:\n       for doc in lazyread(f, delimiter=','):\n           print(doc)\n\nAnother example: we have a file stored in Amazon S3, and we'd like to read it line-by-line.\nThe `boto3 <https://boto3.readthedocs.io/en/stable/>`_ API gives us a file object for reading from S3:\n\n.. code-block:: python\n\n   import boto3\n\n   client = boto3.client('s3')\n   s3_object = client.get_object(Bucket='example-bucket', Key='words.txt')\n   body = s3_object['Body']\n\n   for doc in lazyread(body, delimiter=b'\\n'):\n       print(doc)\n\n(This is the use case for which this code was originally written.)\n\nOne more example: we're fetching an HTML page, and want to read lines separated by ``<br>`` in the underlying HTML.\nLike so:\n\n.. code-block:: python\n\n   import urllib.request\n\n   with urllib.request.urlopen('https://example.org/') as f:\n       for doc in lazyread(f, delimiter=b'<br>'):\n           print(doc)\n\nAdvanced usage\n**************\n\n``lazyread()`` returns a generator, which you can wrap to build a pipeline of generators which do processing on the data.\n\nFirst example: we have a file which contains a list of JSON objects, one per line.\n(This is the format of output files from `elasticdump <https://github.com/taskrabbit/elasticsearch-dump>`_.)\nWhat the caller really needs is Python dictionaries, not JSON strings.\nWe can wrap ``lazyread()`` like so:\n\n.. code-block:: python\n\n   import json\n\n   def lazyjson(f, delimiter=b'\\n'):\n       for doc in lazyread(f, delimiter=delimiter):\n\n           # Ignore empty lines, e.g. the last line in a file\n           if not doc.strip():\n               continue\n\n           yield json.loads(doc)\n\nAnother example: we want to parse a large XML file, but not load it all into memory at once.\nWe can write the following wrapper:\n\n.. code-block:: python\n\n   from lxml import etree\n\n   def lazyxmlstrings(f, opening_tag, closing_tag):\n       for doc in lazyread(f, delimiter=closing_tag):\n           if opening_tag not in doc:\n               continue\n\n           # We want complete XML blocks, so look for the opening tag and\n           # just return its contents\n           block = doc.split(opening_tag)[-1]\n           yield opening_tag + block\n\n   def lazyxml(f, opening_tag, closing_tag):\n       for xml_string in lazyxmlstrings(f, opening_tag, closing_tag):\n            yield etree.fromstring(xml_string)\n\nWe use both of these wrappers at Wellcome to do efficient processing of large files that are kept in Amazon S3.\n\nIsn't this a bit simple to be a module?\n***************************************\n\nMaybe.\nThere are recipes on Stack Overflow that do very similar, but I find it useful to have in a standalone module.\n\nAnd it's not completely trivial -- at least, not for me.\nI made two mistakes when I first wrote this:\n\n*  I was hard-coding the initial running string as\n\n   .. code-block:: python\n\n      running = b''\n\n   That only works if your file object is returning bytestrings.\n   If it's returning Unicode strings, you get a ``TypeError`` (`can't concat bytes to str`) when it first tries to read from the file.\n   String types are important!\n\n*  After I'd read another 1024 characters from the file, I checked for the delimiter like so:\n\n   .. code-block:: python\n\n      running += new_data\n      if delimiter in running:\n          curr, running = running.split(delimiter)\n          yield curr + delimiter\n\n   For my initial use case, individual documents were `much` bigger than 1024 characters, so the new data would never contain multiple delimiters.\n   But with smaller documents, you might get multiple delimiters in one read, and then unpacking the result of ``.split()`` would throw a ``ValueError``.\n   So now the code correctly checks and handles the case where a single read includes more than one delimiter.\n\nNow it's encoded and tested in a module, I don't have to worry about making the same mistakes again.\n\nLicense\n*******\n\nMIT.\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/alexwlchan/lazyreader", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "lazyreader", "package_url": "https://pypi.org/project/lazyreader/", "platform": "", "project_url": "https://pypi.org/project/lazyreader/", "project_urls": {"Homepage": "https://github.com/alexwlchan/lazyreader"}, "release_url": "https://pypi.org/project/lazyreader/1.0.1/", "requires_dist": null, "requires_python": "", "summary": "Lazy reading of file objects for efficient batch processing", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>lazyreader is a Python module for doing lazy reading of file objects.</p>\n<p>The Python standard library lets you read a file a line-at-a-time, saving you from loading the entire file into memory.\nFor example:</p>\n<pre><span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'large_file.txt'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n</pre>\n<p>lazyreader lets you do the same thing, but with an arbitrary delimiter, and for any object that presents a <tt>.read()</tt> method.\nFor example:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">lazyreader</span> <span class=\"kn\">import</span> <span class=\"n\">lazyread</span>\n\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'large_file.txt'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">lazyread</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">delimiter</span><span class=\"o\">=</span><span class=\"s1\">';'</span><span class=\"p\">):</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n</pre>\n<p>This is a snippet of code I spun out from the <a href=\"https://github.com/wellcometrust/platform-api\" rel=\"nofollow\">Wellcome Digital Platform</a>.\nWe have large XML and JSON files stored in S3 \u2013 sometimes multiple GBs \u2013 but each file is really a series of \u201cdocuments\u201d, separated by known delimiters.\nDownloading and parsing the entire file would be prohibitively expensive, but lazyreader allows us to hold just a single document in memory at a time.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>lazyreader is available from PyPI:</p>\n<pre><span class=\"gp\">$</span> pip install lazyreader\n</pre>\n</div>\n<div id=\"examples\">\n<h2>Examples</h2>\n<p>If we have a file stored locally, we can open it and split based on any choice of delimiter.\nFor example, if we had a text file in which record were separated by commas:</p>\n<pre><span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'lots_of_records.txt'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">lazyread</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">delimiter</span><span class=\"o\">=</span><span class=\"s1\">','</span><span class=\"p\">):</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n</pre>\n<p>Another example: we have a file stored in Amazon S3, and we\u2019d like to read it line-by-line.\nThe <a href=\"https://boto3.readthedocs.io/en/stable/\" rel=\"nofollow\">boto3</a> API gives us a file object for reading from S3:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">boto3</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">boto3</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"p\">(</span><span class=\"s1\">'s3'</span><span class=\"p\">)</span>\n<span class=\"n\">s3_object</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">get_object</span><span class=\"p\">(</span><span class=\"n\">Bucket</span><span class=\"o\">=</span><span class=\"s1\">'example-bucket'</span><span class=\"p\">,</span> <span class=\"n\">Key</span><span class=\"o\">=</span><span class=\"s1\">'words.txt'</span><span class=\"p\">)</span>\n<span class=\"n\">body</span> <span class=\"o\">=</span> <span class=\"n\">s3_object</span><span class=\"p\">[</span><span class=\"s1\">'Body'</span><span class=\"p\">]</span>\n\n<span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">lazyread</span><span class=\"p\">(</span><span class=\"n\">body</span><span class=\"p\">,</span> <span class=\"n\">delimiter</span><span class=\"o\">=</span><span class=\"sa\">b</span><span class=\"s1\">'</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n</pre>\n<p>(This is the use case for which this code was originally written.)</p>\n<p>One more example: we\u2019re fetching an HTML page, and want to read lines separated by <tt>&lt;br&gt;</tt> in the underlying HTML.\nLike so:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">urllib.request</span>\n\n<span class=\"k\">with</span> <span class=\"n\">urllib</span><span class=\"o\">.</span><span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">urlopen</span><span class=\"p\">(</span><span class=\"s1\">'https://example.org/'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">lazyread</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">delimiter</span><span class=\"o\">=</span><span class=\"sa\">b</span><span class=\"s1\">'&lt;br&gt;'</span><span class=\"p\">):</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"advanced-usage\">\n<h2>Advanced usage</h2>\n<p><tt>lazyread()</tt> returns a generator, which you can wrap to build a pipeline of generators which do processing on the data.</p>\n<p>First example: we have a file which contains a list of JSON objects, one per line.\n(This is the format of output files from <a href=\"https://github.com/taskrabbit/elasticsearch-dump\" rel=\"nofollow\">elasticdump</a>.)\nWhat the caller really needs is Python dictionaries, not JSON strings.\nWe can wrap <tt>lazyread()</tt> like so:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">lazyjson</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">delimiter</span><span class=\"o\">=</span><span class=\"sa\">b</span><span class=\"s1\">'</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">lazyread</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">delimiter</span><span class=\"o\">=</span><span class=\"n\">delimiter</span><span class=\"p\">):</span>\n\n        <span class=\"c1\"># Ignore empty lines, e.g. the last line in a file</span>\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">():</span>\n            <span class=\"k\">continue</span>\n\n        <span class=\"k\">yield</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n</pre>\n<p>Another example: we want to parse a large XML file, but not load it all into memory at once.\nWe can write the following wrapper:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">lxml</span> <span class=\"kn\">import</span> <span class=\"n\">etree</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">lazyxmlstrings</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">opening_tag</span><span class=\"p\">,</span> <span class=\"n\">closing_tag</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">lazyread</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">delimiter</span><span class=\"o\">=</span><span class=\"n\">closing_tag</span><span class=\"p\">):</span>\n        <span class=\"k\">if</span> <span class=\"n\">opening_tag</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"p\">:</span>\n            <span class=\"k\">continue</span>\n\n        <span class=\"c1\"># We want complete XML blocks, so look for the opening tag and</span>\n        <span class=\"c1\"># just return its contents</span>\n        <span class=\"n\">block</span> <span class=\"o\">=</span> <span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">opening_tag</span><span class=\"p\">)[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n        <span class=\"k\">yield</span> <span class=\"n\">opening_tag</span> <span class=\"o\">+</span> <span class=\"n\">block</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">lazyxml</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">opening_tag</span><span class=\"p\">,</span> <span class=\"n\">closing_tag</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">xml_string</span> <span class=\"ow\">in</span> <span class=\"n\">lazyxmlstrings</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">opening_tag</span><span class=\"p\">,</span> <span class=\"n\">closing_tag</span><span class=\"p\">):</span>\n         <span class=\"k\">yield</span> <span class=\"n\">etree</span><span class=\"o\">.</span><span class=\"n\">fromstring</span><span class=\"p\">(</span><span class=\"n\">xml_string</span><span class=\"p\">)</span>\n</pre>\n<p>We use both of these wrappers at Wellcome to do efficient processing of large files that are kept in Amazon S3.</p>\n</div>\n<div id=\"isn-t-this-a-bit-simple-to-be-a-module\">\n<h2>Isn\u2019t this a bit simple to be a module?</h2>\n<p>Maybe.\nThere are recipes on Stack Overflow that do very similar, but I find it useful to have in a standalone module.</p>\n<p>And it\u2019s not completely trivial \u2013 at least, not for me.\nI made two mistakes when I first wrote this:</p>\n<ul>\n<li><p>I was hard-coding the initial running string as</p>\n<pre><span class=\"n\">running</span> <span class=\"o\">=</span> <span class=\"sa\">b</span><span class=\"s1\">''</span>\n</pre>\n<p>That only works if your file object is returning bytestrings.\nIf it\u2019s returning Unicode strings, you get a <tt>TypeError</tt> (<cite>can\u2019t concat bytes to str</cite>) when it first tries to read from the file.\nString types are important!</p>\n</li>\n<li><p>After I\u2019d read another 1024 characters from the file, I checked for the delimiter like so:</p>\n<pre><span class=\"n\">running</span> <span class=\"o\">+=</span> <span class=\"n\">new_data</span>\n<span class=\"k\">if</span> <span class=\"n\">delimiter</span> <span class=\"ow\">in</span> <span class=\"n\">running</span><span class=\"p\">:</span>\n    <span class=\"n\">curr</span><span class=\"p\">,</span> <span class=\"n\">running</span> <span class=\"o\">=</span> <span class=\"n\">running</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">delimiter</span><span class=\"p\">)</span>\n    <span class=\"k\">yield</span> <span class=\"n\">curr</span> <span class=\"o\">+</span> <span class=\"n\">delimiter</span>\n</pre>\n<p>For my initial use case, individual documents were <cite>much</cite> bigger than 1024 characters, so the new data would never contain multiple delimiters.\nBut with smaller documents, you might get multiple delimiters in one read, and then unpacking the result of <tt>.split()</tt> would throw a <tt>ValueError</tt>.\nSo now the code correctly checks and handles the case where a single read includes more than one delimiter.</p>\n</li>\n</ul>\n<p>Now it\u2019s encoded and tested in a module, I don\u2019t have to worry about making the same mistakes again.</p>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>MIT.</p>\n</div>\n\n          </div>"}, "last_serial": 3153916, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "fe22d9772dd2a3ff9ce9d6f16ec21305", "sha256": "6111f4c0f1d04354dc7fb42dda5a5e770bc98aae5a731cbc506df2c19a3bc0c4"}, "downloads": -1, "filename": "lazyreader-1.0.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "fe22d9772dd2a3ff9ce9d6f16ec21305", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 6772, "upload_time": "2017-09-02T07:17:41", "upload_time_iso_8601": "2017-09-02T07:17:41.175993Z", "url": "https://files.pythonhosted.org/packages/c2/95/9ba53b85543a7e1d44e2330873e4a3967838e59e1b1a13677478d15ba85c/lazyreader-1.0.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1357a466e68d3d0f0d57111689fd42f6", "sha256": "7187dc32d9c926aa9f359adfbdc5bbc7f855f6799020a7b331111c3049ab870d"}, "downloads": -1, "filename": "lazyreader-1.0.0.tar.gz", "has_sig": false, "md5_digest": "1357a466e68d3d0f0d57111689fd42f6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4108, "upload_time": "2017-09-02T07:17:42", "upload_time_iso_8601": "2017-09-02T07:17:42.643617Z", "url": "https://files.pythonhosted.org/packages/3b/bb/54c2a6762878d2ed3a8246b6153dc495c692b4adcb6d290651d5b790209c/lazyreader-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "3e115e5383fa9111911049d102a8d830", "sha256": "629dceca1a9e2b9006170936b34cb58e7dbe163e65fca184286cf44c6e50967b"}, "downloads": -1, "filename": "lazyreader-1.0.1-py2-none-any.whl", "has_sig": false, "md5_digest": "3e115e5383fa9111911049d102a8d830", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 7222, "upload_time": "2017-09-06T15:45:41", "upload_time_iso_8601": "2017-09-06T15:45:41.415892Z", "url": "https://files.pythonhosted.org/packages/5c/7f/a17e8eb62cc5cbd6aa6cfa0faca6d360eca6119433623a5bb4d58d58b079/lazyreader-1.0.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "184799baf8d848a3dfbeb10a93c5cd9f", "sha256": "12065c91f2ad743eecf59c0df2dba4ecc7cc015aa177dee2be1c76b03e3907d8"}, "downloads": -1, "filename": "lazyreader-1.0.1.tar.gz", "has_sig": false, "md5_digest": "184799baf8d848a3dfbeb10a93c5cd9f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4457, "upload_time": "2017-09-06T15:45:42", "upload_time_iso_8601": "2017-09-06T15:45:42.837151Z", "url": "https://files.pythonhosted.org/packages/d6/4b/6b310bced42a4777819d94d1734a6da9978fcde2c297fccb07a3bb5eb3f2/lazyreader-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3e115e5383fa9111911049d102a8d830", "sha256": "629dceca1a9e2b9006170936b34cb58e7dbe163e65fca184286cf44c6e50967b"}, "downloads": -1, "filename": "lazyreader-1.0.1-py2-none-any.whl", "has_sig": false, "md5_digest": "3e115e5383fa9111911049d102a8d830", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 7222, "upload_time": "2017-09-06T15:45:41", "upload_time_iso_8601": "2017-09-06T15:45:41.415892Z", "url": "https://files.pythonhosted.org/packages/5c/7f/a17e8eb62cc5cbd6aa6cfa0faca6d360eca6119433623a5bb4d58d58b079/lazyreader-1.0.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "184799baf8d848a3dfbeb10a93c5cd9f", "sha256": "12065c91f2ad743eecf59c0df2dba4ecc7cc015aa177dee2be1c76b03e3907d8"}, "downloads": -1, "filename": "lazyreader-1.0.1.tar.gz", "has_sig": false, "md5_digest": "184799baf8d848a3dfbeb10a93c5cd9f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4457, "upload_time": "2017-09-06T15:45:42", "upload_time_iso_8601": "2017-09-06T15:45:42.837151Z", "url": "https://files.pythonhosted.org/packages/d6/4b/6b310bced42a4777819d94d1734a6da9978fcde2c297fccb07a3bb5eb3f2/lazyreader-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:35 2020"}