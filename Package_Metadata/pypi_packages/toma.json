{"info": {"author": "Andreas @blackhc Kirsch", "author_email": "blackhc+toma@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Torch Memory-adaptive Algorithms (TOMA)\n\n[![Build Status](https://www.travis-ci.com/BlackHC/toma.svg?branch=master)](https://www.travis-ci.com/BlackHC/toma) [![codecov](https://codecov.io/gh/BlackHC/toma/branch/master/graph/badge.svg)](https://codecov.io/gh/BlackHC/toma) [![PyPI](https://img.shields.io/badge/PyPI-toma-blue.svg)](https://pypi.python.org/pypi/toma/)\n\nA collection of helpers to make it easier to write code that adapts to the available (CUDA) memory.\nSpecifically, it retries code that fails due to OOM (out-of-memory) conditions and lowers batchsizes automatically. \n\nTo avoid failing over repeatedly, a simple cache is implemented that memorizes that last successful batchsize given the call and available free memory.\n\n## Installation\n\nTo install using pip, use:\n\n```\npip install toma\n```\n\nTo run the tests, use:\n\n```\npython setup.py test\n```\n\n## Example\n\n```python\nfrom toma import toma\n\n@toma.batch(initial_batchsize=512)\ndef run_inference(batchsize, model, dataset):\n    # ...\n\nrun_inference(batchsize, model, dataset)\n```\n\nThis will try to execute train_model with batchsize=512. If a memory error is thrown, it will decrease the batchsize until it succeeds.\n\n**Note:** \nThis batch size can be different from the batch size used to accumulate gradients by only calling `optimizer.step()` every so often.\n\nTo make it easier to loop over a ranges, there are also `toma.range` and `toma.chunked`:\n\n```python\n@toma.chunked(initial_step=512)\ndef compute_result(out: torch.Tensor, start: int, end: int):\n    # ...\n\nresult = torch.empty((8192, ...))\ncompute_result(result)\n```\n\nThis will chunk `result` and pass the chunks to `compute_result` one by one. \nAgain, if it fails due to OOM, the step will be halfed etc.\nCompared to `toma.batch`, this allows for reduction of the step size while looping over the chunks.\nThis can save computation.\n\n```python\n@toma.range(initial_step=32)\ndef reduce_data(start: int, end: int, out: torch.Tensor, dataA: torch.Tensor, dataB: torch.Tensor):\n    # ...\n\nreduce_data(0, 1024, result, dataA, dataB)\n``` \n\n`toma.range` iterates over `range(start, end, step)` with `step=initial_step`. If it fails due to OOM, it will lower the step size and continue.\n\n### `toma.execute`\n\nTo make it easier to just execute a block without having to extract it into a function and then call it, we also provide `toma.execute.batch`, `toma.execute.range` and `toma.execute.chunked`, which are somewhat unorthodox and call the function that is passed to them right away. (Mainly because there is no support for anonymous functions in Python beyond lambda expressions.)\n\n```python\ndef function():\n    # ... other code\n\n    @toma.execute.chunked(batched_data, initial_step=128):\n    def compute(chunk, start, end):\n        # ...\n```\n\n## Cache\n\nThere are 3 available cache types at the moment. \nThey can be changed by either setting `toma.DEFAULT_CACHE_TYPE` or by passing `cache_type` to the calls.\n\nFor example:\n```python\n@toma.batch(initial_batchsize=512, cache_type=toma.GlobalBatchsizeCache)\n```\nor\n```python\ntoma.explicit.batch(..., toma_cache_type=toma.GlobalBatchsizeCache)\n```\n\n### `StacktraceMemoryBatchsizeCache`: Stacktrace & Available Memory (*the default*)\n\nThis memorizes the successful batchsizes for a given call trace and available memory at that point.\nFor most machine learning code, this is sufficient to remember the right batchsize without having to look at the actual arguments and understanding more of the semantics.\n\nThe implicit assumption is that after a few iterations a stable state will be reached in regards to GPU and CPU memory usage.\n\nTo limit the CPU memory of the process, toma provides:\n```python\nimport toma.cpu_memory\n\ntoma.cpu_memory.set_cpu_memory_limit(8)\n```\nThis can also be useful to avoid accidental swap thrashing.\n\n### `GlobalBatchsizeCache`: Global per Function\n\nThis reuses the last successful batchsize independently from where the call happened.\n\n### `NoBatchsizeCache`: No Caching\n\nAlways starts with the suggested batchsize and fails over if necessary.\n\n## Benchmark/Overhead\n\nThere is overhead involved. Toma should only be used with otherwise time/memory-consuming operations.\n\n```text\n---------------------------------------------------------------------------------- benchmark: 5 tests ----------------------------------------------------------------------------------\nName (time in ms)          Min                Max               Mean            StdDev             Median                IQR            Outliers       OPS            Rounds  Iterations\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_native             2.1455 (1.0)       3.7733 (1.0)       2.3037 (1.0)      0.1103 (1.0)       2.2935 (1.0)       0.1302 (1.0)          81;5  434.0822 (1.0)         448           1\ntest_simple            17.4657 (8.14)     27.0049 (7.16)     21.0453 (9.14)     2.6233 (23.79)    20.4881 (8.93)      3.4384 (26.42)        13;0   47.5165 (0.11)         39           1\ntest_toma_no_cache     31.4380 (14.65)    40.8567 (10.83)    33.2749 (14.44)    2.2530 (20.43)    32.2698 (14.07)     2.8210 (21.67)         4;1   30.0527 (0.07)         25           1\ntest_explicit          33.0759 (15.42)    52.1866 (13.83)    39.6956 (17.23)    6.9620 (63.14)    38.4929 (16.78)    11.2344 (86.31)         4;0   25.1917 (0.06)         20           1\ntest_toma              36.9633 (17.23)    57.0220 (15.11)    43.5201 (18.89)    6.7318 (61.05)    41.6034 (18.14)     7.2173 (55.45)         2;2   22.9779 (0.05)         13           1\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n```\n\n## Thanks\n\nThanks to [@y0ast](https://github.com/y0ast) for feedback and discussion.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/blackhc/toma", "keywords": "tools pytorch", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "toma", "package_url": "https://pypi.org/project/toma/", "platform": "", "project_url": "https://pypi.org/project/toma/", "project_urls": {"Homepage": "https://github.com/blackhc/toma"}, "release_url": "https://pypi.org/project/toma/1.1.0/", "requires_dist": ["torch", "psutil", "check-manifest ; extra == 'dev'", "coverage ; extra == 'test'", "codecov ; extra == 'test'", "pytest ; extra == 'test'", "pytest-benchmark ; extra == 'test'", "pytest-cov ; extra == 'test'", "pytest-forked ; extra == 'test'"], "requires_python": "", "summary": "Write algorithms in PyTorch that adapt to the available (CUDA) memory", "version": "1.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Torch Memory-adaptive Algorithms (TOMA)</h1>\n<p><a href=\"https://www.travis-ci.com/BlackHC/toma\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b6d6b22ba33e4ddb606211c94d2182b9ab0c4016/68747470733a2f2f7777772e7472617669732d63692e636f6d2f426c61636b48432f746f6d612e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://codecov.io/gh/BlackHC/toma\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/285d44fcc8cbbd4361f3f383dddbe7a0e516effe/68747470733a2f2f636f6465636f762e696f2f67682f426c61636b48432f746f6d612f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a> <a href=\"https://pypi.python.org/pypi/toma/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/69bd7825c3cef9b69481fcdf6feeeeecb531c2ec/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507950492d746f6d612d626c75652e737667\"></a></p>\n<p>A collection of helpers to make it easier to write code that adapts to the available (CUDA) memory.\nSpecifically, it retries code that fails due to OOM (out-of-memory) conditions and lowers batchsizes automatically.</p>\n<p>To avoid failing over repeatedly, a simple cache is implemented that memorizes that last successful batchsize given the call and available free memory.</p>\n<h2>Installation</h2>\n<p>To install using pip, use:</p>\n<pre><code>pip install toma\n</code></pre>\n<p>To run the tests, use:</p>\n<pre><code>python setup.py test\n</code></pre>\n<h2>Example</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">toma</span> <span class=\"kn\">import</span> <span class=\"n\">toma</span>\n\n<span class=\"nd\">@toma</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"n\">initial_batchsize</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">run_inference</span><span class=\"p\">(</span><span class=\"n\">batchsize</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">dataset</span><span class=\"p\">):</span>\n    <span class=\"c1\"># ...</span>\n\n<span class=\"n\">run_inference</span><span class=\"p\">(</span><span class=\"n\">batchsize</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">dataset</span><span class=\"p\">)</span>\n</pre>\n<p>This will try to execute train_model with batchsize=512. If a memory error is thrown, it will decrease the batchsize until it succeeds.</p>\n<p><strong>Note:</strong>\nThis batch size can be different from the batch size used to accumulate gradients by only calling <code>optimizer.step()</code> every so often.</p>\n<p>To make it easier to loop over a ranges, there are also <code>toma.range</code> and <code>toma.chunked</code>:</p>\n<pre><span class=\"nd\">@toma</span><span class=\"o\">.</span><span class=\"n\">chunked</span><span class=\"p\">(</span><span class=\"n\">initial_step</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_result</span><span class=\"p\">(</span><span class=\"n\">out</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">start</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">):</span>\n    <span class=\"c1\"># ...</span>\n\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"mi\">8192</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">))</span>\n<span class=\"n\">compute_result</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n</pre>\n<p>This will chunk <code>result</code> and pass the chunks to <code>compute_result</code> one by one.\nAgain, if it fails due to OOM, the step will be halfed etc.\nCompared to <code>toma.batch</code>, this allows for reduction of the step size while looping over the chunks.\nThis can save computation.</p>\n<pre><span class=\"nd\">@toma</span><span class=\"o\">.</span><span class=\"n\">range</span><span class=\"p\">(</span><span class=\"n\">initial_step</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">reduce_data</span><span class=\"p\">(</span><span class=\"n\">start</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">out</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">dataA</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">dataB</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">):</span>\n    <span class=\"c1\"># ...</span>\n\n<span class=\"n\">reduce_data</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">dataA</span><span class=\"p\">,</span> <span class=\"n\">dataB</span><span class=\"p\">)</span>\n</pre>\n<p><code>toma.range</code> iterates over <code>range(start, end, step)</code> with <code>step=initial_step</code>. If it fails due to OOM, it will lower the step size and continue.</p>\n<h3><code>toma.execute</code></h3>\n<p>To make it easier to just execute a block without having to extract it into a function and then call it, we also provide <code>toma.execute.batch</code>, <code>toma.execute.range</code> and <code>toma.execute.chunked</code>, which are somewhat unorthodox and call the function that is passed to them right away. (Mainly because there is no support for anonymous functions in Python beyond lambda expressions.)</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">function</span><span class=\"p\">():</span>\n    <span class=\"c1\"># ... other code</span>\n\n    <span class=\"nd\">@toma</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"o\">.</span><span class=\"n\">chunked</span><span class=\"p\">(</span><span class=\"n\">batched_data</span><span class=\"p\">,</span> <span class=\"n\">initial_step</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">compute</span><span class=\"p\">(</span><span class=\"n\">chunk</span><span class=\"p\">,</span> <span class=\"n\">start</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"p\">):</span>\n        <span class=\"c1\"># ...</span>\n</pre>\n<h2>Cache</h2>\n<p>There are 3 available cache types at the moment.\nThey can be changed by either setting <code>toma.DEFAULT_CACHE_TYPE</code> or by passing <code>cache_type</code> to the calls.</p>\n<p>For example:</p>\n<pre><span class=\"nd\">@toma</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"n\">initial_batchsize</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">cache_type</span><span class=\"o\">=</span><span class=\"n\">toma</span><span class=\"o\">.</span><span class=\"n\">GlobalBatchsizeCache</span><span class=\"p\">)</span>\n</pre>\n<p>or</p>\n<pre><span class=\"n\">toma</span><span class=\"o\">.</span><span class=\"n\">explicit</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">toma_cache_type</span><span class=\"o\">=</span><span class=\"n\">toma</span><span class=\"o\">.</span><span class=\"n\">GlobalBatchsizeCache</span><span class=\"p\">)</span>\n</pre>\n<h3><code>StacktraceMemoryBatchsizeCache</code>: Stacktrace &amp; Available Memory (<em>the default</em>)</h3>\n<p>This memorizes the successful batchsizes for a given call trace and available memory at that point.\nFor most machine learning code, this is sufficient to remember the right batchsize without having to look at the actual arguments and understanding more of the semantics.</p>\n<p>The implicit assumption is that after a few iterations a stable state will be reached in regards to GPU and CPU memory usage.</p>\n<p>To limit the CPU memory of the process, toma provides:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">toma.cpu_memory</span>\n\n<span class=\"n\">toma</span><span class=\"o\">.</span><span class=\"n\">cpu_memory</span><span class=\"o\">.</span><span class=\"n\">set_cpu_memory_limit</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n</pre>\n<p>This can also be useful to avoid accidental swap thrashing.</p>\n<h3><code>GlobalBatchsizeCache</code>: Global per Function</h3>\n<p>This reuses the last successful batchsize independently from where the call happened.</p>\n<h3><code>NoBatchsizeCache</code>: No Caching</h3>\n<p>Always starts with the suggested batchsize and fails over if necessary.</p>\n<h2>Benchmark/Overhead</h2>\n<p>There is overhead involved. Toma should only be used with otherwise time/memory-consuming operations.</p>\n<pre>---------------------------------------------------------------------------------- benchmark: 5 tests ----------------------------------------------------------------------------------\nName (time in ms)          Min                Max               Mean            StdDev             Median                IQR            Outliers       OPS            Rounds  Iterations\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_native             2.1455 (1.0)       3.7733 (1.0)       2.3037 (1.0)      0.1103 (1.0)       2.2935 (1.0)       0.1302 (1.0)          81;5  434.0822 (1.0)         448           1\ntest_simple            17.4657 (8.14)     27.0049 (7.16)     21.0453 (9.14)     2.6233 (23.79)    20.4881 (8.93)      3.4384 (26.42)        13;0   47.5165 (0.11)         39           1\ntest_toma_no_cache     31.4380 (14.65)    40.8567 (10.83)    33.2749 (14.44)    2.2530 (20.43)    32.2698 (14.07)     2.8210 (21.67)         4;1   30.0527 (0.07)         25           1\ntest_explicit          33.0759 (15.42)    52.1866 (13.83)    39.6956 (17.23)    6.9620 (63.14)    38.4929 (16.78)    11.2344 (86.31)         4;0   25.1917 (0.06)         20           1\ntest_toma              36.9633 (17.23)    57.0220 (15.11)    43.5201 (18.89)    6.7318 (61.05)    41.6034 (18.14)     7.2173 (55.45)         2;2   22.9779 (0.05)         13           1\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</pre>\n<h2>Thanks</h2>\n<p>Thanks to <a href=\"https://github.com/y0ast\" rel=\"nofollow\">@y0ast</a> for feedback and discussion.</p>\n\n          </div>"}, "last_serial": 7091968, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "dd5372186a84e38958d87f5a035d044e", "sha256": "7f3891087a1afaf3360876497c4befcdb9a5b2bdb5df6ce6d75b7a88c5477186"}, "downloads": -1, "filename": "toma-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "dd5372186a84e38958d87f5a035d044e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8919, "upload_time": "2020-04-09T12:59:28", "upload_time_iso_8601": "2020-04-09T12:59:28.105777Z", "url": "https://files.pythonhosted.org/packages/b6/94/1b0d32318155ebd109fc5ffcecdea31b0e98515bea6318c6b639ac67884c/toma-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6abce16fafa3dac118c913a4a4aec124", "sha256": "333e2f12b3955964ef4b5491d6e8b3046ad924bb3314dbdba2763acc3800561b"}, "downloads": -1, "filename": "toma-1.0.0.tar.gz", "has_sig": false, "md5_digest": "6abce16fafa3dac118c913a4a4aec124", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9294, "upload_time": "2020-04-09T12:59:31", "upload_time_iso_8601": "2020-04-09T12:59:31.704433Z", "url": "https://files.pythonhosted.org/packages/a5/54/fa8783bbb48d5086641539069665c882de5066f2894e81d9b5c57a2228f9/toma-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "10470420e801cc67cc990fda4ba7e65e", "sha256": "8c82e4adfebe55c061f2f66afe203fe7365c7ee177d816c05a1ee40eed9d890b"}, "downloads": -1, "filename": "toma-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "10470420e801cc67cc990fda4ba7e65e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9029, "upload_time": "2020-04-09T13:31:08", "upload_time_iso_8601": "2020-04-09T13:31:08.258021Z", "url": "https://files.pythonhosted.org/packages/73/2f/9c9bd4f009734625a822a54e4b75d5c4a01474899b9764bc2325fb7775b3/toma-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "272f9d7892bba44047781cc27423c39b", "sha256": "e62229ad6acd3ebd8101f8c5dc21cf7c41d28681189797ea9230e112fdec709c"}, "downloads": -1, "filename": "toma-1.0.1.tar.gz", "has_sig": false, "md5_digest": "272f9d7892bba44047781cc27423c39b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9330, "upload_time": "2020-04-09T13:31:12", "upload_time_iso_8601": "2020-04-09T13:31:12.348507Z", "url": "https://files.pythonhosted.org/packages/6d/d1/f967422149bb9fcc26a5eae149aad407fee66213f0831e49ba853f9ac05e/toma-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "391be78fa13c9666a4f7171bdb85fc6c", "sha256": "b16bcd734d6c47c4cbbc2679c0342df49695b642d3341dd74884ee65d4f9cc28"}, "downloads": -1, "filename": "toma-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "391be78fa13c9666a4f7171bdb85fc6c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9044, "upload_time": "2020-04-09T14:08:09", "upload_time_iso_8601": "2020-04-09T14:08:09.674936Z", "url": "https://files.pythonhosted.org/packages/23/5c/dc875254721c8fe68509473e3f4a8dc7df2e47d9723104111fc422955f99/toma-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "36ae3cbcb15481ef05d1c66cf53868d5", "sha256": "77e8d8c0c93c957a36ac38d95700cacccf259d5c6f7e19379598c1548db6e954"}, "downloads": -1, "filename": "toma-1.0.2.tar.gz", "has_sig": false, "md5_digest": "36ae3cbcb15481ef05d1c66cf53868d5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9338, "upload_time": "2020-04-09T14:08:10", "upload_time_iso_8601": "2020-04-09T14:08:10.968647Z", "url": "https://files.pythonhosted.org/packages/76/1d/e7fca18e8a80aafa2535a1c845f4304170bd87d166f305852143b4cb4e35/toma-1.0.2.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "1a9b49341a92c349aff33cc4e6f47057", "sha256": "e2d678bd286e8a1e8141277f21aa32bb635dc47e58c492d5f60efbf646927318"}, "downloads": -1, "filename": "toma-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "1a9b49341a92c349aff33cc4e6f47057", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9567, "upload_time": "2020-04-24T11:34:25", "upload_time_iso_8601": "2020-04-24T11:34:25.881797Z", "url": "https://files.pythonhosted.org/packages/0e/63/f0b3f8855591bf9385e2c66f27e99c847e758927f1be72a1aece8be007a1/toma-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "036b5801bf5fb7de06c4c94fa4ad7b17", "sha256": "d4b7d04d3c8a5b4ce4fee30a92282e29e95f4b641db20d1c6f458d13e8793b7a"}, "downloads": -1, "filename": "toma-1.1.0.tar.gz", "has_sig": false, "md5_digest": "036b5801bf5fb7de06c4c94fa4ad7b17", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11600, "upload_time": "2020-04-24T11:34:28", "upload_time_iso_8601": "2020-04-24T11:34:28.192661Z", "url": "https://files.pythonhosted.org/packages/4f/d1/74aad779150b03b6237de39f9d7a0e48d6a1ef2ff59d7501f683a8aa5a8a/toma-1.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1a9b49341a92c349aff33cc4e6f47057", "sha256": "e2d678bd286e8a1e8141277f21aa32bb635dc47e58c492d5f60efbf646927318"}, "downloads": -1, "filename": "toma-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "1a9b49341a92c349aff33cc4e6f47057", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9567, "upload_time": "2020-04-24T11:34:25", "upload_time_iso_8601": "2020-04-24T11:34:25.881797Z", "url": "https://files.pythonhosted.org/packages/0e/63/f0b3f8855591bf9385e2c66f27e99c847e758927f1be72a1aece8be007a1/toma-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "036b5801bf5fb7de06c4c94fa4ad7b17", "sha256": "d4b7d04d3c8a5b4ce4fee30a92282e29e95f4b641db20d1c6f458d13e8793b7a"}, "downloads": -1, "filename": "toma-1.1.0.tar.gz", "has_sig": false, "md5_digest": "036b5801bf5fb7de06c4c94fa4ad7b17", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11600, "upload_time": "2020-04-24T11:34:28", "upload_time_iso_8601": "2020-04-24T11:34:28.192661Z", "url": "https://files.pythonhosted.org/packages/4f/d1/74aad779150b03b6237de39f9d7a0e48d6a1ef2ff59d7501f683a8aa5a8a/toma-1.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:20 2020"}