{"info": {"author": "William Jacques", "author_email": "williamjcqs8@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# \ud83d\ude80 Deploy Transformers \ud83e\udd17\n> Deploy a SOTA model for text-generation in just three lines of code \ud83d\udcbb\n\n\n![image](https://svgshare.com/i/GoN.svg)\n\n\n## Installation\n\n[**Pytorch**](https://pytorch.org/get-started/locally/#start-locally) and [**Transformers**](https://github.com/huggingface/transformers/#installation) are obviously needed.\n\n```bash\npip install deploy-transformers\n```\n\n**For deployment, file structure needs to be like this:**\n```bash\n\u251c\u2500\u2500 static\n\u2502   \u251c\u2500\u2500 script.js\n\u2502   \u251c\u2500\u2500 style.css\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 404.html\n\u2502   \u251c\u2500\u2500 index.html\n|\n\u2514\u2500\u2500 your_file.py\n```\n\nYou can either **clone** this repository to have original files or use the function `website.create_structure()` or **create yourself** the structure.\n\n`website.create_structure()` will automatically create *templates/*, *static/* and all the files that are in it (.html, .js, .css).\n\n\n## Usage\n\nCheck the *[examples/](github.com/aquadzn/deploy-transformers/tree/master/examples)* folder.\n\n```python\n# Deployment\nfrom deploy_transformers import Website\n\nwebsite = Website(model_type=\"gpt2\", model_name=\"distilgpt2\")\n# website.create_folder(homepage_file=\"index.html\", template_folder='templates', static_folder='static')\nwebsite.deploy()\n```\n\n**You can change homepage filename, templates/ and static/ names in `website.deploy()` but it's better to keep them as default.**\n\n```python\n# Only text generation\nfrom deploy_transformers import ListModels, Model\n\n# ListModels() to show available models\nmodel = Model(\"gpt2\", \"distilgpt2\", seed=42, verbose=False)\nmodel.generate(length=20, prompt=\"The quick brown fox jumps over the lazy dog\")\n# If no prompt, input will be ask until exit\n```\n\n##\u00a0Thanks\n\n* [Transformers](https://github.com/huggingface/transformers) package by HuggingFace\n* [gpt-2-cloudrun](https://github.com/minimaxir/gpt-2-cloud-run) by minimaxir\n\n## Notes\n\n* Do the same but for other tasks like sentiment analysis, or Q&A.\n* Add Flask option?\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/aquadzn/deploy-transformers", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "deploy-transformers", "package_url": "https://pypi.org/project/deploy-transformers/", "platform": "", "project_url": "https://pypi.org/project/deploy-transformers/", "project_urls": {"Homepage": "https://github.com/aquadzn/deploy-transformers"}, "release_url": "https://pypi.org/project/deploy-transformers/0.1/", "requires_dist": ["starlette", "uvicorn", "jinja2", "aiofiles"], "requires_python": ">=3.5", "summary": "Easily deploy HuggingFace Transformers on a website", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>\ud83d\ude80 Deploy Transformers \ud83e\udd17</h1>\n<blockquote>\n<p>Deploy a SOTA model for text-generation in just three lines of code \ud83d\udcbb</p>\n</blockquote>\n<p><img alt=\"image\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7a1d4d021483ce47059eede08f38e182d4c0d44a/68747470733a2f2f73766773686172652e636f6d2f692f476f4e2e737667\"></p>\n<h2>Installation</h2>\n<p><a href=\"https://pytorch.org/get-started/locally/#start-locally\" rel=\"nofollow\"><strong>Pytorch</strong></a> and <a href=\"https://github.com/huggingface/transformers/#installation\" rel=\"nofollow\"><strong>Transformers</strong></a> are obviously needed.</p>\n<pre>pip install deploy-transformers\n</pre>\n<p><strong>For deployment, file structure needs to be like this:</strong></p>\n<pre>\u251c\u2500\u2500 static\n\u2502   \u251c\u2500\u2500 script.js\n\u2502   \u251c\u2500\u2500 style.css\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 <span class=\"m\">404</span>.html\n\u2502   \u251c\u2500\u2500 index.html\n<span class=\"p\">|</span>\n\u2514\u2500\u2500 your_file.py\n</pre>\n<p>You can either <strong>clone</strong> this repository to have original files or use the function <code>website.create_structure()</code> or <strong>create yourself</strong> the structure.</p>\n<p><code>website.create_structure()</code> will automatically create <em>templates/</em>, <em>static/</em> and all the files that are in it (.html, .js, .css).</p>\n<h2>Usage</h2>\n<p>Check the <em><a href=\"github.com/aquadzn/deploy-transformers/tree/master/examples\" rel=\"nofollow\">examples/</a></em> folder.</p>\n<pre><span class=\"c1\"># Deployment</span>\n<span class=\"kn\">from</span> <span class=\"nn\">deploy_transformers</span> <span class=\"kn\">import</span> <span class=\"n\">Website</span>\n\n<span class=\"n\">website</span> <span class=\"o\">=</span> <span class=\"n\">Website</span><span class=\"p\">(</span><span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"s2\">\"gpt2\"</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"s2\">\"distilgpt2\"</span><span class=\"p\">)</span>\n<span class=\"c1\"># website.create_folder(homepage_file=\"index.html\", template_folder='templates', static_folder='static')</span>\n<span class=\"n\">website</span><span class=\"o\">.</span><span class=\"n\">deploy</span><span class=\"p\">()</span>\n</pre>\n<p><strong>You can change homepage filename, templates/ and static/ names in <code>website.deploy()</code> but it's better to keep them as default.</strong></p>\n<pre><span class=\"c1\"># Only text generation</span>\n<span class=\"kn\">from</span> <span class=\"nn\">deploy_transformers</span> <span class=\"kn\">import</span> <span class=\"n\">ListModels</span><span class=\"p\">,</span> <span class=\"n\">Model</span>\n\n<span class=\"c1\"># ListModels() to show available models</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"s2\">\"gpt2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"distilgpt2\"</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">prompt</span><span class=\"o\">=</span><span class=\"s2\">\"The quick brown fox jumps over the lazy dog\"</span><span class=\"p\">)</span>\n<span class=\"c1\"># If no prompt, input will be ask until exit</span>\n</pre>\n<p>##\u00a0Thanks</p>\n<ul>\n<li><a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">Transformers</a> package by HuggingFace</li>\n<li><a href=\"https://github.com/minimaxir/gpt-2-cloud-run\" rel=\"nofollow\">gpt-2-cloudrun</a> by minimaxir</li>\n</ul>\n<h2>Notes</h2>\n<ul>\n<li>Do the same but for other tasks like sentiment analysis, or Q&amp;A.</li>\n<li>Add Flask option?</li>\n</ul>\n\n          </div>"}, "last_serial": 6308114, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "21fb02f0dc030082aac6485dad40bdd1", "sha256": "c5d0fe946be7cebeb46d622b6717af4ae171e326b0feb24d6c4570ab7005cf01"}, "downloads": -1, "filename": "deploy_transformers-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "21fb02f0dc030082aac6485dad40bdd1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 15765, "upload_time": "2019-12-15T23:55:12", "upload_time_iso_8601": "2019-12-15T23:55:12.087065Z", "url": "https://files.pythonhosted.org/packages/d8/27/7f06939c3c4663dd88a043565b50e896fbf2f622932a2199d5c99243d454/deploy_transformers-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c57d60d67bb2c60acc773cc56b913536", "sha256": "eced19978a4f4f3d6231dd6b016416f67cdba9404e0e8165f57c27433798b5f7"}, "downloads": -1, "filename": "deploy-transformers-0.1.tar.gz", "has_sig": false, "md5_digest": "c57d60d67bb2c60acc773cc56b913536", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 11975, "upload_time": "2019-12-15T23:55:14", "upload_time_iso_8601": "2019-12-15T23:55:14.955966Z", "url": "https://files.pythonhosted.org/packages/45/88/d12c553f0910660974764d2d2ecdaf5ad28d50ced472bdce1611d0796294/deploy-transformers-0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "21fb02f0dc030082aac6485dad40bdd1", "sha256": "c5d0fe946be7cebeb46d622b6717af4ae171e326b0feb24d6c4570ab7005cf01"}, "downloads": -1, "filename": "deploy_transformers-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "21fb02f0dc030082aac6485dad40bdd1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 15765, "upload_time": "2019-12-15T23:55:12", "upload_time_iso_8601": "2019-12-15T23:55:12.087065Z", "url": "https://files.pythonhosted.org/packages/d8/27/7f06939c3c4663dd88a043565b50e896fbf2f622932a2199d5c99243d454/deploy_transformers-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c57d60d67bb2c60acc773cc56b913536", "sha256": "eced19978a4f4f3d6231dd6b016416f67cdba9404e0e8165f57c27433798b5f7"}, "downloads": -1, "filename": "deploy-transformers-0.1.tar.gz", "has_sig": false, "md5_digest": "c57d60d67bb2c60acc773cc56b913536", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 11975, "upload_time": "2019-12-15T23:55:14", "upload_time_iso_8601": "2019-12-15T23:55:14.955966Z", "url": "https://files.pythonhosted.org/packages/45/88/d12c553f0910660974764d2d2ecdaf5ad28d50ced472bdce1611d0796294/deploy-transformers-0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:39:02 2020"}