{"info": {"author": "Roger Fernandez Guri", "author_email": "rfguri@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "Perceptron\n==========\n\n|PyPI version| |Build Status|\n\nPerceptron implements a *multilayer perceptron* network written in Python.\nThis type of network consists of multiple layers of neurons, the first\nof which takes the input. The last layer gives the ouput. There can be\nmultiple middle layers but in this case, it just uses a single one.\n\n    For further information about multilayer perceptron networks, please\n    read `this <https://en.wikipedia.org/wiki/Multilayer_perceptron>`__\n    entry on the Wikipedia.\n\nRequirements\n------------\n\n-  `python <https://www.python.org/>`__ >= 2.7\n\nInstallation\n------------\n\nYou can install the package via ``easy_install`` or ``pip``:\n\n.. code:: bash\n\n    easy_install perceptron\n    pip install perceptron\n\nFeeding Forward\n---------------\n\nThe neural network uses the *hyperbolic tangent (tanh)* function.\n\n.. figure:: http://mathworld.wolfram.com/images/interactive/TanhReal.gif\n   :alt: Hyperbolic tangent\n\n   Hyperbolic tangent\n\nThe x-axis is the total input to the node. As the input aproaches to 0,\nthe output starts to climb quickly. With an input of 2, the output is\nalmos at 1 and doesn't get much higher. This is a type of *sigmoid*\nfunctions to calculate the output of the neurons.\n\n    Note: Before runing the ``feedforward`` algorithm, the network will\n    have to query the nodes and connections, and build, in memory, the \n    position of the network that is relevant to a specific input.\n\nTraining with Backpropagation\n-----------------------------\n\nThe backpropagation algorithm then performs the following steps.\n\nFor each node in the output layer:\n\n1. Calculate the difference between the node's current output and what it should be.\n2. Use the ``dtanh`` function to determine how much the node's total input has to change.\n3. Change the strenght of every inconming input in proportion to the input's current strength and the learning rate.\n\nFor each node in the hidden layer:\n\n1. Change the output of the node by the sum of the strength of each output value multiplied by how much its targets has to change.\n2. Use the ``dtanh`` function to determine how much the node's total input has to change.\n3. Change the strenght of every inconming input in proportion to the input's current strength and the learning rate.\n\nThe implementation of this algorithm actually calculates all the errors\nin advance and then adjusts the weigths, because all the calculations\nrely on knowing the current weights rather than the updated weights.\n\n    Note: Before runing ``backpropagation`` method, it's necessary to\n    run ``feedforward`` so that the current output of every node will be\n    stored in the instance variables.\n\nUsage\n-----\n\nImport the module at the beginning of your file:\n\n.. code:: python\n\n    from perceptron import mlp\n\nInit the ``neural network``:\n\n.. code:: python\n\n    n = mlp.Net()\n\nExample\n-------\n\nIn this example the neurons in the first layer respont to the ids that\nare used as input. If a id is present, then the neurons that are strongly\nconnected to that word become active. The second layer is fed by the\nfirst layer, so it responds to combinations of ids. Finally, the neurons\nfeed their result to the outputs, and particular combinations may be\nstrongly or weakly associated with the possible results. In the end,\nthe final decision is whichever output is strongest classifying an id.\n\n.. code:: python\n\n    from perceptron import mlp\n\n    def main():\n      n = mlp.Net()\n\n      for i in range(30):\n        n.train([101,103],[201,202,203],201)\n        n.train([102,103],[201,202,203],202)\n        n.train([101],[201,202,203],203)\n\n      print n.eval([101,103,],[201,202,203])\n      print n.eval([102,103],[201,202,203])\n      print n.eval([103],[201,202,203])\n\n    if  __name__=='__main__': main()\n\nThat will give the following output.\n\n.. code::\n\n    [0.8435967735300776, 0.011059223531796199, 0.017992770688108367]\n    [-0.028282207517584094, 0.8775955174169334, 0.0032322039490162353]\n    [0.8459277961565395, -0.011590385221469553, -0.8361964445052618]\n\nLicence\n-------\n\nCopyright \u00a9 2016 Roger Fernandez Guri. It is free software, and may be\nredistributed under the terms specified in the\n`LICENCE <https://github.com/rfguri/perceptron/blob/master/LICENSE>`__ file.\n\n.. |PyPI version| image:: https://badge.fury.io/py/perceptron.svg\n   :target: http://badge.fury.io/py/perceptron\n.. |Build Status| image:: https://travis-ci.org/rfguri/perceptron.svg\n   :target: https://travis-ci.org/rfguri/perceptron", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/rfguri/perceptron/", "keywords": "ml machine learning neuralnetworks mlp python nn", "license": "The MIT License (MIT)\n\nCopyright (c) 2016 Roger Fernandez Guri\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.", "maintainer": null, "maintainer_email": null, "name": "perceptron", "package_url": "https://pypi.org/project/perceptron/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/perceptron/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/rfguri/perceptron/"}, "release_url": "https://pypi.org/project/perceptron/1.1.0/", "requires_dist": null, "requires_python": null, "summary": "Multilayer perceptron network implementation in Python", "version": "1.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"http://badge.fury.io/py/perceptron\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0aba85c0f35f6f6e96031d23a3406104d04ba1f7/68747470733a2f2f62616467652e667572792e696f2f70792f70657263657074726f6e2e737667\"></a> <a href=\"https://travis-ci.org/rfguri/perceptron\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6ac4e085faf1d03732e6d3ae175cbc440cc0ecd3/68747470733a2f2f7472617669732d63692e6f72672f7266677572692f70657263657074726f6e2e737667\"></a></p>\n<p>Perceptron implements a <em>multilayer perceptron</em> network written in Python.\nThis type of network consists of multiple layers of neurons, the first\nof which takes the input. The last layer gives the ouput. There can be\nmultiple middle layers but in this case, it just uses a single one.</p>\n<blockquote>\nFor further information about multilayer perceptron networks, please\nread <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\" rel=\"nofollow\">this</a>\nentry on the Wikipedia.</blockquote>\n<div id=\"requirements\">\n<h2>Requirements</h2>\n<ul>\n<li><a href=\"https://www.python.org/\" rel=\"nofollow\">python</a> &gt;= 2.7</li>\n</ul>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>You can install the package via <tt>easy_install</tt> or <tt>pip</tt>:</p>\n<pre>easy_install perceptron\npip install perceptron\n</pre>\n</div>\n<div id=\"feeding-forward\">\n<h2>Feeding Forward</h2>\n<p>The neural network uses the <em>hyperbolic tangent (tanh)</em> function.</p>\n<div>\n<img alt=\"Hyperbolic tangent\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/726acf9bdb6975232cf9f97b6753ce40be8cb6f1/687474703a2f2f6d617468776f726c642e776f6c6672616d2e636f6d2f696d616765732f696e7465726163746976652f54616e685265616c2e676966\">\n<p>Hyperbolic tangent</p>\n</div>\n<p>The x-axis is the total input to the node. As the input aproaches to 0,\nthe output starts to climb quickly. With an input of 2, the output is\nalmos at 1 and doesn\u2019t get much higher. This is a type of <em>sigmoid</em>\nfunctions to calculate the output of the neurons.</p>\n<blockquote>\nNote: Before runing the <tt>feedforward</tt> algorithm, the network will\nhave to query the nodes and connections, and build, in memory, the\nposition of the network that is relevant to a specific input.</blockquote>\n</div>\n<div id=\"training-with-backpropagation\">\n<h2>Training with Backpropagation</h2>\n<p>The backpropagation algorithm then performs the following steps.</p>\n<p>For each node in the output layer:</p>\n<ol>\n<li>Calculate the difference between the node\u2019s current output and what it should be.</li>\n<li>Use the <tt>dtanh</tt> function to determine how much the node\u2019s total input has to change.</li>\n<li>Change the strenght of every inconming input in proportion to the input\u2019s current strength and the learning rate.</li>\n</ol>\n<p>For each node in the hidden layer:</p>\n<ol>\n<li>Change the output of the node by the sum of the strength of each output value multiplied by how much its targets has to change.</li>\n<li>Use the <tt>dtanh</tt> function to determine how much the node\u2019s total input has to change.</li>\n<li>Change the strenght of every inconming input in proportion to the input\u2019s current strength and the learning rate.</li>\n</ol>\n<p>The implementation of this algorithm actually calculates all the errors\nin advance and then adjusts the weigths, because all the calculations\nrely on knowing the current weights rather than the updated weights.</p>\n<blockquote>\nNote: Before runing <tt>backpropagation</tt> method, it\u2019s necessary to\nrun <tt>feedforward</tt> so that the current output of every node will be\nstored in the instance variables.</blockquote>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>Import the module at the beginning of your file:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">perceptron</span> <span class=\"kn\">import</span> <span class=\"n\">mlp</span>\n</pre>\n<p>Init the <tt>neural network</tt>:</p>\n<pre><span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"n\">mlp</span><span class=\"o\">.</span><span class=\"n\">Net</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"example\">\n<h2>Example</h2>\n<p>In this example the neurons in the first layer respont to the ids that\nare used as input. If a id is present, then the neurons that are strongly\nconnected to that word become active. The second layer is fed by the\nfirst layer, so it responds to combinations of ids. Finally, the neurons\nfeed their result to the outputs, and particular combinations may be\nstrongly or weakly associated with the possible results. In the end,\nthe final decision is whichever output is strongest classifying an id.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">perceptron</span> <span class=\"kn\">import</span> <span class=\"n\">mlp</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n  <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"n\">mlp</span><span class=\"o\">.</span><span class=\"n\">Net</span><span class=\"p\">()</span>\n\n  <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">30</span><span class=\"p\">):</span>\n    <span class=\"n\">n</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">([</span><span class=\"mi\">101</span><span class=\"p\">,</span><span class=\"mi\">103</span><span class=\"p\">],[</span><span class=\"mi\">201</span><span class=\"p\">,</span><span class=\"mi\">202</span><span class=\"p\">,</span><span class=\"mi\">203</span><span class=\"p\">],</span><span class=\"mi\">201</span><span class=\"p\">)</span>\n    <span class=\"n\">n</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">([</span><span class=\"mi\">102</span><span class=\"p\">,</span><span class=\"mi\">103</span><span class=\"p\">],[</span><span class=\"mi\">201</span><span class=\"p\">,</span><span class=\"mi\">202</span><span class=\"p\">,</span><span class=\"mi\">203</span><span class=\"p\">],</span><span class=\"mi\">202</span><span class=\"p\">)</span>\n    <span class=\"n\">n</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">([</span><span class=\"mi\">101</span><span class=\"p\">],[</span><span class=\"mi\">201</span><span class=\"p\">,</span><span class=\"mi\">202</span><span class=\"p\">,</span><span class=\"mi\">203</span><span class=\"p\">],</span><span class=\"mi\">203</span><span class=\"p\">)</span>\n\n  <span class=\"nb\">print</span> <span class=\"n\">n</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">([</span><span class=\"mi\">101</span><span class=\"p\">,</span><span class=\"mi\">103</span><span class=\"p\">,],[</span><span class=\"mi\">201</span><span class=\"p\">,</span><span class=\"mi\">202</span><span class=\"p\">,</span><span class=\"mi\">203</span><span class=\"p\">])</span>\n  <span class=\"nb\">print</span> <span class=\"n\">n</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">([</span><span class=\"mi\">102</span><span class=\"p\">,</span><span class=\"mi\">103</span><span class=\"p\">],[</span><span class=\"mi\">201</span><span class=\"p\">,</span><span class=\"mi\">202</span><span class=\"p\">,</span><span class=\"mi\">203</span><span class=\"p\">])</span>\n  <span class=\"nb\">print</span> <span class=\"n\">n</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">([</span><span class=\"mi\">103</span><span class=\"p\">],[</span><span class=\"mi\">201</span><span class=\"p\">,</span><span class=\"mi\">202</span><span class=\"p\">,</span><span class=\"mi\">203</span><span class=\"p\">])</span>\n\n<span class=\"k\">if</span>  <span class=\"vm\">__name__</span><span class=\"o\">==</span><span class=\"s1\">'__main__'</span><span class=\"p\">:</span> <span class=\"n\">main</span><span class=\"p\">()</span>\n</pre>\n<p>That will give the following output.</p>\n<pre>[0.8435967735300776, 0.011059223531796199, 0.017992770688108367]\n[-0.028282207517584094, 0.8775955174169334, 0.0032322039490162353]\n[0.8459277961565395, -0.011590385221469553, -0.8361964445052618]\n</pre>\n</div>\n<div id=\"licence\">\n<h2>Licence</h2>\n<p>Copyright \u00a9 2016 Roger Fernandez Guri. It is free software, and may be\nredistributed under the terms specified in the\n<a href=\"https://github.com/rfguri/perceptron/blob/master/LICENSE\" rel=\"nofollow\">LICENCE</a> file.</p>\n</div>\n\n          </div>"}, "last_serial": 2089589, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "b7e0f6d26409b1f80be3984ee5d924ed", "sha256": "68bd48a6ecee858e3a2b0cb4de9549cace661e8b90fdc8498d487d3a5ba22aec"}, "downloads": -1, "filename": "perceptron-1.0.0-py2-none-any.whl", "has_sig": false, "md5_digest": "b7e0f6d26409b1f80be3984ee5d924ed", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 10179, "upload_time": "2016-04-28T17:38:37", "upload_time_iso_8601": "2016-04-28T17:38:37.506237Z", "url": "https://files.pythonhosted.org/packages/00/72/548317b0630e309dce20442a1acbe96de31d8c39455e12496560bc3d61e1/perceptron-1.0.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "876e29ca4c309eacdd1b72446287f1d5", "sha256": "35825e19b24dc2d35d4f66785d87eb753098ac72fe3564aefc096ed013c50060"}, "downloads": -1, "filename": "perceptron-1.0.0.tar.gz", "has_sig": false, "md5_digest": "876e29ca4c309eacdd1b72446287f1d5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6488, "upload_time": "2016-04-28T17:38:30", "upload_time_iso_8601": "2016-04-28T17:38:30.928327Z", "url": "https://files.pythonhosted.org/packages/ff/7c/efe2827629cc4f53f3fe179421b50de7e5e23321f1485333ceedc82d2aef/perceptron-1.0.0.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "8f565ec8669d531df742e5a2c762ad88", "sha256": "287f467383a9cd90917a13ce3a16b6d5cf75943ae0f555db4c0072fbc2c4ca87"}, "downloads": -1, "filename": "perceptron-1.1.0-py2-none-any.whl", "has_sig": false, "md5_digest": "8f565ec8669d531df742e5a2c762ad88", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 10272, "upload_time": "2016-04-28T21:47:51", "upload_time_iso_8601": "2016-04-28T21:47:51.767736Z", "url": "https://files.pythonhosted.org/packages/f4/22/1e8e4d250d95de41422a1c01da3b911e15b4032a11d4677e194d3f6e992b/perceptron-1.1.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5f45bb029e5adc022d937c11e4df1222", "sha256": "b2ac4704da9c8d9e026c87aa4b8908ffaf174d98fe8bce3548bfb79ffc868635"}, "downloads": -1, "filename": "perceptron-1.1.0.tar.gz", "has_sig": false, "md5_digest": "5f45bb029e5adc022d937c11e4df1222", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6467, "upload_time": "2016-04-28T21:47:41", "upload_time_iso_8601": "2016-04-28T21:47:41.118885Z", "url": "https://files.pythonhosted.org/packages/7e/8f/8b5f865313b21a0c0c2397df8261a0d5ef5b333c8d3f7151f71294fedd96/perceptron-1.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8f565ec8669d531df742e5a2c762ad88", "sha256": "287f467383a9cd90917a13ce3a16b6d5cf75943ae0f555db4c0072fbc2c4ca87"}, "downloads": -1, "filename": "perceptron-1.1.0-py2-none-any.whl", "has_sig": false, "md5_digest": "8f565ec8669d531df742e5a2c762ad88", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 10272, "upload_time": "2016-04-28T21:47:51", "upload_time_iso_8601": "2016-04-28T21:47:51.767736Z", "url": "https://files.pythonhosted.org/packages/f4/22/1e8e4d250d95de41422a1c01da3b911e15b4032a11d4677e194d3f6e992b/perceptron-1.1.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5f45bb029e5adc022d937c11e4df1222", "sha256": "b2ac4704da9c8d9e026c87aa4b8908ffaf174d98fe8bce3548bfb79ffc868635"}, "downloads": -1, "filename": "perceptron-1.1.0.tar.gz", "has_sig": false, "md5_digest": "5f45bb029e5adc022d937c11e4df1222", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6467, "upload_time": "2016-04-28T21:47:41", "upload_time_iso_8601": "2016-04-28T21:47:41.118885Z", "url": "https://files.pythonhosted.org/packages/7e/8f/8b5f865313b21a0c0c2397df8261a0d5ef5b333c8d3f7151f71294fedd96/perceptron-1.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:22 2020"}