{"info": {"author": "Elias Kassell", "author_email": "eliaskassell@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Visualization"], "description": "![Latent Dimensionality Reduction Header](https://raw.githubusercontent.com/Ekrekr/ldr/master/docs/images/header.png)\n\n[![Status](https://img.shields.io/badge/status-active-success.svg)](eliaskassell.com)\n[![GitHub Issues](https://img.shields.io/github/issues/ekrekr/ldr.svg)](https://github.com/ekrekr/ldr/issues)\n[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/ekrekr/ldr.svg)](https://github.com/ekrekr/ldr/pulls)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n\nLDR stands for **Latent Dimensionality Reduction**. It is a generic method for interpreting models and making accurate predictions when features are missing. It is deployed [here](https://pypi.org/project/ldr/) as a python module.\n\n## About\n\nThe purpose of LDR is to solve a common data science problem. Often models that have a higher predictive accuracy are more complex. These complex models are sensibly referred to as **black box models**. This is frustrating for many data scientists, as they end up with a model that performs well, but **can't explain why**. Inability to explain the model frequently causes the model to **fail in critical situations**, that are difficult to test for. Also, black box models typically require all features to be present to make a prediction. LDR provides a solution to both of these problems.\n\n## Getting Started\n\n### Prerequesites\n\n[Python3](https://www.python.org/download/releases/3.0/).\n\n_Note: Some examples contain additional that are not installed as dependencies by default, such as [PyTorch](https://pytorch.org)._\n\n### Installation\n\n```console\npython3 -m pip install ldr --user\n```\n\n### Examples\n\n<!-- An example analysis of a simple generated distribution can be found [here](examples/distribution_example.ipynb). -->\n\nAn example analysis of a classification problem can be found [here](https://raw.githubusercontent.com/Ekrekr/ldr/master/examples/classification.py).\n\n<!-- An example analysis of a classification problem, using a neural network, can be found [here](examples/classification_example.ipynb). -->\n\n<!-- An example analysis of a regression problem can be found [here](examples/regression_example.ipynb). -->\n\n### Imputation from density\n\nAfter calculating the density estimate of the trained model using `density_estimate()`, using `predict()` on new data will automatically impute missing features.\n\n_Note: currently only missing columns will be imputed, not the odd value here or there._\n\n_Note: This has not been fully implemented, so the optimal accuracy has not been achieved, and it's a bit slow. [Contribution welcome](https://github.com/Ekrekr/ldr/issues) :)._\n\n### Visualizations\n\nThe examples above generate either 1D or 2D visualizations. These demonstrate the certainty of the model across the sample space it is trained on. Take the 1D visualizations from the classification example, which describe the **model's certainty of malignancy classification of mean area and area error of breast cancer** samples in a [study](<https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)>).\n\n#### 1 Dimensional Visualization\n\n![1D visualization example](https://raw.githubusercontent.com/Ekrekr/ldr/master/tests/output/breast_cancer_1d.png)\n\nHere what can be seen is that if a cancer has a **mean area of less than 250** then the model is most likely to **classify it as benign**, whereas if it has a **mean area of more than 1200** then it is almost certainly going to be **classified as malignant**.\n\nWith the area error, what can be seen is that **between 280 and 480**, there is **not enough training data** for the model to reliably make a prediction in that area, as the total model certainty drops to 0 for both classifications. **Between 80 and 210** the model is very biased towards a **malignant classification**.\n\n#### 2 Dimensional Visualization\n\n![2D visualization example](https://raw.githubusercontent.com/Ekrekr/ldr/master/tests/output/breast_cancer_2d.png)\n\nInterpolating two classification certainties of individual features results in the heatmap in the bottom right. What can be seen is that the majority of samples have a low mean area and a low area error. When samples **deviate away from the bottom left**, with increasing mean area and area errors, the **probability of a malignancy classification dramatically increases**.\n\n#### n Dimensional Visualizations\n\nEven though only 1 and 2 dimensional visualizations are given, the entire classifier certainty across the sample space has been mapped. In this study there are actualy **30 features**, but by using LDR the individual components can be easily extracted. In order for a human to interpret an algorithmic model, they must be able to see it. As [Donald Knuth](https://www-cs-faculty.stanford.edu/~knuth/) says:\n\n> \"An algorithm must be seen to be believed, and the best way to learn what an algorithm is all about is to try it.\"\n\n_Note: most successful black box models come from algorithmic techniques, such as neural networks; this is why I draw this similarity._\n\n## Why LDR is Better Than Naive Techniques\n\nNaive techniques for analysis black box models consist of analysing individual features (or composite functions of features) directly against their classifications resulting from each sample in the sample space. Because of this, the extrapolation that happens when applying the model to real world situations is never experimented against. Because LDR draws samples randomly from a KDE (kernel density estimate) of the sample space, by default there is extrapolation. Because of the KDE of the model, features can be imputed during prediction by taking the typical value of variables at the highest density, giving a more accurate representation of the feature space.\n\n## The LDR Recipe\n\nAs machine learning algorithms go, LDR is pretty simple. It's effectively just a [monte-carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration) across the sample space, using the [VEGAS algorithm](https://en.wikipedia.org/wiki/VEGAS_algorithm) as the sampling method.\n\n1. Min-max scale the data so that it falls into [0, 1] intervals.\n\n1. Train a predictive model on the scaled data.\n\n1. Create a kernel density estimate of the training samples.\n\n1. Sample n new points from the kernel density estimate, using the predictive model to make a prediction at each point.\n\n1. Bin the samples according to regular intervals. For each dimension, group points with resolution r, reducing the value of the bin to the mean prediction across it.\n\nSee the [paper](https://raw.githubusercontent.com/Ekrekr/ldr/master/docs/paper/paper.pdf) for more detail.\n\n## Additional Notes\n\nIf you find this package useful, please consider [contributing](contributing.md)!", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ekrekr/pyldr", "keywords": "visualization", "license": "", "maintainer": "", "maintainer_email": "", "name": "ldr", "package_url": "https://pypi.org/project/ldr/", "platform": "", "project_url": "https://pypi.org/project/ldr/", "project_urls": {"Homepage": "https://github.com/ekrekr/pyldr"}, "release_url": "https://pypi.org/project/ldr/0.6/", "requires_dist": null, "requires_python": ">3.5", "summary": "Latent Dimensionality Reduction in Python", "version": "0.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"Latent Dimensionality Reduction Header\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ab69b8c8e26159da483d4e919b677037595cd988/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f456b72656b722f6c64722f6d61737465722f646f63732f696d616765732f6865616465722e706e67\"></p>\n<p><a href=\"eliaskassell.com\" rel=\"nofollow\"><img alt=\"Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3df82fcca92ffecd57766052c224210c0f7e8a9c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374617475732d6163746976652d737563636573732e737667\"></a>\n<a href=\"https://github.com/ekrekr/ldr/issues\" rel=\"nofollow\"><img alt=\"GitHub Issues\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4a800e8508f20fad5ff09d956d25c205fb18a988/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f656b72656b722f6c64722e737667\"></a>\n<a href=\"https://github.com/ekrekr/ldr/pulls\" rel=\"nofollow\"><img alt=\"GitHub Pull Requests\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b79b53b2a55af6714246391e284eb12dc45c8329/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f656b72656b722f6c64722e737667\"></a>\n<a href=\"https://www.gnu.org/licenses/gpl-3.0\" rel=\"nofollow\"><img alt=\"License: GPL v3\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8de17537dd1659a5a076ce547de301e27c839e67/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667\"></a></p>\n<p>LDR stands for <strong>Latent Dimensionality Reduction</strong>. It is a generic method for interpreting models and making accurate predictions when features are missing. It is deployed <a href=\"https://pypi.org/project/ldr/\" rel=\"nofollow\">here</a> as a python module.</p>\n<h2>About</h2>\n<p>The purpose of LDR is to solve a common data science problem. Often models that have a higher predictive accuracy are more complex. These complex models are sensibly referred to as <strong>black box models</strong>. This is frustrating for many data scientists, as they end up with a model that performs well, but <strong>can't explain why</strong>. Inability to explain the model frequently causes the model to <strong>fail in critical situations</strong>, that are difficult to test for. Also, black box models typically require all features to be present to make a prediction. LDR provides a solution to both of these problems.</p>\n<h2>Getting Started</h2>\n<h3>Prerequesites</h3>\n<p><a href=\"https://www.python.org/download/releases/3.0/\" rel=\"nofollow\">Python3</a>.</p>\n<p><em>Note: Some examples contain additional that are not installed as dependencies by default, such as <a href=\"https://pytorch.org\" rel=\"nofollow\">PyTorch</a>.</em></p>\n<h3>Installation</h3>\n<pre><span class=\"go\">python3 -m pip install ldr --user</span>\n</pre>\n<h3>Examples</h3>\n\n<p>An example analysis of a classification problem can be found <a href=\"https://raw.githubusercontent.com/Ekrekr/ldr/master/examples/classification.py\" rel=\"nofollow\">here</a>.</p>\n\n\n<h3>Imputation from density</h3>\n<p>After calculating the density estimate of the trained model using <code>density_estimate()</code>, using <code>predict()</code> on new data will automatically impute missing features.</p>\n<p><em>Note: currently only missing columns will be imputed, not the odd value here or there.</em></p>\n<p><em>Note: This has not been fully implemented, so the optimal accuracy has not been achieved, and it's a bit slow. <a href=\"https://github.com/Ekrekr/ldr/issues\" rel=\"nofollow\">Contribution welcome</a> :).</em></p>\n<h3>Visualizations</h3>\n<p>The examples above generate either 1D or 2D visualizations. These demonstrate the certainty of the model across the sample space it is trained on. Take the 1D visualizations from the classification example, which describe the <strong>model's certainty of malignancy classification of mean area and area error of breast cancer</strong> samples in a <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\" rel=\"nofollow\">study</a>.</p>\n<h4>1 Dimensional Visualization</h4>\n<p><img alt=\"1D visualization example\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fe24b3d0e40806a929119c9e91d6aeec4fd95768/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f456b72656b722f6c64722f6d61737465722f74657374732f6f75747075742f6272656173745f63616e6365725f31642e706e67\"></p>\n<p>Here what can be seen is that if a cancer has a <strong>mean area of less than 250</strong> then the model is most likely to <strong>classify it as benign</strong>, whereas if it has a <strong>mean area of more than 1200</strong> then it is almost certainly going to be <strong>classified as malignant</strong>.</p>\n<p>With the area error, what can be seen is that <strong>between 280 and 480</strong>, there is <strong>not enough training data</strong> for the model to reliably make a prediction in that area, as the total model certainty drops to 0 for both classifications. <strong>Between 80 and 210</strong> the model is very biased towards a <strong>malignant classification</strong>.</p>\n<h4>2 Dimensional Visualization</h4>\n<p><img alt=\"2D visualization example\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bda94254a3e848a28a94965f4dffbcd08ae18d7d/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f456b72656b722f6c64722f6d61737465722f74657374732f6f75747075742f6272656173745f63616e6365725f32642e706e67\"></p>\n<p>Interpolating two classification certainties of individual features results in the heatmap in the bottom right. What can be seen is that the majority of samples have a low mean area and a low area error. When samples <strong>deviate away from the bottom left</strong>, with increasing mean area and area errors, the <strong>probability of a malignancy classification dramatically increases</strong>.</p>\n<h4>n Dimensional Visualizations</h4>\n<p>Even though only 1 and 2 dimensional visualizations are given, the entire classifier certainty across the sample space has been mapped. In this study there are actualy <strong>30 features</strong>, but by using LDR the individual components can be easily extracted. In order for a human to interpret an algorithmic model, they must be able to see it. As <a href=\"https://www-cs-faculty.stanford.edu/%7Eknuth/\" rel=\"nofollow\">Donald Knuth</a> says:</p>\n<blockquote>\n<p>\"An algorithm must be seen to be believed, and the best way to learn what an algorithm is all about is to try it.\"</p>\n</blockquote>\n<p><em>Note: most successful black box models come from algorithmic techniques, such as neural networks; this is why I draw this similarity.</em></p>\n<h2>Why LDR is Better Than Naive Techniques</h2>\n<p>Naive techniques for analysis black box models consist of analysing individual features (or composite functions of features) directly against their classifications resulting from each sample in the sample space. Because of this, the extrapolation that happens when applying the model to real world situations is never experimented against. Because LDR draws samples randomly from a KDE (kernel density estimate) of the sample space, by default there is extrapolation. Because of the KDE of the model, features can be imputed during prediction by taking the typical value of variables at the highest density, giving a more accurate representation of the feature space.</p>\n<h2>The LDR Recipe</h2>\n<p>As machine learning algorithms go, LDR is pretty simple. It's effectively just a <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_integration\" rel=\"nofollow\">monte-carlo integration</a> across the sample space, using the <a href=\"https://en.wikipedia.org/wiki/VEGAS_algorithm\" rel=\"nofollow\">VEGAS algorithm</a> as the sampling method.</p>\n<ol>\n<li>\n<p>Min-max scale the data so that it falls into [0, 1] intervals.</p>\n</li>\n<li>\n<p>Train a predictive model on the scaled data.</p>\n</li>\n<li>\n<p>Create a kernel density estimate of the training samples.</p>\n</li>\n<li>\n<p>Sample n new points from the kernel density estimate, using the predictive model to make a prediction at each point.</p>\n</li>\n<li>\n<p>Bin the samples according to regular intervals. For each dimension, group points with resolution r, reducing the value of the bin to the mean prediction across it.</p>\n</li>\n</ol>\n<p>See the <a href=\"https://raw.githubusercontent.com/Ekrekr/ldr/master/docs/paper/paper.pdf\" rel=\"nofollow\">paper</a> for more detail.</p>\n<h2>Additional Notes</h2>\n<p>If you find this package useful, please consider <a href=\"contributing.md\" rel=\"nofollow\">contributing</a>!</p>\n\n          </div>"}, "last_serial": 6441573, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "a65efd283192dc260119659a601dc1fa", "sha256": "5e887fd601818152e1768104d59e5c4be85747b703c957339f9df0f594391da8"}, "downloads": -1, "filename": "ldr-0.1.tar.gz", "has_sig": false, "md5_digest": "a65efd283192dc260119659a601dc1fa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2242, "upload_time": "2019-12-15T13:03:55", "upload_time_iso_8601": "2019-12-15T13:03:55.801687Z", "url": "https://files.pythonhosted.org/packages/bb/49/96c2d695f31cb4899534c65f6c776ffcf60c2db19cf4b8325c2e494ba67d/ldr-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "9a24b22693eb396033e48b7ad1b7c755", "sha256": "f1ab1480ef9a82e172deabf6dae9aa7e15117fbc7e3df24519fdda937ff2ba89"}, "downloads": -1, "filename": "ldr-0.2.tar.gz", "has_sig": false, "md5_digest": "9a24b22693eb396033e48b7ad1b7c755", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5, <4", "size": 2562, "upload_time": "2019-12-15T18:04:06", "upload_time_iso_8601": "2019-12-15T18:04:06.519855Z", "url": "https://files.pythonhosted.org/packages/53/40/40eacac5a0450e97b665b488be800e5dc56a4c43d1e29ec31f17fc201c6e/ldr-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "4e9359b1806b71ac737ff77b07851977", "sha256": "674ad8553a5917ade76435d8f37656b62aa6e494d8ca9ad6d337441181a4d78e"}, "downloads": -1, "filename": "ldr-0.3.tar.gz", "has_sig": false, "md5_digest": "4e9359b1806b71ac737ff77b07851977", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 10524, "upload_time": "2019-12-15T19:57:56", "upload_time_iso_8601": "2019-12-15T19:57:56.422625Z", "url": "https://files.pythonhosted.org/packages/1f/47/b899c1af72cd88e50fada5380c968dbebc7264075d1266067732403f8a3b/ldr-0.3.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "35d3457cf6b1a7b64c1f0e39135681ec", "sha256": "c2df08c8fa0328bc763f0448ce7dea06c11e642c1ab19eb676a51cb9f4ed52d6"}, "downloads": -1, "filename": "ldr-0.4.tar.gz", "has_sig": false, "md5_digest": "35d3457cf6b1a7b64c1f0e39135681ec", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 16507, "upload_time": "2019-12-22T13:41:43", "upload_time_iso_8601": "2019-12-22T13:41:43.086816Z", "url": "https://files.pythonhosted.org/packages/dc/22/7d6fb65be69e1710cc7bd6525eeba266dbb7e17ec88582a3bf6268a5e6b8/ldr-0.4.tar.gz", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "66dae9a9007d74f7b311f40e702398a4", "sha256": "4e4f74b19d9fadd407ff62ee13b5b1acf59d53f9cccea44eba265c56a40b5757"}, "downloads": -1, "filename": "ldr-0.5.tar.gz", "has_sig": false, "md5_digest": "66dae9a9007d74f7b311f40e702398a4", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 16485, "upload_time": "2019-12-22T14:02:43", "upload_time_iso_8601": "2019-12-22T14:02:43.942427Z", "url": "https://files.pythonhosted.org/packages/9f/45/4856ecc45a53a471fd39f97110477a550ec512669baaf1308358c6bf814b/ldr-0.5.tar.gz", "yanked": false}], "0.6": [{"comment_text": "", "digests": {"md5": "e6758e7a316e553985e79727ab7f091e", "sha256": "8effbb1c862b0bee9afd2324ab718dc3686e1b6ab5cca15cc71f075d75ea804c"}, "downloads": -1, "filename": "ldr-0.6.tar.gz", "has_sig": false, "md5_digest": "e6758e7a316e553985e79727ab7f091e", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 17952, "upload_time": "2020-01-12T23:22:16", "upload_time_iso_8601": "2020-01-12T23:22:16.020372Z", "url": "https://files.pythonhosted.org/packages/eb/63/b0ea9fb21157bf57008d1c8eaa4017f0e20e5d7663140ab7240e60d1f4d6/ldr-0.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e6758e7a316e553985e79727ab7f091e", "sha256": "8effbb1c862b0bee9afd2324ab718dc3686e1b6ab5cca15cc71f075d75ea804c"}, "downloads": -1, "filename": "ldr-0.6.tar.gz", "has_sig": false, "md5_digest": "e6758e7a316e553985e79727ab7f091e", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 17952, "upload_time": "2020-01-12T23:22:16", "upload_time_iso_8601": "2020-01-12T23:22:16.020372Z", "url": "https://files.pythonhosted.org/packages/eb/63/b0ea9fb21157bf57008d1c8eaa4017f0e20e5d7663140ab7240e60d1f4d6/ldr-0.6.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:23 2020"}