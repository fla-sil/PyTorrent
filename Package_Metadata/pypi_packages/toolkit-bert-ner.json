{"info": {"author": "Allen WU", "author_email": "allen.wu18621039969@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# toolkit-bert-ner\nBase Google pre-training model(BERT), then add BiLSTM layer and crf layer, train a Chinese named entity recognition model.\n\n## Download project and install  \nYou can install this project by:  \n```\npip install -i https://test.pypi.org/simple/ toolkit-bert-ner==1.0.0\n```\n\nOR\n```\ngit clone http://git.huimeimt.net:8008/ds/toolkit-bert-ner.git\ncd toolkit-bert-ner/\npython3 setup.py install\n```\n\nif you do not want to install, you just need clone this project and reference the file of <run.py> to train the model or start the service.   \n\n## Train model:\nYou can use -help to view the relevant parameters of the training named entity recognition model, where data_dir, bert_config_file, output_dir, init_checkpoint, vocab_file must be specified.\n```angular2html\ntoolkit-bert-ner-train -help\n```\n\ntrain/dev/test dataset is like this:\n```\n\u6d77 O\n\u9493 O\n\u6bd4 O\n\u8d5b O\n\u5730 O\n\u70b9 O\n\u5728 O\n\u53a6 B-LOC\n\u95e8 I-LOC\n\u4e0e O\n\u91d1 B-LOC\n\u95e8 I-LOC\n\u4e4b O\n\u95f4 O\n\u7684 O\n\u6d77 O\n\u57df O\n\u3002 O\n```\nThe first one of each line is a token, the second is token's label, and the line is divided by a blank line. The maximum length of each sentence is [max_seq_length] params.  \nYou can get training data from above two git repos  \nYou can training ner model by running below command:  \n```\ntoolkit_bert_ner_training \\\n    -data_dir {your dataset dir}\\\n    -output_dir {training output dir}\\\n    -init_checkpoint {Google BERT model dir}\\\n    -bert_config_file {bert_config.json under the Google BERT model dir} \\\n    -vocab_file {vocab.txt under the Google BERT model dir}\n```\nlike my init_checkpoint: \n```\ninit_checkpoint = {$HOME}/pre-trained-models/chinese_L-12_H-768_A-12/bert_model.ckpt\n```\nyou can special labels using -label_list params, the project get labels from training data.  \n```\n# using , split\n-labels 'B-LOC, I-LOC ...'\nOR save label in a file like labels.txt, one line one label\n-labels labels.txt\n```\n\nAfter training model, the NER model will be saved in {output_dir} which you special above cmd line.  \n##### My Training environment\uff1aTesla P40 24G mem  \n\n## As Service\n```\ntoolkit-bert-ner-serving-start -help\n```\n\nand than you can using below cmd start ner service:\n```angular2html\ntoolkit_bert_ner_serving \\\n    -model_dir C:\\workspace\\python\\BERT_Base\\output\\ner2 \\\n    -bert_model_dir F:\\chinese_L-12_H-768_A-12\n    -model_pb_dir C:\\workspace\\python\\BERT_Base\\model_pb_dir\n    -mode NER\n```\n\nyou can using below code test client:  \n#### 1. NER Client\n```angular2html\nimport time\nfrom bert_base.client import BertClient\n\nwith BertClient(show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n    start_t = time.perf_counter()\n    str = '1\u670824\u65e5\uff0c\u65b0\u534e\u793e\u5bf9\u5916\u53d1\u5e03\u4e86\u4e2d\u592e\u5bf9\u96c4\u5b89\u65b0\u533a\u7684\u6307\u5bfc\u610f\u89c1\uff0c\u6d0b\u6d0b\u6d12\u6d121.2\u4e07\u591a\u5b57\uff0c17\u6b21\u63d0\u5230\u5317\u4eac\uff0c4\u6b21\u63d0\u5230\u5929\u6d25\uff0c\u4fe1\u606f\u91cf\u5f88\u5927\uff0c\u5176\u5b9e\u4e5f\u56de\u7b54\u4e86\u4eba\u4eec\u5173\u5fc3\u7684\u5f88\u591a\u95ee\u9898\u3002'\n    rst = bc.encode([str, str])\n    print('rst:', rst)\n    print(time.perf_counter() - start_t)\n```\n```angular2html\nrst = bc.encode([list(str), list(str)], is_tokenized=True)\n```  \n\n## License\nMIT.  \n\n## How to train\n#### 1. Download BERT chinese model:  \n ```\n wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip  \n ```\n#### 2. Put BERT chinese model to $HOME/pre-trained-models/:  \n ```\nmkdir $HOME/pre-trained-models/\nunzip chinese_L-12_H-768_A-12.zip $HOME/pre-trained-models/\n ```\n\n#### 3. Train model\n\n##### first method \n```\n  python3 bert_lstm_ner.py   \\\n                  --task_name=\"NER\"  \\ \n                  --do_train=True   \\\n                  --do_eval=True   \\\n                  --do_predict=True\n                  --data_dir=NERdata   \\\n                  --vocab_file=checkpoint/vocab.txt  \\ \n                  --bert_config_file=checkpoint/bert_config.json \\  \n                  --init_checkpoint=checkpoint/bert_model.ckpt   \\\n                  --max_seq_length=128   \\\n                  --train_batch_size=32   \\\n                  --learning_rate=2e-5   \\\n                  --num_train_epochs=3.0   \\\n                  --output_dir=./output \\\n ```       \n ##### OR replace the BERT path and project path in bert_lstm_ner.py\n ```\n if os.name == 'nt': #windows path config\n    bert_path = '{your BERT model path}'\n    root_path = '{project path}'\nelse: # linux path config\n    bert_path = '{your BERT model path}'\n    root_path = '{project path}'\n ```\n Than Run:\n ```angular2html\npython3 bert_lstm_ner.py\n```\n\n### USING BLSTM-CRF OR ONLY CRF FOR DECODE!\nJust alter bert_lstm_ner.py line of 450, the params of the function of add_blstm_crf_layer: crf_only=True or False  \n\nONLY CRF output layer:\n```\n    blstm_crf = BLSTM_CRF(embedded_chars=embedding, hidden_unit=FLAGS.lstm_size, cell_type=FLAGS.cell, num_layers=FLAGS.num_layers,\n                          dropout_rate=FLAGS.droupout_rate, initializers=initializers, num_labels=num_labels,\n                          seq_length=max_seq_length, labels=labels, lengths=lengths, is_training=is_training)\n    rst = blstm_crf.add_blstm_crf_layer(crf_only=True)\n```\n\n\nBiLSTM with CRF output layer\n```\n    blstm_crf = BLSTM_CRF(embedded_chars=embedding, hidden_unit=FLAGS.lstm_size, cell_type=FLAGS.cell, num_layers=FLAGS.num_layers,\n                          dropout_rate=FLAGS.droupout_rate, initializers=initializers, num_labels=num_labels,\n                          seq_length=max_seq_length, labels=labels, lengths=lengths, is_training=is_training)\n    rst = blstm_crf.add_blstm_crf_layer(crf_only=False)\n```\n\n## ONLINE PREDICT\nIf model is train finished, just run\n```angular2html\npython3 terminal_predict.py\n```\n\n ## Using NER as Service\n\n#### Service \nUsing NER as Service is simple, you just need to run the python script below in the project root path:\n```angular2html\npython3 runs.py \\ \n    -mode NER\n    -bert_model_dir /home/macan/ml/data/chinese_L-12_H-768_A-12 \\\n    -ner_model_dir /home/macan/ml/data/bert_ner \\\n    -model_pd_dir /home/macan/ml/workspace/BERT_Base/output/predict_optimizer \\\n    -num_worker 8\n```\n\n\n#### Client\nThe client using methods can reference client_test.py script\n```angular2html\nimport time\nfrom client.client import BertClient\n\nner_model_dir = 'C:\\workspace\\python\\BERT_Base\\output\\predict_ner'\nwith BertClient( ner_model_dir=ner_model_dir, show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n    start_t = time.perf_counter()\n    str = '1\u670824\u65e5\uff0c\u65b0\u534e\u793e\u5bf9\u5916\u53d1\u5e03\u4e86\u4e2d\u592e\u5bf9\u96c4\u5b89\u65b0\u533a\u7684\u6307\u5bfc\u610f\u89c1\uff0c\u6d0b\u6d0b\u6d12\u6d121.2\u4e07\u591a\u5b57\uff0c17\u6b21\u63d0\u5230\u5317\u4eac\uff0c4\u6b21\u63d0\u5230\u5929\u6d25\uff0c\u4fe1\u606f\u91cf\u5f88\u5927\uff0c\u5176\u5b9e\u4e5f\u56de\u7b54\u4e86\u4eba\u4eec\u5173\u5fc3\u7684\u5f88\u591a\u95ee\u9898\u3002'\n    rst = bc.encode([str])\n    print('rst:', rst)\n    print(time.perf_counter() - start_t)\n```\n\n\nNOTE: input format you can sometime reference bert as service project.    \nWelcome to provide more client language code like java or others.  \n ## Using yourself data to train\n if you want to use yourself data to train ner model,you just modify  the get_labes func.\n ```angular2html\ndef get_labels(self):\n        return [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n```\nNOTE: \"X\", \u201c[CLS]\u201d, \u201c[SEP]\u201d These three are necessary, you just replace your data label to this return list.  \nOr you can use last code lets the program automatically get the label from training data\n```angular2html\ndef get_labels(self):\n        # \u901a\u8fc7\u8bfb\u53d6train\u6587\u4ef6\u83b7\u53d6\u6807\u7b7e\u7684\u65b9\u6cd5\u4f1a\u51fa\u73b0\u4e00\u5b9a\u7684\u98ce\u9669\u3002\n        if os.path.exists(os.path.join(FLAGS.output_dir, 'label_list.pkl')):\n            with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'rb') as rf:\n                self.labels = pickle.load(rf)\n        else:\n            if len(self.labels) > 0:\n                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n                with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'wb') as rf:\n                    pickle.dump(self.labels, rf)\n            else:\n                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n        return self.labels\n\n```\n\n\n## Reference: \n+ The evaluation codes come from:https://github.com/guillaumegenthial/tf_metrics/blob/master/tf_metrics/__init__.py\n\n+ [https://github.com/google-research/bert](https://github.com/google-research/bert)\n\n+ [https://github.com/kyzhouhzau/BERT-NER](https://github.com/kyzhouhzau/BERT-NER)\n\n+ [https://github.com/zjy-ucas/ChineseNER](https://github.com/zjy-ucas/ChineseNER)\n\n+ [https://github.com/hanxiao/bert-as-service](https://github.com/hanxiao/bert-as-service)\n\n+ [https://github.com/macanv/BERT-BiLSTM-CRF-NER](https://github.com/macanv/BERT-BiLSTM-CRF-NER)\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/wxl18039675170", "keywords": "toolkit_bert_ner nlp ner NER named entity recognition bilstm crf tensorflow machine learning sentence encoding embedding serving", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "toolkit-bert-ner", "package_url": "https://pypi.org/project/toolkit-bert-ner/", "platform": "", "project_url": "https://pypi.org/project/toolkit-bert-ner/", "project_urls": {"Homepage": "https://github.com/wxl18039675170"}, "release_url": "https://pypi.org/project/toolkit-bert-ner/1.0.2/", "requires_dist": ["numpy", "six", "pyzmq (>=16.0.0)", "GPUtil (>=1.3.0)", "termcolor (>=1.1)", "tensorflow (>=1.10.0) ; extra == 'cpu'", "tensorflow-gpu (>=1.10.0) ; extra == 'gpu'", "flask ; extra == 'http'", "flask-compress ; extra == 'http'", "flask-cors ; extra == 'http'", "flask-json ; extra == 'http'"], "requires_python": "", "summary": "Use Google's BERT for Chinese natural language processing tasks such as named entity recognition and provide server services", "version": "1.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>toolkit-bert-ner</h1>\n<p>Base Google pre-training model(BERT), then add BiLSTM layer and crf layer, train a Chinese named entity recognition model.</p>\n<h2>Download project and install</h2>\n<p>You can install this project by:</p>\n<pre><code>pip install -i https://test.pypi.org/simple/ toolkit-bert-ner==1.0.0\n</code></pre>\n<p>OR</p>\n<pre><code>git clone http://git.huimeimt.net:8008/ds/toolkit-bert-ner.git\ncd toolkit-bert-ner/\npython3 setup.py install\n</code></pre>\n<p>if you do not want to install, you just need clone this project and reference the file of &lt;run.py&gt; to train the model or start the service.</p>\n<h2>Train model:</h2>\n<p>You can use -help to view the relevant parameters of the training named entity recognition model, where data_dir, bert_config_file, output_dir, init_checkpoint, vocab_file must be specified.</p>\n<pre>toolkit-bert-ner-train -help\n</pre>\n<p>train/dev/test dataset is like this:</p>\n<pre><code>\u6d77 O\n\u9493 O\n\u6bd4 O\n\u8d5b O\n\u5730 O\n\u70b9 O\n\u5728 O\n\u53a6 B-LOC\n\u95e8 I-LOC\n\u4e0e O\n\u91d1 B-LOC\n\u95e8 I-LOC\n\u4e4b O\n\u95f4 O\n\u7684 O\n\u6d77 O\n\u57df O\n\u3002 O\n</code></pre>\n<p>The first one of each line is a token, the second is token's label, and the line is divided by a blank line. The maximum length of each sentence is [max_seq_length] params.<br>\nYou can get training data from above two git repos<br>\nYou can training ner model by running below command:</p>\n<pre><code>toolkit_bert_ner_training \\\n    -data_dir {your dataset dir}\\\n    -output_dir {training output dir}\\\n    -init_checkpoint {Google BERT model dir}\\\n    -bert_config_file {bert_config.json under the Google BERT model dir} \\\n    -vocab_file {vocab.txt under the Google BERT model dir}\n</code></pre>\n<p>like my init_checkpoint:</p>\n<pre><code>init_checkpoint = {$HOME}/pre-trained-models/chinese_L-12_H-768_A-12/bert_model.ckpt\n</code></pre>\n<p>you can special labels using -label_list params, the project get labels from training data.</p>\n<pre><code># using , split\n-labels 'B-LOC, I-LOC ...'\nOR save label in a file like labels.txt, one line one label\n-labels labels.txt\n</code></pre>\n<p>After training model, the NER model will be saved in {output_dir} which you special above cmd line.</p>\n<h5>My Training environment\uff1aTesla P40 24G mem</h5>\n<h2>As Service</h2>\n<pre><code>toolkit-bert-ner-serving-start -help\n</code></pre>\n<p>and than you can using below cmd start ner service:</p>\n<pre>toolkit_bert_ner_serving \\\n    -model_dir C:\\workspace\\python\\BERT_Base\\output\\ner2 \\\n    -bert_model_dir F:\\chinese_L-12_H-768_A-12\n    -model_pb_dir C:\\workspace\\python\\BERT_Base\\model_pb_dir\n    -mode NER\n</pre>\n<p>you can using below code test client:</p>\n<h4>1. NER Client</h4>\n<pre>import time\nfrom bert_base.client import BertClient\n\nwith BertClient(show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n    start_t = time.perf_counter()\n    str = '1\u670824\u65e5\uff0c\u65b0\u534e\u793e\u5bf9\u5916\u53d1\u5e03\u4e86\u4e2d\u592e\u5bf9\u96c4\u5b89\u65b0\u533a\u7684\u6307\u5bfc\u610f\u89c1\uff0c\u6d0b\u6d0b\u6d12\u6d121.2\u4e07\u591a\u5b57\uff0c17\u6b21\u63d0\u5230\u5317\u4eac\uff0c4\u6b21\u63d0\u5230\u5929\u6d25\uff0c\u4fe1\u606f\u91cf\u5f88\u5927\uff0c\u5176\u5b9e\u4e5f\u56de\u7b54\u4e86\u4eba\u4eec\u5173\u5fc3\u7684\u5f88\u591a\u95ee\u9898\u3002'\n    rst = bc.encode([str, str])\n    print('rst:', rst)\n    print(time.perf_counter() - start_t)\n</pre>\n<pre>rst = bc.encode([list(str), list(str)], is_tokenized=True)\n</pre>\n<h2>License</h2>\n<p>MIT.</p>\n<h2>How to train</h2>\n<h4>1. Download BERT chinese model:</h4>\n<pre><code>wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip  \n</code></pre>\n<h4>2. Put BERT chinese model to $HOME/pre-trained-models/:</h4>\n<pre><code>mkdir $HOME/pre-trained-models/\nunzip chinese_L-12_H-768_A-12.zip $HOME/pre-trained-models/\n</code></pre>\n<h4>3. Train model</h4>\n<h5>first method</h5>\n<pre><code>  python3 bert_lstm_ner.py   \\\n                  --task_name=\"NER\"  \\ \n                  --do_train=True   \\\n                  --do_eval=True   \\\n                  --do_predict=True\n                  --data_dir=NERdata   \\\n                  --vocab_file=checkpoint/vocab.txt  \\ \n                  --bert_config_file=checkpoint/bert_config.json \\  \n                  --init_checkpoint=checkpoint/bert_model.ckpt   \\\n                  --max_seq_length=128   \\\n                  --train_batch_size=32   \\\n                  --learning_rate=2e-5   \\\n                  --num_train_epochs=3.0   \\\n                  --output_dir=./output \\\n</code></pre>\n<h5>OR replace the BERT path and project path in bert_lstm_ner.py</h5>\n<pre><code>if os.name == 'nt': #windows path config\n   bert_path = '{your BERT model path}'\n   root_path = '{project path}'\nelse: # linux path config\n   bert_path = '{your BERT model path}'\n   root_path = '{project path}'\n</code></pre>\n<p>Than Run:</p>\n<pre>python3 bert_lstm_ner.py\n</pre>\n<h3>USING BLSTM-CRF OR ONLY CRF FOR DECODE!</h3>\n<p>Just alter bert_lstm_ner.py line of 450, the params of the function of add_blstm_crf_layer: crf_only=True or False</p>\n<p>ONLY CRF output layer:</p>\n<pre><code>    blstm_crf = BLSTM_CRF(embedded_chars=embedding, hidden_unit=FLAGS.lstm_size, cell_type=FLAGS.cell, num_layers=FLAGS.num_layers,\n                          dropout_rate=FLAGS.droupout_rate, initializers=initializers, num_labels=num_labels,\n                          seq_length=max_seq_length, labels=labels, lengths=lengths, is_training=is_training)\n    rst = blstm_crf.add_blstm_crf_layer(crf_only=True)\n</code></pre>\n<p>BiLSTM with CRF output layer</p>\n<pre><code>    blstm_crf = BLSTM_CRF(embedded_chars=embedding, hidden_unit=FLAGS.lstm_size, cell_type=FLAGS.cell, num_layers=FLAGS.num_layers,\n                          dropout_rate=FLAGS.droupout_rate, initializers=initializers, num_labels=num_labels,\n                          seq_length=max_seq_length, labels=labels, lengths=lengths, is_training=is_training)\n    rst = blstm_crf.add_blstm_crf_layer(crf_only=False)\n</code></pre>\n<h2>ONLINE PREDICT</h2>\n<p>If model is train finished, just run</p>\n<pre>python3 terminal_predict.py\n</pre>\n<h2>Using NER as Service</h2>\n<h4>Service</h4>\n<p>Using NER as Service is simple, you just need to run the python script below in the project root path:</p>\n<pre>python3 runs.py \\ \n    -mode NER\n    -bert_model_dir /home/macan/ml/data/chinese_L-12_H-768_A-12 \\\n    -ner_model_dir /home/macan/ml/data/bert_ner \\\n    -model_pd_dir /home/macan/ml/workspace/BERT_Base/output/predict_optimizer \\\n    -num_worker 8\n</pre>\n<h4>Client</h4>\n<p>The client using methods can reference client_test.py script</p>\n<pre>import time\nfrom client.client import BertClient\n\nner_model_dir = 'C:\\workspace\\python\\BERT_Base\\output\\predict_ner'\nwith BertClient( ner_model_dir=ner_model_dir, show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n    start_t = time.perf_counter()\n    str = '1\u670824\u65e5\uff0c\u65b0\u534e\u793e\u5bf9\u5916\u53d1\u5e03\u4e86\u4e2d\u592e\u5bf9\u96c4\u5b89\u65b0\u533a\u7684\u6307\u5bfc\u610f\u89c1\uff0c\u6d0b\u6d0b\u6d12\u6d121.2\u4e07\u591a\u5b57\uff0c17\u6b21\u63d0\u5230\u5317\u4eac\uff0c4\u6b21\u63d0\u5230\u5929\u6d25\uff0c\u4fe1\u606f\u91cf\u5f88\u5927\uff0c\u5176\u5b9e\u4e5f\u56de\u7b54\u4e86\u4eba\u4eec\u5173\u5fc3\u7684\u5f88\u591a\u95ee\u9898\u3002'\n    rst = bc.encode([str])\n    print('rst:', rst)\n    print(time.perf_counter() - start_t)\n</pre>\n<p>NOTE: input format you can sometime reference bert as service project.<br>\nWelcome to provide more client language code like java or others.</p>\n<h2>Using yourself data to train</h2>\n<p>if you want to use yourself data to train ner model,you just modify  the get_labes func.</p>\n<pre>def get_labels(self):\n       return [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n</pre>\n<p>NOTE: \"X\", \u201c[CLS]\u201d, \u201c[SEP]\u201d These three are necessary, you just replace your data label to this return list.<br>\nOr you can use last code lets the program automatically get the label from training data</p>\n<pre>def get_labels(self):\n        # \u901a\u8fc7\u8bfb\u53d6train\u6587\u4ef6\u83b7\u53d6\u6807\u7b7e\u7684\u65b9\u6cd5\u4f1a\u51fa\u73b0\u4e00\u5b9a\u7684\u98ce\u9669\u3002\n        if os.path.exists(os.path.join(FLAGS.output_dir, 'label_list.pkl')):\n            with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'rb') as rf:\n                self.labels = pickle.load(rf)\n        else:\n            if len(self.labels) &gt; 0:\n                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n                with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'wb') as rf:\n                    pickle.dump(self.labels, rf)\n            else:\n                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n        return self.labels\n</pre>\n<h2>Reference:</h2>\n<ul>\n<li>\n<p>The evaluation codes come from:<a href=\"https://github.com/guillaumegenthial/tf_metrics/blob/master/tf_metrics/__init__.py\" rel=\"nofollow\">https://github.com/guillaumegenthial/tf_metrics/blob/master/tf_metrics/__init__.py</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">https://github.com/google-research/bert</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/kyzhouhzau/BERT-NER\" rel=\"nofollow\">https://github.com/kyzhouhzau/BERT-NER</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/zjy-ucas/ChineseNER\" rel=\"nofollow\">https://github.com/zjy-ucas/ChineseNER</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/hanxiao/bert-as-service\" rel=\"nofollow\">https://github.com/hanxiao/bert-as-service</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/macanv/BERT-BiLSTM-CRF-NER\" rel=\"nofollow\">https://github.com/macanv/BERT-BiLSTM-CRF-NER</a></p>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 6456098, "releases": {"1.0.2": [{"comment_text": "", "digests": {"md5": "1c2f309e080313beaa01df8007dd7c49", "sha256": "3ca0298f2384bab62a7f226fa59e1954d98cd22ab8ef4d87fa83e17c1189a714"}, "downloads": -1, "filename": "toolkit_bert_ner-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "1c2f309e080313beaa01df8007dd7c49", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 106272, "upload_time": "2020-01-15T03:30:36", "upload_time_iso_8601": "2020-01-15T03:30:36.175393Z", "url": "https://files.pythonhosted.org/packages/98/c8/b907a8d3933b4ce3dd71be063f8cf266165531d6a425a79eddf9ea5e8b43/toolkit_bert_ner-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3a31b5d6d95653fa885a8945167812f0", "sha256": "fb7f6f67fdafc400ce0b822f9503e24aaf24e6200a645175c52b52a985502c85"}, "downloads": -1, "filename": "toolkit_bert_ner-1.0.2.tar.gz", "has_sig": false, "md5_digest": "3a31b5d6d95653fa885a8945167812f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 91210, "upload_time": "2020-01-15T03:30:40", "upload_time_iso_8601": "2020-01-15T03:30:40.268815Z", "url": "https://files.pythonhosted.org/packages/93/8c/7964762d65d37b8646665bee0153bfb4b7746e5f357de2eabc29bec2fea7/toolkit_bert_ner-1.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1c2f309e080313beaa01df8007dd7c49", "sha256": "3ca0298f2384bab62a7f226fa59e1954d98cd22ab8ef4d87fa83e17c1189a714"}, "downloads": -1, "filename": "toolkit_bert_ner-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "1c2f309e080313beaa01df8007dd7c49", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 106272, "upload_time": "2020-01-15T03:30:36", "upload_time_iso_8601": "2020-01-15T03:30:36.175393Z", "url": "https://files.pythonhosted.org/packages/98/c8/b907a8d3933b4ce3dd71be063f8cf266165531d6a425a79eddf9ea5e8b43/toolkit_bert_ner-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3a31b5d6d95653fa885a8945167812f0", "sha256": "fb7f6f67fdafc400ce0b822f9503e24aaf24e6200a645175c52b52a985502c85"}, "downloads": -1, "filename": "toolkit_bert_ner-1.0.2.tar.gz", "has_sig": false, "md5_digest": "3a31b5d6d95653fa885a8945167812f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 91210, "upload_time": "2020-01-15T03:30:40", "upload_time_iso_8601": "2020-01-15T03:30:40.268815Z", "url": "https://files.pythonhosted.org/packages/93/8c/7964762d65d37b8646665bee0153bfb4b7746e5f357de2eabc29bec2fea7/toolkit_bert_ner-1.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:04 2020"}