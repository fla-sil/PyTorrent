{"info": {"author": "The Deeva Authors", "author_email": "", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Night-to-Day Image-translation System\n## Introduction\nAs mentioned in the following paper, brightness is a very important and sensitive factor in image processing.\n  * Night surveillance is a challenging task because of low brightness, low contrast, low Signal to Noise Ratio (SNR) and low appearance information. [[1](https://ieeexplore.ieee.org/document/5414538)]\n  * Inhomogeneous illumination induces large contrast variations in images. Detection is di\ufb03cult in low-contrast regions and may result in loss of color information and in confusing foreground and background regions. [[2](https://www.robots.ox.ac.uk/~vgg/publications/2018/Neumann18b/)]\n\nIn addition, in the object detection field, brightness is a factor that greatly affects the performance like mAP, and the accuracy of object detection decreases at night.\nWith this in mind, the project proposes a Night-to-Day Image-translation System(hereinafter referred to as \"n2dit\") that transforms night images to day images.\nThe system supports improving the performance of systems that require object recognition, such as ADAS, black boxes, and computer vision.\n\n\n<div align=\"center\">\n<kbd>\n<img src=\"https://drive.google.com/uc?export=view&id=1azFgM1aMZhbIAKMitYaFKTCputV0X4Zx\" width=\"400\">\n</kbd>\n</div>\n<br>\nCycleGAN, an unsupervised learning-based adversary generation model, was used and Alpha blending was introduced to improve the detection rate of the transformed image.\nThrough this, we proved that, there was a improvement in mAP(mean Average Precision) compared to the existing CLAHE(Contrast Limited Adaptive Histogram Equalization) algorithm.\n\n## Implementation\n### CycleGAN\nThe project started with CycleGAN[[3](https://arxiv.org/abs/1703.10593)]. CycleGAN is created by adding cycle-consistency loss to GAN. Cycle-consistency loss helps CycleGAN to only transform the original image to a Fake to the level that can be recovered to Real. In addition, CycleGAN has the great advantage of using a training set that does not require pairing. Thus, allowing CycleGAN to learn domain-to-domain image-translation.  \nWhile following the original implementation, we found out that if the depth of the discriminator is less than a certain amount, generator no longer learns after a certain period of time. Therefore, we tried to find the appropriate number of convolutional layers in the discriminator to improve performance through equivalent competitive learning of the generator and discriminator.\n\n<div align=\"center\">\n<kbd>\n<img src=\"https://hardikbansal.github.io/CycleGANBlog/images/model.jpg\" width=\"600\">\n</kbd>\n</div>\n<br>\n<div align=\"center\">\n<kbd>\n<img src=\"https://hardikbansal.github.io/CycleGANBlog/images/model1.jpg\" width=\"600\">\n</kbd>\n</div>\n<br>\n\n### Alpha Blending[[5](https://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending)]\nAlpha blending is also used in 2D computer graphics to put rasterized foreground elements over a background.\nWe experimentally found that the method of displaying a mixture by overlaying the output image to the input image showed higher object detection rates than the conversion method through CycleGAN.\n\n<div align=\"center\">\n<kbd>\n<img src=\"https://i.imgur.com/Z7YNOkj.jpg\" width=\"600\">\n</kbd>\n</div>\n<br>\n\n### Failed Experiments\n  * #### Perceptual Loss\n    In Ledig et al.[[4](https://arxiv.org/abs/1609.04802)], the authors propose a perceptual loss function which consists of an adversarial loss and a content loss.\n    We use this in the objective function to improve the resolution of the fake image generated by CycleGAN.\n<br>\n\n  * #### Adding CLAHE-Sobel Loss\n    Before we applied the perceptual loss, we experimented with adding the following loss to increase the resolution of the generated images:\n    For the mapping function __*G: X \u2192 Y*__, __*F: Y \u2192 X*__ and CLAHE __*C*__, sobel filter __*S*__<br><br>\n    <div align=center>\n    <img src=\"https://i.imgur.com/OeGB2iT.png\" />\n    </div>\n    So the full objective is:<br><br>\n    <div align=center>\n    <img src=\"https://i.imgur.com/6QOB15V.png\" />\n    </div>\n    The results during the learning process seemed quite plausible, but the test scores did not improve significantly.\n<br>\n\n  * #### Resize Convolution\n    We found that the checkerboard-like pattern that occurs during the generator's upsampling process reduces the object detection rate and causes low resolution.\n    We found the Bilinear-Resize Convolution[[6](https://distill.pub/2016/deconv-checkerboard/)], which is an alternative to the existing upsampling method, \u201ctransposed convolution\u201d.\n    The method resizes the image using the nearest-neighbor interpolation and then upsamples it through the convolutional layer, which improve the \u201cuneven overlap\u201d such as a checkerboard pattern.\n\n<div align=\"center\">\n<kbd>\n<img src=\"https://distill.pub/2016/deconv-checkerboard/assets/upsample_DeconvTypes.svg\" width=\"800\">\n</kbd>\n</div>\n<br>\n\n\n## Test Process\n### Preparing Dataset\nExtract night and day images from the labels in the <a href=\"https://bdd-data.berkeley.edu/\">BerKeley DeepDrive dataset</a>, redefine the class name in the label as appropriate for the darknet setup, and create the correct label by calculating the width and height from the x1, x2, y1, and y2 coordinate.\n\n### Model Training\n Train our model with the training dataset prepared using the various options.\n\n### Image-translation\n Convert the night image in the test dataset into a day image with a trained model.\n\n### Object Detection\n Calculate mAP of real and transformed images using Darknet with COCO-Yolo v3 pre-trained model.\n There are 80 classes in the COCO dataset, but we limit it to three classes(person, car, bus) for the characteristics of the BDD dataset.\n\n\n### Refine Model\n Repeat training by adjusting network or parameters according to whether object detection is improved.\n\n## Results\n### Result Images\nThe image translation results of all experiments, including the failed ones, are as follows.\n\n<div align=\"center\">\n<kbd>\n<img src=\"https://drive.google.com/uc?export=view&id=1l1lcNTi5gHX6T2u6AurkmR4ALxX-xWfN\" width=\"1000\">\n</kbd>\n</div>\n<br>\n\n### Object Detection Performance\nThe following table is the result of object detection of 5,000 night images from a BerKeley DeepDrive Dataset with COCO-Yolo v3 weight.\nThe number of classes and objects in a dataset is as follows: (Person: 3611, Car: 45694, Bus: 378)\nWhen the generator uses 9 resnet blocks, the discriminator uses 6 convolution layers, and alpha blending is applied, the highest object detection rate is shown in the following table.\n\n<div align=\"center\">\n\n||Original|CLAHE|Alpha Blending<br/>(Real:0.8, fake:0.2 ratio)|The discriminator has 6 convolution layers<br/>with Alpha Blending|\n|:---|---:|---:|---:|---:|\n|Person(AP)|29.75|31.90|30.71|30.98|\n|Car(AP)|42.09|44.41|44.54|46.19|\n|Bus(AP)|25.44|26.37|28.34|28.05|\n|mAP|32.43|34.23|34.53|35.07|\n|mAP(Balanced)|41.07|43.36|43.41|44.95|\n|Diff|-|2.30|2.35|3.88|\n\n</div>\n\n## Conclusion\nThe Night-to-Day Image-translation System adds the characteristics of the target domain, which the original domain did not have, to the original domain for image transformation. Night images with low brightness are converted to day images with high brightness, which is a major factor in improving the performance of object detection.\nIn addition, problems such as noise, low resolution, generated from a bad weather environment or a sensor of image processing devices, etc. that degrade the performance of object detection can be solved by learning them into the original domain and converting them to have desired characteristics.\nTherefore, the system is considered to be highly utilized in the field of computer vision, and it is possible to apply a target transformation in image processing of various fields as well as night and day transformation.\n\n## Reference\n  * ### [1] [Object detection and tracking for night surveillance based on salient contrast analysis](https://ieeexplore.ieee.org/document/5414538)\n  * ### [2] [NightOwls: A pedestrians at night dataset](https://www.robots.ox.ac.uk/~vgg/publications/2018/Neumann18b/)\n  * ### [3] [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)\n  * ### [4] [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)\n  * ### [5] [Alpha blending in Wikipedia](https://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending)\n  * ### [6] [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)\n\n## System Requirements\n### Hardware\n  * NVIDIA GPUs 8G\n  * 10.00 GB of available disk space\n\n### Software\n  * #### Linux\n    - #### Ubuntu 18.04\n      + NVIDIA driver - version for CUDA\n      + CUDA - 10.0\n      + cuDNN - 7.6.2.24-1\n      + Python 3.6.8\n      + pip - 19.2.3(python 3.6)\n      + tensorflow-gpu - 1.14\n      + matplotlib - 3.1.1\n      + pillow - 6.2.1\n      + opencv-python - 3.4.2.17\n</br>\n\n  * #### Docker\n    - NVIDIA driver - version for CUDA\n    - Docker Engine - 19.03.4\n    - (Option 1) tensorflow/tensorflow:1.14.0-gpu-py3(Digest:e72e66b3dcb9)\n      + matplotlib - 3.1.1\n      + pillow - 6.2.1\n      + opencv-python - 3.4.2.17\n    - (Option 2) n2dit Docker Image\n      + See below.\n\n## Installation\n### Use pip for Installing\n  * (Optional) To use the GUI, checkout the gui branch with the following command.\n    ```\n    $ git checkout gui\n    ```\n  * Install the list of requirements with the following command.\n    ```\n    $ pip install -r requirements.txt\n    ```\n  * Install n2dit with the following command from the directory path where the setup.py file is located.\n    ```\n    $ pip install .\n    ```\n  * (Optional) If you are using Anaconda, follow these steps.\n    - Remove opencv-python from requirements.txt.\n    - Create and activate the environment you want to use.\n    - Install opencv-python with the following command.\n      ```\n      $ conda install -c menpo opencv\n      ```\n    - Use the following command as in the two steps above.\n      ```\n      $ conda install --file requirements.txt\n      $ pip install .\n      ```\n### Docker\n  * #### Build n2dit docker image\n    - (Optional) To use the GUI, checkout the gui branch with the following command.\n    - Build the docker image by running the following command where the Dockerfile is located:\n      ```\n      $ docker build -t <image-name>:<tag> --target release .\n      ```\n  * #### Docker Hub\n    - You can get the docker image that have been built from the following repository.\n      ```\n      deeva2019/n2dit:<version>\n      ```\n      + version\n        * CLI version: X.X.X\n        * GUI version: gui-X.X.X\n\n## Excecution\n### n2dit\n  * #### Training\n    - Set options via n2dit command options or configuration files.\n      + Sample configuration files are located in the config directory.\n        ```\n        [cyclegan]\n        dirA = ./datasets/day\n        dirB = ./datasets/night\n        results_dir = ./results\n        load_size = 286\n        crop_size = 256\n        channels = 3\n        shuffle = yes\n        pool_size = 50\n        exp_name = test\n        continue = no\n        epoch = 0\n        niter = 100\n        niter_decay = 100\n        disp_loss_freq = 10\n        disp_summary_freq = 100\n        learning_rate = 0.0002\n        lambda_A = 10.0\n        lambda_B = 10.0\n        lambda_idt = 0.5\n        upsample = resize_conv\n        ```\n    - Run with the following command.\n      ```\n      $ n2dit cyclegan train -C config/sample_train.cfg\n      ```\n  * #### Test\n    - Config option values through the n2dit command options or sample configuration file as shown above.\n    - Run with the following command.\n      ```\n      $ n2dit cyclegan test -C config/sample_test.cfg\n      ```\n  * #### Guide to using docker\n    - Create a container with the following command to use the host's GPUs.\n      ```\n      $ docker run --gpus all <image-name>:<tag> <n2dit options>\n      ```\n    - The file used by n2dit corresponds to the following options.\n      + dirA: Source domain image directory you want to transform\n      + dirB: Target domain image directory\n      + results_dir: Common results directory used for training and testing\n      + (Optional) config_file: Configuration file\n    - So unless you use n2dit inside the container, use the option v to bind the host volume to the container as follows.\n      ```\n      $ docker run --gpus all -v /home/deeva/datasets/A/:/work/A\\\n                              -v /home/deeva/datasets/B/:/work/B\\\n                              -v /home/deeva/results/:/work/results\\\n                              n2dit:latest cyclegan train --dirA /work/A --dirB /work/B --results_dir /work/results\n      ```\n### n2dit GUI\n  * (Optional) To use the host's GUI inside the container, The script uses a docker option that exposes xhost to render to the display by reading and writing X11 Unix sockets.\n    - To use GUI's, use the following command. However, This approach is very vulnerable.\n      ```\n      $ xhost +local:root\n      ```\n  * When you are finished with the container's GUI, run the following command.\n    ```\n    $ xhost -local:root\n    ```\n  * Use easily.\n\n<div align=\"center\">\n<kbd>\n<img src=\"https://drive.google.com/uc?export=view&id=1nhH4EZv_KfF88OOlMq-WfI1XDJ070xBp\" width=\"600\">\n</kbd>\n</div>\n<br>\n\n<div align=\"center\">\n<kbd>\n<img src=\"https://drive.google.com/uc?export=view&id=1t3cspi5Hft3KlXU7N-t2p5tIz1Sc8Vyy\" width=\"600\">\n</kbd>\n</div>\n<br>\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://gitlab.com/deeva/Night-to-Day-Image-translation", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "n2dit", "package_url": "https://pypi.org/project/n2dit/", "platform": "", "project_url": "https://pypi.org/project/n2dit/", "project_urls": {"Homepage": "https://gitlab.com/deeva/Night-to-Day-Image-translation"}, "release_url": "https://pypi.org/project/n2dit/1.0.0/", "requires_dist": ["gast (==0.2.2)", "numpy (==1.16.2)", "tensorflow-gpu (==1.14)", "matplotlib (==3.1.1)", "pillow (==6.2.1)", "opencv-python (==3.4.2.17)"], "requires_python": ">=3.6", "summary": "Deep learning-based Night-to-Day image-translation software", "version": "1.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Night-to-Day Image-translation System</h1>\n<h2>Introduction</h2>\n<p>As mentioned in the following paper, brightness is a very important and sensitive factor in image processing.</p>\n<ul>\n<li>Night surveillance is a challenging task because of low brightness, low contrast, low Signal to Noise Ratio (SNR) and low appearance information. [<a href=\"https://ieeexplore.ieee.org/document/5414538\" rel=\"nofollow\">1</a>]</li>\n<li>Inhomogeneous illumination induces large contrast variations in images. Detection is di\ufb03cult in low-contrast regions and may result in loss of color information and in confusing foreground and background regions. [<a href=\"https://www.robots.ox.ac.uk/%7Evgg/publications/2018/Neumann18b/\" rel=\"nofollow\">2</a>]</li>\n</ul>\n<p>In addition, in the object detection field, brightness is a factor that greatly affects the performance like mAP, and the accuracy of object detection decreases at night.\nWith this in mind, the project proposes a Night-to-Day Image-translation System(hereinafter referred to as \"n2dit\") that transforms night images to day images.\nThe system supports improving the performance of systems that require object recognition, such as ADAS, black boxes, and computer vision.</p>\n<div>\n<kbd>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/73cf57d54302a57932297616d6ffcb2773641438/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d31617a46674d31614d5a686249414b4d69745961464b5443707574563058345a78\" width=\"400\">\n</kbd>\n</div>\n<br>\nCycleGAN, an unsupervised learning-based adversary generation model, was used and Alpha blending was introduced to improve the detection rate of the transformed image.\nThrough this, we proved that, there was a improvement in mAP(mean Average Precision) compared to the existing CLAHE(Contrast Limited Adaptive Histogram Equalization) algorithm.\n<h2>Implementation</h2>\n<h3>CycleGAN</h3>\n<p>The project started with CycleGAN[<a href=\"https://arxiv.org/abs/1703.10593\" rel=\"nofollow\">3</a>]. CycleGAN is created by adding cycle-consistency loss to GAN. Cycle-consistency loss helps CycleGAN to only transform the original image to a Fake to the level that can be recovered to Real. In addition, CycleGAN has the great advantage of using a training set that does not require pairing. Thus, allowing CycleGAN to learn domain-to-domain image-translation.<br>\nWhile following the original implementation, we found out that if the depth of the discriminator is less than a certain amount, generator no longer learns after a certain period of time. Therefore, we tried to find the appropriate number of convolutional layers in the discriminator to improve performance through equivalent competitive learning of the generator and discriminator.</p>\n<div>\n<kbd>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f8fb2abb6506e93870c97ef57af683377cedbc1c/68747470733a2f2f68617264696b62616e73616c2e6769746875622e696f2f4379636c6547414e426c6f672f696d616765732f6d6f64656c2e6a7067\" width=\"600\">\n</kbd>\n</div>\n<br>\n<div>\n<kbd>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/64b77743c33fa93804606020c7d33b336439411c/68747470733a2f2f68617264696b62616e73616c2e6769746875622e696f2f4379636c6547414e426c6f672f696d616765732f6d6f64656c312e6a7067\" width=\"600\">\n</kbd>\n</div>\n<br>\n<h3>Alpha Blending[<a href=\"https://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending\" rel=\"nofollow\">5</a>]</h3>\n<p>Alpha blending is also used in 2D computer graphics to put rasterized foreground elements over a background.\nWe experimentally found that the method of displaying a mixture by overlaying the output image to the input image showed higher object detection rates than the conversion method through CycleGAN.</p>\n<div>\n<kbd>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ae013d2be6711b1094f9c9fd97ebd48db7bd52ce/68747470733a2f2f692e696d6775722e636f6d2f5a37594e4f6b6a2e6a7067\" width=\"600\">\n</kbd>\n</div>\n<br>\n<h3>Failed Experiments</h3>\n<ul>\n<li>\n<h4>Perceptual Loss</h4>\nIn Ledig et al.[<a href=\"https://arxiv.org/abs/1609.04802\" rel=\"nofollow\">4</a>], the authors propose a perceptual loss function which consists of an adversarial loss and a content loss.\nWe use this in the objective function to improve the resolution of the fake image generated by CycleGAN.</li>\n</ul>\n<br>\n<ul>\n<li>\n<h4>Adding CLAHE-Sobel Loss</h4>\nBefore we applied the perceptual loss, we experimented with adding the following loss to increase the resolution of the generated images:\nFor the mapping function <strong><em>G: X \u2192 Y</em></strong>, <strong><em>F: Y \u2192 X</em></strong> and CLAHE <strong><em>C</em></strong>, sobel filter <strong><em>S</em></strong><br><br>\n<div>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/475a8eeb8fcc7bc6f8cb5856ea0886e9be43bc7b/68747470733a2f2f692e696d6775722e636f6d2f4f6547423269542e706e67\">\n</div>\nSo the full objective is:<br><br>\n<div>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1591b6b3b08437a169a42fd40cc1efb5c312f0bc/68747470733a2f2f692e696d6775722e636f6d2f36514f423135562e706e67\">\n</div>\nThe results during the learning process seemed quite plausible, but the test scores did not improve significantly.\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<h4>Resize Convolution</h4>\nWe found that the checkerboard-like pattern that occurs during the generator's upsampling process reduces the object detection rate and causes low resolution.\nWe found the Bilinear-Resize Convolution[<a href=\"https://distill.pub/2016/deconv-checkerboard/\" rel=\"nofollow\">6</a>], which is an alternative to the existing upsampling method, \u201ctransposed convolution\u201d.\nThe method resizes the image using the nearest-neighbor interpolation and then upsamples it through the convolutional layer, which improve the \u201cuneven overlap\u201d such as a checkerboard pattern.</li>\n</ul>\n<div>\n<kbd>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f35c2cb086d7dc6a08bf833f710d29c4c56f4f08/68747470733a2f2f64697374696c6c2e7075622f323031362f6465636f6e762d636865636b6572626f6172642f6173736574732f757073616d706c655f4465636f6e7654797065732e737667\" width=\"800\">\n</kbd>\n</div>\n<br>\n<h2>Test Process</h2>\n<h3>Preparing Dataset</h3>\n<p>Extract night and day images from the labels in the <a href=\"https://bdd-data.berkeley.edu/\" rel=\"nofollow\">BerKeley DeepDrive dataset</a>, redefine the class name in the label as appropriate for the darknet setup, and create the correct label by calculating the width and height from the x1, x2, y1, and y2 coordinate.</p>\n<h3>Model Training</h3>\n<p>Train our model with the training dataset prepared using the various options.</p>\n<h3>Image-translation</h3>\n<p>Convert the night image in the test dataset into a day image with a trained model.</p>\n<h3>Object Detection</h3>\n<p>Calculate mAP of real and transformed images using Darknet with COCO-Yolo v3 pre-trained model.\nThere are 80 classes in the COCO dataset, but we limit it to three classes(person, car, bus) for the characteristics of the BDD dataset.</p>\n<h3>Refine Model</h3>\n<p>Repeat training by adjusting network or parameters according to whether object detection is improved.</p>\n<h2>Results</h2>\n<h3>Result Images</h3>\n<p>The image translation results of all experiments, including the failed ones, are as follows.</p>\n<div>\n<kbd>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/50a7e656d9364747e2a75ba5e89a9e5483b86737/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d316c316c634e54693567485836543275364175726b6d5234414c78582d7857664e\" width=\"1000\">\n</kbd>\n</div>\n<br>\n<h3>Object Detection Performance</h3>\n<p>The following table is the result of object detection of 5,000 night images from a BerKeley DeepDrive Dataset with COCO-Yolo v3 weight.\nThe number of classes and objects in a dataset is as follows: (Person: 3611, Car: 45694, Bus: 378)\nWhen the generator uses 9 resnet blocks, the discriminator uses 6 convolution layers, and alpha blending is applied, the highest object detection rate is shown in the following table.</p>\n<div>\n<table>\n<thead>\n<tr>\n<th align=\"left\"></th>\n<th align=\"right\">Original</th>\n<th align=\"right\">CLAHE</th>\n<th align=\"right\">Alpha Blending<br>(Real:0.8, fake:0.2 ratio)</th>\n<th align=\"right\">The discriminator has 6 convolution layers<br>with Alpha Blending</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">Person(AP)</td>\n<td align=\"right\">29.75</td>\n<td align=\"right\">31.90</td>\n<td align=\"right\">30.71</td>\n<td align=\"right\">30.98</td>\n</tr>\n<tr>\n<td align=\"left\">Car(AP)</td>\n<td align=\"right\">42.09</td>\n<td align=\"right\">44.41</td>\n<td align=\"right\">44.54</td>\n<td align=\"right\">46.19</td>\n</tr>\n<tr>\n<td align=\"left\">Bus(AP)</td>\n<td align=\"right\">25.44</td>\n<td align=\"right\">26.37</td>\n<td align=\"right\">28.34</td>\n<td align=\"right\">28.05</td>\n</tr>\n<tr>\n<td align=\"left\">mAP</td>\n<td align=\"right\">32.43</td>\n<td align=\"right\">34.23</td>\n<td align=\"right\">34.53</td>\n<td align=\"right\">35.07</td>\n</tr>\n<tr>\n<td align=\"left\">mAP(Balanced)</td>\n<td align=\"right\">41.07</td>\n<td align=\"right\">43.36</td>\n<td align=\"right\">43.41</td>\n<td align=\"right\">44.95</td>\n</tr>\n<tr>\n<td align=\"left\">Diff</td>\n<td align=\"right\">-</td>\n<td align=\"right\">2.30</td>\n<td align=\"right\">2.35</td>\n<td align=\"right\">3.88</td>\n</tr></tbody></table>\n</div>\n<h2>Conclusion</h2>\n<p>The Night-to-Day Image-translation System adds the characteristics of the target domain, which the original domain did not have, to the original domain for image transformation. Night images with low brightness are converted to day images with high brightness, which is a major factor in improving the performance of object detection.\nIn addition, problems such as noise, low resolution, generated from a bad weather environment or a sensor of image processing devices, etc. that degrade the performance of object detection can be solved by learning them into the original domain and converting them to have desired characteristics.\nTherefore, the system is considered to be highly utilized in the field of computer vision, and it is possible to apply a target transformation in image processing of various fields as well as night and day transformation.</p>\n<h2>Reference</h2>\n<ul>\n<li>\n<h3>[1] <a href=\"https://ieeexplore.ieee.org/document/5414538\" rel=\"nofollow\">Object detection and tracking for night surveillance based on salient contrast analysis</a></h3>\n</li>\n<li>\n<h3>[2] <a href=\"https://www.robots.ox.ac.uk/%7Evgg/publications/2018/Neumann18b/\" rel=\"nofollow\">NightOwls: A pedestrians at night dataset</a></h3>\n</li>\n<li>\n<h3>[3] <a href=\"https://arxiv.org/abs/1703.10593\" rel=\"nofollow\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a></h3>\n</li>\n<li>\n<h3>[4] <a href=\"https://arxiv.org/abs/1609.04802\" rel=\"nofollow\">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a></h3>\n</li>\n<li>\n<h3>[5] <a href=\"https://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending\" rel=\"nofollow\">Alpha blending in Wikipedia</a></h3>\n</li>\n<li>\n<h3>[6] <a href=\"https://distill.pub/2016/deconv-checkerboard/\" rel=\"nofollow\">Deconvolution and Checkerboard Artifacts</a></h3>\n</li>\n</ul>\n<h2>System Requirements</h2>\n<h3>Hardware</h3>\n<ul>\n<li>NVIDIA GPUs 8G</li>\n<li>10.00 GB of available disk space</li>\n</ul>\n<h3>Software</h3>\n<ul>\n<li>\n<h4>Linux</h4>\n<ul>\n<li>\n<h4>Ubuntu 18.04</h4>\n<ul>\n<li>NVIDIA driver - version for CUDA</li>\n<li>CUDA - 10.0</li>\n<li>cuDNN - 7.6.2.24-1</li>\n<li>Python 3.6.8</li>\n<li>pip - 19.2.3(python 3.6)</li>\n<li>tensorflow-gpu - 1.14</li>\n<li>matplotlib - 3.1.1</li>\n<li>pillow - 6.2.1</li>\n<li>opencv-python - 3.4.2.17</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<br>\n<ul>\n<li>\n<h4>Docker</h4>\n<ul>\n<li>NVIDIA driver - version for CUDA</li>\n<li>Docker Engine - 19.03.4</li>\n<li>(Option 1) tensorflow/tensorflow:1.14.0-gpu-py3(Digest:e72e66b3dcb9)\n<ul>\n<li>matplotlib - 3.1.1</li>\n<li>pillow - 6.2.1</li>\n<li>opencv-python - 3.4.2.17</li>\n</ul>\n</li>\n<li>(Option 2) n2dit Docker Image\n<ul>\n<li>See below.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Installation</h2>\n<h3>Use pip for Installing</h3>\n<ul>\n<li>(Optional) To use the GUI, checkout the gui branch with the following command.\n<pre><code>$ git checkout gui\n</code></pre>\n</li>\n<li>Install the list of requirements with the following command.\n<pre><code>$ pip install -r requirements.txt\n</code></pre>\n</li>\n<li>Install n2dit with the following command from the directory path where the setup.py file is located.\n<pre><code>$ pip install .\n</code></pre>\n</li>\n<li>(Optional) If you are using Anaconda, follow these steps.\n<ul>\n<li>Remove opencv-python from requirements.txt.</li>\n<li>Create and activate the environment you want to use.</li>\n<li>Install opencv-python with the following command.\n<pre><code>$ conda install -c menpo opencv\n</code></pre>\n</li>\n<li>Use the following command as in the two steps above.\n<pre><code>$ conda install --file requirements.txt\n$ pip install .\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Docker</h3>\n<ul>\n<li>\n<h4>Build n2dit docker image</h4>\n<ul>\n<li>(Optional) To use the GUI, checkout the gui branch with the following command.</li>\n<li>Build the docker image by running the following command where the Dockerfile is located:\n<pre><code>$ docker build -t &lt;image-name&gt;:&lt;tag&gt; --target release .\n</code></pre>\n</li>\n</ul>\n</li>\n<li>\n<h4>Docker Hub</h4>\n<ul>\n<li>You can get the docker image that have been built from the following repository.\n<pre><code>deeva2019/n2dit:&lt;version&gt;\n</code></pre>\n<ul>\n<li>version\n<ul>\n<li>CLI version: X.X.X</li>\n<li>GUI version: gui-X.X.X</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Excecution</h2>\n<h3>n2dit</h3>\n<ul>\n<li>\n<h4>Training</h4>\n<ul>\n<li>Set options via n2dit command options or configuration files.\n<ul>\n<li>Sample configuration files are located in the config directory.\n<pre><code>[cyclegan]\ndirA = ./datasets/day\ndirB = ./datasets/night\nresults_dir = ./results\nload_size = 286\ncrop_size = 256\nchannels = 3\nshuffle = yes\npool_size = 50\nexp_name = test\ncontinue = no\nepoch = 0\nniter = 100\nniter_decay = 100\ndisp_loss_freq = 10\ndisp_summary_freq = 100\nlearning_rate = 0.0002\nlambda_A = 10.0\nlambda_B = 10.0\nlambda_idt = 0.5\nupsample = resize_conv\n</code></pre>\n</li>\n</ul>\n</li>\n<li>Run with the following command.\n<pre><code>$ n2dit cyclegan train -C config/sample_train.cfg\n</code></pre>\n</li>\n</ul>\n</li>\n<li>\n<h4>Test</h4>\n<ul>\n<li>Config option values through the n2dit command options or sample configuration file as shown above.</li>\n<li>Run with the following command.\n<pre><code>$ n2dit cyclegan test -C config/sample_test.cfg\n</code></pre>\n</li>\n</ul>\n</li>\n<li>\n<h4>Guide to using docker</h4>\n<ul>\n<li>Create a container with the following command to use the host's GPUs.\n<pre><code>$ docker run --gpus all &lt;image-name&gt;:&lt;tag&gt; &lt;n2dit options&gt;\n</code></pre>\n</li>\n<li>The file used by n2dit corresponds to the following options.\n<ul>\n<li>dirA: Source domain image directory you want to transform</li>\n<li>dirB: Target domain image directory</li>\n<li>results_dir: Common results directory used for training and testing</li>\n<li>(Optional) config_file: Configuration file</li>\n</ul>\n</li>\n<li>So unless you use n2dit inside the container, use the option v to bind the host volume to the container as follows.\n<pre><code>$ docker run --gpus all -v /home/deeva/datasets/A/:/work/A\\\n                        -v /home/deeva/datasets/B/:/work/B\\\n                        -v /home/deeva/results/:/work/results\\\n                        n2dit:latest cyclegan train --dirA /work/A --dirB /work/B --results_dir /work/results\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h3>n2dit GUI</h3>\n<ul>\n<li>(Optional) To use the host's GUI inside the container, The script uses a docker option that exposes xhost to render to the display by reading and writing X11 Unix sockets.\n<ul>\n<li>To use GUI's, use the following command. However, This approach is very vulnerable.\n<pre><code>$ xhost +local:root\n</code></pre>\n</li>\n</ul>\n</li>\n<li>When you are finished with the container's GUI, run the following command.\n<pre><code>$ xhost -local:root\n</code></pre>\n</li>\n<li>Use easily.</li>\n</ul>\n<div>\n<kbd>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1046814db6fb11d27ac47b28cf5d3b462fb6673d/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d316e684834455a765f4b664638384f4f6c4d712d5766493158444a303730784270\" width=\"600\">\n</kbd>\n</div>\n<br>\n<div>\n<kbd>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/36f73814a113e360cf837dd1a10e8737832b12d9/68747470733a2f2f64726976652e676f6f676c652e636f6d2f75633f6578706f72743d766965772669643d3174336373706935486674334b6c5855374e2d7432703574497a31536338567979\" width=\"600\">\n</kbd>\n</div>\n<br>\n\n          </div>"}, "last_serial": 6246259, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "668ef7e61fdbc92817214ff8557cbd8b", "sha256": "682a87c0bafdef2e3eb6ebeb32f00cd0164424f17bc1ae3d379990d077aabdf2"}, "downloads": -1, "filename": "n2dit-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "668ef7e61fdbc92817214ff8557cbd8b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 22588, "upload_time": "2019-12-05T08:53:35", "upload_time_iso_8601": "2019-12-05T08:53:35.616697Z", "url": "https://files.pythonhosted.org/packages/f2/5c/54efe853f24c44db064b08e3f653d60e3b215dd9df2e5f5741a7908ce26c/n2dit-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1d9962b3e60c2415d27e912e5b3e9262", "sha256": "8a7487c8f2d9ef93f9cbfe51aae8618bd73caa39447d1fe2c9305bc8b0a6a66e"}, "downloads": -1, "filename": "n2dit-1.0.0.tar.gz", "has_sig": false, "md5_digest": "1d9962b3e60c2415d27e912e5b3e9262", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 23908, "upload_time": "2019-12-05T08:53:37", "upload_time_iso_8601": "2019-12-05T08:53:37.852989Z", "url": "https://files.pythonhosted.org/packages/dc/6f/df993e81513866524ec86526d2450ed13a6b952257ce239e78149e6516d4/n2dit-1.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "668ef7e61fdbc92817214ff8557cbd8b", "sha256": "682a87c0bafdef2e3eb6ebeb32f00cd0164424f17bc1ae3d379990d077aabdf2"}, "downloads": -1, "filename": "n2dit-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "668ef7e61fdbc92817214ff8557cbd8b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 22588, "upload_time": "2019-12-05T08:53:35", "upload_time_iso_8601": "2019-12-05T08:53:35.616697Z", "url": "https://files.pythonhosted.org/packages/f2/5c/54efe853f24c44db064b08e3f653d60e3b215dd9df2e5f5741a7908ce26c/n2dit-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1d9962b3e60c2415d27e912e5b3e9262", "sha256": "8a7487c8f2d9ef93f9cbfe51aae8618bd73caa39447d1fe2c9305bc8b0a6a66e"}, "downloads": -1, "filename": "n2dit-1.0.0.tar.gz", "has_sig": false, "md5_digest": "1d9962b3e60c2415d27e912e5b3e9262", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 23908, "upload_time": "2019-12-05T08:53:37", "upload_time_iso_8601": "2019-12-05T08:53:37.852989Z", "url": "https://files.pythonhosted.org/packages/dc/6f/df993e81513866524ec86526d2450ed13a6b952257ce239e78149e6516d4/n2dit-1.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:54 2020"}