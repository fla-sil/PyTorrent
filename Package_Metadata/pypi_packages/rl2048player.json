{"info": {"author": "aar015", "author_email": "avirlincoln@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6"], "description": "# Reinforcement Learning to Play 2048\nThis project trains agents to play the game 2048 using Q, SARSA, and TD0 learning agents. \n\n## Installation\nThis project is pip installable. On linux it can be installed by entering, ```pip3 install rl2048player``` into your terminal. For other operating systems, follow the pip instructions for that system. \n\n## Features\nIn order to use this package, you must first import it using\n```\nimport rl2048player as rl\n```\nNow, I will review some of the main features of this package. For most of these features, there are addditional options not explained in this review. All options for any command can be found by reviewing the documentation in the code. \n\n### Masks\nPefore creating a learning agent, you must initialize a mask. Masks translate between the game board and the learning agent. This allows you to change the way the agent understands the game board without changing the agent itself. Currently, only one mask has been implemented. This mask breaks down the board into rows, columns, and 2x2 squares. This has the effect of  decoupling parts of the board that do not interact strongly with each other. The mask can be initialized using the code\n```\nmask = rl.masks.Mask_rxcx4()\n```\n\n### Agents\nOnce the mask has been initialized, you can initialize the agents. Currently, there are three agents that have been implemented: one using a Q learning algorithm, one using a SARSA learning algorithm, and one using a TD0 learning algorithm. These agents can be initialized by\n```\nagent = rl.agents.QAgent(mask)\nagent = rl.agents.SARSAAgent(mask)\nagent = rl.agents.TD0Agent(mask)\n```\nrespectively. There are also options to change the agent's hyperparameters when initializing them. The default values are the hyperparameters I have found to work best. \n\n#### Training\nTo train the agents over x number of games and record the scores use the code\n```\nscores = agent.train(x)\n```\nYou can then look at a graph of those scores by \n```\nagent.makeGraph(scores)\nplt.show()\n```\nThere are also options to automatically save the scores and graph to a file. \n\n#### Making Gif's\nOnce the agent has been trained, you can create a gif of the agent playing a game and save it to a file by using the command\n```\nagent.makeGif(gif_file)\n```\n\n#### Saving and Loading\nYou can also save trained agent by using\n```\nagent.save(agent_file)\n```\nThis save function only saves the look-up table for this agent, not the whole object. In order to load the agent, you need to initialize an agent with the same mask and hyperparameters and then load the look-up table using\n```\nagent = rl.agents.AppropriateAgent(AppropriateMask)\nagent.load(agent_file)\n```\n\n### Examples\nIf you don't want to have to do all of this to use this package, there are some examples precoded in the package. Currently there are two examples that can be accessed using\n```\nrl.examples.example1()\nrl.examples.example2()\n```\n\n## Acknowledgments\nThe game logic for this project is based on code by georgwiese which can be found at https://github.com/georgwiese/2048-rl. The implementation of the learning agents is based on method described in the paper Temporal difference learning of n-tuple networks for the game 2048, by M.Szubertand W. Jaskowski. All code is my original work except when noted. \n\n## Future Work\nIn the future, I plan to work on implementing additional masks and learning agents. \n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/aar015/rl-2048-player", "keywords": "2048 RL", "license": "", "maintainer": "", "maintainer_email": "", "name": "rl2048player", "package_url": "https://pypi.org/project/rl2048player/", "platform": "", "project_url": "https://pypi.org/project/rl2048player/", "project_urls": {"Homepage": "https://github.com/aar015/rl-2048-player"}, "release_url": "https://pypi.org/project/rl2048player/0.0.3/", "requires_dist": ["imageio", "matplotlib", "numpy", "opencv-python"], "requires_python": "", "summary": "Reinforcement Learning Agent capble of playing 2048", "version": "0.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Reinforcement Learning to Play 2048</h1>\n<p>This project trains agents to play the game 2048 using Q, SARSA, and TD0 learning agents.</p>\n<h2>Installation</h2>\n<p>This project is pip installable. On linux it can be installed by entering, <code>pip3 install rl2048player</code> into your terminal. For other operating systems, follow the pip instructions for that system.</p>\n<h2>Features</h2>\n<p>In order to use this package, you must first import it using</p>\n<pre><code>import rl2048player as rl\n</code></pre>\n<p>Now, I will review some of the main features of this package. For most of these features, there are addditional options not explained in this review. All options for any command can be found by reviewing the documentation in the code.</p>\n<h3>Masks</h3>\n<p>Pefore creating a learning agent, you must initialize a mask. Masks translate between the game board and the learning agent. This allows you to change the way the agent understands the game board without changing the agent itself. Currently, only one mask has been implemented. This mask breaks down the board into rows, columns, and 2x2 squares. This has the effect of  decoupling parts of the board that do not interact strongly with each other. The mask can be initialized using the code</p>\n<pre><code>mask = rl.masks.Mask_rxcx4()\n</code></pre>\n<h3>Agents</h3>\n<p>Once the mask has been initialized, you can initialize the agents. Currently, there are three agents that have been implemented: one using a Q learning algorithm, one using a SARSA learning algorithm, and one using a TD0 learning algorithm. These agents can be initialized by</p>\n<pre><code>agent = rl.agents.QAgent(mask)\nagent = rl.agents.SARSAAgent(mask)\nagent = rl.agents.TD0Agent(mask)\n</code></pre>\n<p>respectively. There are also options to change the agent's hyperparameters when initializing them. The default values are the hyperparameters I have found to work best.</p>\n<h4>Training</h4>\n<p>To train the agents over x number of games and record the scores use the code</p>\n<pre><code>scores = agent.train(x)\n</code></pre>\n<p>You can then look at a graph of those scores by</p>\n<pre><code>agent.makeGraph(scores)\nplt.show()\n</code></pre>\n<p>There are also options to automatically save the scores and graph to a file.</p>\n<h4>Making Gif's</h4>\n<p>Once the agent has been trained, you can create a gif of the agent playing a game and save it to a file by using the command</p>\n<pre><code>agent.makeGif(gif_file)\n</code></pre>\n<h4>Saving and Loading</h4>\n<p>You can also save trained agent by using</p>\n<pre><code>agent.save(agent_file)\n</code></pre>\n<p>This save function only saves the look-up table for this agent, not the whole object. In order to load the agent, you need to initialize an agent with the same mask and hyperparameters and then load the look-up table using</p>\n<pre><code>agent = rl.agents.AppropriateAgent(AppropriateMask)\nagent.load(agent_file)\n</code></pre>\n<h3>Examples</h3>\n<p>If you don't want to have to do all of this to use this package, there are some examples precoded in the package. Currently there are two examples that can be accessed using</p>\n<pre><code>rl.examples.example1()\nrl.examples.example2()\n</code></pre>\n<h2>Acknowledgments</h2>\n<p>The game logic for this project is based on code by georgwiese which can be found at <a href=\"https://github.com/georgwiese/2048-rl\" rel=\"nofollow\">https://github.com/georgwiese/2048-rl</a>. The implementation of the learning agents is based on method described in the paper Temporal difference learning of n-tuple networks for the game 2048, by M.Szubertand W. Jaskowski. All code is my original work except when noted.</p>\n<h2>Future Work</h2>\n<p>In the future, I plan to work on implementing additional masks and learning agents.</p>\n\n          </div>"}, "last_serial": 5339905, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "b73dc540e066304c9463903d1742439d", "sha256": "0ce0ed3c64877ffa8086f03b69fc23813c1651a444bac0199287747fa78164b2"}, "downloads": -1, "filename": "rl2048player-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b73dc540e066304c9463903d1742439d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8582, "upload_time": "2019-05-25T20:57:49", "upload_time_iso_8601": "2019-05-25T20:57:49.176713Z", "url": "https://files.pythonhosted.org/packages/f8/48/72a587327960153ab00bfb31078c5959e97f21897b309a8bb392fa0aa56c/rl2048player-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fd34a76744c92fa061afb369d514f5c6", "sha256": "e09a04e64412aaf02ce7cddfb87ff330bde22e63a8604dcb56adef08ddba27a4"}, "downloads": -1, "filename": "rl2048player-0.0.1.tar.gz", "has_sig": false, "md5_digest": "fd34a76744c92fa061afb369d514f5c6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6365, "upload_time": "2019-05-25T20:57:51", "upload_time_iso_8601": "2019-05-25T20:57:51.213489Z", "url": "https://files.pythonhosted.org/packages/11/2e/1b990385f02535c8c9253163f0a25781e43bca96b3cf4f9b2e1d204ca789/rl2048player-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "d526dedfd7a4b20be83a79a96abb4f30", "sha256": "3eb16fbc77aa20a874eb96afbda5e6b40b51f4adf4a397354275a5b257050040"}, "downloads": -1, "filename": "rl2048player-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "d526dedfd7a4b20be83a79a96abb4f30", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8591, "upload_time": "2019-05-25T21:18:29", "upload_time_iso_8601": "2019-05-25T21:18:29.127905Z", "url": "https://files.pythonhosted.org/packages/72/6c/304d24f4ce3b790ce6e73168add76806dfabc7065051ebe7896262dafa5b/rl2048player-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b503018ec0818dcce233e36e446c3264", "sha256": "655f20ea2bbd7bb7b9b9bf6aa175a65c23646721b7a4bae2a25277ab259fa2cc"}, "downloads": -1, "filename": "rl2048player-0.0.2.tar.gz", "has_sig": false, "md5_digest": "b503018ec0818dcce233e36e446c3264", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6372, "upload_time": "2019-05-25T21:18:30", "upload_time_iso_8601": "2019-05-25T21:18:30.461550Z", "url": "https://files.pythonhosted.org/packages/94/4c/6008ef252e8e1c26e22c30d8c26cf375cee8bae9eaca8038fd71a9262fc4/rl2048player-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "a9f1b7cc27cb12c6bca36a15acb3de42", "sha256": "77c2a8a8e09cae53ab9c9d41ef60fd7970ecf2a9fbe756d3b94a20aa8e4de45b"}, "downloads": -1, "filename": "rl2048player-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "a9f1b7cc27cb12c6bca36a15acb3de42", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 12118, "upload_time": "2019-05-30T22:16:57", "upload_time_iso_8601": "2019-05-30T22:16:57.814773Z", "url": "https://files.pythonhosted.org/packages/5a/46/8875933528cff82e1c07ce927a41bea0993e0fa1b77659341dca22cbaf5d/rl2048player-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dd3df7144ecba66ef31903b78371ad15", "sha256": "d59425f86fec4a73d648e82faaafcb96aab9911e097d9b580e1d70a623609469"}, "downloads": -1, "filename": "rl2048player-0.0.3.tar.gz", "has_sig": false, "md5_digest": "dd3df7144ecba66ef31903b78371ad15", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10990, "upload_time": "2019-05-30T22:17:00", "upload_time_iso_8601": "2019-05-30T22:17:00.075204Z", "url": "https://files.pythonhosted.org/packages/ae/e3/74b3d0a67a87edb8c46b72a6888c65a985ae369e86c6e5db8853cd7c3c94/rl2048player-0.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a9f1b7cc27cb12c6bca36a15acb3de42", "sha256": "77c2a8a8e09cae53ab9c9d41ef60fd7970ecf2a9fbe756d3b94a20aa8e4de45b"}, "downloads": -1, "filename": "rl2048player-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "a9f1b7cc27cb12c6bca36a15acb3de42", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 12118, "upload_time": "2019-05-30T22:16:57", "upload_time_iso_8601": "2019-05-30T22:16:57.814773Z", "url": "https://files.pythonhosted.org/packages/5a/46/8875933528cff82e1c07ce927a41bea0993e0fa1b77659341dca22cbaf5d/rl2048player-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dd3df7144ecba66ef31903b78371ad15", "sha256": "d59425f86fec4a73d648e82faaafcb96aab9911e097d9b580e1d70a623609469"}, "downloads": -1, "filename": "rl2048player-0.0.3.tar.gz", "has_sig": false, "md5_digest": "dd3df7144ecba66ef31903b78371ad15", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10990, "upload_time": "2019-05-30T22:17:00", "upload_time_iso_8601": "2019-05-30T22:17:00.075204Z", "url": "https://files.pythonhosted.org/packages/ae/e3/74b3d0a67a87edb8c46b72a6888c65a985ae369e86c6e5db8853cd7c3c94/rl2048player-0.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:02:20 2020"}