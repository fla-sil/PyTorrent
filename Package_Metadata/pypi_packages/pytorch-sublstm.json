{"info": {"author": "Russi Chatterjee", "author_email": "root@ixaxaar.in", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# subtractive LSTM (subLSTM), for Pytorch\n\n[![Build Status](https://travis-ci.org/ixaxaar/pytorch-sublstm.svg?branch=master)](https://travis-ci.org/ixaxaar/pytorch-sublstm) [![PyPI version](https://badge.fury.io/py/pytorch-sublstm.svg)](https://badge.fury.io/py/pytorch-sublstm)\n\nThis is an implementation of subLSTM described in the paper [Cortical microcircuits as gated-recurrent neural networks, Rui Ponte Costa et al.](https://arxiv.org/abs/1711.02448)\n\n## Install\n\n```bash\npip install pytorch-sublstm\n```\n\n\n## Usage\n\n**Parameters**:\n\nFollowing are the constructor parameters:\n\n| Argument | Default | Description |\n| --- | --- | --- |\n| input_size | `None` | Size of the input vectors |\n| hidden_size | `None` | Size of hidden units |\n| num_layers | `1` | Number of layers in the network |\n| bias | `True` | Bias |\n| batch_first | `False` | Whether data is fed batch first |\n| dropout | `0` | Dropout between layers in the network |\n| bidirectional | `False` | If the network is bidirectional |\n\n\n### Example usage:\n\n#### nn Interface\n```python\nimport torch\nfrom torch.autograd import Variable\nfrom subLSTM.nn import SubLSTM\n\nhidden_size = 20\ninput_size = 10\nseq_len = 5\nbatch_size = 7\nhidden = None\n\ninput = Variable(torch.randn(batch_size, seq_len, input_size))\n\nrnn = SubLSTM(input_size, hidden_size, num_layers=2, bias=True, batch_first=True)\n\n# forward pass\noutput, hidden = rnn(input, hidden)\n```\n\n#### Cell Interface\n\n```python\nimport torch\nfrom torch.autograd import Variable\nfrom subLSTM.nn import SubLSTMCell\n\nhidden_size = 20\ninput_size = 10\nseq_len = 5\nbatch_size = 7\nhidden = None\n\nhx = Variable(torch.randn(batch_size, hidden_size))\ncx = Variable(torch.randn(batch_size, hidden_size))\n\ninput = Variable(torch.randn(batch_size, input_size))\n\ncell = SubLSTMCell(input_size, hidden_size, bias=True)\n(hx, cx) = cell(input, (hx, cx))\n```\n\n### Tasks:\n\nA language modeling task is included [here](./tasks/word_language_model/).\nRefer to its [README](./tasks/word_language_model/README.md) for more info.\n\n\n### Attributions:\n\nA lot of the code is recycled from [pytorch](https://pytorch.org)\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ixaxaar/pytorch-sublstm", "keywords": "cortical microcircuit pytorch sublstm", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "pytorch-sublstm", "package_url": "https://pypi.org/project/pytorch-sublstm/", "platform": "", "project_url": "https://pypi.org/project/pytorch-sublstm/", "project_urls": {"Homepage": "https://github.com/ixaxaar/pytorch-sublstm"}, "release_url": "https://pypi.org/project/pytorch-sublstm/0.0.2/", "requires_dist": ["numpy", "torch", "check-manifest; extra == 'dev'", "coverage; extra == 'test'"], "requires_python": ">=3", "summary": "Differentiable Neural Computer, for Pytorch", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            # subtractive LSTM (subLSTM), for Pytorch<br><br>[![Build Status](https://travis-ci.org/ixaxaar/pytorch-sublstm.svg?branch=master)](https://travis-ci.org/ixaxaar/pytorch-sublstm) [![PyPI version](https://badge.fury.io/py/pytorch-sublstm.svg)](https://badge.fury.io/py/pytorch-sublstm)<br><br>This is an implementation of subLSTM described in the paper [Cortical microcircuits as gated-recurrent neural networks, Rui Ponte Costa et al.](https://arxiv.org/abs/1711.02448)<br><br>## Install<br><br>```bash<br>pip install pytorch-sublstm<br>```<br><br><br>## Usage<br><br>**Parameters**:<br><br>Following are the constructor parameters:<br><br>| Argument | Default | Description |<br>| --- | --- | --- |<br>| input_size | `None` | Size of the input vectors |<br>| hidden_size | `None` | Size of hidden units |<br>| num_layers | `1` | Number of layers in the network |<br>| bias | `True` | Bias |<br>| batch_first | `False` | Whether data is fed batch first |<br>| dropout | `0` | Dropout between layers in the network |<br>| bidirectional | `False` | If the network is bidirectional |<br><br><br>### Example usage:<br><br>#### nn Interface<br>```python<br>import torch<br>from torch.autograd import Variable<br>from subLSTM.nn import SubLSTM<br><br>hidden_size = 20<br>input_size = 10<br>seq_len = 5<br>batch_size = 7<br>hidden = None<br><br>input = Variable(torch.randn(batch_size, seq_len, input_size))<br><br>rnn = SubLSTM(input_size, hidden_size, num_layers=2, bias=True, batch_first=True)<br><br># forward pass<br>output, hidden = rnn(input, hidden)<br>```<br><br>#### Cell Interface<br><br>```python<br>import torch<br>from torch.autograd import Variable<br>from subLSTM.nn import SubLSTMCell<br><br>hidden_size = 20<br>input_size = 10<br>seq_len = 5<br>batch_size = 7<br>hidden = None<br><br>hx = Variable(torch.randn(batch_size, hidden_size))<br>cx = Variable(torch.randn(batch_size, hidden_size))<br><br>input = Variable(torch.randn(batch_size, input_size))<br><br>cell = SubLSTMCell(input_size, hidden_size, bias=True)<br>(hx, cx) = cell(input, (hx, cx))<br>```<br><br>### Tasks:<br><br>A language modeling task is included [here](./tasks/word_language_model/).<br>Refer to its [README](./tasks/word_language_model/README.md) for more info.<br><br><br>### Attributions:<br><br>A lot of the code is recycled from [pytorch](https://pytorch.org)<br><br><br>\n          </div>"}, "last_serial": 3339331, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "6decbdc66eccab9af109d731c8e20182", "sha256": "1e4b594e15d169e66cab09346dc7c9d08e19c0faac81f83f4d80b0e69e806092"}, "downloads": -1, "filename": "pytorch_sublstm-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "6decbdc66eccab9af109d731c8e20182", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 8539, "upload_time": "2017-11-16T11:46:35", "upload_time_iso_8601": "2017-11-16T11:46:35.528879Z", "url": "https://files.pythonhosted.org/packages/b4/54/7f39f3f2ce49158ab456b181039d4a3b51255ac4e74bf196bbebd537eb29/pytorch_sublstm-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "39847113c703897e90e45a3f2ac0b7dd", "sha256": "8e95c95a42d9269f8a680a70832db640ce41f6920c0b644dea12b7a6e613b665"}, "downloads": -1, "filename": "pytorch-sublstm-0.0.1.tar.gz", "has_sig": false, "md5_digest": "39847113c703897e90e45a3f2ac0b7dd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 6097, "upload_time": "2017-11-16T11:46:36", "upload_time_iso_8601": "2017-11-16T11:46:36.980543Z", "url": "https://files.pythonhosted.org/packages/c3/5e/bafa3725e6e6f1aa5721e17a3834b0a6f5082871da08cc95a02bcd55760e/pytorch-sublstm-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "4a6608ceecc7eed496be3b634d7cda30", "sha256": "d4aa45da82362f7907ab78f78211eca1101f8c7d88ccce63d4f47f2922e84c66"}, "downloads": -1, "filename": "pytorch_sublstm-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "4a6608ceecc7eed496be3b634d7cda30", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 8235, "upload_time": "2017-11-16T17:28:37", "upload_time_iso_8601": "2017-11-16T17:28:37.336001Z", "url": "https://files.pythonhosted.org/packages/08/a5/59a7ff047c5fdaa38f7cc3dd2769a13b1be22661ac1d1c26ff86977c5052/pytorch_sublstm-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dea087cdc44fe8b720f37d9e60ab015b", "sha256": "cc87001bab328d50065b005e8ed7d972da68747718d1f803413ad5e4e1b0c970"}, "downloads": -1, "filename": "pytorch-sublstm-0.0.2.tar.gz", "has_sig": false, "md5_digest": "dea087cdc44fe8b720f37d9e60ab015b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 5929, "upload_time": "2017-11-16T17:28:38", "upload_time_iso_8601": "2017-11-16T17:28:38.896236Z", "url": "https://files.pythonhosted.org/packages/98/72/3265439b8fa84f4ad96f22b7a2387113f3baced1794b41db5dab77e7b06f/pytorch-sublstm-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "4a6608ceecc7eed496be3b634d7cda30", "sha256": "d4aa45da82362f7907ab78f78211eca1101f8c7d88ccce63d4f47f2922e84c66"}, "downloads": -1, "filename": "pytorch_sublstm-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "4a6608ceecc7eed496be3b634d7cda30", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 8235, "upload_time": "2017-11-16T17:28:37", "upload_time_iso_8601": "2017-11-16T17:28:37.336001Z", "url": "https://files.pythonhosted.org/packages/08/a5/59a7ff047c5fdaa38f7cc3dd2769a13b1be22661ac1d1c26ff86977c5052/pytorch_sublstm-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dea087cdc44fe8b720f37d9e60ab015b", "sha256": "cc87001bab328d50065b005e8ed7d972da68747718d1f803413ad5e4e1b0c970"}, "downloads": -1, "filename": "pytorch-sublstm-0.0.2.tar.gz", "has_sig": false, "md5_digest": "dea087cdc44fe8b720f37d9e60ab015b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 5929, "upload_time": "2017-11-16T17:28:38", "upload_time_iso_8601": "2017-11-16T17:28:38.896236Z", "url": "https://files.pythonhosted.org/packages/98/72/3265439b8fa84f4ad96f22b7a2387113f3baced1794b41db5dab77e7b06f/pytorch-sublstm-0.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:13:43 2020"}