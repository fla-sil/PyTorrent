{"info": {"author": "Max Halford", "author_email": "maxhalford25@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: BSD License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy"], "description": "\n<p align=\"center\">\n  <img height=\"200px\" src=\"https://docs.google.com/drawings/d/e/2PACX-1vQ0AFza3nHkrhe0Fam_NAZF5wgGzskKTV5To4cfHAmrCuhr3cZnJiZ3pD1OfXVP72A435b5IlsduoQC/pub?w=580&h=259\" alt=\"chantilly_logo\">\n</p>\n\n<p align=\"center\">\n  <!-- Travis -->\n  <a href=\"https://travis-ci.org/creme-ml/chantilly\">\n    <img src=\"https://img.shields.io/travis/creme-ml/chantilly/master.svg?style=flat-square\" alt=\"travis\">\n  </a>\n  <!-- Codecov -->\n  <a href=\"https://codecov.io/gh/creme-ml/chantilly\">\n    <img src=\"https://img.shields.io/codecov/c/gh/creme-ml/chantilly.svg?style=flat-square\" alt=\"codecov\">\n  </a>\n  <!-- Gitter -->\n  <a href=\"https://gitter.im/creme-ml/community?utm_source=share-link&utm_medium=link&utm_campaign=share-link\">\n    <img src=\"https://img.shields.io/gitter/room/creme-ml/community?color=blueviolet&style=flat-square\" alt=\"gitter\">\n  </a>\n  <!-- PyPI -->\n  <a href=\"https://pypi.org/project/chantilly\">\n    <img src=\"https://img.shields.io/pypi/v/chantilly.svg?label=release&color=blue&style=flat-square\" alt=\"pypi\">\n  </a>\n  <!-- License -->\n  <a href=\"https://opensource.org/licenses/BSD-3-Clause\">\n    <img src=\"https://img.shields.io/badge/License-BSD%203--Clause-blue.svg?style=flat-square\" alt=\"bsd_3_license\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <code>chantilly</code> is a deployment tool for <a href=\"https://www.wikiwand.com/en/Online_machine_learning\">online machine learning</a> models. It is designed to work hand in hand with <a href=\"https://github.com/creme-ml/creme\"><code>creme</code></a>.\n</p>\n\n## Table of contents\n\n- [Table of contents](#table-of-contents)\n- [Introduction](#introduction)\n- [Installation](#installation)\n- [User guide](#user-guide)\n  - [Running the server](#running-the-server)\n  - [Picking a flavor](#picking-a-flavor)\n  - [Uploading a model](#uploading-a-model)\n  - [Making a prediction](#making-a-prediction)\n  - [Updating the model](#updating-the-model)\n  - [Monitoring metrics](#monitoring-metrics)\n  - [Monitoring events](#monitoring-events)\n  - [Visual monitoring](#visual-monitoring)\n  - [Usage statistics](#usage-statistics)\n  - [Using multiple models](#using-multiple-models)\n  - [Configuration handling](#configuration-handling)\n  - [Using a different storage backend](#using-a-different-storage-backend)\n    - [Redis](#redis)\n  - [Importing libraries](#importing-libraries)\n  - [Deployment](#deployment)\n- [Examples](#examples)\n- [Development](#development)\n- [Roadmap](#roadmap)\n- [Technical stack](#technical-stack)\n- [Similar alternatives](#similar-alternatives)\n- [License](#license)\n\n## Introduction\n\nThere are many tools for deploying machine learning models. However, none of them support **online models that can learn on the fly**, but `chantilly` does.\n\nHere are some advantages:\n\n- **Simple**: `chantilly` is essentially just a Flask app.\n- **Straightforward**: you provide a model, `chantilly` provides a few API routes to do the rest.\n- **Convenient**: `chantilly` rids you of the burden of storing features between prediction and learning steps.\n- **Flexible**: use any model you wish, as long as it's written in Python and implements a couple of required methods.\n\nNote that `chantilly` is very young, and is therefore subject to evolve. We're also eager for feedback and are happy to work hand in hand with you if you have specific needs.\n\n## Installation\n\n`chantilly` is intended to work with **Python 3.7 or above**. You can install it from PyPI:\n\n```sh\n> pip install chantilly\n```\n\nYou can also install the latest development version as so:\n\n```sh\n> pip install git+https://github.com/creme-ml/chantilly\n```\n\n## User guide\n\n### Running the server\n\nOnce you've followed the installation step, you'll get access to the `chantilly` CLI. You can see the available commands by running `chantilly --help`. You can start a server with the `run` command:\n\n```sh\n> chantilly run\n```\n\nThis will start a [Flask](https://flask.palletsprojects.com/en/1.0.x/) server with all the necessary routes for uploading a model, training it, making predictions with it, and monitoring it. By default, the server will be accessible at [`localhost:5000`](http://localhost:5000), which is what we will be using in the rest of the examples in the user guide. You can run `chantilly routes` in order to see all the available routes.\n\n### Picking a flavor\n\nThe first thing you need to do is pick a flavor. Currently, the available flavors are:\n\n- `regression` for regression tasks.\n- `binary` for binary classification tasks.\n- `multiclass` for multi-class classification tasks.\n\nYou can set the flavor by sending a POST request to `@/api/init`, as so:\n\n```py\nimport requests\n\nconfig = {'flavor': 'regression'}\nrequests.post('http://localhost:5000/api/init', json=config)\n```\n\nYou can also set the flavor via the CLI:\n\n```py\n> chantilly init regression\n```\n\n:warning: Setting the flavor will erase everything and thus provide a clean slate.\n\nYou can view the current flavor by sending a GET request to `@/api/init`:\n\n```py\nr = requests.get('http://localhost:5000/api/init')\nprint(r.json()['flavor'])\n```\n\n### Uploading a model\n\nYou can upload a model by sending a POST request to the `@/api/model` route. You need to provide a model which has been serialized with [`pickle`](https://docs.python.org/3/library/pickle.html) or [`dill`](https://dill.readthedocs.io/en/latest/dill.html) (we recommend the latter). For example:\n\n```py\nfrom creme import compose\nfrom creme import linear_model\nfrom creme import preprocessing\nimport dill\nimport requests\n\nmodel = compose.Pipeline(\n    preprocessing.StandardScaler(),\n    linear_model.LinearRegression()\n)\n\nrequests.post('http://localhost:5000/api/model', data=dill.dumps(model))\n```\n\nLikewise, the model can be retrieved by sending a GET request to `@/api/model`:\n\n```py\nr = requests.get('http://localhost:5000/api/model')\nmodel = pickle.loads(r.content)\n```\n\nNote that `chantilly` will validate the model you provide to make sure it works with the flavor you picked. For instance, if you picked the `regression` flavor, then the model has to implement `fit_one` and `predict_one`.\n\nYou can also add a upload by using the CLI. First, you need to serialize a model and dump it to a file:\n\n```py\nwith open('model.pkl', 'wb') as file:\n    dill.dump(model, file)\n```\n\nThen, call the `add-model` sub-command:\n\n```sh\n> chantilly add-model model.pkl\n```\n\n### Making a prediction\n\nPredictions can be obtained by sending a POST request to `@/api/predict`. The payload you send has to contain a field named `features`. The value of this field will be passed to the `predict_one` method of the model you uploaded earlier on. If the model you provided `predict_proba_one` then that will be used instead. Here is an example:\n\n```py\nimport requests\n\nr = requests.post('http://localhost:5000/api/predict', json={\n    'id': 42,\n    'features': {\n        'shop': 'Ikea',\n        'item': 'Domb\u00e4s',\n        'date': '2020-03-22T10:42:29Z'\n    }\n})\n\nprint(r.json()['prediction'])\n```\n\nNote that in the previous snippet we've also provided an `id` field. This field is optional. If is is provided, then the features will be stored by the `chantilly` server, along with the prediction. This allows not having to provide the features again when you want to update the model later on.\n\n### Updating the model\n\nThe model can be updated by sending a POST request to `@/api/learn`. If you've provided an ID in an earlier call to `@/api/predict`, then you only have to provide said ID along with the ground truth:\n\n```py\nimport requests\n\nrequests.post('http://localhost:5000/api/learn', json={\n    'id': 42,\n    'ground_truth': 10.21\n})\n```\n\nHowever, if you haven't passed an ID earlier on, then you have to provide the features yourself:\n\n```py\nrequests.post('http://localhost:5000/api/learn', json={\n    'features': {\n        'shop': 'Ikea',\n        'item': 'Domb\u00e4s',\n        'date': '2020-03-22T10:42:29Z'\n    },\n    'ground_truth': 10.21\n})\n```\n\nNote that the `id` field will have precedence in case both of `id` and `features` are provided. We highly recommend you to use the `id` field. First of all it means that you don't have to take care of storing the features between calls to `@/api/predict` and `@/api/learn`. Secondly it makes the metrics more reliable because they will be using the predictions that were made at the time `@/api/predict` was called.\n\n### Monitoring metrics\n\nYou can access the current metrics via a GET request to the `@/api/metrics` route.\n\nAdditionally, you can access a stream of metric updates by using the `@/api/stream/metrics`. This is a streaming route which implements [server-sent events (SSE)](https://www.wikiwand.com/en/Server-sent_events). As such it will notify listeners every time the metrics are updates. For instance, you can use the [`sseclient`](https://github.com/btubbs/sseclient) library to monitor the metrics from a Python script:\n\n```py\nimport json\nimport sseclient\n\nmessages = sseclient.SSEClient('http://localhost:5000/api/stream/metrics')\n\nfor msg in messages:\n    metrics = json.loads(msg.data)\n    print(metrics)\n```\n\nYou can use the following piece of JavaScript to do the same thing in a browser:\n\n```js\nvar es = new EventSource('http://localhost:5000/api/stream/metrics');\nes.onmessage = e => {\n    var metrics = JSON.parse(e.data);\n    console.log(metrics)\n};\n```\n\n### Monitoring events\n\nYou can also listen to all the prediction and learning events via the `@/api/stream/events` route. This will yield SSE events with an event name attached, which is either 'predict' or 'learn'. From a Python interpreter, you can do the following:\n\n```py\nimport json\nimport sseclient\n\nmessages = sseclient.SSEClient('http://localhost:5000/api/stream/events')\n\nfor msg in messages:\n    data = json.loads(msg.data)\n    if msg.event == 'learn':\n        print(data['model'], data['features'], data['prediction'], data['ground_truth'])\n    else:\n        print(data['model'], data['features'], data['prediction'])\n```\n\nIn JavaScript, you can you use the [`addEventListener`](https://developer.mozilla.org/en-US/docs/Web/API/EventTarget/addEventListener) method:\n\n```js\nvar es = new EventSource('http://localhost:5000/api/stream/events');\n\nes.addEventListener('learn', = e => {\n    var data = JSON.parse(e.data);\n    console.log(data.model, data.features, data.prediction, data.ground_truth)\n};\n\nes.addEventListener('predict', = e => {\n    var data = JSON.parse(e.data);\n    console.log(data.model, data.features, data.prediction)\n};\n```\n\n### Visual monitoring\n\nA live dashboard is accessible if you navigate to [`localhost:5000`](http://localhost:5000) in your browser.\n\n<p align=\"center\">\n  <img src=\"demo.gif\" alt=\"demo\">\n</p>\n\nUnder the hood the dashboard is simply listening to the API's streaming routes.\n\n### Usage statistics\n\nYou can obtain some essential statistics, by querying the `@/api/stats` routes:\n\n```py\nimport requests\n\nr = requests.get('http://localhost:5000/api/stats')\nprint(r.json())\n```\n\nHere is an output example:\n\n```json\n{\n\n    \"learn\": {\n        \"ewm_duration\": 3408682,\n        \"ewm_duration_human\": \"3ms408\u03bcs682ns\",\n        \"mean_duration\": 6541916,\n        \"mean_duration_human\": \"6ms541\u03bcs916ns\",\n        \"n_calls\": 98\n    },\n    \"predict\": {\n        \"ewm_duration\": 3190724,\n        \"ewm_duration_human\": \"3ms190\u03bcs724ns\",\n        \"mean_duration\": 5248635,\n        \"mean_duration_human\": \"5ms248\u03bcs635ns\",\n        \"n_calls\": 213\n    }\n\n}\n```\n\nThe `mean_duration` fields contain the average duration of each endpoint. The `ewm_duration` fields contain an [exponential moving average](https://www.wikiwand.com/en/Moving_average#/Exponential_moving_average) of said duration, and therefore gives you an idea of the recent performance, which can allow you to detect arising performance issues. Note that these durations do not include the time it takes to transmit the response over the network. These durations only pertain to the processing time on `chantilly`'s side, including but not limited to calls to the model.\n\nThese statistic are voluntarily very plain. Their only purpose is to provide a quick healthcheck. The proper way to monitor a web application's performance, including a Flask app, is to use purpose-built tools. For instance you could use [Loki](https://github.com/grafana/loki) to monitor the application logs and [Grafana](https://grafana.com/) to visualize and analyze them.\n\n### Using multiple models\n\nYou can use different models by giving them names. You can provide a name to a model by adding a suffix to `@/api/model`:\n\n```py\nfrom creme import tree\nimport dill\nimport requests\n\nmodel = tree.DecisionTreeClassifier()\n\nrequests.post('http://localhost:5000/api/model/barney-stinson', data=dill.dumps(model))\n```\n\nYou can also pick a name when you add the model through the CLI:\n\n```sh\n> chantilly add-model model.pkl --name barney-stinson\n```\n\nYou can then choose which model to use when you make a prediction:\n\n```py\nr = requests.post('http://localhost:5000/api/predict', json={\n    'id': 42,\n    'features': {\n        'shop': 'Ikea',\n        'item': 'Domb\u00e4s',\n        'date': '2020-03-22T10:42:29Z'\n    },\n    'model': 'barney-stinson'\n})\n```\n\nThe model which was provided last will be used by default if the `model` field is not specified. If you provide an `id`, then the model which was used for making the prediction will be the one that is updated once the ground truth is made available. You can also specify which model to update directly as so:\n\n```py\nrequests.post('http://localhost:5000/api/learn', json={\n    'id': 42,\n    'ground_truth': 10.21,\n    'model': 'barney-stinson'\n})\n```\n\nNote that the data associated with the given `id` is deleted once the model has been updated. In other words you can't call the `@/api/model` with the same `id` twice.\n\nYou can view the available models as well as the default model by sending a GET request to `@/api/models`:\n\n```py\nr = requests.get('http://localhost:5000/api/models')\nprint(r.json())\n```\n\nYou can delete a model by sending a DELETE request to `@/api/model`:\n\n```py\nrequests.delete('http://localhost:5000/api/model/barney-stinson')\n```\n\n### Configuration handling\n\n`chantilly` follows Flask's [instance folder](https://flask.palletsprojects.com/en/1.1.x/config/#instance-folders) pattern. This means that you can configure `chantilly` via a file named `instance/config.py`. Note that the location is relative to where you are launching the `chantilly run` command from (more information can be found [here](https://flask.palletsprojects.com/en/1.1.x/config/#instance-folders)). You can also configure `chantilly` by setting environment variables.\n\nYou can set all the [builtin variables](https://flask.palletsprojects.com/en/1.1.x/config/#builtin-configuration-values) that Flask provides. You can also set the following variables which are specific to `chantilly`:\n\n- `STORAGE_BACKEND`: determines which [storage backend](#using-a-different-storage-backend) to use.\n- `SHELVE_PATH`: location of the [shelve](https://docs.python.org/3/library/shelve.html) database file. Only applies if `STORAGE_BACKEND` is set to `shelve`.\n- `REDIS_HOST`: required if `STORAGE_BACKEND` is set to `redis`.\n- `REDIS_PORT`: required if `STORAGE_BACKEND` is set to `redis`.\n- `REDIS_DB`: required if `STORAGE_BACKEND` is set to `redis`.\n\nThe `instance/config.py` is a Python file that gets executed before the app starts, therefore this is also where you can [configure logging](https://flask.palletsprojects.com/en/1.1.x/logging/). Here is an example `instance/config.py` file:\n\n```py\nfrom logging.config import dictConfig\n\ndictConfig({\n    'version': 1,\n    'formatters': {'default': {\n        'format': '[%(asctime)s] %(levelname)s in %(module)s: %(message)s',\n    }},\n    'handlers': {'wsgi': {\n        'class': 'logging.FileHandler',\n        'filename': '/var/log/chantilly/error.log',\n        'formatter': 'default'\n    }},\n    'root': {\n        'level': 'INFO',\n        'handlers': ['wsgi']\n    }\n})\n\nSECRET_KEY = 'keep_it_secret_keep_it_safe'\nSHELVE_PATH = '/usr/local/chantilly'\n```\n\n### Using a different storage backend\n\nCurrently, the default storage backend is based on the [shelve](https://docs.python.org/3/library/shelve.html) module. It's possible to use a different backend by setting the `STORAGE_BACKEND` environment variable.\n\n#### Redis\n\nAdd the following to your `instance/config.py` file:\n\n```py\nSTORAGE_BACKEND = 'redis'\nREDIS_HOST = 'localhost'\nREDIS_PORT = 6379\nREDIS_DB = 0\n```\n\nNaturally, the values have to be chosen according to your Redis setup.\n\n### Importing libraries\n\nIt's highly likely that your model will be using external dependencies. A prime example is the [`datetime`](https://docs.python.org/3/library/datetime.html) module, which you'll probably want to use to parse datetime strings. Instead of specifying which libraries you want `chantilly` to import, the current practice is to import your requirements *within* your model. For instance, here is an excerpt taken from the [New-York city taxi trips example](examples/taxis):\n\n```py\nfrom creme import compose\nfrom creme import linear_model\nfrom creme import preprocessing\n\ndef parse(trip):\n    import datetime as dt\n    trip['pickup_datetime'] = dt.datetime.fromisoformat(trip['pickup_datetime'])\n    return trip\n\ndef datetime_info(trip):\n    import calendar\n    day_no = trip['pickup_datetime'].weekday()\n    return {\n        'hour': trip['pickup_datetime'].hour,\n        **{day: i == day_no for i, day in enumerate(calendar.day_name)}\n    }\n\nmodel = compose.FuncTransformer(parse)\nmodel |= compose.FuncTransformer(datetime_info)\nmodel |= preprocessing.StandardScaler()\nmodel |= linear_model.LinearRegression()\n```\n\nNote that you need to make sure that the Python interpreter you're running `chantilly` with has access to the libraries you want to use.\n\n### Deployment\n\nEssentially, `chantilly` is just a Flask application. Therefore, it allows the same [deployment options](https://flask.palletsprojects.com/en/1.1.x/deploying/) as any other Flask application.\n\n## Examples\n\n- [New-York city taxi trips \ud83d\ude95](examples/taxis)\n- [Deployment with Docker Compose \ud83d\udc0b](example/docker-compose)\n\n## Development\n\n```sh\n> git clone https://github.com/creme-ml/chantilly\n> cd chantilly\n> pip install -e \".[dev]\"\n> python setup.py develop\n```\n\nThere are some extra dependencies that can be installed if necessary.\n\n```sh\n> pip install -e \".[dev,redis]\"\n```\n\nYou can then run tests.\n\n```sh\n> pytest\n```\n\nThe default testing environment uses the [shelve](https://docs.python.org/3/library/shelve.html) module; you can also use redis:\n\n```sh\n> pytest --redis\n```\n\nYou may also run the app in development mode.\n\n```sh\n> export FLASK_ENV=development\n> chantilly run\n```\n\n## Roadmap\n\n- **HTTP long polling**: Currently, clients can interact with `creme` over a straightforward HTTP protocol. Therefore the speed bottleneck comes from the web requests, not from the machine learning. We would like to provide a way to interact with `chantilly` via long-polling. This means that a single connection can be used to process multiple predictions and model updates, which reduces the overall latency.\n- **Scaling**: At the moment `chantilly` is designed to be run as a single server. Ideally we want to allow `chantilly` in a multi-server environment. Predictions are simple to scale because the model can be used concurrently. However, updating the model concurrently leads to [reader-write problems](https://www.wikiwand.com/en/Readers%E2%80%93writers_problem). We have some ideas in the pipe, but this is going to need some careful thinking.\n- **Grafana dashboard**: The current dashboard is a quick-and-dirty proof of concept. In the long term, we would like to provide a straighforward way to connect with a [Grafana](https://grafana.com/) instance without having to get your hands dirty. Ideally, we would like to use SQLite as a data source for simplicity reasons. However, The Grafana team [has not planned](https://github.com/grafana/grafana/issues/1542#issuecomment-425684417) to add support for SQLite. Instead, they encourage users to use [plugins](https://grafana.com/docs/grafana/latest/plugins/developing/datasources/). We might also look into [Prometheus](https://prometheus.io/) and [InfluxDB](https://www.influxdata.com/).\n- **Support more paradigms**: For the moment we cater to regression and classification models. In the future we also want to support other paradigms, such as time series forecasting and recommender systems.\n\n## Technical stack\n\n- [Flask](https://flask.palletsprojects.com/en/1.1.x/) for the web server.\n- [dill](https://dill.readthedocs.io/en/latest/dill.html) for model serialization.\n- [marshmallow](https://marshmallow.readthedocs.io/en/stable/) for the API input validation.\n- [Vue.js](https://vuejs.org/), [Chart.js](https://www.chartjs.org/), and [Moment.js](https://momentjs.com/) for the web interface.\n\n## Similar alternatives\n\nMost machine learning deployment tools only support making predictions with a trained model. They don't cater to online models which can be updated on the fly. Nonetheless, some of them are quite interesting and are very much worth looking into!\n\n- [Cortex](https://github.com/cortexlabs/cortex)\n- [Clipper](https://github.com/ucbrise/clipper)\n\n## License\n\n`creme` is free and open-source software licensed under the [3-clause BSD license](https://github.com/creme-ml/creme/blob/master/LICENSE).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/creme-ml/chantilly", "keywords": "", "license": "BSD-3", "maintainer": "", "maintainer_email": "", "name": "chantilly", "package_url": "https://pypi.org/project/chantilly/", "platform": "", "project_url": "https://pypi.org/project/chantilly/", "project_urls": {"Homepage": "https://github.com/creme-ml/chantilly"}, "release_url": "https://pypi.org/project/chantilly/0.2.0/", "requires_dist": ["creme (>=0.5.0)", "dill (>=0.3.1.1)", "Flask (>=1.1.1)", "marshmallow (>=3.5.1)", "flake8 (>=3.7.9) ; extra == 'dev'", "mypy (>=0.770) ; extra == 'dev'", "pytest (>=5.3.5) ; extra == 'dev'", "pytest-cov (>=2.8.1) ; extra == 'dev'", "redis (>=3.5) ; extra == 'redis'"], "requires_python": ">=3.7.0", "summary": "Deployment tool for online machine learning models", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p align=\"center\">\n  <img alt=\"chantilly_logo\" height=\"200px\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9a75045d9f3993362711e2a016d4ad8006b00450/68747470733a2f2f646f63732e676f6f676c652e636f6d2f64726177696e67732f642f652f32504143582d3176513041467a61336e486b7268653046616d5f4e415a46357767477a736b4b545635546f34636648416d724375687233635a6e4a695a337044314f665856503732413433356235496c7364756f51432f7075623f773d35383026683d323539\">\n</p>\n<p align=\"center\">\n  \n  <a href=\"https://travis-ci.org/creme-ml/chantilly\" rel=\"nofollow\">\n    <img alt=\"travis\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d01e63eb519374b21c5e1774cdf913d57a9dedb4/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6372656d652d6d6c2f6368616e74696c6c792f6d61737465722e7376673f7374796c653d666c61742d737175617265\">\n  </a>\n  \n  <a href=\"https://codecov.io/gh/creme-ml/chantilly\" rel=\"nofollow\">\n    <img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/da7b637f52f1672b2224c548119a3318aa932885/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f67682f6372656d652d6d6c2f6368616e74696c6c792e7376673f7374796c653d666c61742d737175617265\">\n  </a>\n  \n  <a href=\"https://gitter.im/creme-ml/community?utm_source=share-link&amp;utm_medium=link&amp;utm_campaign=share-link\" rel=\"nofollow\">\n    <img alt=\"gitter\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/71b7b0639238471da0158a881ecb4bbf64a39207/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6372656d652d6d6c2f636f6d6d756e6974793f636f6c6f723d626c756576696f6c6574267374796c653d666c61742d737175617265\">\n  </a>\n  \n  <a href=\"https://pypi.org/project/chantilly\" rel=\"nofollow\">\n    <img alt=\"pypi\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/848513ba6301e627a8352071b3a52739b6b5beb0/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6368616e74696c6c792e7376673f6c6162656c3d72656c6561736526636f6c6f723d626c7565267374796c653d666c61742d737175617265\">\n  </a>\n  \n  <a href=\"https://opensource.org/licenses/BSD-3-Clause\" rel=\"nofollow\">\n    <img alt=\"bsd_3_license\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/67035b11180e878dcdd60de3f21f469ef7a9d59c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e7376673f7374796c653d666c61742d737175617265\">\n  </a>\n</p>\n<p align=\"center\">\n  <code>chantilly</code> is a deployment tool for <a href=\"https://www.wikiwand.com/en/Online_machine_learning\" rel=\"nofollow\">online machine learning</a> models. It is designed to work hand in hand with <a href=\"https://github.com/creme-ml/creme\" rel=\"nofollow\"><code>creme</code></a>.\n</p>\n<h2>Table of contents</h2>\n<ul>\n<li><a href=\"#table-of-contents\" rel=\"nofollow\">Table of contents</a></li>\n<li><a href=\"#introduction\" rel=\"nofollow\">Introduction</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#user-guide\" rel=\"nofollow\">User guide</a>\n<ul>\n<li><a href=\"#running-the-server\" rel=\"nofollow\">Running the server</a></li>\n<li><a href=\"#picking-a-flavor\" rel=\"nofollow\">Picking a flavor</a></li>\n<li><a href=\"#uploading-a-model\" rel=\"nofollow\">Uploading a model</a></li>\n<li><a href=\"#making-a-prediction\" rel=\"nofollow\">Making a prediction</a></li>\n<li><a href=\"#updating-the-model\" rel=\"nofollow\">Updating the model</a></li>\n<li><a href=\"#monitoring-metrics\" rel=\"nofollow\">Monitoring metrics</a></li>\n<li><a href=\"#monitoring-events\" rel=\"nofollow\">Monitoring events</a></li>\n<li><a href=\"#visual-monitoring\" rel=\"nofollow\">Visual monitoring</a></li>\n<li><a href=\"#usage-statistics\" rel=\"nofollow\">Usage statistics</a></li>\n<li><a href=\"#using-multiple-models\" rel=\"nofollow\">Using multiple models</a></li>\n<li><a href=\"#configuration-handling\" rel=\"nofollow\">Configuration handling</a></li>\n<li><a href=\"#using-a-different-storage-backend\" rel=\"nofollow\">Using a different storage backend</a>\n<ul>\n<li><a href=\"#redis\" rel=\"nofollow\">Redis</a></li>\n</ul>\n</li>\n<li><a href=\"#importing-libraries\" rel=\"nofollow\">Importing libraries</a></li>\n<li><a href=\"#deployment\" rel=\"nofollow\">Deployment</a></li>\n</ul>\n</li>\n<li><a href=\"#examples\" rel=\"nofollow\">Examples</a></li>\n<li><a href=\"#development\" rel=\"nofollow\">Development</a></li>\n<li><a href=\"#roadmap\" rel=\"nofollow\">Roadmap</a></li>\n<li><a href=\"#technical-stack\" rel=\"nofollow\">Technical stack</a></li>\n<li><a href=\"#similar-alternatives\" rel=\"nofollow\">Similar alternatives</a></li>\n<li><a href=\"#license\" rel=\"nofollow\">License</a></li>\n</ul>\n<h2>Introduction</h2>\n<p>There are many tools for deploying machine learning models. However, none of them support <strong>online models that can learn on the fly</strong>, but <code>chantilly</code> does.</p>\n<p>Here are some advantages:</p>\n<ul>\n<li><strong>Simple</strong>: <code>chantilly</code> is essentially just a Flask app.</li>\n<li><strong>Straightforward</strong>: you provide a model, <code>chantilly</code> provides a few API routes to do the rest.</li>\n<li><strong>Convenient</strong>: <code>chantilly</code> rids you of the burden of storing features between prediction and learning steps.</li>\n<li><strong>Flexible</strong>: use any model you wish, as long as it's written in Python and implements a couple of required methods.</li>\n</ul>\n<p>Note that <code>chantilly</code> is very young, and is therefore subject to evolve. We're also eager for feedback and are happy to work hand in hand with you if you have specific needs.</p>\n<h2>Installation</h2>\n<p><code>chantilly</code> is intended to work with <strong>Python 3.7 or above</strong>. You can install it from PyPI:</p>\n<pre>&gt; pip install chantilly\n</pre>\n<p>You can also install the latest development version as so:</p>\n<pre>&gt; pip install git+https://github.com/creme-ml/chantilly\n</pre>\n<h2>User guide</h2>\n<h3>Running the server</h3>\n<p>Once you've followed the installation step, you'll get access to the <code>chantilly</code> CLI. You can see the available commands by running <code>chantilly --help</code>. You can start a server with the <code>run</code> command:</p>\n<pre>&gt; chantilly run\n</pre>\n<p>This will start a <a href=\"https://flask.palletsprojects.com/en/1.0.x/\" rel=\"nofollow\">Flask</a> server with all the necessary routes for uploading a model, training it, making predictions with it, and monitoring it. By default, the server will be accessible at <a href=\"http://localhost:5000\" rel=\"nofollow\"><code>localhost:5000</code></a>, which is what we will be using in the rest of the examples in the user guide. You can run <code>chantilly routes</code> in order to see all the available routes.</p>\n<h3>Picking a flavor</h3>\n<p>The first thing you need to do is pick a flavor. Currently, the available flavors are:</p>\n<ul>\n<li><code>regression</code> for regression tasks.</li>\n<li><code>binary</code> for binary classification tasks.</li>\n<li><code>multiclass</code> for multi-class classification tasks.</li>\n</ul>\n<p>You can set the flavor by sending a POST request to <code>@/api/init</code>, as so:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n\n<span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">'flavor'</span><span class=\"p\">:</span> <span class=\"s1\">'regression'</span><span class=\"p\">}</span>\n<span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/init'</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"n\">config</span><span class=\"p\">)</span>\n</pre>\n<p>You can also set the flavor via the CLI:</p>\n<pre><span class=\"o\">&gt;</span> <span class=\"n\">chantilly</span> <span class=\"n\">init</span> <span class=\"n\">regression</span>\n</pre>\n<p>:warning: Setting the flavor will erase everything and thus provide a clean slate.</p>\n<p>You can view the current flavor by sending a GET request to <code>@/api/init</code>:</p>\n<pre><span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/init'</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">json</span><span class=\"p\">()[</span><span class=\"s1\">'flavor'</span><span class=\"p\">])</span>\n</pre>\n<h3>Uploading a model</h3>\n<p>You can upload a model by sending a POST request to the <code>@/api/model</code> route. You need to provide a model which has been serialized with <a href=\"https://docs.python.org/3/library/pickle.html\" rel=\"nofollow\"><code>pickle</code></a> or <a href=\"https://dill.readthedocs.io/en/latest/dill.html\" rel=\"nofollow\"><code>dill</code></a> (we recommend the latter). For example:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">creme</span> <span class=\"kn\">import</span> <span class=\"n\">compose</span>\n<span class=\"kn\">from</span> <span class=\"nn\">creme</span> <span class=\"kn\">import</span> <span class=\"n\">linear_model</span>\n<span class=\"kn\">from</span> <span class=\"nn\">creme</span> <span class=\"kn\">import</span> <span class=\"n\">preprocessing</span>\n<span class=\"kn\">import</span> <span class=\"nn\">dill</span>\n<span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">compose</span><span class=\"o\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span>\n    <span class=\"n\">preprocessing</span><span class=\"o\">.</span><span class=\"n\">StandardScaler</span><span class=\"p\">(),</span>\n    <span class=\"n\">linear_model</span><span class=\"o\">.</span><span class=\"n\">LinearRegression</span><span class=\"p\">()</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/model'</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">dill</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">))</span>\n</pre>\n<p>Likewise, the model can be retrieved by sending a GET request to <code>@/api/model</code>:</p>\n<pre><span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/model'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">pickle</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">)</span>\n</pre>\n<p>Note that <code>chantilly</code> will validate the model you provide to make sure it works with the flavor you picked. For instance, if you picked the <code>regression</code> flavor, then the model has to implement <code>fit_one</code> and <code>predict_one</code>.</p>\n<p>You can also add a upload by using the CLI. First, you need to serialize a model and dump it to a file:</p>\n<pre><span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'model.pkl'</span><span class=\"p\">,</span> <span class=\"s1\">'wb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">file</span><span class=\"p\">:</span>\n    <span class=\"n\">dill</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">file</span><span class=\"p\">)</span>\n</pre>\n<p>Then, call the <code>add-model</code> sub-command:</p>\n<pre>&gt; chantilly add-model model.pkl\n</pre>\n<h3>Making a prediction</h3>\n<p>Predictions can be obtained by sending a POST request to <code>@/api/predict</code>. The payload you send has to contain a field named <code>features</code>. The value of this field will be passed to the <code>predict_one</code> method of the model you uploaded earlier on. If the model you provided <code>predict_proba_one</code> then that will be used instead. Here is an example:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n\n<span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/predict'</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">42</span><span class=\"p\">,</span>\n    <span class=\"s1\">'features'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s1\">'shop'</span><span class=\"p\">:</span> <span class=\"s1\">'Ikea'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'item'</span><span class=\"p\">:</span> <span class=\"s1\">'Domb\u00e4s'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'date'</span><span class=\"p\">:</span> <span class=\"s1\">'2020-03-22T10:42:29Z'</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">})</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">json</span><span class=\"p\">()[</span><span class=\"s1\">'prediction'</span><span class=\"p\">])</span>\n</pre>\n<p>Note that in the previous snippet we've also provided an <code>id</code> field. This field is optional. If is is provided, then the features will be stored by the <code>chantilly</code> server, along with the prediction. This allows not having to provide the features again when you want to update the model later on.</p>\n<h3>Updating the model</h3>\n<p>The model can be updated by sending a POST request to <code>@/api/learn</code>. If you've provided an ID in an earlier call to <code>@/api/predict</code>, then you only have to provide said ID along with the ground truth:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n\n<span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/learn'</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">42</span><span class=\"p\">,</span>\n    <span class=\"s1\">'ground_truth'</span><span class=\"p\">:</span> <span class=\"mf\">10.21</span>\n<span class=\"p\">})</span>\n</pre>\n<p>However, if you haven't passed an ID earlier on, then you have to provide the features yourself:</p>\n<pre><span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/learn'</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'features'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s1\">'shop'</span><span class=\"p\">:</span> <span class=\"s1\">'Ikea'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'item'</span><span class=\"p\">:</span> <span class=\"s1\">'Domb\u00e4s'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'date'</span><span class=\"p\">:</span> <span class=\"s1\">'2020-03-22T10:42:29Z'</span>\n    <span class=\"p\">},</span>\n    <span class=\"s1\">'ground_truth'</span><span class=\"p\">:</span> <span class=\"mf\">10.21</span>\n<span class=\"p\">})</span>\n</pre>\n<p>Note that the <code>id</code> field will have precedence in case both of <code>id</code> and <code>features</code> are provided. We highly recommend you to use the <code>id</code> field. First of all it means that you don't have to take care of storing the features between calls to <code>@/api/predict</code> and <code>@/api/learn</code>. Secondly it makes the metrics more reliable because they will be using the predictions that were made at the time <code>@/api/predict</code> was called.</p>\n<h3>Monitoring metrics</h3>\n<p>You can access the current metrics via a GET request to the <code>@/api/metrics</code> route.</p>\n<p>Additionally, you can access a stream of metric updates by using the <code>@/api/stream/metrics</code>. This is a streaming route which implements <a href=\"https://www.wikiwand.com/en/Server-sent_events\" rel=\"nofollow\">server-sent events (SSE)</a>. As such it will notify listeners every time the metrics are updates. For instance, you can use the <a href=\"https://github.com/btubbs/sseclient\" rel=\"nofollow\"><code>sseclient</code></a> library to monitor the metrics from a Python script:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sseclient</span>\n\n<span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"n\">sseclient</span><span class=\"o\">.</span><span class=\"n\">SSEClient</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/stream/metrics'</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">msg</span> <span class=\"ow\">in</span> <span class=\"n\">messages</span><span class=\"p\">:</span>\n    <span class=\"n\">metrics</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">msg</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">metrics</span><span class=\"p\">)</span>\n</pre>\n<p>You can use the following piece of JavaScript to do the same thing in a browser:</p>\n<pre><span class=\"kd\">var</span> <span class=\"nx\">es</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">EventSource</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/stream/metrics'</span><span class=\"p\">);</span>\n<span class=\"nx\">es</span><span class=\"p\">.</span><span class=\"nx\">onmessage</span> <span class=\"o\">=</span> <span class=\"nx\">e</span> <span class=\"p\">=&gt;</span> <span class=\"p\">{</span>\n    <span class=\"kd\">var</span> <span class=\"nx\">metrics</span> <span class=\"o\">=</span> <span class=\"nx\">JSON</span><span class=\"p\">.</span><span class=\"nx\">parse</span><span class=\"p\">(</span><span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">data</span><span class=\"p\">);</span>\n    <span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">log</span><span class=\"p\">(</span><span class=\"nx\">metrics</span><span class=\"p\">)</span>\n<span class=\"p\">};</span>\n</pre>\n<h3>Monitoring events</h3>\n<p>You can also listen to all the prediction and learning events via the <code>@/api/stream/events</code> route. This will yield SSE events with an event name attached, which is either 'predict' or 'learn'. From a Python interpreter, you can do the following:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sseclient</span>\n\n<span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"n\">sseclient</span><span class=\"o\">.</span><span class=\"n\">SSEClient</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/stream/events'</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">msg</span> <span class=\"ow\">in</span> <span class=\"n\">messages</span><span class=\"p\">:</span>\n    <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">msg</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">msg</span><span class=\"o\">.</span><span class=\"n\">event</span> <span class=\"o\">==</span> <span class=\"s1\">'learn'</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">'model'</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">'features'</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">'prediction'</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">'ground_truth'</span><span class=\"p\">])</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">'model'</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">'features'</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">'prediction'</span><span class=\"p\">])</span>\n</pre>\n<p>In JavaScript, you can you use the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/EventTarget/addEventListener\" rel=\"nofollow\"><code>addEventListener</code></a> method:</p>\n<pre><span class=\"kd\">var</span> <span class=\"nx\">es</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">EventSource</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/stream/events'</span><span class=\"p\">);</span>\n\n<span class=\"nx\">es</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"s1\">'learn'</span><span class=\"p\">,</span> <span class=\"o\">=</span> <span class=\"nx\">e</span> <span class=\"p\">=&gt;</span> <span class=\"p\">{</span>\n    <span class=\"kd\">var</span> <span class=\"nx\">data</span> <span class=\"o\">=</span> <span class=\"nx\">JSON</span><span class=\"p\">.</span><span class=\"nx\">parse</span><span class=\"p\">(</span><span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">data</span><span class=\"p\">);</span>\n    <span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">log</span><span class=\"p\">(</span><span class=\"nx\">data</span><span class=\"p\">.</span><span class=\"nx\">model</span><span class=\"p\">,</span> <span class=\"nx\">data</span><span class=\"p\">.</span><span class=\"nx\">features</span><span class=\"p\">,</span> <span class=\"nx\">data</span><span class=\"p\">.</span><span class=\"nx\">prediction</span><span class=\"p\">,</span> <span class=\"nx\">data</span><span class=\"p\">.</span><span class=\"nx\">ground_truth</span><span class=\"p\">)</span>\n<span class=\"p\">};</span>\n\n<span class=\"nx\">es</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"s1\">'predict'</span><span class=\"p\">,</span> <span class=\"o\">=</span> <span class=\"nx\">e</span> <span class=\"p\">=&gt;</span> <span class=\"p\">{</span>\n    <span class=\"kd\">var</span> <span class=\"nx\">data</span> <span class=\"o\">=</span> <span class=\"nx\">JSON</span><span class=\"p\">.</span><span class=\"nx\">parse</span><span class=\"p\">(</span><span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">data</span><span class=\"p\">);</span>\n    <span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">log</span><span class=\"p\">(</span><span class=\"nx\">data</span><span class=\"p\">.</span><span class=\"nx\">model</span><span class=\"p\">,</span> <span class=\"nx\">data</span><span class=\"p\">.</span><span class=\"nx\">features</span><span class=\"p\">,</span> <span class=\"nx\">data</span><span class=\"p\">.</span><span class=\"nx\">prediction</span><span class=\"p\">)</span>\n<span class=\"p\">};</span>\n</pre>\n<h3>Visual monitoring</h3>\n<p>A live dashboard is accessible if you navigate to <a href=\"http://localhost:5000\" rel=\"nofollow\"><code>localhost:5000</code></a> in your browser.</p>\n<p align=\"center\">\n  <img alt=\"demo\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/145b17e3a6db1b1e1f35780c9ce8033f095769da/64656d6f2e676966\">\n</p>\n<p>Under the hood the dashboard is simply listening to the API's streaming routes.</p>\n<h3>Usage statistics</h3>\n<p>You can obtain some essential statistics, by querying the <code>@/api/stats</code> routes:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n\n<span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/stats'</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">json</span><span class=\"p\">())</span>\n</pre>\n<p>Here is an output example:</p>\n<pre><span class=\"p\">{</span>\n\n    <span class=\"nt\">\"learn\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"ewm_duration\"</span><span class=\"p\">:</span> <span class=\"mi\">3408682</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"ewm_duration_human\"</span><span class=\"p\">:</span> <span class=\"s2\">\"3ms408\u03bcs682ns\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"mean_duration\"</span><span class=\"p\">:</span> <span class=\"mi\">6541916</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"mean_duration_human\"</span><span class=\"p\">:</span> <span class=\"s2\">\"6ms541\u03bcs916ns\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"n_calls\"</span><span class=\"p\">:</span> <span class=\"mi\">98</span>\n    <span class=\"p\">},</span>\n    <span class=\"nt\">\"predict\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"ewm_duration\"</span><span class=\"p\">:</span> <span class=\"mi\">3190724</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"ewm_duration_human\"</span><span class=\"p\">:</span> <span class=\"s2\">\"3ms190\u03bcs724ns\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"mean_duration\"</span><span class=\"p\">:</span> <span class=\"mi\">5248635</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"mean_duration_human\"</span><span class=\"p\">:</span> <span class=\"s2\">\"5ms248\u03bcs635ns\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"n_calls\"</span><span class=\"p\">:</span> <span class=\"mi\">213</span>\n    <span class=\"p\">}</span>\n\n<span class=\"p\">}</span>\n</pre>\n<p>The <code>mean_duration</code> fields contain the average duration of each endpoint. The <code>ewm_duration</code> fields contain an <a href=\"https://www.wikiwand.com/en/Moving_average#/Exponential_moving_average\" rel=\"nofollow\">exponential moving average</a> of said duration, and therefore gives you an idea of the recent performance, which can allow you to detect arising performance issues. Note that these durations do not include the time it takes to transmit the response over the network. These durations only pertain to the processing time on <code>chantilly</code>'s side, including but not limited to calls to the model.</p>\n<p>These statistic are voluntarily very plain. Their only purpose is to provide a quick healthcheck. The proper way to monitor a web application's performance, including a Flask app, is to use purpose-built tools. For instance you could use <a href=\"https://github.com/grafana/loki\" rel=\"nofollow\">Loki</a> to monitor the application logs and <a href=\"https://grafana.com/\" rel=\"nofollow\">Grafana</a> to visualize and analyze them.</p>\n<h3>Using multiple models</h3>\n<p>You can use different models by giving them names. You can provide a name to a model by adding a suffix to <code>@/api/model</code>:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">creme</span> <span class=\"kn\">import</span> <span class=\"n\">tree</span>\n<span class=\"kn\">import</span> <span class=\"nn\">dill</span>\n<span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tree</span><span class=\"o\">.</span><span class=\"n\">DecisionTreeClassifier</span><span class=\"p\">()</span>\n\n<span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/model/barney-stinson'</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">dill</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">))</span>\n</pre>\n<p>You can also pick a name when you add the model through the CLI:</p>\n<pre>&gt; chantilly add-model model.pkl --name barney-stinson\n</pre>\n<p>You can then choose which model to use when you make a prediction:</p>\n<pre><span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/predict'</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">42</span><span class=\"p\">,</span>\n    <span class=\"s1\">'features'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s1\">'shop'</span><span class=\"p\">:</span> <span class=\"s1\">'Ikea'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'item'</span><span class=\"p\">:</span> <span class=\"s1\">'Domb\u00e4s'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'date'</span><span class=\"p\">:</span> <span class=\"s1\">'2020-03-22T10:42:29Z'</span>\n    <span class=\"p\">},</span>\n    <span class=\"s1\">'model'</span><span class=\"p\">:</span> <span class=\"s1\">'barney-stinson'</span>\n<span class=\"p\">})</span>\n</pre>\n<p>The model which was provided last will be used by default if the <code>model</code> field is not specified. If you provide an <code>id</code>, then the model which was used for making the prediction will be the one that is updated once the ground truth is made available. You can also specify which model to update directly as so:</p>\n<pre><span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/learn'</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">42</span><span class=\"p\">,</span>\n    <span class=\"s1\">'ground_truth'</span><span class=\"p\">:</span> <span class=\"mf\">10.21</span><span class=\"p\">,</span>\n    <span class=\"s1\">'model'</span><span class=\"p\">:</span> <span class=\"s1\">'barney-stinson'</span>\n<span class=\"p\">})</span>\n</pre>\n<p>Note that the data associated with the given <code>id</code> is deleted once the model has been updated. In other words you can't call the <code>@/api/model</code> with the same <code>id</code> twice.</p>\n<p>You can view the available models as well as the default model by sending a GET request to <code>@/api/models</code>:</p>\n<pre><span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/models'</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">json</span><span class=\"p\">())</span>\n</pre>\n<p>You can delete a model by sending a DELETE request to <code>@/api/model</code>:</p>\n<pre><span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">delete</span><span class=\"p\">(</span><span class=\"s1\">'http://localhost:5000/api/model/barney-stinson'</span><span class=\"p\">)</span>\n</pre>\n<h3>Configuration handling</h3>\n<p><code>chantilly</code> follows Flask's <a href=\"https://flask.palletsprojects.com/en/1.1.x/config/#instance-folders\" rel=\"nofollow\">instance folder</a> pattern. This means that you can configure <code>chantilly</code> via a file named <code>instance/config.py</code>. Note that the location is relative to where you are launching the <code>chantilly run</code> command from (more information can be found <a href=\"https://flask.palletsprojects.com/en/1.1.x/config/#instance-folders\" rel=\"nofollow\">here</a>). You can also configure <code>chantilly</code> by setting environment variables.</p>\n<p>You can set all the <a href=\"https://flask.palletsprojects.com/en/1.1.x/config/#builtin-configuration-values\" rel=\"nofollow\">builtin variables</a> that Flask provides. You can also set the following variables which are specific to <code>chantilly</code>:</p>\n<ul>\n<li><code>STORAGE_BACKEND</code>: determines which <a href=\"#using-a-different-storage-backend\" rel=\"nofollow\">storage backend</a> to use.</li>\n<li><code>SHELVE_PATH</code>: location of the <a href=\"https://docs.python.org/3/library/shelve.html\" rel=\"nofollow\">shelve</a> database file. Only applies if <code>STORAGE_BACKEND</code> is set to <code>shelve</code>.</li>\n<li><code>REDIS_HOST</code>: required if <code>STORAGE_BACKEND</code> is set to <code>redis</code>.</li>\n<li><code>REDIS_PORT</code>: required if <code>STORAGE_BACKEND</code> is set to <code>redis</code>.</li>\n<li><code>REDIS_DB</code>: required if <code>STORAGE_BACKEND</code> is set to <code>redis</code>.</li>\n</ul>\n<p>The <code>instance/config.py</code> is a Python file that gets executed before the app starts, therefore this is also where you can <a href=\"https://flask.palletsprojects.com/en/1.1.x/logging/\" rel=\"nofollow\">configure logging</a>. Here is an example <code>instance/config.py</code> file:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">logging.config</span> <span class=\"kn\">import</span> <span class=\"n\">dictConfig</span>\n\n<span class=\"n\">dictConfig</span><span class=\"p\">({</span>\n    <span class=\"s1\">'version'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s1\">'formatters'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">'default'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s1\">'format'</span><span class=\"p\">:</span> <span class=\"s1\">'[</span><span class=\"si\">%(asctime)s</span><span class=\"s1\">] </span><span class=\"si\">%(levelname)s</span><span class=\"s1\"> in </span><span class=\"si\">%(module)s</span><span class=\"s1\">: </span><span class=\"si\">%(message)s</span><span class=\"s1\">'</span><span class=\"p\">,</span>\n    <span class=\"p\">}},</span>\n    <span class=\"s1\">'handlers'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">'wsgi'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s1\">'class'</span><span class=\"p\">:</span> <span class=\"s1\">'logging.FileHandler'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'filename'</span><span class=\"p\">:</span> <span class=\"s1\">'/var/log/chantilly/error.log'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'formatter'</span><span class=\"p\">:</span> <span class=\"s1\">'default'</span>\n    <span class=\"p\">}},</span>\n    <span class=\"s1\">'root'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s1\">'level'</span><span class=\"p\">:</span> <span class=\"s1\">'INFO'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'handlers'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'wsgi'</span><span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">})</span>\n\n<span class=\"n\">SECRET_KEY</span> <span class=\"o\">=</span> <span class=\"s1\">'keep_it_secret_keep_it_safe'</span>\n<span class=\"n\">SHELVE_PATH</span> <span class=\"o\">=</span> <span class=\"s1\">'/usr/local/chantilly'</span>\n</pre>\n<h3>Using a different storage backend</h3>\n<p>Currently, the default storage backend is based on the <a href=\"https://docs.python.org/3/library/shelve.html\" rel=\"nofollow\">shelve</a> module. It's possible to use a different backend by setting the <code>STORAGE_BACKEND</code> environment variable.</p>\n<h4>Redis</h4>\n<p>Add the following to your <code>instance/config.py</code> file:</p>\n<pre><span class=\"n\">STORAGE_BACKEND</span> <span class=\"o\">=</span> <span class=\"s1\">'redis'</span>\n<span class=\"n\">REDIS_HOST</span> <span class=\"o\">=</span> <span class=\"s1\">'localhost'</span>\n<span class=\"n\">REDIS_PORT</span> <span class=\"o\">=</span> <span class=\"mi\">6379</span>\n<span class=\"n\">REDIS_DB</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n</pre>\n<p>Naturally, the values have to be chosen according to your Redis setup.</p>\n<h3>Importing libraries</h3>\n<p>It's highly likely that your model will be using external dependencies. A prime example is the <a href=\"https://docs.python.org/3/library/datetime.html\" rel=\"nofollow\"><code>datetime</code></a> module, which you'll probably want to use to parse datetime strings. Instead of specifying which libraries you want <code>chantilly</code> to import, the current practice is to import your requirements <em>within</em> your model. For instance, here is an excerpt taken from the <a href=\"examples/taxis\" rel=\"nofollow\">New-York city taxi trips example</a>:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">creme</span> <span class=\"kn\">import</span> <span class=\"n\">compose</span>\n<span class=\"kn\">from</span> <span class=\"nn\">creme</span> <span class=\"kn\">import</span> <span class=\"n\">linear_model</span>\n<span class=\"kn\">from</span> <span class=\"nn\">creme</span> <span class=\"kn\">import</span> <span class=\"n\">preprocessing</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">trip</span><span class=\"p\">):</span>\n    <span class=\"kn\">import</span> <span class=\"nn\">datetime</span> <span class=\"k\">as</span> <span class=\"nn\">dt</span>\n    <span class=\"n\">trip</span><span class=\"p\">[</span><span class=\"s1\">'pickup_datetime'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">dt</span><span class=\"o\">.</span><span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">fromisoformat</span><span class=\"p\">(</span><span class=\"n\">trip</span><span class=\"p\">[</span><span class=\"s1\">'pickup_datetime'</span><span class=\"p\">])</span>\n    <span class=\"k\">return</span> <span class=\"n\">trip</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">datetime_info</span><span class=\"p\">(</span><span class=\"n\">trip</span><span class=\"p\">):</span>\n    <span class=\"kn\">import</span> <span class=\"nn\">calendar</span>\n    <span class=\"n\">day_no</span> <span class=\"o\">=</span> <span class=\"n\">trip</span><span class=\"p\">[</span><span class=\"s1\">'pickup_datetime'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">weekday</span><span class=\"p\">()</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"s1\">'hour'</span><span class=\"p\">:</span> <span class=\"n\">trip</span><span class=\"p\">[</span><span class=\"s1\">'pickup_datetime'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">hour</span><span class=\"p\">,</span>\n        <span class=\"o\">**</span><span class=\"p\">{</span><span class=\"n\">day</span><span class=\"p\">:</span> <span class=\"n\">i</span> <span class=\"o\">==</span> <span class=\"n\">day_no</span> <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">day</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">calendar</span><span class=\"o\">.</span><span class=\"n\">day_name</span><span class=\"p\">)}</span>\n    <span class=\"p\">}</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">compose</span><span class=\"o\">.</span><span class=\"n\">FuncTransformer</span><span class=\"p\">(</span><span class=\"n\">parse</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">|=</span> <span class=\"n\">compose</span><span class=\"o\">.</span><span class=\"n\">FuncTransformer</span><span class=\"p\">(</span><span class=\"n\">datetime_info</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">|=</span> <span class=\"n\">preprocessing</span><span class=\"o\">.</span><span class=\"n\">StandardScaler</span><span class=\"p\">()</span>\n<span class=\"n\">model</span> <span class=\"o\">|=</span> <span class=\"n\">linear_model</span><span class=\"o\">.</span><span class=\"n\">LinearRegression</span><span class=\"p\">()</span>\n</pre>\n<p>Note that you need to make sure that the Python interpreter you're running <code>chantilly</code> with has access to the libraries you want to use.</p>\n<h3>Deployment</h3>\n<p>Essentially, <code>chantilly</code> is just a Flask application. Therefore, it allows the same <a href=\"https://flask.palletsprojects.com/en/1.1.x/deploying/\" rel=\"nofollow\">deployment options</a> as any other Flask application.</p>\n<h2>Examples</h2>\n<ul>\n<li><a href=\"examples/taxis\" rel=\"nofollow\">New-York city taxi trips \ud83d\ude95</a></li>\n<li><a href=\"example/docker-compose\" rel=\"nofollow\">Deployment with Docker Compose \ud83d\udc0b</a></li>\n</ul>\n<h2>Development</h2>\n<pre>&gt; git clone https://github.com/creme-ml/chantilly\n&gt; <span class=\"nb\">cd</span> chantilly\n&gt; pip install -e <span class=\"s2\">\".[dev]\"</span>\n&gt; python setup.py develop\n</pre>\n<p>There are some extra dependencies that can be installed if necessary.</p>\n<pre>&gt; pip install -e <span class=\"s2\">\".[dev,redis]\"</span>\n</pre>\n<p>You can then run tests.</p>\n<pre>&gt; pytest\n</pre>\n<p>The default testing environment uses the <a href=\"https://docs.python.org/3/library/shelve.html\" rel=\"nofollow\">shelve</a> module; you can also use redis:</p>\n<pre>&gt; pytest --redis\n</pre>\n<p>You may also run the app in development mode.</p>\n<pre>&gt; <span class=\"nb\">export</span> <span class=\"nv\">FLASK_ENV</span><span class=\"o\">=</span>development\n&gt; chantilly run\n</pre>\n<h2>Roadmap</h2>\n<ul>\n<li><strong>HTTP long polling</strong>: Currently, clients can interact with <code>creme</code> over a straightforward HTTP protocol. Therefore the speed bottleneck comes from the web requests, not from the machine learning. We would like to provide a way to interact with <code>chantilly</code> via long-polling. This means that a single connection can be used to process multiple predictions and model updates, which reduces the overall latency.</li>\n<li><strong>Scaling</strong>: At the moment <code>chantilly</code> is designed to be run as a single server. Ideally we want to allow <code>chantilly</code> in a multi-server environment. Predictions are simple to scale because the model can be used concurrently. However, updating the model concurrently leads to <a href=\"https://www.wikiwand.com/en/Readers%E2%80%93writers_problem\" rel=\"nofollow\">reader-write problems</a>. We have some ideas in the pipe, but this is going to need some careful thinking.</li>\n<li><strong>Grafana dashboard</strong>: The current dashboard is a quick-and-dirty proof of concept. In the long term, we would like to provide a straighforward way to connect with a <a href=\"https://grafana.com/\" rel=\"nofollow\">Grafana</a> instance without having to get your hands dirty. Ideally, we would like to use SQLite as a data source for simplicity reasons. However, The Grafana team <a href=\"https://github.com/grafana/grafana/issues/1542#issuecomment-425684417\" rel=\"nofollow\">has not planned</a> to add support for SQLite. Instead, they encourage users to use <a href=\"https://grafana.com/docs/grafana/latest/plugins/developing/datasources/\" rel=\"nofollow\">plugins</a>. We might also look into <a href=\"https://prometheus.io/\" rel=\"nofollow\">Prometheus</a> and <a href=\"https://www.influxdata.com/\" rel=\"nofollow\">InfluxDB</a>.</li>\n<li><strong>Support more paradigms</strong>: For the moment we cater to regression and classification models. In the future we also want to support other paradigms, such as time series forecasting and recommender systems.</li>\n</ul>\n<h2>Technical stack</h2>\n<ul>\n<li><a href=\"https://flask.palletsprojects.com/en/1.1.x/\" rel=\"nofollow\">Flask</a> for the web server.</li>\n<li><a href=\"https://dill.readthedocs.io/en/latest/dill.html\" rel=\"nofollow\">dill</a> for model serialization.</li>\n<li><a href=\"https://marshmallow.readthedocs.io/en/stable/\" rel=\"nofollow\">marshmallow</a> for the API input validation.</li>\n<li><a href=\"https://vuejs.org/\" rel=\"nofollow\">Vue.js</a>, <a href=\"https://www.chartjs.org/\" rel=\"nofollow\">Chart.js</a>, and <a href=\"https://momentjs.com/\" rel=\"nofollow\">Moment.js</a> for the web interface.</li>\n</ul>\n<h2>Similar alternatives</h2>\n<p>Most machine learning deployment tools only support making predictions with a trained model. They don't cater to online models which can be updated on the fly. Nonetheless, some of them are quite interesting and are very much worth looking into!</p>\n<ul>\n<li><a href=\"https://github.com/cortexlabs/cortex\" rel=\"nofollow\">Cortex</a></li>\n<li><a href=\"https://github.com/ucbrise/clipper\" rel=\"nofollow\">Clipper</a></li>\n</ul>\n<h2>License</h2>\n<p><code>creme</code> is free and open-source software licensed under the <a href=\"https://github.com/creme-ml/creme/blob/master/LICENSE\" rel=\"nofollow\">3-clause BSD license</a>.</p>\n\n          </div>"}, "last_serial": 7151769, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "91255f6dbfba3d62ab670ba9b6cb22ef", "sha256": "710f78140b7317bad27ba3950e370fd32b3fa7a33469e8fc2334150198f6dbb3"}, "downloads": -1, "filename": "chantilly-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "91255f6dbfba3d62ab670ba9b6cb22ef", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7.0", "size": 22435, "upload_time": "2020-04-12T11:02:36", "upload_time_iso_8601": "2020-04-12T11:02:36.010859Z", "url": "https://files.pythonhosted.org/packages/8e/55/4cfed827ad62c4f54ee1c9f799ec184b829069d8a9e8bd13cd123ee20a0e/chantilly-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "27f2694a4665682d7b5973cd31b03ff6", "sha256": "0205ec5e03870e4fe7867ce08cef3a3bd3d1d6f0c817479b17c59688967e69b1"}, "downloads": -1, "filename": "chantilly-0.1.0.tar.gz", "has_sig": false, "md5_digest": "27f2694a4665682d7b5973cd31b03ff6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7.0", "size": 26209, "upload_time": "2020-04-12T11:02:37", "upload_time_iso_8601": "2020-04-12T11:02:37.533129Z", "url": "https://files.pythonhosted.org/packages/06/dc/5546971ece91342c8d790e1977855d58df4b8ab77d797a5930fe8896f541/chantilly-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "0b67ca7f66d560ba103e381c663780cd", "sha256": "c4d9b2bfe94441eb40c223567f9f414d47b989d6b297d1c24dfe0d4af8bb6e76"}, "downloads": -1, "filename": "chantilly-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0b67ca7f66d560ba103e381c663780cd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7.0", "size": 31585, "upload_time": "2020-05-02T14:47:54", "upload_time_iso_8601": "2020-05-02T14:47:54.592996Z", "url": "https://files.pythonhosted.org/packages/eb/96/4eb42ee8a543ac621a28e0caac2cfe592bb337cd6f2a57f186d39329dda5/chantilly-0.2.0-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0b67ca7f66d560ba103e381c663780cd", "sha256": "c4d9b2bfe94441eb40c223567f9f414d47b989d6b297d1c24dfe0d4af8bb6e76"}, "downloads": -1, "filename": "chantilly-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0b67ca7f66d560ba103e381c663780cd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7.0", "size": 31585, "upload_time": "2020-05-02T14:47:54", "upload_time_iso_8601": "2020-05-02T14:47:54.592996Z", "url": "https://files.pythonhosted.org/packages/eb/96/4eb42ee8a543ac621a28e0caac2cfe592bb337cd6f2a57f186d39329dda5/chantilly-0.2.0-py3-none-any.whl", "yanked": false}], "timestamp": "Thu May  7 22:34:28 2020"}