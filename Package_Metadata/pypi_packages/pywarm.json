{"info": {"author": "blue-season", "author_email": "very.blue.season@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "\n[![PyWarm - A cleaner way to build neural networks for PyTorch](https://github.com/blue-season/pywarm/raw/gh-pages/docs/pywarm-logo.png)](https://blue-season.github.io/pywarm/)\n\n# PyWarm\n\nA cleaner way to build neural networks for PyTorch.\n\n[![PyPI Python Version](https://img.shields.io/pypi/pyversions/pywarm)](https://github.com/blue-season/pywarm)\n[![PyPI Version](https://img.shields.io/pypi/v/pywarm)](https://pypi.org/project/pywarm/)\n[![License](https://img.shields.io/github/license/blue-season/pywarm)](https://github.com/blue-season/pywarm/blob/master/LICENSE)\n\n[Examples](https://blue-season.github.io/pywarm/docs/example/)  |  [Tutorial](https://blue-season.github.io/pywarm/docs/tutorial/)  |   [API reference](https://blue-season.github.io/pywarm/reference/warm/functional/)\n\n----\n\n## Introduction\n\nPyWarm is a lightweight, high-level neural network construction API for PyTorch.\nIt enables defining all parts of NNs in the functional way.\n\nWith PyWarm, you can put *all* network data flow logic in the `forward()` method of\nyour model, without the need to define children modules in the `__init__()` method\nand then call it again in the `forward()`.\nThis result in a much more readable model definition in fewer lines of code.\n\nPyWarm only aims to simplify the network definition, and does not attempt to cover\nmodel training, validation or data handling.\n\n----\n\nFor example, a convnet for MNIST:\n(If needed, click the tabs to switch between Warm and Torch versions)\n\n\n``` Python tab=\"Warm\" linenums=\"1\"\n# powered by PyWarm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport warm\nimport warm.functional as W\n\n\nclass ConvNet(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        warm.up(self, [2, 1, 28, 28])\n\n    def forward(self, x):\n        x = W.conv(x, 20, 5, activation='relu')\n        x = F.max_pool2d(x, 2)\n        x = W.conv(x, 50, 5, activation='relu')\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, 800)\n        x = W.linear(x, 500, activation='relu')\n        x = W.linear(x, 10)\n        return F.log_softmax(x, dim=1)\n```\n\n``` Python tab=\"Torch\" linenums=\"1\"\n# vanilla PyTorch version, taken from\n# pytorch tutorials/beginner_source/blitz/neural_networks_tutorial.py \nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ConvNet(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n```\n\n----\n\nA couple of things you may have noticed:\n\n-   First of all, in the PyWarm version, the entire network definition and\n    data flow logic resides in the `forward()` method. You don't have to look\n    up and down repeatedly to understand what `self.conv1`, `self.fc1` etc.\n    is doing.\n\n-   You do not need to track and specify `in_channels` (or `in_features`, etc.)\n    for network layers. PyWarm can infer the information for you. e.g.\n\n```Python\n# Warm\nx = W.conv(x, 20, 5, activation='relu')\nx = W.conv(x, 50, 5, activation='relu')\n\n\n# Torch\nself.conv1 = nn.Conv2d(1, 20, 5, 1)\nself.conv2 = nn.Conv2d(20, 50, 5, 1)\n```\n\n-   One unified `W.conv` for all 1D, 2D, and 3D cases. Fewer things to keep track of!\n\n-   `activation='relu'`. All `warm.functional` APIs accept an optional `activation` keyword,\n    which is basically equivalent to `F.relu(W.conv(...))`.\n\nFor deeper neural networks, see additional [examples](https://blue-season.github.io/pywarm/docs/example/).\n\n----\n## Installation\n\n    pip3 install pywarm\n\n----\n## Quick start: 30 seconds to PyWarm\n\nIf you already have experinces with PyTorch, using PyWarm is very straightforward:\n\n-   First, import PyWarm in you model file:\n```Python\nimport warm\nimport warm.functional as W\n```\n\n-   Second, remove child module definitions in the model's `__init__()` method.\n    In stead, use `W.conv`, `W.linear` ... etc. in the model's `forward()` method,\n    just like how you would use torch nn functional `F.max_pool2d`, `F.relu` ... etc.\n\n    For example, instead of writing:\n\n```Python\n# Torch\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n        # other child module definitions\n    def forward(self, x):\n        x = self.conv1(x)\n        # more forward steps\n```\n\n-   You can now write in the warm way:\n\n```Python\n# Warm\nclass MyWarmModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        warm.up(self, input_shape_or_data)\n    def forward(self, x):\n        x = W.conv(x, out_channels, kernel_size) # no in_channels needed\n        # more forward steps\n```\n\n-   Finally, don't forget to warmify the model by adding\n    \n    `warm.up(self, input_shape_or_data)`\n\n    at the end of the model's `__init__()` method. You need to supply\n    `input_shape_or_data`, which is either a tensor of input data, \n    or just its shape, e.g. `[2, 1, 28, 28]` for MNIST inputs.\n    \n    The model is now ready to use, just like any other PyTorch models.\n\nCheck out the [tutorial](https://blue-season.github.io/pywarm/docs/tutorial/) \nand [examples](https://blue-season.github.io/pywarm/docs/example/) if you want to learn more!\n\n----\n## Testing\n\nClone the repository first, then\n\n    cd pywarm\n    pytest -v\n\n----\n## Documentation\n\nDocumentations are generated using the excellent [Portray](https://timothycrosley.github.io/portray/) package.\n\n-   [Examples](https://blue-season.github.io/pywarm/docs/example/)\n\n-   [Tutorial](https://blue-season.github.io/pywarm/docs/tutorial/) \n\n-   [API reference](https://blue-season.github.io/pywarm/reference/warm/functional/)\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/blue-season/pywarm", "keywords": "pywarm,pytorch,neural network,deep learning", "license": "MIT", "maintainer": "blue-season", "maintainer_email": "very.blue.season@gmail.com", "name": "pywarm", "package_url": "https://pypi.org/project/pywarm/", "platform": "", "project_url": "https://pypi.org/project/pywarm/", "project_urls": {"Homepage": "https://github.com/blue-season/pywarm", "Repository": "https://github.com/blue-season/pywarm"}, "release_url": "https://pypi.org/project/pywarm/0.4.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "A cleaner way to build neural networks for PyTorch.", "version": "0.4.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://blue-season.github.io/pywarm/\" rel=\"nofollow\"><img alt=\"PyWarm - A cleaner way to build neural networks for PyTorch\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b558c766e97ce5b28604acbaceca32abcac1882b/68747470733a2f2f6769746875622e636f6d2f626c75652d736561736f6e2f70797761726d2f7261772f67682d70616765732f646f63732f70797761726d2d6c6f676f2e706e67\"></a></p>\n<h1>PyWarm</h1>\n<p>A cleaner way to build neural networks for PyTorch.</p>\n<p><a href=\"https://github.com/blue-season/pywarm\" rel=\"nofollow\"><img alt=\"PyPI Python Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ae1f77d048411371f3e8f7ef1e9c865b889c9792/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f70797761726d\"></a>\n<a href=\"https://pypi.org/project/pywarm/\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2ab9f64e6497de651652d0f40ee62022d0f61ac9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f70797761726d\"></a>\n<a href=\"https://github.com/blue-season/pywarm/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5138b42bdc043fbbe7e65eadb51a36ae44c9b556/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f626c75652d736561736f6e2f70797761726d\"></a></p>\n<p><a href=\"https://blue-season.github.io/pywarm/docs/example/\" rel=\"nofollow\">Examples</a>  |  <a href=\"https://blue-season.github.io/pywarm/docs/tutorial/\" rel=\"nofollow\">Tutorial</a>  |   <a href=\"https://blue-season.github.io/pywarm/reference/warm/functional/\" rel=\"nofollow\">API reference</a></p>\n<hr>\n<h2>Introduction</h2>\n<p>PyWarm is a lightweight, high-level neural network construction API for PyTorch.\nIt enables defining all parts of NNs in the functional way.</p>\n<p>With PyWarm, you can put <em>all</em> network data flow logic in the <code>forward()</code> method of\nyour model, without the need to define children modules in the <code>__init__()</code> method\nand then call it again in the <code>forward()</code>.\nThis result in a much more readable model definition in fewer lines of code.</p>\n<p>PyWarm only aims to simplify the network definition, and does not attempt to cover\nmodel training, validation or data handling.</p>\n<hr>\n<p>For example, a convnet for MNIST:\n(If needed, click the tabs to switch between Warm and Torch versions)</p>\n<pre><span class=\"c1\"># powered by PyWarm</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n<span class=\"kn\">import</span> <span class=\"nn\">warm</span>\n<span class=\"kn\">import</span> <span class=\"nn\">warm.functional</span> <span class=\"k\">as</span> <span class=\"nn\">W</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">ConvNet</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"n\">warm</span><span class=\"o\">.</span><span class=\"n\">up</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">])</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">max_pool2d</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">max_pool2d</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">800</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">log_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"c1\"># vanilla PyTorch version, taken from</span>\n<span class=\"c1\"># pytorch tutorials/beginner_source/blitz/neural_networks_tutorial.py </span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">ConvNet</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"o\">*</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">500</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">max_pool2d</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">max_pool2d</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"o\">*</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">log_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<hr>\n<p>A couple of things you may have noticed:</p>\n<ul>\n<li>\n<p>First of all, in the PyWarm version, the entire network definition and\ndata flow logic resides in the <code>forward()</code> method. You don't have to look\nup and down repeatedly to understand what <code>self.conv1</code>, <code>self.fc1</code> etc.\nis doing.</p>\n</li>\n<li>\n<p>You do not need to track and specify <code>in_channels</code> (or <code>in_features</code>, etc.)\nfor network layers. PyWarm can infer the information for you. e.g.</p>\n</li>\n</ul>\n<pre><span class=\"c1\"># Warm</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">)</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\"># Torch</span>\n<span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>\n<p>One unified <code>W.conv</code> for all 1D, 2D, and 3D cases. Fewer things to keep track of!</p>\n</li>\n<li>\n<p><code>activation='relu'</code>. All <code>warm.functional</code> APIs accept an optional <code>activation</code> keyword,\nwhich is basically equivalent to <code>F.relu(W.conv(...))</code>.</p>\n</li>\n</ul>\n<p>For deeper neural networks, see additional <a href=\"https://blue-season.github.io/pywarm/docs/example/\" rel=\"nofollow\">examples</a>.</p>\n<hr>\n<h2>Installation</h2>\n<pre><code>pip3 install pywarm\n</code></pre>\n<hr>\n<h2>Quick start: 30 seconds to PyWarm</h2>\n<p>If you already have experinces with PyTorch, using PyWarm is very straightforward:</p>\n<ul>\n<li>First, import PyWarm in you model file:</li>\n</ul>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">warm</span>\n<span class=\"kn\">import</span> <span class=\"nn\">warm.functional</span> <span class=\"k\">as</span> <span class=\"nn\">W</span>\n</pre>\n<ul>\n<li>\n<p>Second, remove child module definitions in the model's <code>__init__()</code> method.\nIn stead, use <code>W.conv</code>, <code>W.linear</code> ... etc. in the model's <code>forward()</code> method,\njust like how you would use torch nn functional <code>F.max_pool2d</code>, <code>F.relu</code> ... etc.</p>\n<p>For example, instead of writing:</p>\n</li>\n</ul>\n<pre><span class=\"c1\"># Torch</span>\n<span class=\"k\">class</span> <span class=\"nc\">MyModule</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"p\">)</span>\n        <span class=\"c1\"># other child module definitions</span>\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"c1\"># more forward steps</span>\n</pre>\n<ul>\n<li>You can now write in the warm way:</li>\n</ul>\n<pre><span class=\"c1\"># Warm</span>\n<span class=\"k\">class</span> <span class=\"nc\">MyWarmModule</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"n\">warm</span><span class=\"o\">.</span><span class=\"n\">up</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">input_shape_or_data</span><span class=\"p\">)</span>\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"p\">)</span> <span class=\"c1\"># no in_channels needed</span>\n        <span class=\"c1\"># more forward steps</span>\n</pre>\n<ul>\n<li>\n<p>Finally, don't forget to warmify the model by adding</p>\n<p><code>warm.up(self, input_shape_or_data)</code></p>\n<p>at the end of the model's <code>__init__()</code> method. You need to supply\n<code>input_shape_or_data</code>, which is either a tensor of input data,\nor just its shape, e.g. <code>[2, 1, 28, 28]</code> for MNIST inputs.</p>\n<p>The model is now ready to use, just like any other PyTorch models.</p>\n</li>\n</ul>\n<p>Check out the <a href=\"https://blue-season.github.io/pywarm/docs/tutorial/\" rel=\"nofollow\">tutorial</a>\nand <a href=\"https://blue-season.github.io/pywarm/docs/example/\" rel=\"nofollow\">examples</a> if you want to learn more!</p>\n<hr>\n<h2>Testing</h2>\n<p>Clone the repository first, then</p>\n<pre><code>cd pywarm\npytest -v\n</code></pre>\n<hr>\n<h2>Documentation</h2>\n<p>Documentations are generated using the excellent <a href=\"https://timothycrosley.github.io/portray/\" rel=\"nofollow\">Portray</a> package.</p>\n<ul>\n<li>\n<p><a href=\"https://blue-season.github.io/pywarm/docs/example/\" rel=\"nofollow\">Examples</a></p>\n</li>\n<li>\n<p><a href=\"https://blue-season.github.io/pywarm/docs/tutorial/\" rel=\"nofollow\">Tutorial</a></p>\n</li>\n<li>\n<p><a href=\"https://blue-season.github.io/pywarm/reference/warm/functional/\" rel=\"nofollow\">API reference</a></p>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 5869499, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "b02acccab989cb5d27b6d089c0325868", "sha256": "8f4a78a61a7189dbf5a5652a6a36bf350bfd0c1589aedb45b6fbcff7db20a2a5"}, "downloads": -1, "filename": "PyWarm-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b02acccab989cb5d27b6d089c0325868", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 10740, "upload_time": "2019-09-02T23:54:08", "upload_time_iso_8601": "2019-09-02T23:54:08.655944Z", "url": "https://files.pythonhosted.org/packages/84/a4/0ddb5f0270193f5fbf01cf8c805ea7d42f4c0696082b22151f2d60d3c919/PyWarm-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b286e5adb6d71cdb12e894ee3268eb91", "sha256": "2ea538f3d9e7223135f1ec3772891c6b206a76952b1c91ae6c11718463f4c129"}, "downloads": -1, "filename": "PyWarm-0.1.1.tar.gz", "has_sig": false, "md5_digest": "b286e5adb6d71cdb12e894ee3268eb91", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11689, "upload_time": "2019-09-02T23:54:10", "upload_time_iso_8601": "2019-09-02T23:54:10.074875Z", "url": "https://files.pythonhosted.org/packages/8e/c6/d50bfbc24eb5a0f6894b2e6198dca1c6c4740fbe7cbb5887e0cb942b2ebf/PyWarm-0.1.1.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "5c14ec7f0a63aaca2d90a0602d87c5a7", "sha256": "eb04c9f7239e790e235723be231f9dc4607420c4a737780d38897727e61d6adf"}, "downloads": -1, "filename": "PyWarm-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5c14ec7f0a63aaca2d90a0602d87c5a7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 12257, "upload_time": "2019-09-06T05:26:42", "upload_time_iso_8601": "2019-09-06T05:26:42.104069Z", "url": "https://files.pythonhosted.org/packages/85/fa/7aebc79f772c10e97e45c0ae73ba9f413b1439fe5cb4a418475e21a5ea2a/PyWarm-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "508cc527661a81316eed9cfb357c01a2", "sha256": "daea9822d6afe40a44cf74afdf3ad573978d5d1ea23c5226ec53d587f1b97215"}, "downloads": -1, "filename": "PyWarm-0.2.0.tar.gz", "has_sig": false, "md5_digest": "508cc527661a81316eed9cfb357c01a2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13300, "upload_time": "2019-09-06T05:26:43", "upload_time_iso_8601": "2019-09-06T05:26:43.589361Z", "url": "https://files.pythonhosted.org/packages/fd/ba/6f2b7e5e72e4c156c96e6be4239d773f8f51504b8251788cd65ffcc4b35e/PyWarm-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "8b989619d091274da572ab74d26ac552", "sha256": "ee615a7e1616d4b1d2dbee2678bb2b68b64361824a82e54425ff1f2ae2c0c13b"}, "downloads": -1, "filename": "PyWarm-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8b989619d091274da572ab74d26ac552", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 12961, "upload_time": "2019-09-08T15:36:48", "upload_time_iso_8601": "2019-09-08T15:36:48.006420Z", "url": "https://files.pythonhosted.org/packages/17/2f/d680e43406da4fe237a3622f711a829ccab6b4285a5b94d1f7b770ca91f2/PyWarm-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "04547e4a6fd39f0747f951b09ab14b87", "sha256": "cd5d6d981146b590fd1258112c9ed6383623071086cd2f5265a982d3a2004833"}, "downloads": -1, "filename": "PyWarm-0.3.0.tar.gz", "has_sig": false, "md5_digest": "04547e4a6fd39f0747f951b09ab14b87", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14080, "upload_time": "2019-09-08T15:36:49", "upload_time_iso_8601": "2019-09-08T15:36:49.511791Z", "url": "https://files.pythonhosted.org/packages/0a/79/67e9519a75fe10448da9b8cf5317b2f727d7e2a08cb139ebc23948da2419/PyWarm-0.3.0.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "44a266144c608966596255817745313f", "sha256": "8191023428991edfa65ec0582e5cd5786658c46de4fba8da847e36ac995f2375"}, "downloads": -1, "filename": "PyWarm-0.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "44a266144c608966596255817745313f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 13027, "upload_time": "2019-09-11T03:20:56", "upload_time_iso_8601": "2019-09-11T03:20:56.343439Z", "url": "https://files.pythonhosted.org/packages/db/c2/8a01146110d74b62863ec369c4be7f71daf46ed6484440f26801fc05cc80/PyWarm-0.4.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7e8d3d7df03203f383b44ed400854ee7", "sha256": "abdf08f3473cd3a1b187ccfdb4a4736689dc1c4e20b7daa59f36d0434dc187ba"}, "downloads": -1, "filename": "PyWarm-0.4.0.tar.gz", "has_sig": false, "md5_digest": "7e8d3d7df03203f383b44ed400854ee7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14019, "upload_time": "2019-09-11T03:20:58", "upload_time_iso_8601": "2019-09-11T03:20:58.028510Z", "url": "https://files.pythonhosted.org/packages/29/2c/0f65c7ebf088ae73157a1b6b3156fcc15b570ded135a3fab9fddcda0a9ce/PyWarm-0.4.0.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "1bdde02b3a04494b7e8e4cd41d8985d3", "sha256": "8fa3ef1c59bc3e52d18f653d7eb164ac9f877174954de7ee0f33e23d2b5827d7"}, "downloads": -1, "filename": "PyWarm-0.4.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1bdde02b3a04494b7e8e4cd41d8985d3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 13371, "upload_time": "2019-09-22T15:53:05", "upload_time_iso_8601": "2019-09-22T15:53:05.897444Z", "url": "https://files.pythonhosted.org/packages/5f/40/f3b27d611ec6502dcda24d57cef96898ebd864fa5a0bc827b4c7bd615329/PyWarm-0.4.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3752b25bd7ae42dfe90d57fbeb4b374c", "sha256": "bbfe5753cf44cfdb994369c2c2343787daec0dfdde26c61daf0c2dd17aaaa99f"}, "downloads": -1, "filename": "PyWarm-0.4.1.tar.gz", "has_sig": false, "md5_digest": "3752b25bd7ae42dfe90d57fbeb4b374c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14263, "upload_time": "2019-09-22T15:53:08", "upload_time_iso_8601": "2019-09-22T15:53:08.706784Z", "url": "https://files.pythonhosted.org/packages/ea/ca/a6e774eb305dd880c6d7d942f1aebc0a47eacd6f386f5df64dd5e5cf0bb5/PyWarm-0.4.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1bdde02b3a04494b7e8e4cd41d8985d3", "sha256": "8fa3ef1c59bc3e52d18f653d7eb164ac9f877174954de7ee0f33e23d2b5827d7"}, "downloads": -1, "filename": "PyWarm-0.4.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1bdde02b3a04494b7e8e4cd41d8985d3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 13371, "upload_time": "2019-09-22T15:53:05", "upload_time_iso_8601": "2019-09-22T15:53:05.897444Z", "url": "https://files.pythonhosted.org/packages/5f/40/f3b27d611ec6502dcda24d57cef96898ebd864fa5a0bc827b4c7bd615329/PyWarm-0.4.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3752b25bd7ae42dfe90d57fbeb4b374c", "sha256": "bbfe5753cf44cfdb994369c2c2343787daec0dfdde26c61daf0c2dd17aaaa99f"}, "downloads": -1, "filename": "PyWarm-0.4.1.tar.gz", "has_sig": false, "md5_digest": "3752b25bd7ae42dfe90d57fbeb4b374c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14263, "upload_time": "2019-09-22T15:53:08", "upload_time_iso_8601": "2019-09-22T15:53:08.706784Z", "url": "https://files.pythonhosted.org/packages/ea/ca/a6e774eb305dd880c6d7d942f1aebc0a47eacd6f386f5df64dd5e5cf0bb5/PyWarm-0.4.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:11:42 2020"}