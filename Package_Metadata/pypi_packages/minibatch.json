{"info": {"author": "Patrick Senti", "author_email": "patrick.senti@omegaml.io", "bugtrack_url": null, "classifiers": [], "description": "Minibatch - Python Stream Processing for humans\n===============================================\n\nDependencies:\n    * a running MongoDB accessible to minibatch\n    * Python 3.x\n\nomega|ml provides a straight-forward, Python-native approach to mini-batch streaming and complex-event\nprocessing that is easily scalable. Streaming primarily consists of\n\n* a producer, which is some function inserting data into the stream\n* a consumer, which is some function retrieving data from the stream\n* transform and windowing functions to process the data in small batches\n\nFeatures\n--------\n\n* native Python producers and consumers\n* includes three basic Window strategies: CountWindow, FixedTimeWindow, RelaxedTimeWindow\n* extensible Window strategies by subclassing and overriding a few methods\n* scalable, persistent streams - parallel inserts, parallel processing of windows\n\nA few hightlights\n\n* creating a stream and appending data is just 2 lines of code\n* producer and consumer stream code runs anywhere\n* no dependencies other than mongoengine, pymongo\n* extensible sources and sinks (already available: Kafka)\n\nQuick start\n-----------\n\n1. Install and setup\n\n   .. code::\n\n      $ pip install minibatch\n      $ docker run -d -p 27017:27017 mongo\n\n2. Create a stream producer or attach to a source\n\n   .. code::\n\n        from minibatch import stream\n        stream = Stream.get_or_create('test')\n        for i in range(100):\n            stream.append({'date': datetime.datetime.now().isoformat()})\n            sleep(.5)\n\n   Currently there is support for Kafka and MQTT sources. However\n   arbitrary other sources can be added.\n\n   .. code::\n\n      from minibatch.contrib.kafka import KafkaSource\n      source = KafkaSource('topic', urls=['kafka:port'])\n      stream.attach(source)\n\n\n3. Consume the stream\n\n   .. code::\n\n        from minibatch import streaming\n\t    @streaming('test', size=2, keep=True)\n\t    def myprocess(window):\n\t        print(window.data)\n\t    return window\n\n\t    =>\n\t    [{'date': '2018-04-30T20:18:22.918060'}, {'date': '2018-04-30T20:18:23.481320'}]\n\t    [{'date': '2018-04-30T20:18:24.041337'}, {'date': '2018-04-30T20:18:24.593545'}\n\t    ...\n\n   `myprocess` is called for every N-tuple of items (`size=2`)  appended to the stream by the producer(s).\n   The frequency is determined by the emitter strategy. This can be configured or changed for a custom\n   emitter strategy, as shown in the next step.\n\n4. Configure the emitter strategy\n\n   Note the `@streaming` decorator. It implements a blocking consumer that delivers batches\n   of data according to some strategy implemented by a WindowEmitter. Currently `@streaming`\n   provides the following interface:\n\n    * `size=N` - uses the :code:`CountWindow` emitter\n    * `interval=SECONDS` - uses the :code:`RelaxedTimeWindow` emitter\n    * `interval=SECONDS, relaxed=False` - uses the :code:`FixedTimeWindow` emitter\n    * `emitter=CLASS:WindowEmitter` - uses the given subclass of a :code:`WindowEmitter`\n    * `workers=N` - set the number of workers to process the decorated function, defaults to number of CPUs\n    * `executor=CLASS:Executor` - the asynchronous executor to use, defaults to :code:`concurrent.futures.ProcessPoolExecutor`\n\nStream sources\n--------------\n\nCurrently provided in :code:`minibatch.contrib`:\n\n* KafkaSource - attach a stream to a Apache Kafka topic\n* MQTTSource - attach to an MQTT broker\n* MongoSource - attach to a MongoDB collection\n\nStream sources are arbitrary objects that support the :code:`stream()`\nmethod, as follows.\n\n.. code::\n\n    class SomeSource:\n        ...\n        def stream(self, stream):\n            for data in source:\n                stream.append(data)\n\n\nStream Sinks\n------------\n\nThe result of a stream can be forwarded to a sink. Currently\nprovided sinks in :code:`minibatch.contrib` are:\n\n* KafkaSink - forward messagess to a Apache Kafka topic\n* MQTTSink  - forward messages to an MQTT broker\n* MongoSink - forward messages to a MongoDB collection\n\nStream sinks are arbitrary objects that support the :code:`put()`\nmethod, as follows.\n\n.. code::\n\n    class SomeSink:\n        ...\n        def put(self, message):\n            sink.send(message)\n\n\nWindow emitters\n---------------\n\nminibatch provides the following window emitters out of the box:\n\n* :code:`CountWindow` - emit fixed-sized windows. Waits until at least *n* messages are\n   available before emitting a new window\n* :code:`FixedTimeWindow`- emit all messages retrieved within specific, time-fixed windows of\n   a given interval of *n* seconds. This guarantees that messages were received in the specific\n   window.\n* :code:`RelaxedTimeWindow` - every interval of *n* seconds emit all messages retrieved since\n   the last window was created. This does not guarantee that messages were received in a given\n   window.\n\n\nImplementing a custom WindowEmitter\n-----------------------------------\n\nCustom emitter strategies are implemented as a subclass to :code:`WindowEmitter`. The main methods\nto implement are\n\n* :code:`window_ready` - returns the tuple :code:`(ready, data)`, where ready is True if there is data\n     to emit\n* :code:`query` - returns the data for the new window. This function retrieves the :code:`data` part\n     of the return value of :code:`window_ready`\n\nSee the API reference for more details.\n\n.. code::\n\n    class SortedWindow(WindowEmitter):\n        \"\"\"\n        sort all data by value and output only multiples of 2 in batches of interval size\n        \"\"\"\n        def window_ready(self):\n            qs = Buffer.objects.no_cache().filter(processed=False)\n            data = []\n            for obj in sorted(qs, key=lambda obj : obj.data['value']):\n                if obj.data['value'] % 2 == 0:\n                    data.append(obj)\n                    if len(data) >= self.interval:\n                        break\n            self._data = data\n            return len(self._data) == self.interval, ()\n\n        def query(self, *args):\n            return self._data\n\n\nWhat is streaming and how does minibatch implement it?\n------------------------------------------------------\n\n*Concepts*\n\nInstead of directly connection producers and consumers, a producer sends messages to a stream. Think\nof a stream as an endless buffer, or a pipeline, that takes input from many producers on one end, and\noutputs messages to a consumer on the other end. This transfer of messages happens asynchronously, that\nis the producer can send messages to the stream independent of whether the consumer is ready to receive, and the  consumer can take messages from the stream independent of whether the producer is ready to send.\n\nUnlike usual asynchronous messaging, however, we want the consumer to receive messages in small batches to optimize throughput. That is, we want the pipeline to *emit* messages only subject to some criteria\nof grouping messages, where each group is called a *mini-batch*. The function that determines whether the\nbatching criteria is met (e.g. time elapsed, number of messages in the pipeline) is called *emitter strategy*,\nand the output it produces is called *window*.\n\nThus in order to connect producers and consumers we need the following parts to our streaming system:\n\n* a :code:`Stream`, keeping metadata for the stream such as its name and when it was created, last read etc.\n* a :code:`Buffer` acting as the buffer where messages sent by producers are stored until the emitting\n* a :code:`WindowEmitter` implementing the emitter strategy\n* a :code:`Window` representing the output produced by the emitter strategy\n\n.. note::\n\n    The producer accepts input from some external system, say an MQTT end point. The producer's responsibility is to enter the data into the streaming buffer.\n    The consumer uses an emitter strategy to produce a Window of data that is then forwarded to the user's processing code.\n\n*Implementation*\n\nminibatch uses MongoDB to implement Streams, Buffers and Windows. Specifically, the following collections are used:\n\n* `stream` - represents instances of `Stream`, each document is a stream with a unique name\n* `buffer` - a virtually endless buffer for all streams in the system, each document contains one message of a stream\n* `window`- each document represents the data as emitted by the particular emitter strategy\n\nBy default messages go through the following states\n\n1. upon append by a producer: message is inserted into `buffer`, with flag `processed = False`\n2. upon being seen by an emitter: message is marked as `processed = True`\n3. upon being emitted: message is copied to `window`, marked `processed = False` (in Window)\n4. upon emit success (no exceptions raised by the emit function): message is deleted from `buffer`\n   and marked `processed = True` in `window`\n\nNotes:\n\n* emitters typically act on a collection of messages, that is steps 2 - 4 are applied to more\n  than one message at a time\n\n* to avoid deleting messages from the buffer, pass `@streaming(..., keep=True)`\n\n* custom emitters can modify the behavior of both creating windows and handling the buffer by\n  overriding the `process()`, `emit()` and `commit()` methods for each of the above steps\n  2/3/4, respectively.\n\nFurther development\n-------------------\n\nHere are a couple of ideas to extend minibatch. Contributions are welcome.\n\n* more examples, following typical streaming examples like word count, filtering\n* more emitter strategies, e.g. for sliding windows\n* performance testing, benchmarking\n* distributed processing of windows via distributed framework such as celery, ray, dask\n* extend emitters by typical stream operations e.g. to support operations like count, filter, map, groupby, merge, join\n* add other storage backends (e.g. Redis, or some Python-native in-memory db that provides network access and an easy to use ORM layer, like mongoengine does for MongoDB)\n\nLicense\n-------\n\nMIT licensed. See LICENSE file.\n\n\n\n\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/omegaml/minibatch", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "minibatch", "package_url": "https://pypi.org/project/minibatch/", "platform": "", "project_url": "https://pypi.org/project/minibatch/", "project_urls": {"Homepage": "http://github.com/omegaml/minibatch"}, "release_url": "https://pypi.org/project/minibatch/0.2.4/", "requires_dist": ["mongoengine (>=0.18)", "dill", "kafka-python (==1.4.7) ; extra == 'all'", "paho-mqtt (==1.5.0) ; extra == 'all'", "pymongo (==3.10.1) ; extra == 'all'", "kafka-python (==1.4.7) ; extra == 'devall'", "paho-mqtt (==1.5.0) ; extra == 'devall'", "pymongo (==3.10.1) ; extra == 'devall'", "omegaml (==0.12.1) ; extra == 'devall'", "kafka-python (==1.4.7) ; extra == 'kafka'", "pymongo (==3.10.1) ; extra == 'mongodb'", "paho-mqtt (==1.5.0) ; extra == 'mqtt'", "pymongo (==3.10.1) ; extra == 'omegaml'", "omegaml (==0.12.1) ; extra == 'omegaml'"], "requires_python": "", "summary": "Python stream processing for humans", "version": "0.2.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <dl>\n<dt>Dependencies:</dt>\n<dd><ul>\n<li>a running MongoDB accessible to minibatch</li>\n<li>Python 3.x</li>\n</ul>\n</dd>\n</dl>\n<p>omega|ml provides a straight-forward, Python-native approach to mini-batch streaming and complex-event\nprocessing that is easily scalable. Streaming primarily consists of</p>\n<ul>\n<li>a producer, which is some function inserting data into the stream</li>\n<li>a consumer, which is some function retrieving data from the stream</li>\n<li>transform and windowing functions to process the data in small batches</li>\n</ul>\n<div id=\"features\">\n<h2>Features</h2>\n<ul>\n<li>native Python producers and consumers</li>\n<li>includes three basic Window strategies: CountWindow, FixedTimeWindow, RelaxedTimeWindow</li>\n<li>extensible Window strategies by subclassing and overriding a few methods</li>\n<li>scalable, persistent streams - parallel inserts, parallel processing of windows</li>\n</ul>\n<p>A few hightlights</p>\n<ul>\n<li>creating a stream and appending data is just 2 lines of code</li>\n<li>producer and consumer stream code runs anywhere</li>\n<li>no dependencies other than mongoengine, pymongo</li>\n<li>extensible sources and sinks (already available: Kafka)</li>\n</ul>\n</div>\n<div id=\"quick-start\">\n<h2>Quick start</h2>\n<ol>\n<li><p>Install and setup</p>\n<pre>$ pip install minibatch\n$ docker run -d -p 27017:27017 mongo\n</pre>\n</li>\n<li><p>Create a stream producer or attach to a source</p>\n<pre>from minibatch import stream\nstream = Stream.get_or_create('test')\nfor i in range(100):\n    stream.append({'date': datetime.datetime.now().isoformat()})\n    sleep(.5)\n</pre>\n<p>Currently there is support for Kafka and MQTT sources. However\narbitrary other sources can be added.</p>\n<pre>from minibatch.contrib.kafka import KafkaSource\nsource = KafkaSource('topic', urls=['kafka:port'])\nstream.attach(source)\n</pre>\n</li>\n<li><p>Consume the stream</p>\n<pre>from minibatch import streaming\n    @streaming('test', size=2, keep=True)\n    def myprocess(window):\n        print(window.data)\n    return window\n\n    =&gt;\n    [{'date': '2018-04-30T20:18:22.918060'}, {'date': '2018-04-30T20:18:23.481320'}]\n    [{'date': '2018-04-30T20:18:24.041337'}, {'date': '2018-04-30T20:18:24.593545'}\n    ...\n</pre>\n<p><cite>myprocess</cite> is called for every N-tuple of items (<cite>size=2</cite>)  appended to the stream by the producer(s).\nThe frequency is determined by the emitter strategy. This can be configured or changed for a custom\nemitter strategy, as shown in the next step.</p>\n</li>\n<li><p>Configure the emitter strategy</p>\n<p>Note the <cite>@streaming</cite> decorator. It implements a blocking consumer that delivers batches\nof data according to some strategy implemented by a WindowEmitter. Currently <cite>@streaming</cite>\nprovides the following interface:</p>\n<blockquote>\n<ul>\n<li><cite>size=N</cite> - uses the <code>CountWindow</code> emitter</li>\n<li><cite>interval=SECONDS</cite> - uses the <code>RelaxedTimeWindow</code> emitter</li>\n<li><cite>interval=SECONDS, relaxed=False</cite> - uses the <code>FixedTimeWindow</code> emitter</li>\n<li><cite>emitter=CLASS:WindowEmitter</cite> - uses the given subclass of a <code>WindowEmitter</code></li>\n<li><cite>workers=N</cite> - set the number of workers to process the decorated function, defaults to number of CPUs</li>\n<li><cite>executor=CLASS:Executor</cite> - the asynchronous executor to use, defaults to <code>concurrent.futures.ProcessPoolExecutor</code></li>\n</ul>\n</blockquote>\n</li>\n</ol>\n</div>\n<div id=\"stream-sources\">\n<h2>Stream sources</h2>\n<p>Currently provided in <code>minibatch.contrib</code>:</p>\n<ul>\n<li>KafkaSource - attach a stream to a Apache Kafka topic</li>\n<li>MQTTSource - attach to an MQTT broker</li>\n<li>MongoSource - attach to a MongoDB collection</li>\n</ul>\n<p>Stream sources are arbitrary objects that support the <code>stream()</code>\nmethod, as follows.</p>\n<pre>class SomeSource:\n    ...\n    def stream(self, stream):\n        for data in source:\n            stream.append(data)\n</pre>\n</div>\n<div id=\"stream-sinks\">\n<h2>Stream Sinks</h2>\n<p>The result of a stream can be forwarded to a sink. Currently\nprovided sinks in <code>minibatch.contrib</code> are:</p>\n<ul>\n<li>KafkaSink - forward messagess to a Apache Kafka topic</li>\n<li>MQTTSink  - forward messages to an MQTT broker</li>\n<li>MongoSink - forward messages to a MongoDB collection</li>\n</ul>\n<p>Stream sinks are arbitrary objects that support the <code>put()</code>\nmethod, as follows.</p>\n<pre>class SomeSink:\n    ...\n    def put(self, message):\n        sink.send(message)\n</pre>\n</div>\n<div id=\"window-emitters\">\n<h2>Window emitters</h2>\n<p>minibatch provides the following window emitters out of the box:</p>\n<ul>\n<li><dl>\n<dt><code>CountWindow</code> - emit fixed-sized windows. Waits until at least <em>n</em> messages are</dt>\n<dd>available before emitting a new window</dd>\n</dl>\n</li>\n<li><dl>\n<dt><code>FixedTimeWindow</code>- emit all messages retrieved within specific, time-fixed windows of</dt>\n<dd>a given interval of <em>n</em> seconds. This guarantees that messages were received in the specific\nwindow.</dd>\n</dl>\n</li>\n<li><dl>\n<dt><code>RelaxedTimeWindow</code> - every interval of <em>n</em> seconds emit all messages retrieved since</dt>\n<dd>the last window was created. This does not guarantee that messages were received in a given\nwindow.</dd>\n</dl>\n</li>\n</ul>\n</div>\n<div id=\"implementing-a-custom-windowemitter\">\n<h2>Implementing a custom WindowEmitter</h2>\n<p>Custom emitter strategies are implemented as a subclass to <code>WindowEmitter</code>. The main methods\nto implement are</p>\n<ul>\n<li><dl>\n<dt><code>window_ready</code> - returns the tuple <code>(ready, data)</code>, where ready is True if there is data</dt>\n<dd>to emit</dd>\n</dl>\n</li>\n<li><dl>\n<dt><code>query</code> - returns the data for the new window. This function retrieves the <code>data</code> part</dt>\n<dd>of the return value of <code>window_ready</code></dd>\n</dl>\n</li>\n</ul>\n<p>See the API reference for more details.</p>\n<pre>class SortedWindow(WindowEmitter):\n    \"\"\"\n    sort all data by value and output only multiples of 2 in batches of interval size\n    \"\"\"\n    def window_ready(self):\n        qs = Buffer.objects.no_cache().filter(processed=False)\n        data = []\n        for obj in sorted(qs, key=lambda obj : obj.data['value']):\n            if obj.data['value'] % 2 == 0:\n                data.append(obj)\n                if len(data) &gt;= self.interval:\n                    break\n        self._data = data\n        return len(self._data) == self.interval, ()\n\n    def query(self, *args):\n        return self._data\n</pre>\n</div>\n<div id=\"what-is-streaming-and-how-does-minibatch-implement-it\">\n<h2>What is streaming and how does minibatch implement it?</h2>\n<p><em>Concepts</em></p>\n<p>Instead of directly connection producers and consumers, a producer sends messages to a stream. Think\nof a stream as an endless buffer, or a pipeline, that takes input from many producers on one end, and\noutputs messages to a consumer on the other end. This transfer of messages happens asynchronously, that\nis the producer can send messages to the stream independent of whether the consumer is ready to receive, and the  consumer can take messages from the stream independent of whether the producer is ready to send.</p>\n<p>Unlike usual asynchronous messaging, however, we want the consumer to receive messages in small batches to optimize throughput. That is, we want the pipeline to <em>emit</em> messages only subject to some criteria\nof grouping messages, where each group is called a <em>mini-batch</em>. The function that determines whether the\nbatching criteria is met (e.g. time elapsed, number of messages in the pipeline) is called <em>emitter strategy</em>,\nand the output it produces is called <em>window</em>.</p>\n<p>Thus in order to connect producers and consumers we need the following parts to our streaming system:</p>\n<ul>\n<li>a <code>Stream</code>, keeping metadata for the stream such as its name and when it was created, last read etc.</li>\n<li>a <code>Buffer</code> acting as the buffer where messages sent by producers are stored until the emitting</li>\n<li>a <code>WindowEmitter</code> implementing the emitter strategy</li>\n<li>a <code>Window</code> representing the output produced by the emitter strategy</li>\n</ul>\n<div>\n<p>Note</p>\n<p>The producer accepts input from some external system, say an MQTT end point. The producer\u2019s responsibility is to enter the data into the streaming buffer.\nThe consumer uses an emitter strategy to produce a Window of data that is then forwarded to the user\u2019s processing code.</p>\n</div>\n<p><em>Implementation</em></p>\n<p>minibatch uses MongoDB to implement Streams, Buffers and Windows. Specifically, the following collections are used:</p>\n<ul>\n<li><cite>stream</cite> - represents instances of <cite>Stream</cite>, each document is a stream with a unique name</li>\n<li><cite>buffer</cite> - a virtually endless buffer for all streams in the system, each document contains one message of a stream</li>\n<li><cite>window</cite>- each document represents the data as emitted by the particular emitter strategy</li>\n</ul>\n<p>By default messages go through the following states</p>\n<ol>\n<li>upon append by a producer: message is inserted into <cite>buffer</cite>, with flag <cite>processed = False</cite></li>\n<li>upon being seen by an emitter: message is marked as <cite>processed = True</cite></li>\n<li>upon being emitted: message is copied to <cite>window</cite>, marked <cite>processed = False</cite> (in Window)</li>\n<li>upon emit success (no exceptions raised by the emit function): message is deleted from <cite>buffer</cite>\nand marked <cite>processed = True</cite> in <cite>window</cite></li>\n</ol>\n<p>Notes:</p>\n<ul>\n<li>emitters typically act on a collection of messages, that is steps 2 - 4 are applied to more\nthan one message at a time</li>\n<li>to avoid deleting messages from the buffer, pass <cite>@streaming(\u2026, keep=True)</cite></li>\n<li>custom emitters can modify the behavior of both creating windows and handling the buffer by\noverriding the <cite>process()</cite>, <cite>emit()</cite> and <cite>commit()</cite> methods for each of the above steps\n2/3/4, respectively.</li>\n</ul>\n</div>\n<div id=\"further-development\">\n<h2>Further development</h2>\n<p>Here are a couple of ideas to extend minibatch. Contributions are welcome.</p>\n<ul>\n<li>more examples, following typical streaming examples like word count, filtering</li>\n<li>more emitter strategies, e.g. for sliding windows</li>\n<li>performance testing, benchmarking</li>\n<li>distributed processing of windows via distributed framework such as celery, ray, dask</li>\n<li>extend emitters by typical stream operations e.g. to support operations like count, filter, map, groupby, merge, join</li>\n<li>add other storage backends (e.g. Redis, or some Python-native in-memory db that provides network access and an easy to use ORM layer, like mongoengine does for MongoDB)</li>\n</ul>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>MIT licensed. See LICENSE file.</p>\n</div>\n\n          </div>"}, "last_serial": 6885584, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "afbe3768b8c485303e408e58959d44e3", "sha256": "b53983cb19c78f146743391104a2904e36a2f92cd00f9ee885e18ce3acd73b88"}, "downloads": -1, "filename": "minibatch-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "afbe3768b8c485303e408e58959d44e3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 6204, "upload_time": "2020-01-30T10:52:13", "upload_time_iso_8601": "2020-01-30T10:52:13.224764Z", "url": "https://files.pythonhosted.org/packages/d7/51/a5b8bf38af0e323c64d452f009f4054ade82cf55ad3af370a49e06769e87/minibatch-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "605e2b5e81274ad23f13ec644e589375", "sha256": "5bfe2d6bbc5d8c0f3548563ce129886f0b0f5544d0521cc958edbfd0c723d2c8"}, "downloads": -1, "filename": "minibatch-0.1.tar.gz", "has_sig": false, "md5_digest": "605e2b5e81274ad23f13ec644e589375", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5021, "upload_time": "2020-01-30T10:52:15", "upload_time_iso_8601": "2020-01-30T10:52:15.022342Z", "url": "https://files.pythonhosted.org/packages/a3/4d/255685d3ef84c23afa6e4bb99f5aa2c0d03ffcb85dac68d6edd43f177689/minibatch-0.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "86731d9e0a7bff4dff8d2ddbcccb9627", "sha256": "2136667ce13ba7dbcaf80ee198ea59807d98cd14241c940cf7b26d027d12eef3"}, "downloads": -1, "filename": "minibatch-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "86731d9e0a7bff4dff8d2ddbcccb9627", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9473, "upload_time": "2020-01-30T11:31:54", "upload_time_iso_8601": "2020-01-30T11:31:54.797158Z", "url": "https://files.pythonhosted.org/packages/27/75/809ae7aa7df1bf0468e597cc3cb4a9d6c54543e0c26f68368fdcc56dd835/minibatch-0.2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "20907dd3a4cc11b6f3b92166960c8882", "sha256": "cf0b10b1bc642f960a0099ec99b49b7f1a755b03de1d7fc8914ffc9460dcb3b5"}, "downloads": -1, "filename": "minibatch-0.2.2.tar.gz", "has_sig": false, "md5_digest": "20907dd3a4cc11b6f3b92166960c8882", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8480, "upload_time": "2020-01-30T11:31:56", "upload_time_iso_8601": "2020-01-30T11:31:56.720047Z", "url": "https://files.pythonhosted.org/packages/8a/b7/93cdcbb6dbba9767c78d8c4ed7a2b4d9f50f4117e8e688757de18ec52e8a/minibatch-0.2.2.tar.gz", "yanked": false}], "0.2.2rc2": [{"comment_text": "", "digests": {"md5": "fca756b03ea09cf071d8f4e34403e635", "sha256": "5cb44831fbf3d1449bc2a07faa00af3ea375562766789dc12fe9c36b7e54fc11"}, "downloads": -1, "filename": "minibatch-0.2.2rc2-py3-none-any.whl", "has_sig": false, "md5_digest": "fca756b03ea09cf071d8f4e34403e635", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9515, "upload_time": "2020-01-30T11:31:29", "upload_time_iso_8601": "2020-01-30T11:31:29.499562Z", "url": "https://files.pythonhosted.org/packages/0e/b1/3834851246dc1a1ea1a6ea65fd0dbd62a5950fc54bf231d61b51e23126f2/minibatch-0.2.2rc2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5b3c73dfddc21beb861bc4e0444805af", "sha256": "8722863c388bdd6802b95ee52911c494d1027b2b3299d1e66938a791e9a80ab3"}, "downloads": -1, "filename": "minibatch-0.2.2rc2.tar.gz", "has_sig": false, "md5_digest": "5b3c73dfddc21beb861bc4e0444805af", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8481, "upload_time": "2020-01-30T11:31:31", "upload_time_iso_8601": "2020-01-30T11:31:31.549018Z", "url": "https://files.pythonhosted.org/packages/3c/df/cbb1511ff5bcb4261b0c2fdc17235baa243962e686e520d5eb922bb36f3f/minibatch-0.2.2rc2.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "7628e1fa00c8abdc497a56bec6a5650e", "sha256": "fc8cffade0d41005a655404a6b97f15debf055285022139f1e47df28c69e3df7"}, "downloads": -1, "filename": "minibatch-0.2.3-py3-none-any.whl", "has_sig": false, "md5_digest": "7628e1fa00c8abdc497a56bec6a5650e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 11033, "upload_time": "2020-02-02T10:43:58", "upload_time_iso_8601": "2020-02-02T10:43:58.211660Z", "url": "https://files.pythonhosted.org/packages/75/62/d206bbea6bf819abb007fe643bbbf29095b32ba8afc18d78d5e45c21cbb5/minibatch-0.2.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5a5b4184b304c93e649aa64806762e68", "sha256": "516a676a676ee8ca2a0118b974b20bf2e462cc77564ba221f40dc2cf6f31fef7"}, "downloads": -1, "filename": "minibatch-0.2.3.tar.gz", "has_sig": false, "md5_digest": "5a5b4184b304c93e649aa64806762e68", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13225, "upload_time": "2020-02-02T10:44:00", "upload_time_iso_8601": "2020-02-02T10:44:00.160165Z", "url": "https://files.pythonhosted.org/packages/ae/5a/79de1e8eed5ce0274cae2af6cadad7713c1af777448d27208f8902d3ee63/minibatch-0.2.3.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "06920946e0dafb41f36eaa6336e0c6b8", "sha256": "8e4cc6a978618095eca6e6be835339fa9cdf906338763e05b8a32419e4508550"}, "downloads": -1, "filename": "minibatch-0.2.4-py3-none-any.whl", "has_sig": false, "md5_digest": "06920946e0dafb41f36eaa6336e0c6b8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 31683, "upload_time": "2020-03-25T22:51:50", "upload_time_iso_8601": "2020-03-25T22:51:50.999867Z", "url": "https://files.pythonhosted.org/packages/3b/e1/9fcef634d7a17875a13be314c5bfab4a476050878db52b7e61c2ba346aa6/minibatch-0.2.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f6246d8fc29eda0dcfd190710bb8d251", "sha256": "46e029094e3dabc3c9bcc73fcc4ea8e75340975ad07643b6e77eba4ac2a664d0"}, "downloads": -1, "filename": "minibatch-0.2.4.tar.gz", "has_sig": false, "md5_digest": "f6246d8fc29eda0dcfd190710bb8d251", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26874, "upload_time": "2020-03-25T22:51:52", "upload_time_iso_8601": "2020-03-25T22:51:52.349943Z", "url": "https://files.pythonhosted.org/packages/52/4e/aa9bf539141290ddf15cfaf73392a9103589cdd029a7266ccf3832565501/minibatch-0.2.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "06920946e0dafb41f36eaa6336e0c6b8", "sha256": "8e4cc6a978618095eca6e6be835339fa9cdf906338763e05b8a32419e4508550"}, "downloads": -1, "filename": "minibatch-0.2.4-py3-none-any.whl", "has_sig": false, "md5_digest": "06920946e0dafb41f36eaa6336e0c6b8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 31683, "upload_time": "2020-03-25T22:51:50", "upload_time_iso_8601": "2020-03-25T22:51:50.999867Z", "url": "https://files.pythonhosted.org/packages/3b/e1/9fcef634d7a17875a13be314c5bfab4a476050878db52b7e61c2ba346aa6/minibatch-0.2.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f6246d8fc29eda0dcfd190710bb8d251", "sha256": "46e029094e3dabc3c9bcc73fcc4ea8e75340975ad07643b6e77eba4ac2a664d0"}, "downloads": -1, "filename": "minibatch-0.2.4.tar.gz", "has_sig": false, "md5_digest": "f6246d8fc29eda0dcfd190710bb8d251", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26874, "upload_time": "2020-03-25T22:51:52", "upload_time_iso_8601": "2020-03-25T22:51:52.349943Z", "url": "https://files.pythonhosted.org/packages/52/4e/aa9bf539141290ddf15cfaf73392a9103589cdd029a7266ccf3832565501/minibatch-0.2.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:54:27 2020"}