{"info": {"author": "hysts", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "# A demo program of MPIIGaze and MPIIFaceGaze\n\nWith this program, you can runs gaze estimation on images and videos.\nBy default, the video from a webcam is used.\n\n![MPIIGaze video result](figures/mpiigaze_video.gif)\n![MPIIFaceGaze video result](figures/mpiifacegaze_video.gif)\n\n(The original video is from [this public domain](https://www.pexels.com/video/woman-in-a-group-having-a-drink-while-listening-3201742/).)\n\n![MPIIGaze image result](figures/mpiigaze_image.jpg)\n\n(The original image is from [this public domain](https://www.pexels.com/photo/photography-of-a-beautiful-woman-smiling-1024311/).)\n\nTo train a model, use [this repository](https://github.com/hysts/pytorch_mpiigaze).\n\n## Quick start\n\n### Installation\n\n```bash\npip install ptgaze\n```\n\n\n### Run demo\n\n```bash\nptgaze --mode eye\n```\n\n\n### Usage\n\n\n```\nusage: ptgaze [-h] [--config CONFIG] [--mode {eye,face}]\n              [--face-detector {dlib,face_alignment_dlib,face_alignement_sfd}]\n              [--device {cpu,cuda}] [--image IMAGE] [--video VIDEO]\n              [--camera CAMERA] [--output-dir OUTPUT_DIR] [--ext {avi,mp4}]\n              [--no-screen] [--debug]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config CONFIG       Config file for YACS. When using a config file, all\n                        the other commandline arguments are ignored. See https\n                        ://github.com/hysts/pytorch_mpiigaze_demo/configs/demo\n                        _mpiigaze.yaml\n  --mode {eye,face}     With 'eye', MPIIGaze model will be used. With 'face',\n                        MPIIFaceGaze model will be used. (default: 'eye')\n  --face-detector {dlib,face_alignment_dlib,face_alignement_sfd}\n                        The method used to detect faces and find face\n                        landmarks (default: 'dlib')\n  --device {cpu,cuda}   Device used for model inference.\n  --image IMAGE         Path to an input image file.\n  --video VIDEO         Path to an input video file.\n  --camera CAMERA       Camera calibration file. See https://github.com/hysts/\n                        pytorch_mpiigaze_demo/ptgaze/data/calib/sample_params.\n                        yaml\n  --output-dir OUTPUT_DIR, -o OUTPUT_DIR\n                        If specified, the overlaid video will be saved to this\n                        directory.\n  --ext {avi,mp4}, -e {avi,mp4}\n                        Output video file extension.\n  --no-screen           If specified, the video is not displayed on screen,\n                        and saved to the output directory.\n  --debug\n```\n\nWhile processing an image or video, press the following keys on the window\nto show or hide intermediate results:\n\n* `l`: landmarks\n* `h`: head pose\n* `t`: projected points of 3D face model\n* `b`: face bounding box\n\n\n## References\n\n* Zhang, Xucong, Yusuke Sugano, Mario Fritz, and Andreas Bulling. \"Appearance-based Gaze Estimation in the Wild.\" Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. [arXiv:1504.02863](https://arxiv.org/abs/1504.02863), [Project Page](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild/)\n* Zhang, Xucong, Yusuke Sugano, Mario Fritz, and Andreas Bulling. \"It's Written All Over Your Face: Full-Face Appearance-Based Gaze Estimation.\" Proc. of the IEEE Conference on Computer Vision and Pattern Recognition Workshops(CVPRW), 2017. [arXiv:1611.08860](https://arxiv.org/abs/1611.08860), [Project Page](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/its-written-all-over-your-face-full-face-appearance-based-gaze-estimation/)\n* Zhang, Xucong, Yusuke Sugano, Mario Fritz, and Andreas Bulling. \"MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation.\" IEEE transactions on pattern analysis and machine intelligence 41 (2017). [arXiv:1711.09017](https://arxiv.org/abs/1711.09017)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hysts/pytorch_mpiigaze_demo", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "ptgaze", "package_url": "https://pypi.org/project/ptgaze/", "platform": "", "project_url": "https://pypi.org/project/ptgaze/", "project_urls": {"Homepage": "https://github.com/hysts/pytorch_mpiigaze_demo"}, "release_url": "https://pypi.org/project/ptgaze/0.0.3/", "requires_dist": null, "requires_python": ">=3.7", "summary": "Gaze estimation using MPIIGaze and MPIIFaceGaze", "version": "0.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>A demo program of MPIIGaze and MPIIFaceGaze</h1>\n<p>With this program, you can runs gaze estimation on images and videos.\nBy default, the video from a webcam is used.</p>\n<p><img alt=\"MPIIGaze video result\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/766ffc3292b754b0caa6d7b9eaddb1e4254d3868/666967757265732f6d70696967617a655f766964656f2e676966\">\n<img alt=\"MPIIFaceGaze video result\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/41e1d5c0d602e12454621685797fa7bb0e743ae6/666967757265732f6d7069696661636567617a655f766964656f2e676966\"></p>\n<p>(The original video is from <a href=\"https://www.pexels.com/video/woman-in-a-group-having-a-drink-while-listening-3201742/\" rel=\"nofollow\">this public domain</a>.)</p>\n<p><img alt=\"MPIIGaze image result\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1481e893b78832629fc7bdc5d4000efaa4f3ff9d/666967757265732f6d70696967617a655f696d6167652e6a7067\"></p>\n<p>(The original image is from <a href=\"https://www.pexels.com/photo/photography-of-a-beautiful-woman-smiling-1024311/\" rel=\"nofollow\">this public domain</a>.)</p>\n<p>To train a model, use <a href=\"https://github.com/hysts/pytorch_mpiigaze\" rel=\"nofollow\">this repository</a>.</p>\n<h2>Quick start</h2>\n<h3>Installation</h3>\n<pre>pip install ptgaze\n</pre>\n<h3>Run demo</h3>\n<pre>ptgaze --mode eye\n</pre>\n<h3>Usage</h3>\n<pre><code>usage: ptgaze [-h] [--config CONFIG] [--mode {eye,face}]\n              [--face-detector {dlib,face_alignment_dlib,face_alignement_sfd}]\n              [--device {cpu,cuda}] [--image IMAGE] [--video VIDEO]\n              [--camera CAMERA] [--output-dir OUTPUT_DIR] [--ext {avi,mp4}]\n              [--no-screen] [--debug]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config CONFIG       Config file for YACS. When using a config file, all\n                        the other commandline arguments are ignored. See https\n                        ://github.com/hysts/pytorch_mpiigaze_demo/configs/demo\n                        _mpiigaze.yaml\n  --mode {eye,face}     With 'eye', MPIIGaze model will be used. With 'face',\n                        MPIIFaceGaze model will be used. (default: 'eye')\n  --face-detector {dlib,face_alignment_dlib,face_alignement_sfd}\n                        The method used to detect faces and find face\n                        landmarks (default: 'dlib')\n  --device {cpu,cuda}   Device used for model inference.\n  --image IMAGE         Path to an input image file.\n  --video VIDEO         Path to an input video file.\n  --camera CAMERA       Camera calibration file. See https://github.com/hysts/\n                        pytorch_mpiigaze_demo/ptgaze/data/calib/sample_params.\n                        yaml\n  --output-dir OUTPUT_DIR, -o OUTPUT_DIR\n                        If specified, the overlaid video will be saved to this\n                        directory.\n  --ext {avi,mp4}, -e {avi,mp4}\n                        Output video file extension.\n  --no-screen           If specified, the video is not displayed on screen,\n                        and saved to the output directory.\n  --debug\n</code></pre>\n<p>While processing an image or video, press the following keys on the window\nto show or hide intermediate results:</p>\n<ul>\n<li><code>l</code>: landmarks</li>\n<li><code>h</code>: head pose</li>\n<li><code>t</code>: projected points of 3D face model</li>\n<li><code>b</code>: face bounding box</li>\n</ul>\n<h2>References</h2>\n<ul>\n<li>Zhang, Xucong, Yusuke Sugano, Mario Fritz, and Andreas Bulling. \"Appearance-based Gaze Estimation in the Wild.\" Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. <a href=\"https://arxiv.org/abs/1504.02863\" rel=\"nofollow\">arXiv:1504.02863</a>, <a href=\"https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild/\" rel=\"nofollow\">Project Page</a></li>\n<li>Zhang, Xucong, Yusuke Sugano, Mario Fritz, and Andreas Bulling. \"It's Written All Over Your Face: Full-Face Appearance-Based Gaze Estimation.\" Proc. of the IEEE Conference on Computer Vision and Pattern Recognition Workshops(CVPRW), 2017. <a href=\"https://arxiv.org/abs/1611.08860\" rel=\"nofollow\">arXiv:1611.08860</a>, <a href=\"https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/its-written-all-over-your-face-full-face-appearance-based-gaze-estimation/\" rel=\"nofollow\">Project Page</a></li>\n<li>Zhang, Xucong, Yusuke Sugano, Mario Fritz, and Andreas Bulling. \"MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation.\" IEEE transactions on pattern analysis and machine intelligence 41 (2017). <a href=\"https://arxiv.org/abs/1711.09017\" rel=\"nofollow\">arXiv:1711.09017</a></li>\n</ul>\n\n          </div>"}, "last_serial": 6849828, "releases": {"0.0.3": [{"comment_text": "", "digests": {"md5": "c688f7b16132a24254b20ecb19eec5f0", "sha256": "743caf22eb8fe2c3c3b00ce2d3576c1e9ddc41f6fe04587cafe226358b85e735"}, "downloads": -1, "filename": "ptgaze-0.0.3.tar.gz", "has_sig": false, "md5_digest": "c688f7b16132a24254b20ecb19eec5f0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 19269, "upload_time": "2020-03-20T13:29:53", "upload_time_iso_8601": "2020-03-20T13:29:53.511310Z", "url": "https://files.pythonhosted.org/packages/c8/1a/ae5808526818b7a4ad8e7bc08586d50dc5a2ba056c36b3004b0b810d046a/ptgaze-0.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c688f7b16132a24254b20ecb19eec5f0", "sha256": "743caf22eb8fe2c3c3b00ce2d3576c1e9ddc41f6fe04587cafe226358b85e735"}, "downloads": -1, "filename": "ptgaze-0.0.3.tar.gz", "has_sig": false, "md5_digest": "c688f7b16132a24254b20ecb19eec5f0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 19269, "upload_time": "2020-03-20T13:29:53", "upload_time_iso_8601": "2020-03-20T13:29:53.511310Z", "url": "https://files.pythonhosted.org/packages/c8/1a/ae5808526818b7a4ad8e7bc08586d50dc5a2ba056c36b3004b0b810d046a/ptgaze-0.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:15:13 2020"}