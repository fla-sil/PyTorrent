{"info": {"author": "Miroslav Dudik, Richard Edgar, Brandon Horn, Roman Lutz", "author_email": "fairlearn@microsoft.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "[![Build Status](https://dev.azure.com/responsibleai/fairlearn/_apis/build/status/Nightly?branchName=master)](https://dev.azure.com/responsibleai/fairlearn/_build/latest?definitionId=23&branchName=master) ![MIT license](https://img.shields.io/badge/License-MIT-blue.svg) ![PyPI](https://img.shields.io/pypi/v/fairlearn?color=blue)\n\n# Fairlearn\n\nFairlearn is a Python package that empowers developers of artificial intelligence (AI) systems to assess their system's fairness and mitigate any observed unfairness issues. Fairlearn contains mitigation algorithms as well as a Jupyter widget for model assessment. Besides the source code, this repository also contains Jupyter notebooks with examples of Fairlearn usage.\n\n- [Current release](https://github.com/fairlearn/fairlearn/tree/v0.4.5#current-release)\n- [What we mean by _fairness_](https://github.com/fairlearn/fairlearn/tree/v0.4.5#what-we-mean-by-fairness)\n- [Overview of Fairlearn](https://github.com/fairlearn/fairlearn/tree/v0.4.5#overview-of-fairlearn)\n  - [Fairlearn algorithms](https://github.com/fairlearn/fairlearn/tree/v0.4.5#fairlearn-algorithms)\n  - [Fairlearn dashboard](https://github.com/fairlearn/fairlearn/tree/v0.4.5#fairlearn-dashboard)\n- [Install Fairlearn](https://github.com/fairlearn/fairlearn/tree/v0.4.5#install-fairlearn)\n- [Usage](https://github.com/fairlearn/fairlearn/tree/v0.4.5#usage)\n- [Contributing](https://github.com/fairlearn/fairlearn/tree/v0.4.5#contributing)\n- [Maintainers](https://github.com/fairlearn/fairlearn/tree/v0.4.5#maintainers)\n- [Issues](https://github.com/fairlearn/fairlearn/tree/v0.4.5#issues)\n\n## Current release\n\n- The current stable release is available at [Fairlearn v0.4.5](https://github.com/fairlearn/fairlearn/tree/v0.4.5).\n\n- Our current version differs substantially from version 0.2 or earlier. Users of these older versions should visit our [onboarding guide](https://github.com/fairlearn/fairlearn/tree/v0.4.5#onboarding-guide).\n\n## What we mean by _fairness_\n\nAn AI system can behave unfairly for a variety of reasons. In Fairlearn, we define whether an AI system is behaving unfairly in terms of its impact on people &ndash; i.e., in terms of harms. We focus on two kinds of harms:\n\n- _Allocation harms._ These harms can occur when AI systems extend or withhold opportunities, resources, or information. Some of the key applications are in hiring, school admissions, and lending.\n\n- _Quality-of-service harms._ Quality of service refers to whether a system works as well for one person as it does for another, even if no opportunities, resources, or information are extended or withheld.\n\nWe follow the approach known as **group fairness**, which asks: _Which groups of individuals are at risk for experiencing harms?_ The relevant groups need to be specified by the data scientist and are application specific.\n\nGroup fairness is formalized by a set of constraints, which require that some aspect (or aspects) of the AI system's behavior be comparable across the groups. The Fairlearn package enables assessment and mitigation of unfairness under several common definitions.\nTo learn more about our definitions of fairness, please visit our [terminology page](https://github.com/fairlearn/fairlearn/tree/v0.4.5/TERMINOLOGY.md#fairness-of-ai-systems).\n\n>_Note_:\n> Fairness is fundamentally a sociotechnical challenge. Many aspects of fairness, such as justice and due process, are not captured by quantitative fairness metrics. Furthermore, there are many quantitative fairness metrics which cannot all be satisfied simultaneously. Our goal is to enable humans to assess different mitigation strategies and then make trade-offs appropriate to their scenario.\n\n## Overview of Fairlearn\n\nThe Fairlearn package has two components:\n\n- A _dashboard_ for assessing which groups are negatively impacted by a model, and for comparing multiple models in terms of various fairness and accuracy metrics.\n\n- _Algorithms_ for mitigating unfairness in a variety of AI tasks and along a variety of fairness definitions.\n\n### Fairlearn algorithms\n\nFairlearn contains the following algorithms for mitigating unfairness in binary classification and regression:\n\n| algorithm | description | classification/regression | sensitive features | supported fairness definitions |\n| --- | --- | --- | --- | --- |\n| `fairlearn.` `reductions.` `ExponentiatedGradient` | Black-box approach to fair classification described in [A Reductions Approach to Fair Classification](https://arxiv.org/abs/1803.02453)| binary classification | categorical | DP, EO |\n| `fairlearn.` `reductions.` `GridSearch` | Black-box approach described in Section 3.4 of [A Reductions Approach to Fair Classification](https://arxiv.org/abs/1803.02453)| binary classification | binary | DP, EO |\n| `fairlearn.` `reductions.` `GridSearch` | Black-box approach that implements a grid-search variant of the algorithm described in Section 5 of [Fair Regression: Quantitative Definitions and Reduction-based Algorithms](https://arxiv.org/abs/1905.12843) | regression | binary | BGL |\n| `fairlearn.` `postprocessing.` `ThresholdOptimizer` | Postprocessing algorithm based on the paper [Equality of Opportunity in Supervised Learning](https://arxiv.org/abs/1610.02413). This technique takes as input an existing classifier and the sensitive feature, and derives a monotone transformation of the classifier's prediction to enforce the specified parity constraints. | binary classification | categorical | DP, EO |\n\n> _Note_:\n> DP refers to demographic parity, EO to equalized odds, and BGL to bounded group loss. For more information on these and other terms we use in this repository please refer to the [terminology page](https://github.com/fairlearn/fairlearn/tree/v0.4.5/TERMINOLOGY.md). To request additional algorithms or fairness definitions, please open a [new issue](https://github.com/fairlearn/fairlearn/issues).\n\n\n### Fairlearn dashboard\n\nFairlearn dashboard is a Jupyter notebook widget for assessing how a model's predictions impact different groups (e.g., different ethnicities), and also for comparing multiple models along different fairness and accuracy metrics.  \n\n#### Set-up and a single-model assessment\n\nTo assess a single model's fairness and accuracy, the dashboard widget can be launched within a Jupyter notebook as follows:\n\n```python\nfrom fairlearn.widget import FairlearnDashboard\n\n# A_test containts your sensitive features (e.g., age, binary gender)\n# sensitive_feature_names containts your sensitive feature names\n# y_true contains ground truth labels\n# y_pred contains prediction labels\n\nFairlearnDashboard(sensitive_features=A_test,\n                   sensitive_feature_names=['BinaryGender', 'Age'],\n                   y_true=Y_test.tolist(),\n                   y_pred=[y_pred.tolist()])\n```\n\n\nAfter the launch, the widget walks the user through the assessment set-up, where the user is asked to select (1) the sensitive feature of interest (e.g., binary gender or age), and (2) the accuracy metric (e.g., model precision) along which to evaluate the overall model performance as well as any disparities across groups. These selections are then used to obtain the visualization of the model's impact on the subgroups (e.g., model precision for females and model precision for males).\n\nThe following figures illustrate the set-up steps, where _binary gender_ is selected as a sensitive feature and _accuracy rate_ is selected as the accuracy metric.\n\n\n\n![Dashboard set-up](img/fairlearn-dashboard-config.png)\n\n\n\nAfter the set-up, the dashboard presents the model assessment in two panels:\n\n\n|Panel|Description|\n|-----|-----------|\n| Disparity in accuracy | This panel shows: (1) the accuracy of your model with respect to your selected accuracy metric (e.g., _accuracy rate_) overall as well as on different subgroups based on your selected sensitive feature (e.g., _accuracy rate_ for females, _accuracy rate_ for males); (2) the disparity (difference) in the values  of the selected accuracy metric across different subgroups; (3) the distribution of errors in each subgroup (e.g., female, male). For binary classification, the errors are further split into overprediction (predicting 1 when the true label is 0), and underprediction (predicting 0 when the true label is 1). |\n| Disparity in predictions | This panel shows a bar chart that contains the selection rate in each group, meaning the fraction of data classified as 1 (in binary classification) or distribution of prediction values (in regression). |\n\n\n![Fairness Insights](img/fairlearn-dashboard-results.png)\n\n#### Comparing multiple models\n\nThe dashboard also enables comparison of multiple models, such as the models produced by different learning algorithms and different mitigation approaches, including `fairlearn.reductions.GridSearch`, `fairlearn.reductions.ExponentiatedGradient` and `fairlearn.postprocessing.ThresholdOptimizer`.\n\nAs before, the user is first asked to select the sensitive feature and the accuracy metric. The _model comparison_ view then depicts the accuracy and disparity of all the provided models in a scatter plot. This allows the user to examine trade-offs between accuracy and fairness. Each of the dots can be clicked to open the assessment of the corresponding model. The figure below shows the model comparison view with `binary gender` selected as a sensitive feature and `accuracy rate` selected as the accuracy metric.\n\n\n\n![Accuracy Fairness Tradeoff](img/fairlearn-dashboard-models.png)\n\n\n\n\n\n## Install Fairlearn\n\nThe package can be installed via\n\n```python\npip install fairlearn\n```\n\nor optionally with a full feature set by adding extras, e.g. `pip install fairlearn[customplots]`.\n\nor you can clone the repository locally via\n\n```python\ngit clone git@github.com:fairlearn/fairlearn.git\n```\n\nTo verify that the cloned repository works (the pip package does not include the tests), run\n\n```python\npip install -r requirements.txt\npython -m pytest -s ./test/unit\n```\n\n\n<details name=\"onboarding-guide\">\n<summary>\n<strong>\n<em>\nOnboarding guide for users of version 0.2 or earlier\n</em>\n</strong>\n</summary>\n\nUp to version 0.2, Fairlearn contained only the exponentiated gradient method. The Fairlearn repository now has a more comprehensive scope and aims to incorporate other methods as specified above. The same exponentiated gradient technique is now the class `fairlearn.reductions.ExponentiatedGradient`. While in the past exponentiated gradient was invoked via\n\n```python\nimport numpy as np\nfrom fairlearn.classred import expgrad\nfrom fairlearn.moments import DP\n\nestimator = LogisticRegression()  # or any other estimator\nexponentiated_gradient_result = expgrad(X, sensitive_features, y, estimator, constraints=DP())\npositive_probabilities = exponentiated_gradient_result.best_classifier(X)\nrandomized_predictions = (positive_probabilities >= np.random.rand(len(positive_probabilities))) * 1\n```\n\nthe equivalent operation is now\n\n```python\nfrom fairlearn.reductions import ExponentiatedGradient, DemographicParity\n\nestimator = LogisticRegression()  # or any other estimator\nexponentiated_gradient = ExponentiatedGradient(estimator, constraints=DemographicParity())\nexponentiated_gradient.fit(X, y, sensitive_features=sensitive_features)\nrandomized_predictions = exponentiated_gradient.predict(X)\n```\n\nPlease open a [new issue](https://github.com/fairlearn/fairlearn/issues) if you encounter any problems.\n\n</details>\n\n## Usage\n\nFor common usage refer to the [Jupyter notebooks](./notebooks) and our [API guide](https://github.com/fairlearn/fairlearn/tree/v0.4.5/CONTRIBUTING.md#api)\n\n## Contributing\n\nTo contribute please check our [contributing guide](https://github.com/fairlearn/fairlearn/tree/v0.4.5/CONTRIBUTING.md).\n\n## Maintainers\n\nThe Fairlearn project is maintained by:\n\n- **@MiroDudik**\n- **@riedgar-ms**\n- **@rihorn2**\n- **@romanlutz**\n\nFor a full list of contributors refer to the [authors page](https://github.com/fairlearn/fairlearn/tree/v0.4.5/AUTHORS.md)\n\n## Issues\n\n### Regular (non-security) issues\n\nPlease submit a report through [GitHub issues](https://github.com/fairlearn/fairlearn/issues). A maintainer will respond promptly as follows:\n- **bug**: triage as `bug` and provide estimated timeline based on severity\n- **feature request**: triage as `feature request` and provide estimated timeline\n- **question** or **discussion**: triage as `question` and either respond or notify/identify a suitable expert to respond\n\nMaintainers will try to link duplicate issues when possible.\n\n### Reporting security issues\n\nPlease take a look at our guidelines for reporting [security issues](https://github.com/fairlearn/fairlearn/tree/v0.4.5/SECURITY.md).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/fairlearn/fairlearn", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "fairlearn", "package_url": "https://pypi.org/project/fairlearn/", "platform": "", "project_url": "https://pypi.org/project/fairlearn/", "project_urls": {"Homepage": "https://github.com/fairlearn/fairlearn"}, "release_url": "https://pypi.org/project/fairlearn/0.4.5/", "requires_dist": ["ipywidgets (>=7.5.0)", "numpy (>=1.17.2)", "pandas (>=0.25.1)", "scikit-learn (>=0.22.1)", "scipy (>=1.3.1)", "matplotlib (>=3.0.3) ; extra == 'customplots'"], "requires_python": ">=3.5", "summary": "Algorithms for mitigating unfairness in supervised machine learning", "version": "0.4.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://dev.azure.com/responsibleai/fairlearn/_build/latest?definitionId=23&amp;branchName=master\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/85e4ec5cd43952b64c7c64f23863e9ac31a9d228/68747470733a2f2f6465762e617a7572652e636f6d2f726573706f6e7369626c6561692f666169726c6561726e2f5f617069732f6275696c642f7374617475732f4e696768746c793f6272616e63684e616d653d6d6173746572\"></a> <img alt=\"MIT license\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4150014b4dfdd7b565fa18de88e9bb1b8ccd7c08/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667\"> <img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/65298af2db0ea427521763b1bd3725d79eac1cb2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f666169726c6561726e3f636f6c6f723d626c7565\"></p>\n<h1>Fairlearn</h1>\n<p>Fairlearn is a Python package that empowers developers of artificial intelligence (AI) systems to assess their system's fairness and mitigate any observed unfairness issues. Fairlearn contains mitigation algorithms as well as a Jupyter widget for model assessment. Besides the source code, this repository also contains Jupyter notebooks with examples of Fairlearn usage.</p>\n<ul>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#current-release\" rel=\"nofollow\">Current release</a></li>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#what-we-mean-by-fairness\" rel=\"nofollow\">What we mean by <em>fairness</em></a></li>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#overview-of-fairlearn\" rel=\"nofollow\">Overview of Fairlearn</a>\n<ul>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#fairlearn-algorithms\" rel=\"nofollow\">Fairlearn algorithms</a></li>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#fairlearn-dashboard\" rel=\"nofollow\">Fairlearn dashboard</a></li>\n</ul>\n</li>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#install-fairlearn\" rel=\"nofollow\">Install Fairlearn</a></li>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#usage\" rel=\"nofollow\">Usage</a></li>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#contributing\" rel=\"nofollow\">Contributing</a></li>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#maintainers\" rel=\"nofollow\">Maintainers</a></li>\n<li><a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#issues\" rel=\"nofollow\">Issues</a></li>\n</ul>\n<h2>Current release</h2>\n<ul>\n<li>\n<p>The current stable release is available at <a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5\" rel=\"nofollow\">Fairlearn v0.4.5</a>.</p>\n</li>\n<li>\n<p>Our current version differs substantially from version 0.2 or earlier. Users of these older versions should visit our <a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5#onboarding-guide\" rel=\"nofollow\">onboarding guide</a>.</p>\n</li>\n</ul>\n<h2>What we mean by <em>fairness</em></h2>\n<p>An AI system can behave unfairly for a variety of reasons. In Fairlearn, we define whether an AI system is behaving unfairly in terms of its impact on people \u2013 i.e., in terms of harms. We focus on two kinds of harms:</p>\n<ul>\n<li>\n<p><em>Allocation harms.</em> These harms can occur when AI systems extend or withhold opportunities, resources, or information. Some of the key applications are in hiring, school admissions, and lending.</p>\n</li>\n<li>\n<p><em>Quality-of-service harms.</em> Quality of service refers to whether a system works as well for one person as it does for another, even if no opportunities, resources, or information are extended or withheld.</p>\n</li>\n</ul>\n<p>We follow the approach known as <strong>group fairness</strong>, which asks: <em>Which groups of individuals are at risk for experiencing harms?</em> The relevant groups need to be specified by the data scientist and are application specific.</p>\n<p>Group fairness is formalized by a set of constraints, which require that some aspect (or aspects) of the AI system's behavior be comparable across the groups. The Fairlearn package enables assessment and mitigation of unfairness under several common definitions.\nTo learn more about our definitions of fairness, please visit our <a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5/TERMINOLOGY.md#fairness-of-ai-systems\" rel=\"nofollow\">terminology page</a>.</p>\n<blockquote>\n<p><em>Note</em>:\nFairness is fundamentally a sociotechnical challenge. Many aspects of fairness, such as justice and due process, are not captured by quantitative fairness metrics. Furthermore, there are many quantitative fairness metrics which cannot all be satisfied simultaneously. Our goal is to enable humans to assess different mitigation strategies and then make trade-offs appropriate to their scenario.</p>\n</blockquote>\n<h2>Overview of Fairlearn</h2>\n<p>The Fairlearn package has two components:</p>\n<ul>\n<li>\n<p>A <em>dashboard</em> for assessing which groups are negatively impacted by a model, and for comparing multiple models in terms of various fairness and accuracy metrics.</p>\n</li>\n<li>\n<p><em>Algorithms</em> for mitigating unfairness in a variety of AI tasks and along a variety of fairness definitions.</p>\n</li>\n</ul>\n<h3>Fairlearn algorithms</h3>\n<p>Fairlearn contains the following algorithms for mitigating unfairness in binary classification and regression:</p>\n<table>\n<thead>\n<tr>\n<th>algorithm</th>\n<th>description</th>\n<th>classification/regression</th>\n<th>sensitive features</th>\n<th>supported fairness definitions</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>fairlearn.</code> <code>reductions.</code> <code>ExponentiatedGradient</code></td>\n<td>Black-box approach to fair classification described in <a href=\"https://arxiv.org/abs/1803.02453\" rel=\"nofollow\">A Reductions Approach to Fair Classification</a></td>\n<td>binary classification</td>\n<td>categorical</td>\n<td>DP, EO</td>\n</tr>\n<tr>\n<td><code>fairlearn.</code> <code>reductions.</code> <code>GridSearch</code></td>\n<td>Black-box approach described in Section 3.4 of <a href=\"https://arxiv.org/abs/1803.02453\" rel=\"nofollow\">A Reductions Approach to Fair Classification</a></td>\n<td>binary classification</td>\n<td>binary</td>\n<td>DP, EO</td>\n</tr>\n<tr>\n<td><code>fairlearn.</code> <code>reductions.</code> <code>GridSearch</code></td>\n<td>Black-box approach that implements a grid-search variant of the algorithm described in Section 5 of <a href=\"https://arxiv.org/abs/1905.12843\" rel=\"nofollow\">Fair Regression: Quantitative Definitions and Reduction-based Algorithms</a></td>\n<td>regression</td>\n<td>binary</td>\n<td>BGL</td>\n</tr>\n<tr>\n<td><code>fairlearn.</code> <code>postprocessing.</code> <code>ThresholdOptimizer</code></td>\n<td>Postprocessing algorithm based on the paper <a href=\"https://arxiv.org/abs/1610.02413\" rel=\"nofollow\">Equality of Opportunity in Supervised Learning</a>. This technique takes as input an existing classifier and the sensitive feature, and derives a monotone transformation of the classifier's prediction to enforce the specified parity constraints.</td>\n<td>binary classification</td>\n<td>categorical</td>\n<td>DP, EO</td>\n</tr></tbody></table>\n<blockquote>\n<p><em>Note</em>:\nDP refers to demographic parity, EO to equalized odds, and BGL to bounded group loss. For more information on these and other terms we use in this repository please refer to the <a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5/TERMINOLOGY.md\" rel=\"nofollow\">terminology page</a>. To request additional algorithms or fairness definitions, please open a <a href=\"https://github.com/fairlearn/fairlearn/issues\" rel=\"nofollow\">new issue</a>.</p>\n</blockquote>\n<h3>Fairlearn dashboard</h3>\n<p>Fairlearn dashboard is a Jupyter notebook widget for assessing how a model's predictions impact different groups (e.g., different ethnicities), and also for comparing multiple models along different fairness and accuracy metrics.</p>\n<h4>Set-up and a single-model assessment</h4>\n<p>To assess a single model's fairness and accuracy, the dashboard widget can be launched within a Jupyter notebook as follows:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">fairlearn.widget</span> <span class=\"kn\">import</span> <span class=\"n\">FairlearnDashboard</span>\n\n<span class=\"c1\"># A_test containts your sensitive features (e.g., age, binary gender)</span>\n<span class=\"c1\"># sensitive_feature_names containts your sensitive feature names</span>\n<span class=\"c1\"># y_true contains ground truth labels</span>\n<span class=\"c1\"># y_pred contains prediction labels</span>\n\n<span class=\"n\">FairlearnDashboard</span><span class=\"p\">(</span><span class=\"n\">sensitive_features</span><span class=\"o\">=</span><span class=\"n\">A_test</span><span class=\"p\">,</span>\n                   <span class=\"n\">sensitive_feature_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'BinaryGender'</span><span class=\"p\">,</span> <span class=\"s1\">'Age'</span><span class=\"p\">],</span>\n                   <span class=\"n\">y_true</span><span class=\"o\">=</span><span class=\"n\">Y_test</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n                   <span class=\"n\">y_pred</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">y_pred</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()])</span>\n</pre>\n<p>After the launch, the widget walks the user through the assessment set-up, where the user is asked to select (1) the sensitive feature of interest (e.g., binary gender or age), and (2) the accuracy metric (e.g., model precision) along which to evaluate the overall model performance as well as any disparities across groups. These selections are then used to obtain the visualization of the model's impact on the subgroups (e.g., model precision for females and model precision for males).</p>\n<p>The following figures illustrate the set-up steps, where <em>binary gender</em> is selected as a sensitive feature and <em>accuracy rate</em> is selected as the accuracy metric.</p>\n<p><img alt=\"Dashboard set-up\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8e9c7fc76873c9b31d4cefe0cbc922b3385a5951/696d672f666169726c6561726e2d64617368626f6172642d636f6e6669672e706e67\"></p>\n<p>After the set-up, the dashboard presents the model assessment in two panels:</p>\n<table>\n<thead>\n<tr>\n<th>Panel</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Disparity in accuracy</td>\n<td>This panel shows: (1) the accuracy of your model with respect to your selected accuracy metric (e.g., <em>accuracy rate</em>) overall as well as on different subgroups based on your selected sensitive feature (e.g., <em>accuracy rate</em> for females, <em>accuracy rate</em> for males); (2) the disparity (difference) in the values  of the selected accuracy metric across different subgroups; (3) the distribution of errors in each subgroup (e.g., female, male). For binary classification, the errors are further split into overprediction (predicting 1 when the true label is 0), and underprediction (predicting 0 when the true label is 1).</td>\n</tr>\n<tr>\n<td>Disparity in predictions</td>\n<td>This panel shows a bar chart that contains the selection rate in each group, meaning the fraction of data classified as 1 (in binary classification) or distribution of prediction values (in regression).</td>\n</tr></tbody></table>\n<p><img alt=\"Fairness Insights\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4008790aad6a1a4690ab5644b630b7314f1d586b/696d672f666169726c6561726e2d64617368626f6172642d726573756c74732e706e67\"></p>\n<h4>Comparing multiple models</h4>\n<p>The dashboard also enables comparison of multiple models, such as the models produced by different learning algorithms and different mitigation approaches, including <code>fairlearn.reductions.GridSearch</code>, <code>fairlearn.reductions.ExponentiatedGradient</code> and <code>fairlearn.postprocessing.ThresholdOptimizer</code>.</p>\n<p>As before, the user is first asked to select the sensitive feature and the accuracy metric. The <em>model comparison</em> view then depicts the accuracy and disparity of all the provided models in a scatter plot. This allows the user to examine trade-offs between accuracy and fairness. Each of the dots can be clicked to open the assessment of the corresponding model. The figure below shows the model comparison view with <code>binary gender</code> selected as a sensitive feature and <code>accuracy rate</code> selected as the accuracy metric.</p>\n<p><img alt=\"Accuracy Fairness Tradeoff\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4934db4ba2240b9f27222d553165c563beed2483/696d672f666169726c6561726e2d64617368626f6172642d6d6f64656c732e706e67\"></p>\n<h2>Install Fairlearn</h2>\n<p>The package can be installed via</p>\n<pre><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">fairlearn</span>\n</pre>\n<p>or optionally with a full feature set by adding extras, e.g. <code>pip install fairlearn[customplots]</code>.</p>\n<p>or you can clone the repository locally via</p>\n<pre><span class=\"n\">git</span> <span class=\"n\">clone</span> <span class=\"n\">git</span><span class=\"nd\">@github</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"p\">:</span><span class=\"n\">fairlearn</span><span class=\"o\">/</span><span class=\"n\">fairlearn</span><span class=\"o\">.</span><span class=\"n\">git</span>\n</pre>\n<p>To verify that the cloned repository works (the pip package does not include the tests), run</p>\n<pre><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"o\">-</span><span class=\"n\">r</span> <span class=\"n\">requirements</span><span class=\"o\">.</span><span class=\"n\">txt</span>\n<span class=\"n\">python</span> <span class=\"o\">-</span><span class=\"n\">m</span> <span class=\"n\">pytest</span> <span class=\"o\">-</span><span class=\"n\">s</span> <span class=\"o\">./</span><span class=\"n\">test</span><span class=\"o\">/</span><span class=\"n\">unit</span>\n</pre>\n<details>\n<summary>\n<strong>\n<em>\nOnboarding guide for users of version 0.2 or earlier\n</em>\n</strong>\n</summary>\n<p>Up to version 0.2, Fairlearn contained only the exponentiated gradient method. The Fairlearn repository now has a more comprehensive scope and aims to incorporate other methods as specified above. The same exponentiated gradient technique is now the class <code>fairlearn.reductions.ExponentiatedGradient</code>. While in the past exponentiated gradient was invoked via</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">fairlearn.classred</span> <span class=\"kn\">import</span> <span class=\"n\">expgrad</span>\n<span class=\"kn\">from</span> <span class=\"nn\">fairlearn.moments</span> <span class=\"kn\">import</span> <span class=\"n\">DP</span>\n\n<span class=\"n\">estimator</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">()</span>  <span class=\"c1\"># or any other estimator</span>\n<span class=\"n\">exponentiated_gradient_result</span> <span class=\"o\">=</span> <span class=\"n\">expgrad</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">sensitive_features</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">estimator</span><span class=\"p\">,</span> <span class=\"n\">constraints</span><span class=\"o\">=</span><span class=\"n\">DP</span><span class=\"p\">())</span>\n<span class=\"n\">positive_probabilities</span> <span class=\"o\">=</span> <span class=\"n\">exponentiated_gradient_result</span><span class=\"o\">.</span><span class=\"n\">best_classifier</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n<span class=\"n\">randomized_predictions</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">positive_probabilities</span> <span class=\"o\">&gt;=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">positive_probabilities</span><span class=\"p\">)))</span> <span class=\"o\">*</span> <span class=\"mi\">1</span>\n</pre>\n<p>the equivalent operation is now</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">fairlearn.reductions</span> <span class=\"kn\">import</span> <span class=\"n\">ExponentiatedGradient</span><span class=\"p\">,</span> <span class=\"n\">DemographicParity</span>\n\n<span class=\"n\">estimator</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">()</span>  <span class=\"c1\"># or any other estimator</span>\n<span class=\"n\">exponentiated_gradient</span> <span class=\"o\">=</span> <span class=\"n\">ExponentiatedGradient</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span> <span class=\"n\">constraints</span><span class=\"o\">=</span><span class=\"n\">DemographicParity</span><span class=\"p\">())</span>\n<span class=\"n\">exponentiated_gradient</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">sensitive_features</span><span class=\"o\">=</span><span class=\"n\">sensitive_features</span><span class=\"p\">)</span>\n<span class=\"n\">randomized_predictions</span> <span class=\"o\">=</span> <span class=\"n\">exponentiated_gradient</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n</pre>\n<p>Please open a <a href=\"https://github.com/fairlearn/fairlearn/issues\" rel=\"nofollow\">new issue</a> if you encounter any problems.</p>\n</details>\n<h2>Usage</h2>\n<p>For common usage refer to the <a href=\"./notebooks\" rel=\"nofollow\">Jupyter notebooks</a> and our <a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5/CONTRIBUTING.md#api\" rel=\"nofollow\">API guide</a></p>\n<h2>Contributing</h2>\n<p>To contribute please check our <a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5/CONTRIBUTING.md\" rel=\"nofollow\">contributing guide</a>.</p>\n<h2>Maintainers</h2>\n<p>The Fairlearn project is maintained by:</p>\n<ul>\n<li><strong>@MiroDudik</strong></li>\n<li><strong>@riedgar-ms</strong></li>\n<li><strong>@rihorn2</strong></li>\n<li><strong>@romanlutz</strong></li>\n</ul>\n<p>For a full list of contributors refer to the <a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5/AUTHORS.md\" rel=\"nofollow\">authors page</a></p>\n<h2>Issues</h2>\n<h3>Regular (non-security) issues</h3>\n<p>Please submit a report through <a href=\"https://github.com/fairlearn/fairlearn/issues\" rel=\"nofollow\">GitHub issues</a>. A maintainer will respond promptly as follows:</p>\n<ul>\n<li><strong>bug</strong>: triage as <code>bug</code> and provide estimated timeline based on severity</li>\n<li><strong>feature request</strong>: triage as <code>feature request</code> and provide estimated timeline</li>\n<li><strong>question</strong> or <strong>discussion</strong>: triage as <code>question</code> and either respond or notify/identify a suitable expert to respond</li>\n</ul>\n<p>Maintainers will try to link duplicate issues when possible.</p>\n<h3>Reporting security issues</h3>\n<p>Please take a look at our guidelines for reporting <a href=\"https://github.com/fairlearn/fairlearn/tree/v0.4.5/SECURITY.md\" rel=\"nofollow\">security issues</a>.</p>\n\n          </div>"}, "last_serial": 6960774, "releases": {"0.2.0": [{"comment_text": "", "digests": {"md5": "4a37ec77b2bccf37d7c9477ed3c9a2c5", "sha256": "b6d0bb41c5522a2c50457f666b74de28af95f07ded0cab6032dbd85dfa29c3dd"}, "downloads": -1, "filename": "fairlearn-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "4a37ec77b2bccf37d7c9477ed3c9a2c5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8207, "upload_time": "2018-06-21T01:03:54", "upload_time_iso_8601": "2018-06-21T01:03:54.362367Z", "url": "https://files.pythonhosted.org/packages/7e/62/3420664a50c19646c94e3a27da09b47d44dbac0da9df565b8bbfd8a7e74e/fairlearn-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2de158c42b78f4045ef78f0322286d95", "sha256": "79f35b0daa3e6f0d7e9d860a875c03199a6810734675ceefee2a44b1e8af23d9"}, "downloads": -1, "filename": "fairlearn-0.2.0.tar.gz", "has_sig": false, "md5_digest": "2de158c42b78f4045ef78f0322286d95", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7587, "upload_time": "2018-06-21T01:03:55", "upload_time_iso_8601": "2018-06-21T01:03:55.535969Z", "url": "https://files.pythonhosted.org/packages/30/21/e4d3c914674af57273b64e49d874d8cc20b1d1518044dc6d9179bbf5c334/fairlearn-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "fd42c76caf52574405212a5853d72157", "sha256": "32ae570614b621870281e68c4036b02bc8e5770f0e3639f45b14fe2aa348aaec"}, "downloads": -1, "filename": "fairlearn-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "fd42c76caf52574405212a5853d72157", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 45738, "upload_time": "2019-11-01T19:48:56", "upload_time_iso_8601": "2019-11-01T19:48:56.517656Z", "url": "https://files.pythonhosted.org/packages/9a/f0/943d08c8a724e96c536b42fe470889b2aae0c92dd20f616a73cf681444af/fairlearn-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b3f004f329a52ca45b77644888a6e7f6", "sha256": "af6d48ded4b3dd4cb2a01143721b8f3756f54abe98187d4ff6886eee3a52a2b8"}, "downloads": -1, "filename": "fairlearn-0.3.0.tar.gz", "has_sig": false, "md5_digest": "b3f004f329a52ca45b77644888a6e7f6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 30455, "upload_time": "2019-11-01T19:48:58", "upload_time_iso_8601": "2019-11-01T19:48:58.192842Z", "url": "https://files.pythonhosted.org/packages/36/eb/b0978535fd64359c1edf9f89ab16bea836fb834627ec1c3374c123ee54f8/fairlearn-0.3.0.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "384911092716938bb3c86d37ed2a5cfc", "sha256": "f60db80475f1a2fa3197788cd89d6edf2ce9cba10839bdcf1dbcfb3cdda70bca"}, "downloads": -1, "filename": "fairlearn-0.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "384911092716938bb3c86d37ed2a5cfc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 21620417, "upload_time": "2019-12-09T15:02:30", "upload_time_iso_8601": "2019-12-09T15:02:30.384718Z", "url": "https://files.pythonhosted.org/packages/3b/04/eccc94001eca76b1b55e1e73dcba0be0912640fc4817549ea21e4a114035/fairlearn-0.4.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "57fb6812f9b95cfd7d9a20e110e21371", "sha256": "032feab1a2dcf0a942a74c397ebc5fbeb2364cfda8a93ac537d91a3a03c5ca63"}, "downloads": -1, "filename": "fairlearn-0.4.0.tar.gz", "has_sig": false, "md5_digest": "57fb6812f9b95cfd7d9a20e110e21371", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10718934, "upload_time": "2019-12-09T15:02:35", "upload_time_iso_8601": "2019-12-09T15:02:35.017958Z", "url": "https://files.pythonhosted.org/packages/3b/df/eddb47dcb6828e0ef70aaf97959a25c298bc74971e4663a58ca0ce8fdbcc/fairlearn-0.4.0.tar.gz", "yanked": false}], "0.4.0a0": [{"comment_text": "", "digests": {"md5": "efb4e118d24c271edc485bd87bb4133c", "sha256": "d92cc2998b7dd018115d601a9fe326443911484e736dc64b4a6384ca00bf8f93"}, "downloads": -1, "filename": "fairlearn-0.4.0a0-py3-none-any.whl", "has_sig": false, "md5_digest": "efb4e118d24c271edc485bd87bb4133c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 24178785, "upload_time": "2019-11-19T21:19:13", "upload_time_iso_8601": "2019-11-19T21:19:13.874615Z", "url": "https://files.pythonhosted.org/packages/35/91/fc7e909970c7de9a0a9ac525df531d72ceac1db7446b78b7205fb0686cff/fairlearn-0.4.0a0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "89600cf7f0dc2b10623f77465481c14b", "sha256": "22042599b078b6fe2bbe5004f73938d22240489060fbce681623372fd03f0681"}, "downloads": -1, "filename": "fairlearn-0.4.0a0.tar.gz", "has_sig": false, "md5_digest": "89600cf7f0dc2b10623f77465481c14b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 11995641, "upload_time": "2019-11-19T21:19:18", "upload_time_iso_8601": "2019-11-19T21:19:18.093883Z", "url": "https://files.pythonhosted.org/packages/5d/e7/9c2253838124a904c90a303ac4a62d4ff8c1885a2be573e4d399a536f032/fairlearn-0.4.0a0.tar.gz", "yanked": false}], "0.4.0a1": [{"comment_text": "", "digests": {"md5": "3ed962a2fae81b460d14eebf684c9510", "sha256": "f26405a8cb1f979e2ccb1fe48b4cf427f7218cf6c1b46306429d9590e006e41c"}, "downloads": -1, "filename": "fairlearn-0.4.0a1-py3-none-any.whl", "has_sig": false, "md5_digest": "3ed962a2fae81b460d14eebf684c9510", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 24178399, "upload_time": "2019-11-21T18:18:32", "upload_time_iso_8601": "2019-11-21T18:18:32.038199Z", "url": "https://files.pythonhosted.org/packages/6d/09/3b88fe8c84aa4179801d94940e0e5327d532804f0e96f3dccf586a8ea070/fairlearn-0.4.0a1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "92ae1f726d01a1e342cab436c43c77c0", "sha256": "7158a9ea4ded6f73c01e83f197e8ab1f94ff9ace0c461cc60cf2649a63e36fa8"}, "downloads": -1, "filename": "fairlearn-0.4.0a1.tar.gz", "has_sig": false, "md5_digest": "92ae1f726d01a1e342cab436c43c77c0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 11997176, "upload_time": "2019-11-21T18:18:35", "upload_time_iso_8601": "2019-11-21T18:18:35.990792Z", "url": "https://files.pythonhosted.org/packages/85/0f/dffeb0dfef8c911e78732d9f7745b8ba8c2118f214f9489c0d4aa28422e2/fairlearn-0.4.0a1.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "08c423e4872d4dd5d4b96a0599096aef", "sha256": "ece8fa0b890631293c8224ba54a37104a5b67406e706c5a2b3ab1785184d4648"}, "downloads": -1, "filename": "fairlearn-0.4.1-py3-none-any.whl", "has_sig": false, "md5_digest": "08c423e4872d4dd5d4b96a0599096aef", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 21634824, "upload_time": "2020-01-10T17:34:13", "upload_time_iso_8601": "2020-01-10T17:34:13.668773Z", "url": "https://files.pythonhosted.org/packages/3b/ee/35c4895df3ddd3395d7d155ca6eebee635e26945acadffce8769e5b6536e/fairlearn-0.4.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ee732363cadd2183b855936c8e8e7060", "sha256": "d8049a60c3307f53970bd4191c748b6b022e10d6be83c51d3923ace6babf3eab"}, "downloads": -1, "filename": "fairlearn-0.4.1.tar.gz", "has_sig": false, "md5_digest": "ee732363cadd2183b855936c8e8e7060", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10727114, "upload_time": "2020-01-10T17:34:17", "upload_time_iso_8601": "2020-01-10T17:34:17.823455Z", "url": "https://files.pythonhosted.org/packages/35/8a/4da161946f0ff9243f200e514217916e4425a57a4306f6a8346495f77b37/fairlearn-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "2ec92b4a169595b077243871e5220461", "sha256": "8e79cb016e64367d78e87d049b6d08812a55a79e619dea130501f059ebeaffa3"}, "downloads": -1, "filename": "fairlearn-0.4.2-py3-none-any.whl", "has_sig": false, "md5_digest": "2ec92b4a169595b077243871e5220461", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 21640728, "upload_time": "2020-01-28T20:57:38", "upload_time_iso_8601": "2020-01-28T20:57:38.086842Z", "url": "https://files.pythonhosted.org/packages/b4/93/adf703fa6b640f6557f4721fb484d71ccff1578c9baf370a259da4a60c9e/fairlearn-0.4.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d62a47a2facebcc4475753f38ab6ef6b", "sha256": "f52c6e8d715432dd0fa213ea8b4238ce82119bbe71598d1c32d91bcfe1927c7d"}, "downloads": -1, "filename": "fairlearn-0.4.2.tar.gz", "has_sig": false, "md5_digest": "d62a47a2facebcc4475753f38ab6ef6b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10731458, "upload_time": "2020-01-28T20:57:41", "upload_time_iso_8601": "2020-01-28T20:57:41.740629Z", "url": "https://files.pythonhosted.org/packages/97/4a/a0fef341f2d8ba59a2956837498f64fc075df2ee1c904b9be76cfdf97c0e/fairlearn-0.4.2.tar.gz", "yanked": false}], "0.4.3": [{"comment_text": "", "digests": {"md5": "5d5b455f0b4dc665e78675dd9f021df0", "sha256": "212422f587a5fa4bed6ee274c763f16c3e829d47b69d41072ff433c691a45d78"}, "downloads": -1, "filename": "fairlearn-0.4.3-py3-none-any.whl", "has_sig": false, "md5_digest": "5d5b455f0b4dc665e78675dd9f021df0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 21643655, "upload_time": "2020-02-04T17:52:06", "upload_time_iso_8601": "2020-02-04T17:52:06.426121Z", "url": "https://files.pythonhosted.org/packages/f3/a5/77c0758b8444dcb7c0e37a9079f756baff89296782d32d87b865d8c493cc/fairlearn-0.4.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f21e5b5ac8efa6ee7b33d5f8d0dd28cc", "sha256": "71073c5956eba2f5af2094f297f75474793ea148c625c421fc7d5951dc2a4e9e"}, "downloads": -1, "filename": "fairlearn-0.4.3.tar.gz", "has_sig": false, "md5_digest": "f21e5b5ac8efa6ee7b33d5f8d0dd28cc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10733380, "upload_time": "2020-02-04T17:52:10", "upload_time_iso_8601": "2020-02-04T17:52:10.498480Z", "url": "https://files.pythonhosted.org/packages/89/f8/7e7442b041d1b5ce75e43517deec0a05373a02920c19c7bace987c792660/fairlearn-0.4.3.tar.gz", "yanked": false}], "0.4.4": [{"comment_text": "", "digests": {"md5": "e1ba097c4970e5a73369365fe53a3db9", "sha256": "d27038bbbdbdf7dd6804477cf99c7ae9f7be8f8f6c7d3a0f79840f7cb0c86619"}, "downloads": -1, "filename": "fairlearn-0.4.4-py3-none-any.whl", "has_sig": false, "md5_digest": "e1ba097c4970e5a73369365fe53a3db9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 21643406, "upload_time": "2020-02-19T00:41:13", "upload_time_iso_8601": "2020-02-19T00:41:13.705283Z", "url": "https://files.pythonhosted.org/packages/a9/8a/e4a0d3eeac8b0edf1f3611951c0114c676f17b43f0680c6cf710923a8e52/fairlearn-0.4.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "72e417bede0dc958348a5686681e3207", "sha256": "2dd23833e1c07ed0264a0c5d24c75be3ae0518b1413ae537c060d0146889f09b"}, "downloads": -1, "filename": "fairlearn-0.4.4.tar.gz", "has_sig": false, "md5_digest": "72e417bede0dc958348a5686681e3207", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10736850, "upload_time": "2020-02-19T00:41:17", "upload_time_iso_8601": "2020-02-19T00:41:17.567869Z", "url": "https://files.pythonhosted.org/packages/94/eb/b2d01eb406531c1e2c11a554497fa0f8f9e4c73d8a460b236744647d133e/fairlearn-0.4.4.tar.gz", "yanked": false}], "0.4.5": [{"comment_text": "", "digests": {"md5": "fb9bf6e584714a0dfbe2edcb4ec3f58d", "sha256": "25fa075621fcece4158671e3080751b925133f28137f0a6b31f16c24c63c541e"}, "downloads": -1, "filename": "fairlearn-0.4.5-py3-none-any.whl", "has_sig": false, "md5_digest": "fb9bf6e584714a0dfbe2edcb4ec3f58d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 21204057, "upload_time": "2020-04-06T10:59:40", "upload_time_iso_8601": "2020-04-06T10:59:40.679314Z", "url": "https://files.pythonhosted.org/packages/1b/c3/61e5f2df5dec4e20a69a746021b8e88844a340e9b02518591f5021cabaa2/fairlearn-0.4.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "852749e89a360d9ec9dbdc74a112d1eb", "sha256": "7bd4bc9f807f49a3ce8e058ac1c944477445d4d17894162d52b91227e1acaa3b"}, "downloads": -1, "filename": "fairlearn-0.4.5.tar.gz", "has_sig": false, "md5_digest": "852749e89a360d9ec9dbdc74a112d1eb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10526672, "upload_time": "2020-04-06T10:59:43", "upload_time_iso_8601": "2020-04-06T10:59:43.863323Z", "url": "https://files.pythonhosted.org/packages/b4/99/0dccdc37d31f5c8c82a6bf007f90478296097677381e052e92dc9f484c37/fairlearn-0.4.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fb9bf6e584714a0dfbe2edcb4ec3f58d", "sha256": "25fa075621fcece4158671e3080751b925133f28137f0a6b31f16c24c63c541e"}, "downloads": -1, "filename": "fairlearn-0.4.5-py3-none-any.whl", "has_sig": false, "md5_digest": "fb9bf6e584714a0dfbe2edcb4ec3f58d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 21204057, "upload_time": "2020-04-06T10:59:40", "upload_time_iso_8601": "2020-04-06T10:59:40.679314Z", "url": "https://files.pythonhosted.org/packages/1b/c3/61e5f2df5dec4e20a69a746021b8e88844a340e9b02518591f5021cabaa2/fairlearn-0.4.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "852749e89a360d9ec9dbdc74a112d1eb", "sha256": "7bd4bc9f807f49a3ce8e058ac1c944477445d4d17894162d52b91227e1acaa3b"}, "downloads": -1, "filename": "fairlearn-0.4.5.tar.gz", "has_sig": false, "md5_digest": "852749e89a360d9ec9dbdc74a112d1eb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10526672, "upload_time": "2020-04-06T10:59:43", "upload_time_iso_8601": "2020-04-06T10:59:43.863323Z", "url": "https://files.pythonhosted.org/packages/b4/99/0dccdc37d31f5c8c82a6bf007f90478296097677381e052e92dc9f484c37/fairlearn-0.4.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:59 2020"}