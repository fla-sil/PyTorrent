{"info": {"author": "Nurtas Makhazhanov", "author_email": "makhazhanovn@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Topic :: Software Development :: Build Tools"], "description": "[![Build Status](https://travis-ci.org/cmusphinx/g2p-seq2seq.svg?branch=master)](https://travis-ci.org/cmusphinx/g2p-seq2seq)\n\n# Sequence-to-Sequence G2P toolkit\n\nThe tool does Grapheme-to-Phoneme (G2P) conversion using recurrent\nneural network (RNN) with long short-term memory units (LSTM). LSTM\nsequence-to-sequence models were successfully applied in various tasks,\nincluding machine translation [1] and grapheme-to-phoneme [2].\n\nThis implementation is based on python\n[TensorFlow](https://www.tensorflow.org/tutorials/seq2seq/),\nwhich allows an efficient training on both CPU and GPU.\n\n## Installation\n\nThe tool requires TensorFlow at least version 1.3.0 and Tensor2Tensor with version 1.3.0 or higher. Please see the installation\n[guide](https://www.tensorflow.org/install/)\nfor TensorFlow installation details.\n\nYou can install tensorflow with the following command:\n\n```\nsudo pip install tensorflow-gpu\n```\n\nAnd for installing Tensor2Tensor run:\n\n```\nsudo pip install tensor2tensor\n```\n\nThe g2p_seq2seq package itself uses setuptools, so you can follow standard installation process:\n\n```\nsudo python setup.py install\n```\n\nYou can also run the tests\n\n```\npython setup.py test\n```\n\nThe runnable script `g2p-seq2seq` is installed in  `/usr/local/bin` folder by default (you can adjust it with `setup.py` options if needed) . You need to make sure you have this folder included in your `PATH` so you can run this script from command line.\n\n## Running G2P\n\nA pretrained 2-layer transformer model with 512 hidden units is [available for download on cmusphinx website](https://sourceforge.net/projects/cmusphinx/files/G2P%20Models/g2p-seq2seq-cmudict.tar.gz/download).\nUnpack the model after download. The model is trained on [CMU English dictionary](http://github.com/cmusphinx/cmudict)\n\n```\nwget -O g2p-seq2seq-cmudict.tar.gz https://sourceforge.net/projects/cmusphinx/files/G2P%20Models/g2p-seq2seq-cmudict.tar.gz/download \ntar xf g2p-seq2seq-cmudict.tar.gz\n```\n\nThe easiest way to check how the tool works is to run it the interactive mode and type the words\n\n```\n$ g2p-seq2seq --interactive --model_dir model_folder_path\n...\n> hello\n...\nINFO:tensorflow:HH EH L OW\n...\n>\n```\n\nTo generate pronunciations for an English word list with a trained model, run\n\n```\n  g2p-seq2seq --decode your_wordlist --model_dir model_folder_path [--output decode_output_file_path]\n```\n\nThe wordlist is a text file with one word per line\n\nIf you wish to list top N variants of decoding, set return_beams flag and specify beam_size:\n\n```\n  g2p-seq2seq --decode your_wordlist --model_dir model_folder_path --return_beams --beam_size number_returned_beams [--output decode_output_file_path]\n```\n\nTo evaluate Word Error Rate of the trained model, run\n\n```\n  g2p-seq2seq --evaluate your_test_dictionary --model_dir model_folder_path\n```\n\nThe test dictionary should be a dictionary in standard format:\nHELLO HH EH L OW\nBYE B AY\n\nYou may also calculate Word Error Rate considering all top N beams.\nIn this case we consider word decoding as error only if none of the decoded beams will match with the ground true pronunciation of the word.\n\n## Training G2P system\n\nTo train G2P you need a dictionary (word and phone sequence per line).\nSee an [example dictionary](http://github.com/cmusphinx/cmudict)\n\n```\n  g2p-seq2seq --train train_dictionary.dic --model_dir model_folder_path\n```\n\nYou can set up maximum training steps:\n```\n  \"--max_steps\" - Maximum number of training steps (Default: 0).\n     If 0 train until no improvement is observed\n```\n\nIt is a good idea to play with the following parameters:\n```\n  \"--size\" - Size of each model layer (Default: 64).\n     We observed much better results with 256 units, but the training becomes slower\n\n  \"--num_layers\" - Number of layers in the model (Default: 2). \n     For example, you can try 1 if the train set is not large enough, \n     or 3 to hopefully get better results\n\n  \"--filter_size\" - The size of the filter layer in a convolutional layer (Default: 256)\n\n  \"--dropout\" - The proportion of dropping out units in hidden layers (Default: 0.5)\n\n  \"--attention_dropout\" - The proportion of dropping out units in an attention layer (Default: 0.5)\n\n  \"--num_heads\" - Number of applied heads in Multi-attention mechanism (Default: 2)\n```\n\nYou can manually point out Development and Test datasets:\n```\n  \"--valid\" - Development dictionary (Default: created from train_dictionary.dic)\n  \"--test\" - Test dictionary (Default: created from train_dictionary.dic)\n```\n\nIf you need to continue train a saved model just point out the directory with the existing model:\n```\n  g2p-seq2seq --train train_dictionary.dic --model_dir model_folder_path\n```\n\nAnd, if you want to start training from scratch:\n```\n  \"--reinit\" - Rewrite model in model_folder_path\n```\n\nThe differences in pronunciations between short and long words can be significant. So, seq2seq models applies bucketing technique to take account of such problems. On the other hand, splitting initial data into too many buckets can worse the final results. Because in this case there will be not enough amount of examples in each particular bucket. To get a better results, you may tune following three parameters that change number and size of the buckets:\n```\n  \"--min_length_bucket\" - the size of the minimal bucket (Default: 5)\n  \"--max_length\" - maximal possible length of words or maximal number of phonemes in pronunciations (Default: 40)\n  \"--length_bucket_step\" - multiplier that controls the number of length buckets in the data. The buckets have maximum lengths from min_bucket_length to max_length, increasing by factors of length_bucket_step (Default: 2.0)\n```\n\nTo reproduce the following results, train the model on CMUdict dictionaries during 50 epochs:\n```\n--max_epochs 50\n```\n\n#### Word error rate on CMU dictionary data sets\n\nSystem | WER ([CMUdict PRONALSYL 2007](https://sourceforge.net/projects/cmusphinx/files/G2P%20Models/phonetisaurus-cmudict-split.tar.gz)), % | WER ([CMUdict latest\\*](https://github.com/cmusphinx/cmudict)), %\n--- | --- | ---\nBaseline WFST (Phonetisaurus) | 24.4 | 33.89\nTransformer num_layers=2, size=256   | 22.2 | ~31\n\\* These results pointed out for dictionary without stress.\n\n## References\n---------------------------------------\n\n[1] Lukasz Kaiser. \"Accelerating Deep Learning Research with the Tensor2Tensor Library.\" In Google Research Blog, 2017.\n\n[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lucasz Kaiser, and Illia Polosukhin. \"Attention Is All You Need.\"\narXiv preprint\narXiv:1706.03762, 2017.", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/cmusphinx/g2p-seq2seq", "keywords": "g2p seq2seq tensor2tensor rnnlm", "license": "Apache Software License", "maintainer": "", "maintainer_email": "", "name": "g2p-seq2seq", "package_url": "https://pypi.org/project/g2p-seq2seq/", "platform": "", "project_url": "https://pypi.org/project/g2p-seq2seq/", "project_urls": {"Homepage": "https://github.com/cmusphinx/g2p-seq2seq"}, "release_url": "https://pypi.org/project/g2p-seq2seq/6.0.0a0/", "requires_dist": null, "requires_python": "", "summary": "Grapheme to phoneme module based on Seq2Seq", "version": "6.0.0a0", "yanked": false, "html_description": "<div class=\"project-description\">\n            [![Build Status](https://travis-ci.org/cmusphinx/g2p-seq2seq.svg?branch=master)](https://travis-ci.org/cmusphinx/g2p-seq2seq)<br><br># Sequence-to-Sequence G2P toolkit<br><br>The tool does Grapheme-to-Phoneme (G2P) conversion using recurrent<br>neural network (RNN) with long short-term memory units (LSTM). LSTM<br>sequence-to-sequence models were successfully applied in various tasks,<br>including machine translation [1] and grapheme-to-phoneme [2].<br><br>This implementation is based on python<br>[TensorFlow](https://www.tensorflow.org/tutorials/seq2seq/),<br>which allows an efficient training on both CPU and GPU.<br><br>## Installation<br><br>The tool requires TensorFlow at least version 1.3.0 and Tensor2Tensor with version 1.3.0 or higher. Please see the installation<br>[guide](https://www.tensorflow.org/install/)<br>for TensorFlow installation details.<br><br>You can install tensorflow with the following command:<br><br>```<br>sudo pip install tensorflow-gpu<br>```<br><br>And for installing Tensor2Tensor run:<br><br>```<br>sudo pip install tensor2tensor<br>```<br><br>The g2p_seq2seq package itself uses setuptools, so you can follow standard installation process:<br><br>```<br>sudo python setup.py install<br>```<br><br>You can also run the tests<br><br>```<br>python setup.py test<br>```<br><br>The runnable script `g2p-seq2seq` is installed in  `/usr/local/bin` folder by default (you can adjust it with `setup.py` options if needed) . You need to make sure you have this folder included in your `PATH` so you can run this script from command line.<br><br>## Running G2P<br><br>A pretrained 2-layer transformer model with 512 hidden units is [available for download on cmusphinx website](https://sourceforge.net/projects/cmusphinx/files/G2P%20Models/g2p-seq2seq-cmudict.tar.gz/download).<br>Unpack the model after download. The model is trained on [CMU English dictionary](http://github.com/cmusphinx/cmudict)<br><br>```<br>wget -O g2p-seq2seq-cmudict.tar.gz https://sourceforge.net/projects/cmusphinx/files/G2P%20Models/g2p-seq2seq-cmudict.tar.gz/download <br>tar xf g2p-seq2seq-cmudict.tar.gz<br>```<br><br>The easiest way to check how the tool works is to run it the interactive mode and type the words<br><br>```<br>$ g2p-seq2seq --interactive --model_dir model_folder_path<br>...<br>&gt; hello<br>...<br>INFO:tensorflow:HH EH L OW<br>...<br>&gt;<br>```<br><br>To generate pronunciations for an English word list with a trained model, run<br><br>```<br>  g2p-seq2seq --decode your_wordlist --model_dir model_folder_path [--output decode_output_file_path]<br>```<br><br>The wordlist is a text file with one word per line<br><br>If you wish to list top N variants of decoding, set return_beams flag and specify beam_size:<br><br>```<br>  g2p-seq2seq --decode your_wordlist --model_dir model_folder_path --return_beams --beam_size number_returned_beams [--output decode_output_file_path]<br>```<br><br>To evaluate Word Error Rate of the trained model, run<br><br>```<br>  g2p-seq2seq --evaluate your_test_dictionary --model_dir model_folder_path<br>```<br><br>The test dictionary should be a dictionary in standard format:<br>HELLO HH EH L OW<br>BYE B AY<br><br>You may also calculate Word Error Rate considering all top N beams.<br>In this case we consider word decoding as error only if none of the decoded beams will match with the ground true pronunciation of the word.<br><br>## Training G2P system<br><br>To train G2P you need a dictionary (word and phone sequence per line).<br>See an [example dictionary](http://github.com/cmusphinx/cmudict)<br><br>```<br>  g2p-seq2seq --train train_dictionary.dic --model_dir model_folder_path<br>```<br><br>You can set up maximum training steps:<br>```<br>  \"--max_steps\" - Maximum number of training steps (Default: 0).<br>     If 0 train until no improvement is observed<br>```<br><br>It is a good idea to play with the following parameters:<br>```<br>  \"--size\" - Size of each model layer (Default: 64).<br>     We observed much better results with 256 units, but the training becomes slower<br><br>  \"--num_layers\" - Number of layers in the model (Default: 2). <br>     For example, you can try 1 if the train set is not large enough, <br>     or 3 to hopefully get better results<br><br>  \"--filter_size\" - The size of the filter layer in a convolutional layer (Default: 256)<br><br>  \"--dropout\" - The proportion of dropping out units in hidden layers (Default: 0.5)<br><br>  \"--attention_dropout\" - The proportion of dropping out units in an attention layer (Default: 0.5)<br><br>  \"--num_heads\" - Number of applied heads in Multi-attention mechanism (Default: 2)<br>```<br><br>You can manually point out Development and Test datasets:<br>```<br>  \"--valid\" - Development dictionary (Default: created from train_dictionary.dic)<br>  \"--test\" - Test dictionary (Default: created from train_dictionary.dic)<br>```<br><br>If you need to continue train a saved model just point out the directory with the existing model:<br>```<br>  g2p-seq2seq --train train_dictionary.dic --model_dir model_folder_path<br>```<br><br>And, if you want to start training from scratch:<br>```<br>  \"--reinit\" - Rewrite model in model_folder_path<br>```<br><br>The differences in pronunciations between short and long words can be significant. So, seq2seq models applies bucketing technique to take account of such problems. On the other hand, splitting initial data into too many buckets can worse the final results. Because in this case there will be not enough amount of examples in each particular bucket. To get a better results, you may tune following three parameters that change number and size of the buckets:<br>```<br>  \"--min_length_bucket\" - the size of the minimal bucket (Default: 5)<br>  \"--max_length\" - maximal possible length of words or maximal number of phonemes in pronunciations (Default: 40)<br>  \"--length_bucket_step\" - multiplier that controls the number of length buckets in the data. The buckets have maximum lengths from min_bucket_length to max_length, increasing by factors of length_bucket_step (Default: 2.0)<br>```<br><br>To reproduce the following results, train the model on CMUdict dictionaries during 50 epochs:<br>```<br>--max_epochs 50<br>```<br><br>#### Word error rate on CMU dictionary data sets<br><br>System | WER ([CMUdict PRONALSYL 2007](https://sourceforge.net/projects/cmusphinx/files/G2P%20Models/phonetisaurus-cmudict-split.tar.gz)), % | WER ([CMUdict latest\\*](https://github.com/cmusphinx/cmudict)), %<br>--- | --- | ---<br>Baseline WFST (Phonetisaurus) | 24.4 | 33.89<br>Transformer num_layers=2, size=256   | 22.2 | ~31<br>\\* These results pointed out for dictionary without stress.<br><br>## References<br>---------------------------------------<br><br>[1] Lukasz Kaiser. \"Accelerating Deep Learning Research with the Tensor2Tensor Library.\" In Google Research Blog, 2017.<br><br>[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lucasz Kaiser, and Illia Polosukhin. \"Attention Is All You Need.\"<br>arXiv preprint<br>arXiv:1706.03762, 2017.\n          </div>"}, "last_serial": 3532399, "releases": {"6.0.0a0": [{"comment_text": "", "digests": {"md5": "5002829bfc024c38bcbb7443298396f4", "sha256": "8368e3ad5a60218e0690db88ecd9e15ffe0dc83d46b6e69f848f4a937538e71a"}, "downloads": -1, "filename": "g2p_seq2seq-6.0.0a0.tar.gz", "has_sig": false, "md5_digest": "5002829bfc024c38bcbb7443298396f4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45977, "upload_time": "2018-01-29T18:53:50", "upload_time_iso_8601": "2018-01-29T18:53:50.222401Z", "url": "https://files.pythonhosted.org/packages/4f/f3/ac03b1a0c1ced6b73cc4be1b658b82bd6011f834011ead01b6fdb4e37496/g2p_seq2seq-6.0.0a0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5002829bfc024c38bcbb7443298396f4", "sha256": "8368e3ad5a60218e0690db88ecd9e15ffe0dc83d46b6e69f848f4a937538e71a"}, "downloads": -1, "filename": "g2p_seq2seq-6.0.0a0.tar.gz", "has_sig": false, "md5_digest": "5002829bfc024c38bcbb7443298396f4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45977, "upload_time": "2018-01-29T18:53:50", "upload_time_iso_8601": "2018-01-29T18:53:50.222401Z", "url": "https://files.pythonhosted.org/packages/4f/f3/ac03b1a0c1ced6b73cc4be1b658b82bd6011f834011ead01b6fdb4e37496/g2p_seq2seq-6.0.0a0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:59:25 2020"}