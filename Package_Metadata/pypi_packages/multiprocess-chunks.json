{"info": {"author": "Malcolm van Raalte", "author_email": "malcolm@van.raalte.ca", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Software Development :: Build Tools"], "description": "# multiprocess_chunks\n\nChunk-based, multiprocess processing of iterables.\nUses the `multiprocess` package to perform the multiprocessization.\nUses the `cloudpickle` to pickle hard-to-pickle objects.\n\n#### Why is this useful?\n\nWhen using the built-in Python `multiprocessing.Pool.map` method the items being iterated are individually pickled. This can lead to a lot of pickling which can negatively affect performance. This is particularly true, and not necessarily obvious, if extra data is passed into the `f` function via a lambda. For example:\n\n```\nfrom multiprocessing import Pool\nd = {...} # a large dict of some sort\np.map(lamda x: x + d[x], [1, 2, 3, ...])\n```\n\nIn this case both `x` and `d` are pickled, individually, for every item in `[1, 2, 3, ...]`.\n\nThe methods in this package divide the `[1, 2, 3, ...]` list into chunks and pickle each chunk and `d` a small number of times.\n\n## Installation\n\n```\npip install multiprocess-chunks\n```\n\n## Usage\n\nThere are two methods to choose from: `map_list_as_chunks` and `map_list_in_chunks`.\n\n#### map_list_as_chunks\n\nThis method divides the iterable that is passed to it into chunks.\nThe chunks are then processed in multiprocess.\nIt returns the mapped chunks.\n\nParameters:\n`def map_list_as_chunks(l, f, extra_data, cpus=None, max_chunk_size=None`)\n\n- `l`: The iterable to process in multiprocess.\n- `f`: The function that processes each chunk. It takes two parameters: - `chunk, extra_data`\n- `extra_data`: Data that is passed into `f` for each chunk.\n- `cpus`: The number of CPUs to use. If `None` the number of cores on the system will be used. This value decides how many chunks to create.\n- `max_chunk_size`: Limits the chunk size.\n\nExample:\n\n```\nfrom multiprocess_chunks import map_list_as_chunks\n\nl = range(0, 10)\nf = lambda chunk, ed: [c * ed for c in chunk]\nresult = map_list_as_chunks(l, f, 5, 2)\n# result = [ [0, 5, 10, 15, 20], [25, 30, 35, 40, 45] ]\n```\n\n#### map_list_in_chunks\n\nThis method divides the iterable that is passed to it into chunks.\nThe chunks are then processed in multiprocess.\nIt unwinds the processed chunks to return the processed items.\n\nParameters:\n`def map_list_in_chunks(l, f, extra_data)`\n\n- `l`: The iterable to process in multiprocess.\n- `f`: The function that processes each chunk. It takes two parameters: `item, extra_data`\n- `extra_data`: Data that is passed into `f` for each chunk.\n\nExample:\n\n```\nfrom multiprocess_chunks import map_list_in_chunks\n\nl = range(0, 10)\nf = lambda item, ed: item * ed\nresult = map_list_in_chunks(l, f, 5)\n# result = [0, 5, 10, 15, 20 25, 30, 35, 40, 45]\n```\n\nEssentially, `map_list_in_chunks` gives the same output as `multiprocessing.Pool.map` but, behind the scenes, it is chunking and being efficient about pickling.\n\n#### A note on pickling\n\nThis package uses the `pathos` package to perform the multiprocessization and the `cloudpickle` package to perform pickling. This allows it to pickle objects that Python's built-in `multiprocessing` cannot.\n\n#### Performance\n\nHow much better will your code perform? There are many factors at play here. The only way to know is to do your own timings.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/malcolmvr/multiprocess-chunks/archive/v1.0.0.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/malcolmvr/multiprocess-chunks", "keywords": "multiprocess,multiprocessing,pickle,pickling,chunks,map", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "multiprocess-chunks", "package_url": "https://pypi.org/project/multiprocess-chunks/", "platform": "", "project_url": "https://pypi.org/project/multiprocess-chunks/", "project_urls": {"Download": "https://github.com/malcolmvr/multiprocess-chunks/archive/v1.0.0.tar.gz", "Homepage": "https://github.com/malcolmvr/multiprocess-chunks"}, "release_url": "https://pypi.org/project/multiprocess-chunks/1.0.0/", "requires_dist": null, "requires_python": ">=3.3", "summary": "Chunk-based, multiprocess processing of iterables.", "version": "1.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>multiprocess_chunks</h1>\n<p>Chunk-based, multiprocess processing of iterables.\nUses the <code>multiprocess</code> package to perform the multiprocessization.\nUses the <code>cloudpickle</code> to pickle hard-to-pickle objects.</p>\n<h4>Why is this useful?</h4>\n<p>When using the built-in Python <code>multiprocessing.Pool.map</code> method the items being iterated are individually pickled. This can lead to a lot of pickling which can negatively affect performance. This is particularly true, and not necessarily obvious, if extra data is passed into the <code>f</code> function via a lambda. For example:</p>\n<pre><code>from multiprocessing import Pool\nd = {...} # a large dict of some sort\np.map(lamda x: x + d[x], [1, 2, 3, ...])\n</code></pre>\n<p>In this case both <code>x</code> and <code>d</code> are pickled, individually, for every item in <code>[1, 2, 3, ...]</code>.</p>\n<p>The methods in this package divide the <code>[1, 2, 3, ...]</code> list into chunks and pickle each chunk and <code>d</code> a small number of times.</p>\n<h2>Installation</h2>\n<pre><code>pip install multiprocess-chunks\n</code></pre>\n<h2>Usage</h2>\n<p>There are two methods to choose from: <code>map_list_as_chunks</code> and <code>map_list_in_chunks</code>.</p>\n<h4>map_list_as_chunks</h4>\n<p>This method divides the iterable that is passed to it into chunks.\nThe chunks are then processed in multiprocess.\nIt returns the mapped chunks.</p>\n<p>Parameters:\n<code>def map_list_as_chunks(l, f, extra_data, cpus=None, max_chunk_size=None</code>)</p>\n<ul>\n<li><code>l</code>: The iterable to process in multiprocess.</li>\n<li><code>f</code>: The function that processes each chunk. It takes two parameters: - <code>chunk, extra_data</code></li>\n<li><code>extra_data</code>: Data that is passed into <code>f</code> for each chunk.</li>\n<li><code>cpus</code>: The number of CPUs to use. If <code>None</code> the number of cores on the system will be used. This value decides how many chunks to create.</li>\n<li><code>max_chunk_size</code>: Limits the chunk size.</li>\n</ul>\n<p>Example:</p>\n<pre><code>from multiprocess_chunks import map_list_as_chunks\n\nl = range(0, 10)\nf = lambda chunk, ed: [c * ed for c in chunk]\nresult = map_list_as_chunks(l, f, 5, 2)\n# result = [ [0, 5, 10, 15, 20], [25, 30, 35, 40, 45] ]\n</code></pre>\n<h4>map_list_in_chunks</h4>\n<p>This method divides the iterable that is passed to it into chunks.\nThe chunks are then processed in multiprocess.\nIt unwinds the processed chunks to return the processed items.</p>\n<p>Parameters:\n<code>def map_list_in_chunks(l, f, extra_data)</code></p>\n<ul>\n<li><code>l</code>: The iterable to process in multiprocess.</li>\n<li><code>f</code>: The function that processes each chunk. It takes two parameters: <code>item, extra_data</code></li>\n<li><code>extra_data</code>: Data that is passed into <code>f</code> for each chunk.</li>\n</ul>\n<p>Example:</p>\n<pre><code>from multiprocess_chunks import map_list_in_chunks\n\nl = range(0, 10)\nf = lambda item, ed: item * ed\nresult = map_list_in_chunks(l, f, 5)\n# result = [0, 5, 10, 15, 20 25, 30, 35, 40, 45]\n</code></pre>\n<p>Essentially, <code>map_list_in_chunks</code> gives the same output as <code>multiprocessing.Pool.map</code> but, behind the scenes, it is chunking and being efficient about pickling.</p>\n<h4>A note on pickling</h4>\n<p>This package uses the <code>pathos</code> package to perform the multiprocessization and the <code>cloudpickle</code> package to perform pickling. This allows it to pickle objects that Python's built-in <code>multiprocessing</code> cannot.</p>\n<h4>Performance</h4>\n<p>How much better will your code perform? There are many factors at play here. The only way to know is to do your own timings.</p>\n\n          </div>"}, "last_serial": 6758300, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "d9cf53429420c9d18a711c9eb14a5187", "sha256": "dfdb6e18979779340b7289c4c188168577218ccfd1f40b6ec841b1c4b328c3c6"}, "downloads": -1, "filename": "multiprocess_chunks-1.0.0.tar.gz", "has_sig": false, "md5_digest": "d9cf53429420c9d18a711c9eb14a5187", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.3", "size": 3901, "upload_time": "2020-03-05T22:11:51", "upload_time_iso_8601": "2020-03-05T22:11:51.752389Z", "url": "https://files.pythonhosted.org/packages/99/bc/67af1aeab9efce27301e01b7e0fc94634d843c9a27f21ecfc12b97829201/multiprocess_chunks-1.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d9cf53429420c9d18a711c9eb14a5187", "sha256": "dfdb6e18979779340b7289c4c188168577218ccfd1f40b6ec841b1c4b328c3c6"}, "downloads": -1, "filename": "multiprocess_chunks-1.0.0.tar.gz", "has_sig": false, "md5_digest": "d9cf53429420c9d18a711c9eb14a5187", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.3", "size": 3901, "upload_time": "2020-03-05T22:11:51", "upload_time_iso_8601": "2020-03-05T22:11:51.752389Z", "url": "https://files.pythonhosted.org/packages/99/bc/67af1aeab9efce27301e01b7e0fc94634d843c9a27f21ecfc12b97829201/multiprocess_chunks-1.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:22 2020"}