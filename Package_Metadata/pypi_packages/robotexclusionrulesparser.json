{"info": {"author": "Philip Semanchuk", "author_email": "philip@pyspoken.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Win32 (MS Windows)", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Operating System :: MacOS :: MacOS X", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 3", "Topic :: Utilities"], "description": "Robotexclusionrulesparser is an alternative to the Python standard library\nmodule robotparser. It fetches and parses robots.txt files and can answer\nquestions as to whether or not a given user agent is permitted to visit a \ncertain URL.\n\nThis module has some features that the standard library module robotparser \ndoes not, including the ability to decode non-ASCII robots.txt files, respect\nfor Expires headers and understanding of Crawl-delay and Sitemap directives \nand wildcard syntax in path names.\n\nComplete documentation (including a comparison with the standard library\nmodule robotparser) is available in ReadMe.html.\n\nRobotexclusionrulesparser is released under a BSD license.", "description_content_type": null, "docs_url": null, "download_url": "http://nikitathespider.com/python/rerp/robotexclusionrulesparser-1.7.1.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://nikitathespider.com/python/rerp/", "keywords": "robots.txt robot parser", "license": "http://creativecommons.org/licenses/BSD/", "maintainer": null, "maintainer_email": null, "name": "robotexclusionrulesparser", "package_url": "https://pypi.org/project/robotexclusionrulesparser/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/robotexclusionrulesparser/", "project_urls": {"Download": "http://nikitathespider.com/python/rerp/robotexclusionrulesparser-1.7.1.tar.gz", "Homepage": "http://nikitathespider.com/python/rerp/"}, "release_url": "https://pypi.org/project/robotexclusionrulesparser/1.7.1/", "requires_dist": null, "requires_python": null, "summary": "A robots.txt parser alternative to Python's robotparser module", "version": "1.7.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Robotexclusionrulesparser is an alternative to the Python standard library\nmodule robotparser. It fetches and parses robots.txt files and can answer\nquestions as to whether or not a given user agent is permitted to visit a\ncertain URL.</p>\n<p>This module has some features that the standard library module robotparser\ndoes not, including the ability to decode non-ASCII robots.txt files, respect\nfor Expires headers and understanding of Crawl-delay and Sitemap directives\nand wildcard syntax in path names.</p>\n<p>Complete documentation (including a comparison with the standard library\nmodule robotparser) is available in ReadMe.html.</p>\n<p>Robotexclusionrulesparser is released under a BSD license.</p>\n\n          </div>"}, "last_serial": 2278265, "releases": {"1.0": [], "1.1": [], "1.2": [], "1.2.1": [], "1.2.2": [], "1.3": [], "1.6.1": [], "1.6.2": [{"comment_text": "", "digests": {"md5": "a4e9e55f621d0495b26f43d31dd7585a", "sha256": "1e7cde5d9ccd1ba9b02f91001665b0ac6ba029b46b3ab5fd1e34023dd4d40a51"}, "downloads": -1, "filename": "robotexclusionrulesparser-1.6.2.tar.gz", "has_sig": false, "md5_digest": "a4e9e55f621d0495b26f43d31dd7585a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 27429, "upload_time": "2014-03-25T18:40:14", "upload_time_iso_8601": "2014-03-25T18:40:14.282153Z", "url": "https://files.pythonhosted.org/packages/bc/ef/8970b5237fe60a401688b6c1bb9845bbff86afffc87070f4388adc116756/robotexclusionrulesparser-1.6.2.tar.gz", "yanked": false}], "1.7.0": [{"comment_text": "", "digests": {"md5": "44baeb13c9e08b8e76391635ba202564", "sha256": "62dd21db1d2e7860cad26fd4017948d27849b29b417bd929d4c0022f557bb47b"}, "downloads": -1, "filename": "robotexclusionrulesparser-1.7.0.tar.gz", "has_sig": false, "md5_digest": "44baeb13c9e08b8e76391635ba202564", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31161, "upload_time": "2015-12-29T19:11:57", "upload_time_iso_8601": "2015-12-29T19:11:57.606534Z", "url": "https://files.pythonhosted.org/packages/dd/67/043137d3f3ebb1b1af92e9a3927ec0208ace277d3bcca6baca986e0b1b4f/robotexclusionrulesparser-1.7.0.tar.gz", "yanked": false}], "1.7.1": [{"comment_text": "", "digests": {"md5": "f11ccefc9ec9397db8fc8e62b79c93ef", "sha256": "d23aa14ae8145c13c95612d696736bad52a4bd0819ce8c9437ee745098fb8388"}, "downloads": -1, "filename": "robotexclusionrulesparser-1.7.1.tar.gz", "has_sig": false, "md5_digest": "f11ccefc9ec9397db8fc8e62b79c93ef", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31494, "upload_time": "2016-08-12T15:53:17", "upload_time_iso_8601": "2016-08-12T15:53:17.737520Z", "url": "https://files.pythonhosted.org/packages/39/97/74634de03a0856160a8c2fa92f03cdf1827c3b1d3d42378d4b79119cd9fa/robotexclusionrulesparser-1.7.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f11ccefc9ec9397db8fc8e62b79c93ef", "sha256": "d23aa14ae8145c13c95612d696736bad52a4bd0819ce8c9437ee745098fb8388"}, "downloads": -1, "filename": "robotexclusionrulesparser-1.7.1.tar.gz", "has_sig": false, "md5_digest": "f11ccefc9ec9397db8fc8e62b79c93ef", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31494, "upload_time": "2016-08-12T15:53:17", "upload_time_iso_8601": "2016-08-12T15:53:17.737520Z", "url": "https://files.pythonhosted.org/packages/39/97/74634de03a0856160a8c2fa92f03cdf1827c3b1d3d42378d4b79119cd9fa/robotexclusionrulesparser-1.7.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:01:59 2020"}