{"info": {"author": "Fridolin Pokorny", "author_email": "fridolin@redhat.com", "bugtrack_url": null, "classifiers": [], "description": "Thoth Storages\n--------------\n\nThis library provides a library called `thoth-storages\n<https://pypi.org/project/thoth-storages>`_ used in project `Thoth\n<https://thoth-station.ninja>`_.  The library exposes core queries and methods\nfor PostgreSQL database as well as adapters for manipulating with Ceph via its\nS3 compatible API.\n\nInstallation and Usage\n======================\n\nThe library can be installed via pip or Pipenv from\n`PyPI <https://pypi.org/project/thoth-storages>`_:\n\n.. code-block:: console\n\n   pipenv install thoth-storages\n\nThe library does not provide any CLI, it is rather a low level library\nsupporting other parts of Thoth.\n\nYou can run prepared testsuite via the following command:\n\n.. code-block:: console\n\n  pipenv install --dev\n  pipenv run python3 setup.py test\n\n  # To generate docs:\n  pipenv run python3 setup.py build_sphinx\n\nRunning PostgreSQL locally\n==========================\n\nYou can use `docker-compose.yaml` present in this repository to run a local PostgreSQL instance, (make sure you installed `podman-compose <https://github.com/containers/podman-compose>`_):\n\n.. code-block:: console\n\n  $ podman-compose up\n\nAfter running the command above, you should be able to access a local PostgreSQL instance at `localhost:5432`. This is also the default configuration for PostgreSQL's adapter - you don't need to provide `GRAPH_SERVICE_HOST` explicitly. The default configuration uses database named `postgres` which can be accessed using `postgres` user and `postgres` password (SSL is disabled).\n\nThe provided `docker-compose.yaml` has also PGweb enabled for to have an UI for the database content. To access it visit `http://localhost:8081/ <http://localhost:8081>`_.\n\nThe provided `docker-compose.yaml` does not use any volume. After you containers restart, the content will not be available anymore.\n\nIf you would like to experiment with PostgreSQL programatically, you can use the following code snippet as a starting point:\n\n.. code-block:: python\n\n  from thoth.storages import GraphDatabase\n\n  graph = GraphDatabase()\n  graph.connect()\n  # To clear database:\n  # graph.drop_all()\n  # To initialize schema in the graph database:\n  # graph.initialize_schema()\n\nGenerating migrations and schema adjustment in deployment\n=========================================================\n\nIf you make any changes to data model of the main PostgreSQL database, you need\nto generate migrations. These migrations state how to adjust already existing\ndatabase with data in deployments. For this purpose, `Alembic migrations\n<https://alembic.sqlalchemy.org>`_ are used. Alembic can (`partially\n<https://alembic.sqlalchemy.org/en/latest/autogenerate.html#what-does-autogenerate-detect-and-what-does-it-not-detect>`_)\nautomatically detect what has changed and how to adjust already existing\ndatabase in a deployment.\n\nAlembic uses incremental version control, where each migration is versioned and\nstates how to migrate from previous state of database to the desired next state - these\nversions are present in `alembic/versions` directory and are automatically\ngenerated with procedure described bellow.\n\nIf you make any changes, follow the following steps which will generate version\nfor you:\n\n1. make sure your local PostgreSQL instance is running (follow `Running\n   PostgreSQL locally` instructions above):\n\n  .. code-block:: console\n\n    $ podman-compose up\n\n2. Run Alembic CLI to generate versions for you:\n\n  .. code-block:: console\n\n    # Make sure you have your environment setup:\n    # pipenv install --dev\n    # Make sure you are running the most recent version of schema:\n    $ PYTHONPATH=. pipenv run alembic upgrade head\n    # Actually generate a new version:\n    $ PYTHONPATH=. pipenv run alembic revision --autogenerate -m \"Added row to calculate sum of sums which will be divided by 42\"\n\n3. Review migrations generated by Alembic. Note `NOT all changes are\n   automatically detected by Alembic\n   <https://alembic.sqlalchemy.org/en/latest/autogenerate.html#what-does-autogenerate-detect-and-what-does-it-not-detect>`_.\n\n4. Make sure generated migrations are part of your pull request so changes are\n   propagated to deployments:\n\n\n  .. code-block:: console\n\n    $ git add thoth/storages/data/alembic/versions/\n\n4. In a deployment, use Management API and its `/graph/initialize` endpoint to\n   propagate database schema changes in deployment (Management API has to have\n   recent schema changes present which are populated with new `thoth-storages`\n   releases).\n\n5. If running locally and you would like to propagate changes, run the following Alembic command to update migrations to the latest version:\n\n  .. code-block:: console\n\n    $ PYTHONPATH=. pipenv run alembic upgrade head\n\n\n  If you would like to update schema programmatically run the following Python code:\n\n  .. code-block:: python\n\n    from thoth.storages import GraphDatabase\n\n    graph = GraphDatabase()\n    graph.connect()\n    graph.initilize_schema()\n\nGenerate schema images\n======================\n\nYou can use shipped CLI ``thoth-storages`` to automatically generate schema images out of the current models:\n\n.. code-block:: console\n\n  # First, make sure you have dev packages installed:\n  pipenv install --dev\n  PYTHONPATH=. pipenv run python3 ./thoth-storages generate-schema\n\nThe command above will produce 2 images named ``schema.png`` and\n``schema_cache.png``. The first PNG file shows schema for the main PostgreSQL\ninstance and the latter one, as the name suggests, shows how cache schema looks\nlike.\n\n\nIf the command above fails with the following exception:\n\n.. code-block:: python\n\n  FileNotFoundError: [Errno 2] \"dot\" not found in path.\n\nmake sure you have `graphviz` package installed:\n\n.. code-block:: console\n\n  dnf install -y graphviz\n\nCreating own performance indicators\n===================================\n\nYou can create your own performance indicators. To create own performance\nindicator, create a script which tests desired functionality of a library. An\nexample can be matrix multiplication script present in `performance\n<https://github.com/thoth-station/performance/blob/master/tensorflow/matmul.py>`_\nrepository. This script can be supplied to Dependency Monkey to validate\ncertain combination of libraries in desired runtime and buildtime environment\nor directly on Amun API which will run the given script using desired software\nand hardware configuration. Please follow instructions on how to create a\nperformance script shown in the `README of performance repo\n<https://github.com/thoth-station/performance>`_.\n\nTo create relevant models, adjust `thoth/storages/graph/models_performance.py` file\nand add your model. Describe parameters (reported in `@parameters` section of\nperformance indicator result) and result (reported in `@result`). The name of\nclass should match `name` which is reported by performance indicator run.\n\n.. code-block:: python\n\n  class PiMatmul(Base, BaseExtension, PerformanceIndicatorBase):\n      \"\"\"A class for representing a matrix multiplication micro-performance test.\"\"\"\n\n      # Device used during performance indicator run - CPU/GPU/TPU/...\n      device = Column(String(128), nullable=False)\n      matrix_size = Column(Integer, nullable=False)\n      dtype = Column(String(128), nullable=False)\n      reps = Column(Integer, nullable=False)\n      elapsed = Column(Float, nullable=False)\n      rate = Column(Float, nullable=False)\n\nAll the models use `SQLAchemy <https://www.sqlalchemy.org/>`_.\nSee `docs <https://docs.sqlalchemy.org/>`_ for more info.\n\nOnline debugging of queries\n===========================\n\nYou can print to logger all the queries that are performed to a PostgreSQL instance. To do so, set the following environment variable:\n\n.. code-block::\n\n  export THOTH_STORAGES_DEBUG_QUERIES=1\n\nOnline debugging of queries\n===========================\n\nYou can print information about PostgreSQL adapter together with statisics on\nthe graph cache and memory cache usage to logger (it has to have at least level\n`INFO` set). To do so, set the following environment variable:\n\n.. code-block::\n\n  export THOTH_STORAGES_LOG_STATS=1\n\nThese statistics will be printed once the database adapter is destructed.\n\nCreating backups from Thoth deployment\n======================================\n\nYou can use `pg_dump` and `psql` utilities to create dumps and restore\nthe database content from dumps. This tool is pre-installed in the container image\nwhich is running PostgreSQL so the only thing you need to do is execute\n`pg_dump` in Thoth's deployment in a PostgreSQL container to create a dump, use\n`oc cp` to retrieve dump (or directly use `oc exec` and create the dump from the\ncluster) and subsequently `psql` to restore the database content. The\nprerequisite for this is to have access to the running container (edit rights).\n\n.. code-block:: console\n\n  # Execute the following commands from the root of this Git repo:\n  # List PostgreSQL pods running:\n  $ oc get pod -l name=postgresql\n  NAME                 READY     STATUS    RESTARTS   AGE\n  postgresql-1-glwnr   1/1       Running   0          3d\n  # Open remote shell to the running container in the PostgreSQL pod:\n  $ oc rsh -t postgresql-1-glwnr bash\n  # Perform dump of the database:\n  (cluster-postgres) $ pg_dump > pg_dump-$(date +\"%s\").sql\n  (cluster-postgres) $ ls pg_dump-*.sql   # Remember the current dump name\n  (cluster-postgres) pg_dump-1569491024.sql\n  (cluster-postgres) $ exit\n  # Copy the dump to the current dir:\n  $ oc cp thoth-test-core/postgresql-1-glwnr:/opt/app-root/src/pg_dump-1569491024.sql  .\n  # Start local PostgreSQL instance:\n  $ podman-compose up --detach\n  <logs will show up>\n  $ psql -h localhost -p 5432 --username=postgres < pg_dump-1569491024.sql\n  password: <type password \"postgres\" here>\n  <logs will show up>\n\nSyncing results of jobs run in the cluster\n==========================================\n\nEach job in the cluster reports a JSON which states necessary information about\nthe job run (metadata) and actual job results. These results of jobs are stored\non object storage `Ceph <https://ceph.io/>`_ via S3 compatible API and later on\nsynced via graph syncs to the knowledge graph. The component responsible for\ngraph syncs is `graph-sync-job\n<https://github.com/thoth-station/graph-sync-job>`_ which is written generic\nenough to sync any data and report metrics about synced data so you don't need\nto provide such logic on each new workload registered in the system. To sync\nyour own results of job results (workload) done in the cluster, implement\nrelated syncing logic in the `sync.py\n<https://github.com/thoth-station/storages/blob/master/thoth/storages/sync.py>`_\nand register handler in the ``_HANDLERS_MAPPING`` in the same file. The mapping\nmaps prefix of the document id to the handler (function) which is responsible\nfor syncing data into the knowledge base (please mind signatures of existing\nsyncing funcions to automatically integrate with ``sync_documents`` function\nwhich is called from ``graph-sync-job``).\n\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/thoth-station/storages", "keywords": "", "license": "GPLv3+", "maintainer": "Francesco Murdaca", "maintainer_email": "fmurdaca@redhat.com", "name": "thoth-storage", "package_url": "https://pypi.org/project/thoth-storage/", "platform": "", "project_url": "https://pypi.org/project/thoth-storage/", "project_urls": {"Homepage": "https://github.com/thoth-station/storages"}, "release_url": "https://pypi.org/project/thoth-storage/0.19.17/", "requires_dist": ["click", "voluptuous", "boto3", "thoth-common", "amun", "python-dateutil", "thoth-python", "pyyaml", "methodtools", "sqlalchemy", "psycopg2-binary", "sqlalchemy-utils", "alembic"], "requires_python": "", "summary": "Storage and database adapters available in project Thoth", "version": "0.19.17", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This library provides a library called <a href=\"https://pypi.org/project/thoth-storages\" rel=\"nofollow\">thoth-storages</a> used in project <a href=\"https://thoth-station.ninja\" rel=\"nofollow\">Thoth</a>.  The library exposes core queries and methods\nfor PostgreSQL database as well as adapters for manipulating with Ceph via its\nS3 compatible API.</p>\n<div id=\"installation-and-usage\">\n<h2>Installation and Usage</h2>\n<p>The library can be installed via pip or Pipenv from\n<a href=\"https://pypi.org/project/thoth-storages\" rel=\"nofollow\">PyPI</a>:</p>\n<pre><span class=\"go\">pipenv install thoth-storages</span>\n</pre>\n<p>The library does not provide any CLI, it is rather a low level library\nsupporting other parts of Thoth.</p>\n<p>You can run prepared testsuite via the following command:</p>\n<pre><span class=\"go\">pipenv install --dev\npipenv run python3 setup.py test\n\n</span><span class=\"gp\">#</span> To generate docs:\n<span class=\"go\">pipenv run python3 setup.py build_sphinx</span>\n</pre>\n</div>\n<div id=\"running-postgresql-locally\">\n<h2>Running PostgreSQL locally</h2>\n<p>You can use <cite>docker-compose.yaml</cite> present in this repository to run a local PostgreSQL instance, (make sure you installed <a href=\"https://github.com/containers/podman-compose\" rel=\"nofollow\">podman-compose</a>):</p>\n<pre><span class=\"gp\">$</span> podman-compose up\n</pre>\n<p>After running the command above, you should be able to access a local PostgreSQL instance at <cite>localhost:5432</cite>. This is also the default configuration for PostgreSQL\u2019s adapter - you don\u2019t need to provide <cite>GRAPH_SERVICE_HOST</cite> explicitly. The default configuration uses database named <cite>postgres</cite> which can be accessed using <cite>postgres</cite> user and <cite>postgres</cite> password (SSL is disabled).</p>\n<p>The provided <cite>docker-compose.yaml</cite> has also PGweb enabled for to have an UI for the database content. To access it visit <a href=\"http://localhost:8081\" rel=\"nofollow\">http://localhost:8081/</a>.</p>\n<p>The provided <cite>docker-compose.yaml</cite> does not use any volume. After you containers restart, the content will not be available anymore.</p>\n<p>If you would like to experiment with PostgreSQL programatically, you can use the following code snippet as a starting point:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">thoth.storages</span> <span class=\"kn\">import</span> <span class=\"n\">GraphDatabase</span>\n\n<span class=\"n\">graph</span> <span class=\"o\">=</span> <span class=\"n\">GraphDatabase</span><span class=\"p\">()</span>\n<span class=\"n\">graph</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">()</span>\n<span class=\"c1\"># To clear database:</span>\n<span class=\"c1\"># graph.drop_all()</span>\n<span class=\"c1\"># To initialize schema in the graph database:</span>\n<span class=\"c1\"># graph.initialize_schema()</span>\n</pre>\n</div>\n<div id=\"generating-migrations-and-schema-adjustment-in-deployment\">\n<h2>Generating migrations and schema adjustment in deployment</h2>\n<p>If you make any changes to data model of the main PostgreSQL database, you need\nto generate migrations. These migrations state how to adjust already existing\ndatabase with data in deployments. For this purpose, <a href=\"https://alembic.sqlalchemy.org\" rel=\"nofollow\">Alembic migrations</a> are used. Alembic can (<a href=\"https://alembic.sqlalchemy.org/en/latest/autogenerate.html#what-does-autogenerate-detect-and-what-does-it-not-detect\" rel=\"nofollow\">partially</a>)\nautomatically detect what has changed and how to adjust already existing\ndatabase in a deployment.</p>\n<p>Alembic uses incremental version control, where each migration is versioned and\nstates how to migrate from previous state of database to the desired next state - these\nversions are present in <cite>alembic/versions</cite> directory and are automatically\ngenerated with procedure described bellow.</p>\n<p>If you make any changes, follow the following steps which will generate version\nfor you:</p>\n<ol>\n<li>make sure your local PostgreSQL instance is running (follow <cite>Running\nPostgreSQL locally</cite> instructions above):</li>\n</ol>\n<blockquote>\n<pre><span class=\"gp\">$</span> podman-compose up\n</pre>\n</blockquote>\n<ol>\n<li>Run Alembic CLI to generate versions for you:</li>\n</ol>\n<blockquote>\n<pre><span class=\"gp\">#</span> Make sure you have your environment setup:\n<span class=\"gp\">#</span> pipenv install --dev\n<span class=\"gp\">#</span> Make sure you are running the most recent version of schema:\n<span class=\"gp\">$</span> <span class=\"nv\">PYTHONPATH</span><span class=\"o\">=</span>. pipenv run alembic upgrade head\n<span class=\"gp\">#</span> Actually generate a new version:\n<span class=\"gp\">$</span> <span class=\"nv\">PYTHONPATH</span><span class=\"o\">=</span>. pipenv run alembic revision --autogenerate -m <span class=\"s2\">\"Added row to calculate sum of sums which will be divided by 42\"</span>\n</pre>\n</blockquote>\n<ol>\n<li>Review migrations generated by Alembic. Note <a href=\"https://alembic.sqlalchemy.org/en/latest/autogenerate.html#what-does-autogenerate-detect-and-what-does-it-not-detect\" rel=\"nofollow\">NOT all changes are\nautomatically detected by Alembic</a>.</li>\n<li>Make sure generated migrations are part of your pull request so changes are\npropagated to deployments:</li>\n</ol>\n<blockquote>\n<pre><span class=\"gp\">$</span> git add thoth/storages/data/alembic/versions/\n</pre>\n</blockquote>\n<ol>\n<li>In a deployment, use Management API and its <cite>/graph/initialize</cite> endpoint to\npropagate database schema changes in deployment (Management API has to have\nrecent schema changes present which are populated with new <cite>thoth-storages</cite>\nreleases).</li>\n<li>If running locally and you would like to propagate changes, run the following Alembic command to update migrations to the latest version:</li>\n</ol>\n<blockquote>\n<pre><span class=\"gp\">$</span> <span class=\"nv\">PYTHONPATH</span><span class=\"o\">=</span>. pipenv run alembic upgrade head\n</pre>\n<p>If you would like to update schema programmatically run the following Python code:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">thoth.storages</span> <span class=\"kn\">import</span> <span class=\"n\">GraphDatabase</span>\n\n<span class=\"n\">graph</span> <span class=\"o\">=</span> <span class=\"n\">GraphDatabase</span><span class=\"p\">()</span>\n<span class=\"n\">graph</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">()</span>\n<span class=\"n\">graph</span><span class=\"o\">.</span><span class=\"n\">initilize_schema</span><span class=\"p\">()</span>\n</pre>\n</blockquote>\n</div>\n<div id=\"generate-schema-images\">\n<h2>Generate schema images</h2>\n<p>You can use shipped CLI <tt><span class=\"pre\">thoth-storages</span></tt> to automatically generate schema images out of the current models:</p>\n<pre><span class=\"gp\">#</span> First, make sure you have dev packages installed:\n<span class=\"go\">pipenv install --dev\nPYTHONPATH=. pipenv run python3 ./thoth-storages generate-schema</span>\n</pre>\n<p>The command above will produce 2 images named <tt>schema.png</tt> and\n<tt>schema_cache.png</tt>. The first PNG file shows schema for the main PostgreSQL\ninstance and the latter one, as the name suggests, shows how cache schema looks\nlike.</p>\n<p>If the command above fails with the following exception:</p>\n<pre><span class=\"ne\">FileNotFoundError</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">Errno</span> <span class=\"mi\">2</span><span class=\"p\">]</span> <span class=\"s2\">\"dot\"</span> <span class=\"ow\">not</span> <span class=\"n\">found</span> <span class=\"ow\">in</span> <span class=\"n\">path</span><span class=\"o\">.</span>\n</pre>\n<p>make sure you have <cite>graphviz</cite> package installed:</p>\n<pre><span class=\"go\">dnf install -y graphviz</span>\n</pre>\n</div>\n<div id=\"creating-own-performance-indicators\">\n<h2>Creating own performance indicators</h2>\n<p>You can create your own performance indicators. To create own performance\nindicator, create a script which tests desired functionality of a library. An\nexample can be matrix multiplication script present in <a href=\"https://github.com/thoth-station/performance/blob/master/tensorflow/matmul.py\" rel=\"nofollow\">performance</a>\nrepository. This script can be supplied to Dependency Monkey to validate\ncertain combination of libraries in desired runtime and buildtime environment\nor directly on Amun API which will run the given script using desired software\nand hardware configuration. Please follow instructions on how to create a\nperformance script shown in the <a href=\"https://github.com/thoth-station/performance\" rel=\"nofollow\">README of performance repo</a>.</p>\n<p>To create relevant models, adjust <cite>thoth/storages/graph/models_performance.py</cite> file\nand add your model. Describe parameters (reported in <cite>@parameters</cite> section of\nperformance indicator result) and result (reported in <cite>@result</cite>). The name of\nclass should match <cite>name</cite> which is reported by performance indicator run.</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">PiMatmul</span><span class=\"p\">(</span><span class=\"n\">Base</span><span class=\"p\">,</span> <span class=\"n\">BaseExtension</span><span class=\"p\">,</span> <span class=\"n\">PerformanceIndicatorBase</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"A class for representing a matrix multiplication micro-performance test.\"\"\"</span>\n\n    <span class=\"c1\"># Device used during performance indicator run - CPU/GPU/TPU/...</span>\n    <span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">Column</span><span class=\"p\">(</span><span class=\"n\">String</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">),</span> <span class=\"n\">nullable</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n    <span class=\"n\">matrix_size</span> <span class=\"o\">=</span> <span class=\"n\">Column</span><span class=\"p\">(</span><span class=\"n\">Integer</span><span class=\"p\">,</span> <span class=\"n\">nullable</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n    <span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">Column</span><span class=\"p\">(</span><span class=\"n\">String</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">),</span> <span class=\"n\">nullable</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n    <span class=\"n\">reps</span> <span class=\"o\">=</span> <span class=\"n\">Column</span><span class=\"p\">(</span><span class=\"n\">Integer</span><span class=\"p\">,</span> <span class=\"n\">nullable</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n    <span class=\"n\">elapsed</span> <span class=\"o\">=</span> <span class=\"n\">Column</span><span class=\"p\">(</span><span class=\"n\">Float</span><span class=\"p\">,</span> <span class=\"n\">nullable</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n    <span class=\"n\">rate</span> <span class=\"o\">=</span> <span class=\"n\">Column</span><span class=\"p\">(</span><span class=\"n\">Float</span><span class=\"p\">,</span> <span class=\"n\">nullable</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n<p>All the models use <a href=\"https://www.sqlalchemy.org/\" rel=\"nofollow\">SQLAchemy</a>.\nSee <a href=\"https://docs.sqlalchemy.org/\" rel=\"nofollow\">docs</a> for more info.</p>\n</div>\n<div id=\"online-debugging-of-queries\">\n<h2>Online debugging of queries</h2>\n<p>You can print to logger all the queries that are performed to a PostgreSQL instance. To do so, set the following environment variable:</p>\n<pre>export THOTH_STORAGES_DEBUG_QUERIES=1\n</pre>\n</div>\n<div id=\"id2\">\n<h2>Online debugging of queries</h2>\n<p>You can print information about PostgreSQL adapter together with statisics on\nthe graph cache and memory cache usage to logger (it has to have at least level\n<cite>INFO</cite> set). To do so, set the following environment variable:</p>\n<pre>export THOTH_STORAGES_LOG_STATS=1\n</pre>\n<p>These statistics will be printed once the database adapter is destructed.</p>\n</div>\n<div id=\"creating-backups-from-thoth-deployment\">\n<h2>Creating backups from Thoth deployment</h2>\n<p>You can use <cite>pg_dump</cite> and <cite>psql</cite> utilities to create dumps and restore\nthe database content from dumps. This tool is pre-installed in the container image\nwhich is running PostgreSQL so the only thing you need to do is execute\n<cite>pg_dump</cite> in Thoth\u2019s deployment in a PostgreSQL container to create a dump, use\n<cite>oc cp</cite> to retrieve dump (or directly use <cite>oc exec</cite> and create the dump from the\ncluster) and subsequently <cite>psql</cite> to restore the database content. The\nprerequisite for this is to have access to the running container (edit rights).</p>\n<pre><span class=\"gp\">#</span> Execute the following commands from the root of this Git repo:\n<span class=\"gp\">#</span> List PostgreSQL pods running:\n<span class=\"gp\">$</span> oc get pod -l <span class=\"nv\">name</span><span class=\"o\">=</span>postgresql\n<span class=\"go\">NAME                 READY     STATUS    RESTARTS   AGE\npostgresql-1-glwnr   1/1       Running   0          3d\n</span><span class=\"gp\">#</span> Open remote shell to the running container in the PostgreSQL pod:\n<span class=\"gp\">$</span> oc rsh -t postgresql-1-glwnr bash\n<span class=\"gp\">#</span> Perform dump of the database:\n<span class=\"gp-VirtualEnv\">(cluster-postgres)</span> <span class=\"gp\">$</span> pg_dump &gt; pg_dump-<span class=\"k\">$(</span>date +<span class=\"s2\">\"%s\"</span><span class=\"k\">)</span>.sql\n<span class=\"gp-VirtualEnv\">(cluster-postgres)</span> <span class=\"gp\">$</span> ls pg_dump-*.sql   <span class=\"c1\"># Remember the current dump name\n</span><span class=\"gp-VirtualEnv\">(cluster-postgres)</span><span class=\"c1\"></span> <span class=\"c1\"></span><span class=\"go\">pg_dump-1569491024.sql\n</span><span class=\"gp-VirtualEnv\">(cluster-postgres)</span> <span class=\"gp\">$</span> <span class=\"nb\">exit</span>\n<span class=\"gp\">#</span> Copy the dump to the current dir:\n<span class=\"gp\">$</span> oc cp thoth-test-core/postgresql-1-glwnr:/opt/app-root/src/pg_dump-1569491024.sql  .\n<span class=\"gp\">#</span> Start <span class=\"nb\">local</span> PostgreSQL instance:\n<span class=\"gp\">$</span> podman-compose up --detach\n<span class=\"go\">&lt;logs will show up&gt;\n</span><span class=\"gp\">$</span> psql -h localhost -p <span class=\"m\">5432</span> --username<span class=\"o\">=</span>postgres &lt; pg_dump-1569491024.sql\n<span class=\"go\">password: &lt;type password \"postgres\" here&gt;\n&lt;logs will show up&gt;</span>\n</pre>\n</div>\n<div id=\"syncing-results-of-jobs-run-in-the-cluster\">\n<h2>Syncing results of jobs run in the cluster</h2>\n<p>Each job in the cluster reports a JSON which states necessary information about\nthe job run (metadata) and actual job results. These results of jobs are stored\non object storage <a href=\"https://ceph.io/\" rel=\"nofollow\">Ceph</a> via S3 compatible API and later on\nsynced via graph syncs to the knowledge graph. The component responsible for\ngraph syncs is <a href=\"https://github.com/thoth-station/graph-sync-job\" rel=\"nofollow\">graph-sync-job</a> which is written generic\nenough to sync any data and report metrics about synced data so you don\u2019t need\nto provide such logic on each new workload registered in the system. To sync\nyour own results of job results (workload) done in the cluster, implement\nrelated syncing logic in the <a href=\"https://github.com/thoth-station/storages/blob/master/thoth/storages/sync.py\" rel=\"nofollow\">sync.py</a>\nand register handler in the <tt>_HANDLERS_MAPPING</tt> in the same file. The mapping\nmaps prefix of the document id to the handler (function) which is responsible\nfor syncing data into the knowledge base (please mind signatures of existing\nsyncing funcions to automatically integrate with <tt>sync_documents</tt> function\nwhich is called from <tt><span class=\"pre\">graph-sync-job</span></tt>).</p>\n</div>\n\n          </div>"}, "last_serial": 6093601, "releases": {"0.19.17": [{"comment_text": "", "digests": {"md5": "1f748509600f31aa28245b3dfb7e896d", "sha256": "28561a7d6ae653fbd377b5c2f12a32e90bf7e12e66882873f20b407a8b0cef4d"}, "downloads": -1, "filename": "thoth_storage-0.19.17-py3-none-any.whl", "has_sig": false, "md5_digest": "1f748509600f31aa28245b3dfb7e896d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 94546, "upload_time": "2019-11-07T14:20:19", "upload_time_iso_8601": "2019-11-07T14:20:19.791523Z", "url": "https://files.pythonhosted.org/packages/16/96/990909026c43b0b1da03c6a1ed2b492970b6961c29292012ac870ca042b9/thoth_storage-0.19.17-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1f748509600f31aa28245b3dfb7e896d", "sha256": "28561a7d6ae653fbd377b5c2f12a32e90bf7e12e66882873f20b407a8b0cef4d"}, "downloads": -1, "filename": "thoth_storage-0.19.17-py3-none-any.whl", "has_sig": false, "md5_digest": "1f748509600f31aa28245b3dfb7e896d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 94546, "upload_time": "2019-11-07T14:20:19", "upload_time_iso_8601": "2019-11-07T14:20:19.791523Z", "url": "https://files.pythonhosted.org/packages/16/96/990909026c43b0b1da03c6a1ed2b492970b6961c29292012ac870ca042b9/thoth_storage-0.19.17-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 02:53:49 2020"}