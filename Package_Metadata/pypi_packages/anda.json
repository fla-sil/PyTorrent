{"info": {"author": "Vojtech Kase", "author_email": "vojtech.kase@gmail.com", "bugtrack_url": null, "classifiers": ["License :: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# anda\n\n[toc]\n\n```bash\npip install anda\n```\n\n\n\nThis is a Python package for collecting, manipulation and visualizing various ancient Mediterranean data. It focus on their temporal, textual and spatial aspects. It is structured into several gradually evolving submodules, namely `gr`, `imda`, `concs`, and `textnet`.\n\n## anda.gr\n\n```python\nfrom anda import gr\n```\n\nThis module is dedicated to preprocessing of ancient Greek textual data. It contains functions for lemmatization, posttagging and translation. It relies heavely on Morhesus Dictionary. \n\n### Lemmatization\n\nA minimal usage is to lemmatize individual word. You can  either ask for only the first lemma (`return_first_lemma()`) or for all possibilities (`return_all_unique_lemmata()`.  In most cases , the outcome is the same:\n\n```python\ngr.return_first_lemma(\"\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03bd\")\n> '\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7'\n\ngr.return_all_unique_lemmata(\"\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03bd\")\n> '\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7'\n```\n\nAbove these are functions `lemmatize_string()` and `gr.get_lemmatized_sentences()`. Both work with string of any length. The first returns a list of lemmata. The second returns a list of lemmatized sentences.\n\n```python\nstring = \"\u03a0\u03c1\u03cc\u03c4\u03b1\u03c3\u03b9\u03c2 \u03bc\u1f72\u03bd \u03bf\u1f56\u03bd \u1f10\u03c3\u03c4\u1f76 \u03bb\u03cc\u03b3\u03bf\u03c2 \u03ba\u03b1\u03c4\u03b1\u03c6\u03b1\u03c4\u03b9\u03ba\u1f78\u03c2 \u1f22 \u1f00\u03c0\u03bf\u03c6\u03b1\u03c4\u03b9\u03ba\u1f78\u03c2 \u03c4\u03b9\u03bd\u1f78\u03c2 \u03ba\u03b1\u03c4\u03ac \u03c4\u03b9\u03bd\u03bf\u03c2. \u039f\u1f57\u03c4\u03bf\u03c2 \u03b4\u1f72 \u1f22 \u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5 \u1f22 \u1f10\u03bd \u03bc\u03ad\u03c1\u03b5\u03b9 \u1f22 \u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03c2. \u039b\u03ad\u03b3\u03c9 \u03b4\u1f72 \u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5 \u03bc\u1f72\u03bd \u03c4\u1f78 \u03c0\u03b1\u03bd\u03c4\u1f76 \u1f22 \u03bc\u03b7\u03b4\u03b5\u03bd\u1f76 \u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd, \u1f10\u03bd \u03bc\u03ad\u03c1\u03b5\u03b9 \u03b4\u1f72 \u03c4\u1f78 \u03c4\u03b9\u03bd\u1f76 \u1f22 \u03bc\u1f74 \u03c4\u03b9\u03bd\u1f76 \u1f22 \u03bc\u1f74 \u03c0\u03b1\u03bd\u03c4\u1f76 \u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd, \u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03bd \u03b4\u1f72 \u03c4\u1f78 \u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd \u1f22 \u03bc\u1f74 \u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd \u1f04\u03bd\u03b5\u03c5 \u03c4\u03bf\u1fe6 \u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5, \u1f22 \u03ba\u03b1\u03c4\u1f70 \u03bc\u03ad\u03c1\u03bf\u03c2, \u03bf\u1f37\u03bf\u03bd \u03c4\u1f78 \u03c4\u1ff6\u03bd \u1f10\u03bd\u03b1\u03bd\u03c4\u03af\u03c9\u03bd \u03b5\u1f36\u03bd\u03b1\u03b9 \u03c4\u1f74\u03bd \u03b1\u1f50\u03c4\u1f74\u03bd \u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03bd \u1f22 \u03c4\u1f78 \u03c4\u1f74\u03bd \u1f21\u03b4\u03bf\u03bd\u1f74\u03bd \u03bc\u1f74 \u03b5\u1f36\u03bd\u03b1\u03b9 \u1f00\u03b3\u03b1\u03b8\u03cc\u03bd.\"\n\ngr.lemmatize_string(string)\n> ['\u03c0\u03c1\u03cc\u03c4\u03b1\u03c3\u03b9\u03c2', '\u03bb\u03cc\u03b3\u03bf\u03c2', '\u03ba\u03b1\u03c4\u03b1\u03c6\u03b1\u03c4\u03b9\u03ba\u03cc\u03c2', '\u1f00\u03c0\u03bf\u03c6\u03b1\u03c4\u03b9\u03ba\u03cc\u03c2', '\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5', '\u03bc\u03ad\u03c1\u03bf\u03c2', '\u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03c2', '\u03bb\u03ad\u03b3\u03c9', '\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5', '\u03c0\u1fb6\u03c2', '\u03bc\u03b7\u03b4\u03b5\u03af\u03c2', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u03bc\u03ad\u03c1\u03bf\u03c2', '\u03c0\u1fb6\u03c2', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03bd', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u1f04\u03bd\u03b5\u03c5', '\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5', '\u03bc\u03ad\u03c1\u03bf\u03c2', '\u03bf\u1f37\u03bf\u03c2', '\u1f10\u03bd\u03b1\u03bd\u03c4\u03af\u03bf\u03c2', '\u03b1\u1f50\u03c4\u03b7\u03bd', '\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7', '\u1f21\u03b4\u03bf\u03bd\u03b7\u03bd', '\u1f00\u03b3\u03b1\u03b8\u03cc\u03c2']\n\ngr.get_lemmatized_sentences(string)\n> [['\u03c0\u03c1\u03cc\u03c4\u03b1\u03c3\u03b9\u03c2', '\u03bb\u03cc\u03b3\u03bf\u03c2', '\u03ba\u03b1\u03c4\u03b1\u03c6\u03b1\u03c4\u03b9\u03ba\u03cc\u03c2', '\u1f00\u03c0\u03bf\u03c6\u03b1\u03c4\u03b9\u03ba\u03cc\u03c2'], ['\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5', '\u03bc\u03ad\u03c1\u03bf\u03c2', '\u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03c2'], ['\u03bb\u03ad\u03b3\u03c9', '\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5', '\u03c0\u1fb6\u03c2', '\u03bc\u03b7\u03b4\u03b5\u03af\u03c2', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u03bc\u03ad\u03c1\u03bf\u03c2', '\u03c0\u1fb6\u03c2', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03bd', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u1f04\u03bd\u03b5\u03c5', '\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5', '\u03bc\u03ad\u03c1\u03bf\u03c2', '\u03bf\u1f37\u03bf\u03c2', '\u1f10\u03bd\u03b1\u03bd\u03c4\u03af\u03bf\u03c2', '\u03b1\u1f50\u03c4\u03b7\u03bd', '\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7', '\u1f21\u03b4\u03bf\u03bd\u03b7\u03bd', '\u1f00\u03b3\u03b1\u03b8\u03cc\u03c2']]\n```\n\nAll lemmatization functions can be further parametrized by several arguments\n\n* `all_lemmata=False` : \n* `filter_by_postag=[\"n\",\"a\",\"v\"]`: returns only nouns (\"n\"), adjectives (\"a\") and verbs (\"v\")\n* `involve_unknown=True`, if `False`, it returns only words found in the dictionary\n\nThus, you can run:\n\n```python\nlemmatized_sentences = gr.get_lemmatized_sentences(string, all_lemmata=False, filter_by_postag=[\"n\",\"a\",\"v\"], involve_unknown=False)\nprint(lemmatized_sentences)\n> [['\u03bb\u03cc\u03b3\u03bf\u03c2'], ['\u03bc\u03ad\u03c1\u03bf\u03c2'], ['\u03c0\u1fb6\u03c2', '\u03bc\u03b7\u03b4\u03b5\u03af\u03c2', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u03bc\u03ad\u03c1\u03bf\u03c2', '\u03c0\u1fb6\u03c2', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9', '\u1f04\u03bd\u03c9/\u1f00\u03bd\u03af\u03b7\u03bc\u03b9', '\u03bc\u03ad\u03c1\u03bf\u03c2', '\u03bf\u1f37\u03bf\u03c2', '\u1f10\u03bd\u03b1\u03bd\u03c4\u03af\u03bf\u03c2', '\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7', '\u1f00\u03b3\u03b1\u03b8\u03cc\u03c2']]\n```\n\n\n\n(1) `get_lemmatized_sentences(string, all_lemmata=False, filter_by_postag=None, involve_unknown=False)`:  it receives a raw Greek text of any kind and extent as its input  Such input is  processed by a series of subsequent functions embedded within each other, which might be also used independently\n\n(1) `get_sentences()` splits the string into sentences by common sentence separators.\n\n(2) `lemmatize_string(sentence)`  first calls `tokenize_string()`, which makes a basic cleaning and stopwords filtering for the sentence, and returns a list of words. Subsequently, each word from the tokenized sentence is sent either to `return_first_lemma()` or to `return_all_unique_lemmata()`, on the basis of the value of the parameter `all_lemmata=` (set to `False` by default). \n\n(4) `return_all_unique_lemmata()`goes to the `morpheus_dict` values and returns all unique lemmata.\n\n(5) Parameter `filter_by_postag=` (default `None`) enables to sub-select  chosen word types from the tokens, on the basis of first character in the tag \"p\" . Thus, to choose only  nouns, adjectives, and verbs, you can set  `filter_by_postag=[\"n\", \"a\", \"v\"].`\n\n### Translation\n\nNext to the lemmatization, there is also a series of functions for translations, like `return_all_unique_translations(word, filter_by_postag=None, involve_unknown=False)`, useful for any wordform, and `lemma_translator(word)`, where we already have a lemma.\n\n```python\ngr.return_all_unique_translations(\"\u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd\", filter_by_postag=None, involve_unknown=False)\n> 'to begin, make a beginning'\n\ngr.lemma_translator(\"\u03bb\u03cc\u03b3\u03bf\u03c2\")\n> 'the word'\n```\n\n### Morphological analysis\n\nYou can also do a morphological analysis of a string\n\n```python\ngr.morphological_analysis(string)[1:4]\n> [{'i': '564347',\n  'f': '\u03bc\u03ad\u03bd',\n  'b': '\u03bc\u03b5\u03bd',\n  'l': '\u03bc\u03ad\u03bd',\n  'e': '\u03bc\u03b5\u03bd',\n  'p': 'g--------',\n  'd': '20753',\n  's': 'on the one hand, on the other hand',\n  'a': None},\n {'i': '642363',\n  'f': '\u03bf\u1f56\u03bd',\n  'b': '\u03bf\u03c5\u03bd',\n  'l': '\u03bf\u1f56\u03bd',\n  'e': '\u03bf\u03c5\u03bd',\n  'p': 'g--------',\n  'd': '23870',\n  's': 'really, at all events',\n  'a': None},\n {'i': '264221',\n  'f': '\u1f10\u03c3\u03c4\u03af',\n  'b': '\u03b5\u03c3\u03c4\u03b9',\n  'l': '\u03b5\u1f30\u03bc\u03af',\n  'e': '\u03b5\u03b9\u03bc\u03b9',\n  'p': 'v3spia---',\n  'd': '9722',\n  's': 'I have',\n  'a': None}]\n```\n\n## imda\n\nThis module will serve for importing various ancient Mediterranean resources. Most of them will be imported directly from open third-party online resources. However, some of them have been preprocessed as part of the SDAM project.\n\nThe ideal is that it will work like this:\n\n```\nimda.list_datasets()\n>>> ['roman_provinces_117', 'EDH', 'roman_cities_hanson', 'orbis_network']\n```\n\nAnd:\n\n```python\nrp = imda.import_dataset(\"roman_provinces_117\", \"gdf\")\ntype(rp)\n>>>geopandas.geodataframe\n```\n\n\n\n## concs\n\nThis module contains functions for working\n\n## textnet\n\nThis module contains functions for generating, analyzing and visualizing word co-occurrence networks. It has been designed especially for working with textual data in ancient Greek. \n\n## Versions history\n\n* 0.0.5 - greek dictionaries included within the package\n* 0.0.5 - experimenting with data inclusion\n* 0.0.4 - docs\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/sdam-au/anda", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "anda", "package_url": "https://pypi.org/project/anda/", "platform": "", "project_url": "https://pypi.org/project/anda/", "project_urls": {"Homepage": "https://github.com/sdam-au/anda"}, "release_url": "https://pypi.org/project/anda/0.0.6/", "requires_dist": null, "requires_python": ">=3.4", "summary": "A package collecting various functions to work with ancient Mediterranean datasets (textual, spatial, etc.)", "version": "0.0.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>anda</h1>\n<p>[toc]</p>\n<pre>pip install anda\n</pre>\n<p>This is a Python package for collecting, manipulation and visualizing various ancient Mediterranean data. It focus on their temporal, textual and spatial aspects. It is structured into several gradually evolving submodules, namely <code>gr</code>, <code>imda</code>, <code>concs</code>, and <code>textnet</code>.</p>\n<h2>anda.gr</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">anda</span> <span class=\"kn\">import</span> <span class=\"n\">gr</span>\n</pre>\n<p>This module is dedicated to preprocessing of ancient Greek textual data. It contains functions for lemmatization, posttagging and translation. It relies heavely on Morhesus Dictionary.</p>\n<h3>Lemmatization</h3>\n<p>A minimal usage is to lemmatize individual word. You can  either ask for only the first lemma (<code>return_first_lemma()</code>) or for all possibilities (<code>return_all_unique_lemmata()</code>.  In most cases , the outcome is the same:</p>\n<pre><span class=\"n\">gr</span><span class=\"o\">.</span><span class=\"n\">return_first_lemma</span><span class=\"p\">(</span><span class=\"s2\">\"\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03bd\"</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;</span> <span class=\"s1\">'\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7'</span>\n\n<span class=\"n\">gr</span><span class=\"o\">.</span><span class=\"n\">return_all_unique_lemmata</span><span class=\"p\">(</span><span class=\"s2\">\"\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03bd\"</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;</span> <span class=\"s1\">'\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7'</span>\n</pre>\n<p>Above these are functions <code>lemmatize_string()</code> and <code>gr.get_lemmatized_sentences()</code>. Both work with string of any length. The first returns a list of lemmata. The second returns a list of lemmatized sentences.</p>\n<pre><span class=\"n\">string</span> <span class=\"o\">=</span> <span class=\"s2\">\"\u03a0\u03c1\u03cc\u03c4\u03b1\u03c3\u03b9\u03c2 \u03bc\u1f72\u03bd \u03bf\u1f56\u03bd \u1f10\u03c3\u03c4\u1f76 \u03bb\u03cc\u03b3\u03bf\u03c2 \u03ba\u03b1\u03c4\u03b1\u03c6\u03b1\u03c4\u03b9\u03ba\u1f78\u03c2 \u1f22 \u1f00\u03c0\u03bf\u03c6\u03b1\u03c4\u03b9\u03ba\u1f78\u03c2 \u03c4\u03b9\u03bd\u1f78\u03c2 \u03ba\u03b1\u03c4\u03ac \u03c4\u03b9\u03bd\u03bf\u03c2. \u039f\u1f57\u03c4\u03bf\u03c2 \u03b4\u1f72 \u1f22 \u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5 \u1f22 \u1f10\u03bd \u03bc\u03ad\u03c1\u03b5\u03b9 \u1f22 \u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03c2. \u039b\u03ad\u03b3\u03c9 \u03b4\u1f72 \u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5 \u03bc\u1f72\u03bd \u03c4\u1f78 \u03c0\u03b1\u03bd\u03c4\u1f76 \u1f22 \u03bc\u03b7\u03b4\u03b5\u03bd\u1f76 \u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd, \u1f10\u03bd \u03bc\u03ad\u03c1\u03b5\u03b9 \u03b4\u1f72 \u03c4\u1f78 \u03c4\u03b9\u03bd\u1f76 \u1f22 \u03bc\u1f74 \u03c4\u03b9\u03bd\u1f76 \u1f22 \u03bc\u1f74 \u03c0\u03b1\u03bd\u03c4\u1f76 \u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd, \u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03bd \u03b4\u1f72 \u03c4\u1f78 \u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd \u1f22 \u03bc\u1f74 \u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd \u1f04\u03bd\u03b5\u03c5 \u03c4\u03bf\u1fe6 \u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5, \u1f22 \u03ba\u03b1\u03c4\u1f70 \u03bc\u03ad\u03c1\u03bf\u03c2, \u03bf\u1f37\u03bf\u03bd \u03c4\u1f78 \u03c4\u1ff6\u03bd \u1f10\u03bd\u03b1\u03bd\u03c4\u03af\u03c9\u03bd \u03b5\u1f36\u03bd\u03b1\u03b9 \u03c4\u1f74\u03bd \u03b1\u1f50\u03c4\u1f74\u03bd \u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7\u03bd \u1f22 \u03c4\u1f78 \u03c4\u1f74\u03bd \u1f21\u03b4\u03bf\u03bd\u1f74\u03bd \u03bc\u1f74 \u03b5\u1f36\u03bd\u03b1\u03b9 \u1f00\u03b3\u03b1\u03b8\u03cc\u03bd.\"</span>\n\n<span class=\"n\">gr</span><span class=\"o\">.</span><span class=\"n\">lemmatize_string</span><span class=\"p\">(</span><span class=\"n\">string</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;</span> <span class=\"p\">[</span><span class=\"s1\">'\u03c0\u03c1\u03cc\u03c4\u03b1\u03c3\u03b9\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bb\u03cc\u03b3\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03ba\u03b1\u03c4\u03b1\u03c6\u03b1\u03c4\u03b9\u03ba\u03cc\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f00\u03c0\u03bf\u03c6\u03b1\u03c4\u03b9\u03ba\u03cc\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03ad\u03c1\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bb\u03ad\u03b3\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03c0\u1fb6\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03b7\u03b4\u03b5\u03af\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03ad\u03c1\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03c0\u1fb6\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03bd'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f04\u03bd\u03b5\u03c5'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03ad\u03c1\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bf\u1f37\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f10\u03bd\u03b1\u03bd\u03c4\u03af\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03b1\u1f50\u03c4\u03b7\u03bd'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f21\u03b4\u03bf\u03bd\u03b7\u03bd'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f00\u03b3\u03b1\u03b8\u03cc\u03c2'</span><span class=\"p\">]</span>\n\n<span class=\"n\">gr</span><span class=\"o\">.</span><span class=\"n\">get_lemmatized_sentences</span><span class=\"p\">(</span><span class=\"n\">string</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;</span> <span class=\"p\">[[</span><span class=\"s1\">'\u03c0\u03c1\u03cc\u03c4\u03b1\u03c3\u03b9\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bb\u03cc\u03b3\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03ba\u03b1\u03c4\u03b1\u03c6\u03b1\u03c4\u03b9\u03ba\u03cc\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f00\u03c0\u03bf\u03c6\u03b1\u03c4\u03b9\u03ba\u03cc\u03c2'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03ad\u03c1\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03c2'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'\u03bb\u03ad\u03b3\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03c0\u1fb6\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03b7\u03b4\u03b5\u03af\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03ad\u03c1\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03c0\u1fb6\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f00\u03b4\u03b9\u03cc\u03c1\u03b9\u03c3\u03c4\u03bf\u03bd'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f04\u03bd\u03b5\u03c5'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03ba\u03b1\u03b8\u03cc\u03bb\u03bf\u03c5'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03ad\u03c1\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bf\u1f37\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f10\u03bd\u03b1\u03bd\u03c4\u03af\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03b1\u1f50\u03c4\u03b7\u03bd'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f21\u03b4\u03bf\u03bd\u03b7\u03bd'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f00\u03b3\u03b1\u03b8\u03cc\u03c2'</span><span class=\"p\">]]</span>\n</pre>\n<p>All lemmatization functions can be further parametrized by several arguments</p>\n<ul>\n<li><code>all_lemmata=False</code> :</li>\n<li><code>filter_by_postag=[\"n\",\"a\",\"v\"]</code>: returns only nouns (\"n\"), adjectives (\"a\") and verbs (\"v\")</li>\n<li><code>involve_unknown=True</code>, if <code>False</code>, it returns only words found in the dictionary</li>\n</ul>\n<p>Thus, you can run:</p>\n<pre><span class=\"n\">lemmatized_sentences</span> <span class=\"o\">=</span> <span class=\"n\">gr</span><span class=\"o\">.</span><span class=\"n\">get_lemmatized_sentences</span><span class=\"p\">(</span><span class=\"n\">string</span><span class=\"p\">,</span> <span class=\"n\">all_lemmata</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">filter_by_postag</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"n\"</span><span class=\"p\">,</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span><span class=\"s2\">\"v\"</span><span class=\"p\">],</span> <span class=\"n\">involve_unknown</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">lemmatized_sentences</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;</span> <span class=\"p\">[[</span><span class=\"s1\">'\u03bb\u03cc\u03b3\u03bf\u03c2'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'\u03bc\u03ad\u03c1\u03bf\u03c2'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'\u03c0\u1fb6\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03b7\u03b4\u03b5\u03af\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03ad\u03c1\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03c0\u1fb6\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f51\u03c0\u03ac\u03c1\u03c7\u03c9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f04\u03bd\u03c9/\u1f00\u03bd\u03af\u03b7\u03bc\u03b9'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bc\u03ad\u03c1\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u03bf\u1f37\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f10\u03bd\u03b1\u03bd\u03c4\u03af\u03bf\u03c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7'</span><span class=\"p\">,</span> <span class=\"s1\">'\u1f00\u03b3\u03b1\u03b8\u03cc\u03c2'</span><span class=\"p\">]]</span>\n</pre>\n<p>(1) <code>get_lemmatized_sentences(string, all_lemmata=False, filter_by_postag=None, involve_unknown=False)</code>:  it receives a raw Greek text of any kind and extent as its input  Such input is  processed by a series of subsequent functions embedded within each other, which might be also used independently</p>\n<p>(1) <code>get_sentences()</code> splits the string into sentences by common sentence separators.</p>\n<p>(2) <code>lemmatize_string(sentence)</code>  first calls <code>tokenize_string()</code>, which makes a basic cleaning and stopwords filtering for the sentence, and returns a list of words. Subsequently, each word from the tokenized sentence is sent either to <code>return_first_lemma()</code> or to <code>return_all_unique_lemmata()</code>, on the basis of the value of the parameter <code>all_lemmata=</code> (set to <code>False</code> by default).</p>\n<p>(4) <code>return_all_unique_lemmata()</code>goes to the <code>morpheus_dict</code> values and returns all unique lemmata.</p>\n<p>(5) Parameter <code>filter_by_postag=</code> (default <code>None</code>) enables to sub-select  chosen word types from the tokens, on the basis of first character in the tag \"p\" . Thus, to choose only  nouns, adjectives, and verbs, you can set  <code>filter_by_postag=[\"n\", \"a\", \"v\"].</code></p>\n<h3>Translation</h3>\n<p>Next to the lemmatization, there is also a series of functions for translations, like <code>return_all_unique_translations(word, filter_by_postag=None, involve_unknown=False)</code>, useful for any wordform, and <code>lemma_translator(word)</code>, where we already have a lemma.</p>\n<pre><span class=\"n\">gr</span><span class=\"o\">.</span><span class=\"n\">return_all_unique_translations</span><span class=\"p\">(</span><span class=\"s2\">\"\u1f51\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9\u03bd\"</span><span class=\"p\">,</span> <span class=\"n\">filter_by_postag</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">involve_unknown</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;</span> <span class=\"s1\">'to begin, make a beginning'</span>\n\n<span class=\"n\">gr</span><span class=\"o\">.</span><span class=\"n\">lemma_translator</span><span class=\"p\">(</span><span class=\"s2\">\"\u03bb\u03cc\u03b3\u03bf\u03c2\"</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;</span> <span class=\"s1\">'the word'</span>\n</pre>\n<h3>Morphological analysis</h3>\n<p>You can also do a morphological analysis of a string</p>\n<pre><span class=\"n\">gr</span><span class=\"o\">.</span><span class=\"n\">morphological_analysis</span><span class=\"p\">(</span><span class=\"n\">string</span><span class=\"p\">)[</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">4</span><span class=\"p\">]</span>\n<span class=\"o\">&gt;</span> <span class=\"p\">[{</span><span class=\"s1\">'i'</span><span class=\"p\">:</span> <span class=\"s1\">'564347'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'f'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03bc\u03ad\u03bd'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'b'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03bc\u03b5\u03bd'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'l'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03bc\u03ad\u03bd'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'e'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03bc\u03b5\u03bd'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'p'</span><span class=\"p\">:</span> <span class=\"s1\">'g--------'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'d'</span><span class=\"p\">:</span> <span class=\"s1\">'20753'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'s'</span><span class=\"p\">:</span> <span class=\"s1\">'on the one hand, on the other hand'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'a'</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">},</span>\n <span class=\"p\">{</span><span class=\"s1\">'i'</span><span class=\"p\">:</span> <span class=\"s1\">'642363'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'f'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03bf\u1f56\u03bd'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'b'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03bf\u03c5\u03bd'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'l'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03bf\u1f56\u03bd'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'e'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03bf\u03c5\u03bd'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'p'</span><span class=\"p\">:</span> <span class=\"s1\">'g--------'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'d'</span><span class=\"p\">:</span> <span class=\"s1\">'23870'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'s'</span><span class=\"p\">:</span> <span class=\"s1\">'really, at all events'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'a'</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">},</span>\n <span class=\"p\">{</span><span class=\"s1\">'i'</span><span class=\"p\">:</span> <span class=\"s1\">'264221'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'f'</span><span class=\"p\">:</span> <span class=\"s1\">'\u1f10\u03c3\u03c4\u03af'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'b'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03b5\u03c3\u03c4\u03b9'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'l'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03b5\u1f30\u03bc\u03af'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'e'</span><span class=\"p\">:</span> <span class=\"s1\">'\u03b5\u03b9\u03bc\u03b9'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'p'</span><span class=\"p\">:</span> <span class=\"s1\">'v3spia---'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'d'</span><span class=\"p\">:</span> <span class=\"s1\">'9722'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'s'</span><span class=\"p\">:</span> <span class=\"s1\">'I have'</span><span class=\"p\">,</span>\n  <span class=\"s1\">'a'</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">}]</span>\n</pre>\n<h2>imda</h2>\n<p>This module will serve for importing various ancient Mediterranean resources. Most of them will be imported directly from open third-party online resources. However, some of them have been preprocessed as part of the SDAM project.</p>\n<p>The ideal is that it will work like this:</p>\n<pre><code>imda.list_datasets()\n&gt;&gt;&gt; ['roman_provinces_117', 'EDH', 'roman_cities_hanson', 'orbis_network']\n</code></pre>\n<p>And:</p>\n<pre><span class=\"n\">rp</span> <span class=\"o\">=</span> <span class=\"n\">imda</span><span class=\"o\">.</span><span class=\"n\">import_dataset</span><span class=\"p\">(</span><span class=\"s2\">\"roman_provinces_117\"</span><span class=\"p\">,</span> <span class=\"s2\">\"gdf\"</span><span class=\"p\">)</span>\n<span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">rp</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span><span class=\"n\">geopandas</span><span class=\"o\">.</span><span class=\"n\">geodataframe</span>\n</pre>\n<h2>concs</h2>\n<p>This module contains functions for working</p>\n<h2>textnet</h2>\n<p>This module contains functions for generating, analyzing and visualizing word co-occurrence networks. It has been designed especially for working with textual data in ancient Greek.</p>\n<h2>Versions history</h2>\n<ul>\n<li>0.0.5 - greek dictionaries included within the package</li>\n<li>0.0.5 - experimenting with data inclusion</li>\n<li>0.0.4 - docs</li>\n</ul>\n\n          </div>"}, "last_serial": 7186939, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "dde8b4283dc5c371374d17859a46c483", "sha256": "4db51d5d9b54af1f8ae40591a9dd6e7b56c87a7d383d8a3b8eb870b67369cd8c"}, "downloads": -1, "filename": "anda-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "dde8b4283dc5c371374d17859a46c483", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 12183, "upload_time": "2020-01-31T14:24:04", "upload_time_iso_8601": "2020-01-31T14:24:04.247206Z", "url": "https://files.pythonhosted.org/packages/90/b2/0b01478d9117bd87c708a5cfdae7302215a583afe3389a80555bed3bb7fd/anda-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2b1bc42c4b1281094dab11d7f1e77f28", "sha256": "6a95af9b7659631b0ebb8d75df6669555ea7035c9ddab6c6a49eac521c6af136"}, "downloads": -1, "filename": "anda-0.0.1.tar.gz", "has_sig": false, "md5_digest": "2b1bc42c4b1281094dab11d7f1e77f28", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 10556, "upload_time": "2020-01-31T14:24:06", "upload_time_iso_8601": "2020-01-31T14:24:06.916241Z", "url": "https://files.pythonhosted.org/packages/2c/dd/41e5e9d2a894591f2957a4dec66d1a2d2c1cf97865af7e0817abceceed3d/anda-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "c0c108cdf371ad2b3582c704b243d498", "sha256": "7e86a456eff189f1ce24013b9f5ad21f4094f47bbe2ca198d50a953082fac660"}, "downloads": -1, "filename": "anda-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "c0c108cdf371ad2b3582c704b243d498", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 12480, "upload_time": "2020-01-31T14:30:30", "upload_time_iso_8601": "2020-01-31T14:30:30.330601Z", "url": "https://files.pythonhosted.org/packages/e4/6b/e4f8a53e9e34dd78bf7a54b7f93d024dfc2265f5a5f74dfd093f2653ae42/anda-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1043df0ac5724d4570c8d0019fad01d2", "sha256": "039ca7222867a3f7b32ae3a745a0ca9bf4259b6e924d963eedae06581c8d215d"}, "downloads": -1, "filename": "anda-0.0.2.tar.gz", "has_sig": false, "md5_digest": "1043df0ac5724d4570c8d0019fad01d2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 12291, "upload_time": "2020-01-31T14:30:31", "upload_time_iso_8601": "2020-01-31T14:30:31.630324Z", "url": "https://files.pythonhosted.org/packages/7c/ff/39c455116a9fce6322c67425c4c8ed0136c8351c5c4e42841322857501ad/anda-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "309eb47bac347f4278ec637bc29c112c", "sha256": "8d2e33bef202c6c6228ad3eafc415d88716e7c567456b8a0b2b2c0f01f896a67"}, "downloads": -1, "filename": "anda-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "309eb47bac347f4278ec637bc29c112c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 21267254, "upload_time": "2020-05-05T13:42:28", "upload_time_iso_8601": "2020-05-05T13:42:28.681913Z", "url": "https://files.pythonhosted.org/packages/ce/e6/695d103e7f755a7f07310c84242e902eb8dc169803b700c754760cd4f568/anda-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5630a23f9916949fcf4253f2017ecbf3", "sha256": "a325a85f9e70393180cc30a58adff3304faf0fc43f0c8e2b9c563d151f2c519d"}, "downloads": -1, "filename": "anda-0.0.3.tar.gz", "has_sig": false, "md5_digest": "5630a23f9916949fcf4253f2017ecbf3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 18993550, "upload_time": "2020-05-05T13:42:45", "upload_time_iso_8601": "2020-05-05T13:42:45.715432Z", "url": "https://files.pythonhosted.org/packages/a9/3f/60872e707d2e487f8365087694e50d2db11e7c7ffc80b6397a9584429357/anda-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "b350a2b17e8d62e6277c02b065bbfabd", "sha256": "b0020040d67ccf62dc357c77a159460daa6344383026b15edfdf528a5cf0fb91"}, "downloads": -1, "filename": "anda-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "b350a2b17e8d62e6277c02b065bbfabd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 21267238, "upload_time": "2020-05-05T13:53:14", "upload_time_iso_8601": "2020-05-05T13:53:14.803874Z", "url": "https://files.pythonhosted.org/packages/ff/97/eb2ba7f475666bfc089cdcdb9cdcb5947cd690e13d7328b5623d9adaad3c/anda-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a5c6eed5043a51ac8b2dba1e8ae4696c", "sha256": "559b06c6ade2f905cdd49b5e77c58cf3461baa41ee859eb918f4da0bdbddd379"}, "downloads": -1, "filename": "anda-0.0.4.tar.gz", "has_sig": false, "md5_digest": "a5c6eed5043a51ac8b2dba1e8ae4696c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 18993532, "upload_time": "2020-05-05T13:53:21", "upload_time_iso_8601": "2020-05-05T13:53:21.555266Z", "url": "https://files.pythonhosted.org/packages/e9/e6/79b825d2bf899a5231b5589245832e5301bc65a3a0f84f78c4b7e88f4d89/anda-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "21d481dcd8f36a21bb2594a80b3b7428", "sha256": "92685553cb0361dc6291152b64cd36c9e553ee98bfe5291234206039fc9e243a"}, "downloads": -1, "filename": "anda-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "21d481dcd8f36a21bb2594a80b3b7428", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 21267265, "upload_time": "2020-05-07T09:08:56", "upload_time_iso_8601": "2020-05-07T09:08:56.628818Z", "url": "https://files.pythonhosted.org/packages/b4/d8/e8c7b446c8810ccf27186068e1e47e2ccbd283f7660a88b761f9e5532b1f/anda-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c82645bae8953faaf87cf9fd5bad2887", "sha256": "edca075ce4c78e9ebfe5a4267fb030d2741732344ee12a6f24ec90267784fe84"}, "downloads": -1, "filename": "anda-0.0.5.tar.gz", "has_sig": false, "md5_digest": "c82645bae8953faaf87cf9fd5bad2887", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 18993644, "upload_time": "2020-05-07T09:09:13", "upload_time_iso_8601": "2020-05-07T09:09:13.260998Z", "url": "https://files.pythonhosted.org/packages/a6/14/83458d08f24f9a4c0fcb30a9c3649f8e1c55bce774c2a20a21691e8b0825/anda-0.0.5.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "a79ef0e5df77841ba8cbf68b6e78db53", "sha256": "c80831da929a93ecb329055b86dcf403d4185bf30aa77511662847d9cc786bb3"}, "downloads": -1, "filename": "anda-0.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "a79ef0e5df77841ba8cbf68b6e78db53", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 21267298, "upload_time": "2020-05-07T09:39:20", "upload_time_iso_8601": "2020-05-07T09:39:20.096471Z", "url": "https://files.pythonhosted.org/packages/36/1d/5a7fdf53d3420e78fda3cdb5a9f4c5d16a3f5edcdb2928b4d4c6f26f6628/anda-0.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7f6bc7c4ee2782d3197b7e407d0a6649", "sha256": "5e62cdd334a40c167f7346672f0928064d5b26f6797c536e04a86ade73b6834f"}, "downloads": -1, "filename": "anda-0.0.6.tar.gz", "has_sig": false, "md5_digest": "7f6bc7c4ee2782d3197b7e407d0a6649", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 18993707, "upload_time": "2020-05-07T09:39:31", "upload_time_iso_8601": "2020-05-07T09:39:31.757924Z", "url": "https://files.pythonhosted.org/packages/cc/51/3f53054461ff71391b5cd04d32431b30e9bc7aff516f023790ba833d5da0/anda-0.0.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a79ef0e5df77841ba8cbf68b6e78db53", "sha256": "c80831da929a93ecb329055b86dcf403d4185bf30aa77511662847d9cc786bb3"}, "downloads": -1, "filename": "anda-0.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "a79ef0e5df77841ba8cbf68b6e78db53", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 21267298, "upload_time": "2020-05-07T09:39:20", "upload_time_iso_8601": "2020-05-07T09:39:20.096471Z", "url": "https://files.pythonhosted.org/packages/36/1d/5a7fdf53d3420e78fda3cdb5a9f4c5d16a3f5edcdb2928b4d4c6f26f6628/anda-0.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7f6bc7c4ee2782d3197b7e407d0a6649", "sha256": "5e62cdd334a40c167f7346672f0928064d5b26f6797c536e04a86ade73b6834f"}, "downloads": -1, "filename": "anda-0.0.6.tar.gz", "has_sig": false, "md5_digest": "7f6bc7c4ee2782d3197b7e407d0a6649", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 18993707, "upload_time": "2020-05-07T09:39:31", "upload_time_iso_8601": "2020-05-07T09:39:31.757924Z", "url": "https://files.pythonhosted.org/packages/cc/51/3f53054461ff71391b5cd04d32431b30e9bc7aff516f023790ba833d5da0/anda-0.0.6.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:18:14 2020"}