{"info": {"author": "Disaiah Benentt", "author_email": "dlbennett365@students.ecsu.edu", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: X11 Applications :: GTK", "Intended Audience :: Developers", "Intended Audience :: End Users/Desktop", "License :: OSI Approved :: GNU General Public License (GPL)", "Operating System :: POSIX :: Linux", "Programming Language :: Python", "Topic :: Desktop Environment", "Topic :: Text Processing :: Fonts"], "description": "Building Web-Crawler\nBuilding a Web-Crawler Software is easy, and helps you take advantage of a data mining software. This document will help guide you through understanding this build process.\n\n## Requirements\n\n1. BeautifulSoup4, module is needed and can be installed with pip install BeautifulSoup4.\n2. re, module is needed and can be installed with pip install re.\n3. matplotlib, module is needed to plot the graphs for the scatterplot and boxplot.\n4. pandas, module is needed to load the csv data into a dataframe.\n\n2. Web-Crawler, using one of the following configurations:\n* **macOS** You can either use Web-Crawler for Mac or  See installation instructions.\n* **Linux**  Install Web-Crawler according to the [instructions] for your OS.\n\n## Overview\n\nWhile it is possible to build a web-crawler using a local python installation, we have a build process that runs on a local environment.  This simplifies initial set up and provides for a very consistent build and test environment.\n\n## Key scripts\n\nThe following scripts are found in the `build/` directory. Note that all scripts must be run from the Web-Crawler root directory.\n1. `src/webcrawler/move_csv.sh`\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://dislbenn.github.io", "keywords": "", "license": "Creative Commons Attribution-Noncommercial-Share Alike license", "maintainer": "", "maintainer_email": "", "name": "Final_Project", "package_url": "https://pypi.org/project/Final_Project/", "platform": "", "project_url": "https://pypi.org/project/Final_Project/", "project_urls": {"Homepage": "http://dislbenn.github.io"}, "release_url": "https://pypi.org/project/Final_Project/1.0/", "requires_dist": null, "requires_python": "", "summary": "Extracting home information from zillow.com", "version": "1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Building Web-Crawler\nBuilding a Web-Crawler Software is easy, and helps you take advantage of a data mining software. This document will help guide you through understanding this build process.</p>\n<p>## Requirements</p>\n<ol>\n<li>BeautifulSoup4, module is needed and can be installed with pip install BeautifulSoup4.</li>\n<li>re, module is needed and can be installed with pip install re.</li>\n<li>matplotlib, module is needed to plot the graphs for the scatterplot and boxplot.</li>\n<li>pandas, module is needed to load the csv data into a dataframe.</li>\n</ol>\n<p>2. Web-Crawler, using one of the following configurations:\n* <strong>macOS</strong> You can either use Web-Crawler for Mac or  See installation instructions.\n* <strong>Linux</strong>  Install Web-Crawler according to the [instructions] for your OS.</p>\n<p>## Overview</p>\n<p>While it is possible to build a web-crawler using a local python installation, we have a build process that runs on a local environment.  This simplifies initial set up and provides for a very consistent build and test environment.</p>\n<p>## Key scripts</p>\n<p>The following scripts are found in the <cite>build/</cite> directory. Note that all scripts must be run from the Web-Crawler root directory.\n1. <cite>src/webcrawler/move_csv.sh</cite></p>\n\n          </div>"}, "last_serial": 4553683, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "cc8cbb759428482b676f9497b1a6226f", "sha256": "610081d8eb2de7afa9cb8ea1c004801d4ebdbe3ee0ad9721f913d01f265a26d2"}, "downloads": -1, "filename": "Final_Project-1.0.tar.gz", "has_sig": false, "md5_digest": "cc8cbb759428482b676f9497b1a6226f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 254713, "upload_time": "2018-12-02T23:51:18", "upload_time_iso_8601": "2018-12-02T23:51:18.757772Z", "url": "https://files.pythonhosted.org/packages/c8/42/7a532b8bec5bdb1e0784d1c117c21f675693ddf71936ec1a6e6b9541fb9b/Final_Project-1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cc8cbb759428482b676f9497b1a6226f", "sha256": "610081d8eb2de7afa9cb8ea1c004801d4ebdbe3ee0ad9721f913d01f265a26d2"}, "downloads": -1, "filename": "Final_Project-1.0.tar.gz", "has_sig": false, "md5_digest": "cc8cbb759428482b676f9497b1a6226f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 254713, "upload_time": "2018-12-02T23:51:18", "upload_time_iso_8601": "2018-12-02T23:51:18.757772Z", "url": "https://files.pythonhosted.org/packages/c8/42/7a532b8bec5bdb1e0784d1c117c21f675693ddf71936ec1a6e6b9541fb9b/Final_Project-1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:42:27 2020"}