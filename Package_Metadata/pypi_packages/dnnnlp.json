{"info": {"author": "KzXuan", "author_email": "kaizhouxuan@gmail.com", "bugtrack_url": null, "classifiers": ["License :: Free for non-commercial use", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "## PyTorch - Deep Neural Network - Natural Language Processing\n\nVersion 1.1 by KzXuan\n\n**Contains CNN, RNN and Transformer layers and models implemented by pytorch for classification and sequence labeling tasks in NLP.**\n\n* Newly designed modules.\n* Reduce usage complexity.\n* Use `mask` as the sequence length identifier.\n* Multi-GPU parallel for grid search.\n\n<br>\n\n### Dependecies\n\npython 3.5+ & pytorch 1.2.0+\n\n<br>\n\n### [Install](https://pypi.org/project/dnnnlp/)\n\n```bash\n> pip install dnnnlp\n```\n\n<br>\n\n### [API Document](./docs.md) (In Chinese)\n\n  * [Layer](./docs.md#Layer) - [layer.py](./dnnnlp/layer.py)\n  * [Model](./docs.md#Model) -  [model.py](./dnnnlp/model.py)\n  * [Execution](./docs.md#Execution) - [exec.py](./dnnnlp/exec.py)\n  * [Utility](./docs.md#Utility) - [utils.py](./dnnnlp/utils.py)\n\n<br>\n\n### Hyperparameters\n\n| Name          | Type  | Default     | Description                                                    |\n| ------------- | ----- | ----------- | -------------------------------------------------------------- |\n| n_gpu         | int   | 1           | The number of GPUs (0 means no GPU acceleration).              |\n| space_turbo   | bool  | True        | Accelerate with more GPU memories.                             |\n| rand_seed     | int   | 100         | Random seed setting.                                           |\n| data_shuffle  | bool  | Ture        | Disrupt data for training.                                     |\n| emb_type      | str   | None        | Embedding modes contain None, 'const' or 'variable'.           |\n| emb_dim       | int   | 300         | Embedding dimension (or feature dimension).                    |\n| n_class       | int   | 2           | Number of target classes.                                      |\n| n_hidden      | int   | 50          | Number of hidden nodes, or output channels of CNN.             |\n| learning_rate | float | 0.01        | Learning rate.                                                 |\n| l2_reg        | float | 1e-6        | L2 regular.                                                    |\n| batch_size    | int   | 32          | Number of samples for one batch.                               |\n| iter_times    | int   | 30          | Number of iterations.                                          |\n| display_step  | int   | 2           | The number of iterations between each output of the result.    |\n| drop_prob     | float | 0.1         | Dropout ratio.                                                 |\n| eval_metric   | str   | 'accuracy'  | Evaluation metrics contain 'accuracy', 'macro', 'class1', etc. |\n\n<br>\n\n### Usage\n\n```python\n# import our modules\nfrom dnnnlp.model import RNNModel\nfrom dnnnlp.exec import default_args, Classify\n\n# load the embedding matrix\nemb_mat = np.array((-1, 300))\n# load the train data\ntrain_x = np.array((800, 50))\ntrain_y = np.array((800,))\ntrain_mask = np.array((800, 50))\n# load the test data\ntest_x = np.array((200, 50))\ntest_y = np.array((200,))\ntest_mask = np.array((200, 50))\n\n# get the default arguments\nargs = default_args()\n# modify part of the arguments\nargs.space_turbo = False\nargs.n_hidden = 100\nargs.batch_size = 32\n```\n\n* Classification\n\n```python\n# initilize a model\nmodel = RNNModel(args, emb_mat, bi_direction=False, rnn_type='GRU', use_attention=True)\n# initilize a classifier\nnn = Classify(model, args, train_x, train_y, train_mask, test_x, test_y, test_mask)\n# do training and testing\nevals = nn.train_test(device_id=0)\n```\n\n* Run several times and get the average score.\n\n````python\n# initilize a model\nmodel = CNNModel(args, emb_mat, kernel_widths=[2, 3, 4])\n# initilize a classifier\nnn = Classify(model, args, train_x, train_y, train_mask)\n# run the model several times\navg_evals = average_several_run(nn.train_test, args, n_times=8, n_paral=4, fold=5)\n````\n\n* Parameters' grid search.\n\n````python\n# initilize a model\nmodel = TransformerModel(args, n_layer=12, n_head=8)\n# initilize a classifier\nnn = Classify(model, args, train_x, train_y, train_mask, test_x, test_y, test_mask)\n# set searching params\nparams_search = {'learning_rate': [0.1, 0.01], 'n_hidden': [50, 100]}\n# run grid search\nmax_evals = grid_search(nn, nn.train_test, args, params_search)\n````\n\n* Sequence labeling\n\n```python\nfrom dnnnlp.model import RNNCRFModel\nfrom dnnnlp.exec import default_args, SequenceLabeling\n\n# load the train data\ntrain_x = np.array((1000, 50))\ntrain_y = np.array((1000, 50))\ntrain_mask = np.array((1000, 50))\n\n# initilize a model\nmodel = RNNCRFModel(args)\n# initilize a labeler\nnn = SequenceLabeling(model, args, train_x, train_y, train_mask)\n# do cross validation\nnn.cross_validation(fold=5)\n```\n\n<br>\n\n### History\n\n**version 1.1**\n  * Add `CRFLayer`: packaging CRF for both training and testing.\n  * Add `RNNCRFModel`: a integrated RNN-CRF sequence labeling model.\n  * Add `SequenceLabeling`: a sequence labeling execution module that inherits from Classify.\n  * Fix errors in judging whether a tensor is None.\n\n**version 1.0**\n  * Rename project `dnn` to `dnnnlp`.\n  * Remove file `base`, add file `utils`.\n  * Optimize and rename `SoftmaxLayer` and `SoftAttentionLayer`.\n  * Rewrite and rename `EmbeddingLayer`, `CNNLayer` and `RNNLayer`.\n  * Rewrite `MultiheadAttentionLayer`: a packaging attention layer based on `nn.MultiheadAttention`.\n  * Rewrite `TransformerLayer`: support new `MultiheadAttentionLayer`.\n  * Optimize and rename `CNNModel`, `RNNModel` and `TransformerModel`.\n  * Optimize and rename `Classify`: a highly applicable classification execution module.\n  * Rewrite `average_several_run` and `grid_search`: support multi-GPU parallel.\n  * Support pytorch 1.2.0.\n\n**version 0.12**\n  * Update `RNN_layer`: fully support for tanh, LSTM and GRU.\n  * Fix errors in some mask operations.\n  * Support pytorch 1.1.0.\n\nOld version [0.12.3](https://github.com/NUSTM/pytorch-dnnnlp/tree/8d2d6c4e432076e13020ae54954aa419f3bb9bce).\n\n**version 0.11**\n  * Provides an acceleration method by using more GPU memories.\n  * Fix the problem of memory consumption caused by abnormal data reading.\n  * Add `multi_head_attention_layer`: packaging multi-head attention for Transformer.\n  * Add `Transformer_layer` and `Transformer_model`: packaging Transformer layer and model written by ourself.\n  * Support data disruption for training.\n\n**version 0.10**\n  * Split the code into four files: `base`, `layer`, `model`, `exec`.\n  * Add `CNN_layer` and `CNN_model`: packaging CNN layer and model.\n  * Support multi-GPU parallel for each model.\n\n**version 0.9**\n  * Fix the problem of output format.\n  * Fix the statistical errors in cross-validation part of `LSTM_classify`.\n  * Rename: `LSTM_model` to `RNN_layer`, `self_attention` to `self_attention_layer`.\n  * Add `softmax_layer`: a packaging fully-connected layer.\n\n**version 0.8**\n  * Adjust the applicability of functions in `LSTM_classify` to avoid rewriting in `LSTM_sequence`.\n  * Optimize the way of parameter transfer.\n  * A more complete evaluation mechanism.\n\n**version 0.7**\n  * Add `LSTM_sequence`: a sequence labeling module for `LSTM_model`.\n  * Fix the nan-value problem in hierarchical classification.\n  * Support pytorch 1.0.0.\n\n**version 0.6**\n  * Update `LSTM_classify`: support hierarchical classification.\n  * The `GRU_model` is merged into the `LSTM_model`.\n  * Adapt to CPU operation.\n\n**version 0.5**\n  * Split the running part of `LSTM_classify` to reduce the rewrite of custom models.\n  * Add control for visual output.\n  * Create function `average_several_run`: support to get the average score after several training and testing.\n  * Create function `grid_search`: support parameters' grid search.\n\n**version 0.4**\n  * Add `GRU_model`: a packaging GRU model based on `nn.GRU`.\n  * Support L2 regular.\n\n**version 0.3**\n  * Add `self_attention`: provides attention mechanism support.\n  * Update `LSTM_classify`: adapts to complex custom models.\n\n**version 0.2**\n  * Support mode selection of embedding.\n  * Default usage of `nn.Dropout`.\n  * Create function `default_args` to provide default hyperparameters.\n\n**version 0.1**\n  * Initilization of project `dnn`: based on pytorch 0.4.1.\n  * Add `LSTM_model`: a packaging LSTM model based on `nn.LSTM`.\n  * Add `LSTM_classify`: a classification module for LSTM model, which supports train-test and corss-validation.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/NUSTM/pytorch-dnnnlp", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "dnnnlp", "package_url": "https://pypi.org/project/dnnnlp/", "platform": "", "project_url": "https://pypi.org/project/dnnnlp/", "project_urls": {"Homepage": "https://github.com/NUSTM/pytorch-dnnnlp"}, "release_url": "https://pypi.org/project/dnnnlp/1.1.4/", "requires_dist": ["numpy", "scikit-learn (>=0.21.0)", "torch (>=1.2.0)", "torchcrf (>=0.7.2)"], "requires_python": "", "summary": "Deep Neural Networks for Natural Language Processing classification or sequential task written by PyTorch.", "version": "1.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h2>PyTorch - Deep Neural Network - Natural Language Processing</h2>\n<p>Version 1.1 by KzXuan</p>\n<p><strong>Contains CNN, RNN and Transformer layers and models implemented by pytorch for classification and sequence labeling tasks in NLP.</strong></p>\n<ul>\n<li>Newly designed modules.</li>\n<li>Reduce usage complexity.</li>\n<li>Use <code>mask</code> as the sequence length identifier.</li>\n<li>Multi-GPU parallel for grid search.</li>\n</ul>\n<br>\n<h3>Dependecies</h3>\n<p>python 3.5+ &amp; pytorch 1.2.0+</p>\n<br>\n<h3><a href=\"https://pypi.org/project/dnnnlp/\" rel=\"nofollow\">Install</a></h3>\n<pre>&gt; pip install dnnnlp\n</pre>\n<br>\n<h3><a href=\"./docs.md\" rel=\"nofollow\">API Document</a> (In Chinese)</h3>\n<ul>\n<li><a href=\"./docs.md#Layer\" rel=\"nofollow\">Layer</a> - <a href=\"./dnnnlp/layer.py\" rel=\"nofollow\">layer.py</a></li>\n<li><a href=\"./docs.md#Model\" rel=\"nofollow\">Model</a> -  <a href=\"./dnnnlp/model.py\" rel=\"nofollow\">model.py</a></li>\n<li><a href=\"./docs.md#Execution\" rel=\"nofollow\">Execution</a> - <a href=\"./dnnnlp/exec.py\" rel=\"nofollow\">exec.py</a></li>\n<li><a href=\"./docs.md#Utility\" rel=\"nofollow\">Utility</a> - <a href=\"./dnnnlp/utils.py\" rel=\"nofollow\">utils.py</a></li>\n</ul>\n<br>\n<h3>Hyperparameters</h3>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Default</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>n_gpu</td>\n<td>int</td>\n<td>1</td>\n<td>The number of GPUs (0 means no GPU acceleration).</td>\n</tr>\n<tr>\n<td>space_turbo</td>\n<td>bool</td>\n<td>True</td>\n<td>Accelerate with more GPU memories.</td>\n</tr>\n<tr>\n<td>rand_seed</td>\n<td>int</td>\n<td>100</td>\n<td>Random seed setting.</td>\n</tr>\n<tr>\n<td>data_shuffle</td>\n<td>bool</td>\n<td>Ture</td>\n<td>Disrupt data for training.</td>\n</tr>\n<tr>\n<td>emb_type</td>\n<td>str</td>\n<td>None</td>\n<td>Embedding modes contain None, 'const' or 'variable'.</td>\n</tr>\n<tr>\n<td>emb_dim</td>\n<td>int</td>\n<td>300</td>\n<td>Embedding dimension (or feature dimension).</td>\n</tr>\n<tr>\n<td>n_class</td>\n<td>int</td>\n<td>2</td>\n<td>Number of target classes.</td>\n</tr>\n<tr>\n<td>n_hidden</td>\n<td>int</td>\n<td>50</td>\n<td>Number of hidden nodes, or output channels of CNN.</td>\n</tr>\n<tr>\n<td>learning_rate</td>\n<td>float</td>\n<td>0.01</td>\n<td>Learning rate.</td>\n</tr>\n<tr>\n<td>l2_reg</td>\n<td>float</td>\n<td>1e-6</td>\n<td>L2 regular.</td>\n</tr>\n<tr>\n<td>batch_size</td>\n<td>int</td>\n<td>32</td>\n<td>Number of samples for one batch.</td>\n</tr>\n<tr>\n<td>iter_times</td>\n<td>int</td>\n<td>30</td>\n<td>Number of iterations.</td>\n</tr>\n<tr>\n<td>display_step</td>\n<td>int</td>\n<td>2</td>\n<td>The number of iterations between each output of the result.</td>\n</tr>\n<tr>\n<td>drop_prob</td>\n<td>float</td>\n<td>0.1</td>\n<td>Dropout ratio.</td>\n</tr>\n<tr>\n<td>eval_metric</td>\n<td>str</td>\n<td>'accuracy'</td>\n<td>Evaluation metrics contain 'accuracy', 'macro', 'class1', etc.</td>\n</tr></tbody></table>\n<br>\n<h3>Usage</h3>\n<pre><span class=\"c1\"># import our modules</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dnnnlp.model</span> <span class=\"kn\">import</span> <span class=\"n\">RNNModel</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dnnnlp.exec</span> <span class=\"kn\">import</span> <span class=\"n\">default_args</span><span class=\"p\">,</span> <span class=\"n\">Classify</span>\n\n<span class=\"c1\"># load the embedding matrix</span>\n<span class=\"n\">emb_mat</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">300</span><span class=\"p\">))</span>\n<span class=\"c1\"># load the train data</span>\n<span class=\"n\">train_x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"mi\">800</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">))</span>\n<span class=\"n\">train_y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"mi\">800</span><span class=\"p\">,))</span>\n<span class=\"n\">train_mask</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"mi\">800</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">))</span>\n<span class=\"c1\"># load the test data</span>\n<span class=\"n\">test_x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">))</span>\n<span class=\"n\">test_y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"mi\">200</span><span class=\"p\">,))</span>\n<span class=\"n\">test_mask</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># get the default arguments</span>\n<span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">default_args</span><span class=\"p\">()</span>\n<span class=\"c1\"># modify part of the arguments</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">space_turbo</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">n_hidden</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n</pre>\n<ul>\n<li>Classification</li>\n</ul>\n<pre><span class=\"c1\"># initilize a model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">RNNModel</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">emb_mat</span><span class=\"p\">,</span> <span class=\"n\">bi_direction</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">rnn_type</span><span class=\"o\">=</span><span class=\"s1\">'GRU'</span><span class=\"p\">,</span> <span class=\"n\">use_attention</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># initilize a classifier</span>\n<span class=\"n\">nn</span> <span class=\"o\">=</span> <span class=\"n\">Classify</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">train_x</span><span class=\"p\">,</span> <span class=\"n\">train_y</span><span class=\"p\">,</span> <span class=\"n\">train_mask</span><span class=\"p\">,</span> <span class=\"n\">test_x</span><span class=\"p\">,</span> <span class=\"n\">test_y</span><span class=\"p\">,</span> <span class=\"n\">test_mask</span><span class=\"p\">)</span>\n<span class=\"c1\"># do training and testing</span>\n<span class=\"n\">evals</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">train_test</span><span class=\"p\">(</span><span class=\"n\">device_id</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>Run several times and get the average score.</li>\n</ul>\n<pre><span class=\"c1\"># initilize a model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">CNNModel</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">emb_mat</span><span class=\"p\">,</span> <span class=\"n\">kernel_widths</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">])</span>\n<span class=\"c1\"># initilize a classifier</span>\n<span class=\"n\">nn</span> <span class=\"o\">=</span> <span class=\"n\">Classify</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">train_x</span><span class=\"p\">,</span> <span class=\"n\">train_y</span><span class=\"p\">,</span> <span class=\"n\">train_mask</span><span class=\"p\">)</span>\n<span class=\"c1\"># run the model several times</span>\n<span class=\"n\">avg_evals</span> <span class=\"o\">=</span> <span class=\"n\">average_several_run</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">train_test</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">n_times</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">n_paral</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">fold</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>Parameters' grid search.</li>\n</ul>\n<pre><span class=\"c1\"># initilize a model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TransformerModel</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">n_layer</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"n\">n_head</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n<span class=\"c1\"># initilize a classifier</span>\n<span class=\"n\">nn</span> <span class=\"o\">=</span> <span class=\"n\">Classify</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">train_x</span><span class=\"p\">,</span> <span class=\"n\">train_y</span><span class=\"p\">,</span> <span class=\"n\">train_mask</span><span class=\"p\">,</span> <span class=\"n\">test_x</span><span class=\"p\">,</span> <span class=\"n\">test_y</span><span class=\"p\">,</span> <span class=\"n\">test_mask</span><span class=\"p\">)</span>\n<span class=\"c1\"># set searching params</span>\n<span class=\"n\">params_search</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">'learning_rate'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span><span class=\"p\">],</span> <span class=\"s1\">'n_hidden'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">]}</span>\n<span class=\"c1\"># run grid search</span>\n<span class=\"n\">max_evals</span> <span class=\"o\">=</span> <span class=\"n\">grid_search</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">,</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">train_test</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">params_search</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>Sequence labeling</li>\n</ul>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">dnnnlp.model</span> <span class=\"kn\">import</span> <span class=\"n\">RNNCRFModel</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dnnnlp.exec</span> <span class=\"kn\">import</span> <span class=\"n\">default_args</span><span class=\"p\">,</span> <span class=\"n\">SequenceLabeling</span>\n\n<span class=\"c1\"># load the train data</span>\n<span class=\"n\">train_x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">))</span>\n<span class=\"n\">train_y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">))</span>\n<span class=\"n\">train_mask</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">((</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># initilize a model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">RNNCRFModel</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">)</span>\n<span class=\"c1\"># initilize a labeler</span>\n<span class=\"n\">nn</span> <span class=\"o\">=</span> <span class=\"n\">SequenceLabeling</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">train_x</span><span class=\"p\">,</span> <span class=\"n\">train_y</span><span class=\"p\">,</span> <span class=\"n\">train_mask</span><span class=\"p\">)</span>\n<span class=\"c1\"># do cross validation</span>\n<span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">cross_validation</span><span class=\"p\">(</span><span class=\"n\">fold</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n</pre>\n<br>\n<h3>History</h3>\n<p><strong>version 1.1</strong></p>\n<ul>\n<li>Add <code>CRFLayer</code>: packaging CRF for both training and testing.</li>\n<li>Add <code>RNNCRFModel</code>: a integrated RNN-CRF sequence labeling model.</li>\n<li>Add <code>SequenceLabeling</code>: a sequence labeling execution module that inherits from Classify.</li>\n<li>Fix errors in judging whether a tensor is None.</li>\n</ul>\n<p><strong>version 1.0</strong></p>\n<ul>\n<li>Rename project <code>dnn</code> to <code>dnnnlp</code>.</li>\n<li>Remove file <code>base</code>, add file <code>utils</code>.</li>\n<li>Optimize and rename <code>SoftmaxLayer</code> and <code>SoftAttentionLayer</code>.</li>\n<li>Rewrite and rename <code>EmbeddingLayer</code>, <code>CNNLayer</code> and <code>RNNLayer</code>.</li>\n<li>Rewrite <code>MultiheadAttentionLayer</code>: a packaging attention layer based on <code>nn.MultiheadAttention</code>.</li>\n<li>Rewrite <code>TransformerLayer</code>: support new <code>MultiheadAttentionLayer</code>.</li>\n<li>Optimize and rename <code>CNNModel</code>, <code>RNNModel</code> and <code>TransformerModel</code>.</li>\n<li>Optimize and rename <code>Classify</code>: a highly applicable classification execution module.</li>\n<li>Rewrite <code>average_several_run</code> and <code>grid_search</code>: support multi-GPU parallel.</li>\n<li>Support pytorch 1.2.0.</li>\n</ul>\n<p><strong>version 0.12</strong></p>\n<ul>\n<li>Update <code>RNN_layer</code>: fully support for tanh, LSTM and GRU.</li>\n<li>Fix errors in some mask operations.</li>\n<li>Support pytorch 1.1.0.</li>\n</ul>\n<p>Old version <a href=\"https://github.com/NUSTM/pytorch-dnnnlp/tree/8d2d6c4e432076e13020ae54954aa419f3bb9bce\" rel=\"nofollow\">0.12.3</a>.</p>\n<p><strong>version 0.11</strong></p>\n<ul>\n<li>Provides an acceleration method by using more GPU memories.</li>\n<li>Fix the problem of memory consumption caused by abnormal data reading.</li>\n<li>Add <code>multi_head_attention_layer</code>: packaging multi-head attention for Transformer.</li>\n<li>Add <code>Transformer_layer</code> and <code>Transformer_model</code>: packaging Transformer layer and model written by ourself.</li>\n<li>Support data disruption for training.</li>\n</ul>\n<p><strong>version 0.10</strong></p>\n<ul>\n<li>Split the code into four files: <code>base</code>, <code>layer</code>, <code>model</code>, <code>exec</code>.</li>\n<li>Add <code>CNN_layer</code> and <code>CNN_model</code>: packaging CNN layer and model.</li>\n<li>Support multi-GPU parallel for each model.</li>\n</ul>\n<p><strong>version 0.9</strong></p>\n<ul>\n<li>Fix the problem of output format.</li>\n<li>Fix the statistical errors in cross-validation part of <code>LSTM_classify</code>.</li>\n<li>Rename: <code>LSTM_model</code> to <code>RNN_layer</code>, <code>self_attention</code> to <code>self_attention_layer</code>.</li>\n<li>Add <code>softmax_layer</code>: a packaging fully-connected layer.</li>\n</ul>\n<p><strong>version 0.8</strong></p>\n<ul>\n<li>Adjust the applicability of functions in <code>LSTM_classify</code> to avoid rewriting in <code>LSTM_sequence</code>.</li>\n<li>Optimize the way of parameter transfer.</li>\n<li>A more complete evaluation mechanism.</li>\n</ul>\n<p><strong>version 0.7</strong></p>\n<ul>\n<li>Add <code>LSTM_sequence</code>: a sequence labeling module for <code>LSTM_model</code>.</li>\n<li>Fix the nan-value problem in hierarchical classification.</li>\n<li>Support pytorch 1.0.0.</li>\n</ul>\n<p><strong>version 0.6</strong></p>\n<ul>\n<li>Update <code>LSTM_classify</code>: support hierarchical classification.</li>\n<li>The <code>GRU_model</code> is merged into the <code>LSTM_model</code>.</li>\n<li>Adapt to CPU operation.</li>\n</ul>\n<p><strong>version 0.5</strong></p>\n<ul>\n<li>Split the running part of <code>LSTM_classify</code> to reduce the rewrite of custom models.</li>\n<li>Add control for visual output.</li>\n<li>Create function <code>average_several_run</code>: support to get the average score after several training and testing.</li>\n<li>Create function <code>grid_search</code>: support parameters' grid search.</li>\n</ul>\n<p><strong>version 0.4</strong></p>\n<ul>\n<li>Add <code>GRU_model</code>: a packaging GRU model based on <code>nn.GRU</code>.</li>\n<li>Support L2 regular.</li>\n</ul>\n<p><strong>version 0.3</strong></p>\n<ul>\n<li>Add <code>self_attention</code>: provides attention mechanism support.</li>\n<li>Update <code>LSTM_classify</code>: adapts to complex custom models.</li>\n</ul>\n<p><strong>version 0.2</strong></p>\n<ul>\n<li>Support mode selection of embedding.</li>\n<li>Default usage of <code>nn.Dropout</code>.</li>\n<li>Create function <code>default_args</code> to provide default hyperparameters.</li>\n</ul>\n<p><strong>version 0.1</strong></p>\n<ul>\n<li>Initilization of project <code>dnn</code>: based on pytorch 0.4.1.</li>\n<li>Add <code>LSTM_model</code>: a packaging LSTM model based on <code>nn.LSTM</code>.</li>\n<li>Add <code>LSTM_classify</code>: a classification module for LSTM model, which supports train-test and corss-validation.</li>\n</ul>\n\n          </div>"}, "last_serial": 6045757, "releases": {"1.0.3": [{"comment_text": "", "digests": {"md5": "5cab194cc4aa07c63d40c45021b5b16c", "sha256": "6bb6f712fe83b196bc70964f131d0ae7b1db692779e209cfdd66c1fd72b820b2"}, "downloads": -1, "filename": "dnnnlp-1.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "5cab194cc4aa07c63d40c45021b5b16c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17395, "upload_time": "2019-08-23T02:39:22", "upload_time_iso_8601": "2019-08-23T02:39:22.447196Z", "url": "https://files.pythonhosted.org/packages/5a/03/e99f234932a81a4dc1d9bf0ac34a6fbff71fc0b2eb60151b65257eeef0fb/dnnnlp-1.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d65b29161ccb1183b61b418ee9b3f6e8", "sha256": "636c48d1520c45738207d5211e9e6d9b24faa6bdde63871e1160aa0f40714577"}, "downloads": -1, "filename": "dnnnlp-1.0.3.tar.gz", "has_sig": false, "md5_digest": "d65b29161ccb1183b61b418ee9b3f6e8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14178, "upload_time": "2019-08-23T02:39:25", "upload_time_iso_8601": "2019-08-23T02:39:25.769694Z", "url": "https://files.pythonhosted.org/packages/6c/36/4750b0163e72deed3680d30adbe50077a6414ea3161a2477851422f500dc/dnnnlp-1.0.3.tar.gz", "yanked": false}], "1.1.4": [{"comment_text": "", "digests": {"md5": "0552b2fb489ea97a98d280f0a80ddd93", "sha256": "eb03c41ff8ccdb592fc2de29a3b885a5e56e8e214e69a6256348dab29401d408"}, "downloads": -1, "filename": "dnnnlp-1.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "0552b2fb489ea97a98d280f0a80ddd93", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18344, "upload_time": "2019-10-29T07:50:06", "upload_time_iso_8601": "2019-10-29T07:50:06.755277Z", "url": "https://files.pythonhosted.org/packages/51/2e/545ea7d0cbc648d12e6777ed13c8e012b4c54d028085f1b659f6c98d6b26/dnnnlp-1.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5ad7873a3512660103af73e64a30e288", "sha256": "aa37a56ccf089b3450c68d546ca4e9e2bd887c5020fef8076460445077ca7fe3"}, "downloads": -1, "filename": "dnnnlp-1.1.4.tar.gz", "has_sig": false, "md5_digest": "5ad7873a3512660103af73e64a30e288", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15056, "upload_time": "2019-10-29T07:50:08", "upload_time_iso_8601": "2019-10-29T07:50:08.924396Z", "url": "https://files.pythonhosted.org/packages/c1/9c/a5e635017e2ef33f0795c584044842c5a3ca22a403994f469b7b0b3823b7/dnnnlp-1.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0552b2fb489ea97a98d280f0a80ddd93", "sha256": "eb03c41ff8ccdb592fc2de29a3b885a5e56e8e214e69a6256348dab29401d408"}, "downloads": -1, "filename": "dnnnlp-1.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "0552b2fb489ea97a98d280f0a80ddd93", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18344, "upload_time": "2019-10-29T07:50:06", "upload_time_iso_8601": "2019-10-29T07:50:06.755277Z", "url": "https://files.pythonhosted.org/packages/51/2e/545ea7d0cbc648d12e6777ed13c8e012b4c54d028085f1b659f6c98d6b26/dnnnlp-1.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5ad7873a3512660103af73e64a30e288", "sha256": "aa37a56ccf089b3450c68d546ca4e9e2bd887c5020fef8076460445077ca7fe3"}, "downloads": -1, "filename": "dnnnlp-1.1.4.tar.gz", "has_sig": false, "md5_digest": "5ad7873a3512660103af73e64a30e288", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15056, "upload_time": "2019-10-29T07:50:08", "upload_time_iso_8601": "2019-10-29T07:50:08.924396Z", "url": "https://files.pythonhosted.org/packages/c1/9c/a5e635017e2ef33f0795c584044842c5a3ca22a403994f469b7b0b3823b7/dnnnlp-1.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:51:08 2020"}