{"info": {"author": "Simon W\u00f6rpel", "author_email": "simon.woerpel@medienrevolte.de", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Topic :: Utilities"], "description": "Run Panda Run\n=============\n\n\n\n\nA simple interface written in python for reproducible & persistent data\nwarehousing around small data analysis / processing projects with\n```pandas`` <https://pandas.pydata.org/>`__.\n\nCurrently supports ``csv`` and ``json`` resources.\n\nUseful to build an automated workflow like this:\n\n**1. Download** fetch datasets from different remote or local sources,\nstore them somewhere (with respect to versioning / incremental updates),\ndo some general cleaning, `processing <#operations>`__ and get a nice\n```pandas.DataFrame`` <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`__\nfor each dataset and\n\n**2. Wrangle** your data somehow, store and load\n`revisions <#revisions>`__ to share *source of truth* between notebooks\nor scripts.\n\n`3. Publish <#publish>`__ some of the wrangled data somewhere where\nother services (Google Spreadsheet, Datawrapper, even another\n``Datastore``) can work on further\n\nIt includes a simple command-line interface, e.g.\u00a0for automated\nprocessing via cronjobs.\n\nQuickstart\n----------\n\n`Install via pip <#installation>`__\n\nSpecify your datasets via ``yaml`` syntax:\n\n.. code:: yaml\n\n   datasets:\n     my_dataset:\n       csv_url: http://example.org/data.csv  # url to fetch csv file from\n       columns:                              # only use specified columns\n         - id: identifier                    # rename original column `identifier` to `id`\n         - name\n         - date\n       dt_index: date                        # set date-based index\n     another_dataset:\n       json_url: !ENV ${SECRET_URL}          # see below for env vars\n       ...\n\nstore this as a file, and set the env var ``CONFIG`` to the path:\n\n::\n\n       export CONFIG=./path/to/config.yml\n\n.. code:: python\n\n   from runpandarun.datasets import my_dataset, another_dataset\n\n   df = my_dataset.get_df()\n   df['name'].plot.hist()\n\n   another_dataset.daily.mean().plot()  # some handy shorthands for pandas\n\nHandle data persistence and state of datasets:\n\n.. code:: python\n\n   from runpandarun.datasets import my_dataset\n\n   # update data from remote source:\n   my_dataset = my_dataset.update()\n\n   # update complete store:\n   from runpandarun import Datastore\n\n   store = Datastore()\n   store.update()\n\n   # save a revision\n   df = my_dataset.get_df()\n   df = wrangle(df)\n   my_dataset.save(df, 'wrangled')\n\n   # get this revision (in another script)\n   df = my_dataset['wrangled']\n\n   # publish\n   df = my_dataset.get_df()\n   clean(df)\n   my_dataset.publish(df, name='cleaned', overwrite=True)\n\nUpdate your datastore from the command-line (for use in cronjobs e.g.)\n\nSpecify config path either via ``--config`` or env var ``CONFIG``\n\nUpdate all:\n\n::\n\n   runpandarun update --config /path/to/config.yml\n\nOnly specific datasets and with env var:\n\n::\n\n   CONFIG=/path/to/config.yml runpandarun update my_dataset my_other_dataset ...\n\nInstallation\n------------\n\nRequires python3. Virtualenv use recommended.\n\nAdditional dependencies (``pandas`` et. al.) will be installed\nautomatically:\n\n::\n\n   pip install runpandarun\n\nAfter this, you should be able to execute in your terminal:\n\n::\n\n   runpandarun -h\n\nYou should as well be able to import it in your python scripts:\n\n.. code:: python\n\n   from runpandarun import Datastore\n\n   # start the party...\n\nConfig\n------\n\n**Easy**\n\nSet an environment variable ``CONFIG`` pointing to your yaml file.\n\n``runpandarun`` will find your config and you are all set (see\n`quickstart <#quickstart>`__)\n\n**Manually**\n\nOf course you can initialize the config manually:\n\n-  from a file\n-  as yaml string\n-  as a python dict\n\n.. code:: python\n\n   from runpandarun import Datastore\n\n   store = Datastore('./path/to/config.yml')\n   store = Datastore(config_dict)\n   store = Datastore(\"\"\"\n       datasets:\n         my_dataset:\n         ...\n   \"\"\")\n\nTo quickly test your config for a dataset named ``my_dataset``, you can\nuse the command-line (this will print the generated csv to stdout):\n\n::\n\n   CONFIG=config.yml runpandarun print my_dataset\n\nexamples\n~~~~~~~~\n\nSee the yaml files in `./example/ <./example/>`__\n\ntop-level options\n~~~~~~~~~~~~~~~~~\n\n.. code:: yaml\n\n   storage:\n     data_root: ./path/                    # absolute or relative path where to store the files\n   publish:\n     handlers:\n       filesystem:\n         public_root: !ENV ${PUBLIC_ROOT}  # where to store published data, e.g. a path to a webserver root via env var\n         enabled: true\n       gcloud:\n         bucket: !ENV ${GOOGLE_BUCKET}     # or in a google cloud storage bucket...\n         enabled: !ENV ${GOOGLE_PUBLISH}   # enable or disable a publish handler based on environment\n   combine:\n     - dataset1                            # keys of defined datasets for quick merging\n     - dataset2\n   datasets:                               # definition for datasets\n     dataset1:\n       csv_url: ...\n\ndataset options\n~~~~~~~~~~~~~~~\n\n**Source link**\n\n-  *required*\n-  any of:\n\n.. code:: yaml\n\n       csv_url:            # url to remote csv, the response must be the direct csv content\n                           # this can also be a Google Spreadsheet \"published to the web\" in csv format\n       csv_local:          # absolute or relative path to a file on disk\n       json_url:           # url to remote json, the response should be text or application/json\n       json_local:         # absolute or relative path to a file on disk\n\n**Request params**\n\n-  *optional*\n-  for each source url, you can pass ``params`` and ``headers`` that\n   will feed into\n   ```requests.get()`` <https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request>`__\n\n.. code:: yaml\n\n       csv_url: https://example.org\n       request:\n         params:\n           format: csv\n         header:\n           \"api-key\": 123abc\n\n**Incremental**\n\n-  *optional*\n-  instead of versioning the downloaded datasets and only use the latest\n   one, this flag allows to combine all the downloaded data over time to\n   one dataset. Use case example: A publisher releases updated data each\n   day under the same url\n\n.. code:: yaml\n\n       incremental: true\n\n**Columns**\n\n-  *optional*\n-  specify list of subset columns to use\n\n.. code:: yaml\n\n       columns:\n         - column1\n         - column2: origColumnName     # optional renaming mapping (rename `origColumnName` to `column2`)\n\n**Index**\n\n-  *optional*\n-  specify which column (after renaming was applied) should be the index\n-  default: ``id``\n\n.. code:: yaml\n\n       index: person_id                # set column `person_id` as index\n\n.. code:: yaml\n\n       dt_index: event_date            # specify a date/time-based index instead\n\n.. code:: yaml\n\n       dt_index:\n         column: event_date\n         format: \"%d.%m.%Y\"\n\nOperations\n~~~~~~~~~~\n\n-  *optional*\n\nApply `any valid operation that is a function attribute of\n``pandas.DataFrame`` <https://pandas.pydata.org/pandas-docs/stable/reference/frame.html>`__\n(like ``drop_duplicates``, ``sort_values``, ``fillna`` \u2026) in the given\norder with optional function arguments that will be passed to the call.\n\nDefault operations: ``['drop_duplicates', 'sort_index']``\n\nDisable:\n\n.. code:: yaml\n\n       ...\n       ops: false\n\nHere are examples:\n\n**Sort**\n\n```pandas.DataFrame.sort_values()`` <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html>`__\n\n.. code:: yaml\n\n       ...\n       ops:\n         sort_values:                    # pass parameters for pandas function `sort_values`\n           by:\n             - column1\n             - column2\n           ascending: false\n\n**De-duplicate**\n\n```pandas.DataFrame.drop_duplicates()`` <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html>`__\n- when using a subset, use in conjunction with ``sort_values`` to make\nsure to keep the right records\n\n.. code:: yaml\n\n       ...\n       ops:\n         drop_duplicates:              # pass parameters for pandas function `drop_duplicates`\n           subset:\n             - column1\n             - column2\n           keep: last\n\ncombining\n~~~~~~~~~\n\nA quick top-level option for easy combining datasets from different\nsources.\n\nThis happens via\n```pandas.concat`` <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html>`__\nand decides if it should concat *long* or *wide* (aka\n``pandas.concat(..., axis=1)``)\n\n-  **long**: if index name and column names accross the specified\n   datasets in config ``combine`` are the same\n-  **wide**: if index is the same accross the specified datasets in\n   config ``combine``\n\nTODO: *more to come\u2026 (aka merging)*\n\nenv vars\n~~~~~~~~\n\nFor api keys or other secrets, you can put environment variables into\nthe config:\n\n.. code:: yaml\n\n   storage:\n     data_root: !ENV '${DATA_ROOT}/data/'\n   datasets:\n     google_places:\n       json_url: https://maps.googleapis.com/maps/api/place/findplacefromtext/json\n       request:\n         params:\n           key: !ENV ${GOOGLE_APY_KEY}\n       ...\n\nUsage in your scripts\n---------------------\n\nOnce set up, you can start moving the data warehousing out of your\nanalysis scripts and focus on the analysis itself\u2026\n\n.. code:: python\n\n   from runpandarun import Datastore\n\n   store = Datastore(config)\n\n   # all your datasets become direct attributes of the store:\n   ds = store.my_dataset\n\n   # all your datasets have their computed (according to your config) `pandas.DataFrame` as attribute:\n   df = store.my_dataset.get_df()\n\n   # get combined df (if specified in the config)\n   df = store.combined\n\nresampling\n~~~~~~~~~~\n\nsome time-based shorthands (if you have a ``dt_index: true`` in your\nconfig) based on\n```pandas.DataFrame.resample`` <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html>`__\n\nThe resulting ``pandas.DataFrame`` will only have columns with numeric\ndata in it.\n\n.. code:: python\n\n   dataset = store.my_time_based_dataset\n\n   s = dataset.daily.mean()\n   s.plot()\n\n   s = dataset.yearly.count().cumsum()\n   s.plot()\n\nAvailable time aggregations: - minutely - hourly - daily - weekly -\nmonthly - yearly\n\nAvailable aggregation methods: - sum - mean - max - min - count\n\nFor more advanced resampling, just work on your dataframes directly\u2026\n\nPublish\n-------\n\nAfter the workflow is done, you can publish some (or all) results.\n\n.. code:: python\n\n   dataset = store.my_dataset\n   df1 = do_something_with(dataset.get_df())\n   dataset.publish(df1, overwrite=True, include_source=True)\n   df2 = do_something_else_with(dataset.get_df())\n   dataset.publish(df2, name='filtered_for_europe', format='json')\n\nFor behaviour reasons the ``overwrite``-flag must be set explicitly\n(during function execution or in config, see below), otherwise it will\nraise if a public file already exists. To avoid overwriting, set a\ndifferent name.\n\nThe ``publish()`` parameters can be set in the config as well, either\nglobally or per dataset, specified for each handler (currently\n``filesystem`` or ``gcloud``). Dataset-specific settings overwrite\nglobal ones for the storage handler.\n\n.. code:: yaml\n\n   publish:\n     overwrite: true               # global option for all handlers: always overwrite existing files\n     with_timestamp: true          # include current timestamp in filename\n     handlers:\n       filesystem:\n         public_root: /path/to/a/dir/a/webserver/can/serve/\n         include_source: true\n       gcloud:\n         bucket: my-bucket-name\n         include_source: false\n   ...\n   datasets:\n     my_dataset:\n       ...\n       publish:\n         gcloud:\n           bucket: another-bucket\n           include_source: true\n           format: json\n           name: something\n\n**TODO**: currently only storing to a filesystem or google cloud storage\nimplemented.\n\nBut features could be: - goolge spreadsheet - ftp - s3 - \u2026\n\nRevisions\n---------\n\nAt any time between reading data in and publishing you can store and get\nrevisions of a dataset. This is usually a ``pd.DataFrame`` in an\nintermediate state, e.g.\u00a0after date enriching but before analysis.\n\nThis feature can be used in an automated processing workflow consisting\nof multiple notebooks to share DataFrames between each other. The\nunderlying storage mechanism is\n`pickle <https://docs.python.org/3/library/pickle.html>`__ to make sure\na DataFrame revision behaves as expected. This comes with the downside\nthat pickle\u2019s are not safe to share between different systems, but to\nre-create them in another environment, that\u2019s what a reproducible\nworkflow is for, right?\n\n**store a revision**\n\n.. code:: python\n\n   ds = store.my_dataset\n   df = ds.get_df()\n   ds.revisions.save('tansformed', df.T)\n\n**load a revision**\n\n.. code:: python\n\n   ds = store.my_dataset\n   df = ds['transformed']\n\n**show available revisions**\n\n.. code:: python\n\n   ds = store.my_dataset\n   ds.revisions.show()\n\n**iterate through revisions**\n\n.. code:: python\n\n   ds = store.my_dataset\n   for df in ds.revisions:\n     do_something(df)\n\n*Pro tip* you can go crazy and use this mechanism to store & retrieve\n*any* object that is serializable via\n`pickle <https://docs.python.org/3/library/pickle.html>`__\n\ncli\n---\n\n.. code:: bash\n\n   usage: runpandarun [-h] [--loglevel LOGLEVEL] {update,print,publish} ...\n\n   positional arguments:\n     {update,print,publish}\n                           commands help: run `runpandarun <command> -h`\n\n   optional arguments:\n     -h, --help            show this help message and exit\n     --loglevel LOGLEVEL\n\ndevelopement\n------------\n\nInstall testing requirements:\n\n::\n\n   make install\n\nTest:\n\n::\n\n   make test\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/simonwoerpel/runpandarun", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "runpandarun", "package_url": "https://pypi.org/project/runpandarun/", "platform": "", "project_url": "https://pypi.org/project/runpandarun/", "project_urls": {"Homepage": "https://github.com/simonwoerpel/runpandarun"}, "release_url": "https://pypi.org/project/runpandarun/0.1.4/", "requires_dist": ["banal", "pyyaml", "pandas", "requests", "awesome-slugify", "python-dateutil", "google-cloud-storage"], "requires_python": "", "summary": "A simple toolkit for managing data from different sources.", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>A simple interface written in python for reproducible &amp; persistent data\nwarehousing around small data analysis / processing projects with\n<tt>`pandas</tt> &lt;<a href=\"https://pandas.pydata.org/\" rel=\"nofollow\">https://pandas.pydata.org/</a>&gt;`__.</p>\n<p>Currently supports <tt>csv</tt> and <tt>json</tt> resources.</p>\n<p>Useful to build an automated workflow like this:</p>\n<p><strong>1. Download</strong> fetch datasets from different remote or local sources,\nstore them somewhere (with respect to versioning / incremental updates),\ndo some general cleaning, <a href=\"#operations\" rel=\"nofollow\">processing</a> and get a nice\n<tt>`pandas.DataFrame</tt> &lt;<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\" rel=\"nofollow\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html</a>&gt;`__\nfor each dataset and</p>\n<p><strong>2. Wrangle</strong> your data somehow, store and load\n<a href=\"#revisions\" rel=\"nofollow\">revisions</a> to share <em>source of truth</em> between notebooks\nor scripts.</p>\n<p><a href=\"#publish\" rel=\"nofollow\">3. Publish</a> some of the wrangled data somewhere where\nother services (Google Spreadsheet, Datawrapper, even another\n<tt>Datastore</tt>) can work on further</p>\n<p>It includes a simple command-line interface, e.g.\u00a0for automated\nprocessing via cronjobs.</p>\n<div id=\"quickstart\">\n<h2>Quickstart</h2>\n<p><a href=\"#installation\" rel=\"nofollow\">Install via pip</a></p>\n<p>Specify your datasets via <tt>yaml</tt> syntax:</p>\n<pre><span class=\"nt\">datasets</span><span class=\"p\">:</span>\n  <span class=\"nt\">my_dataset</span><span class=\"p\">:</span>\n    <span class=\"nt\">csv_url</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">http://example.org/data.csv</span>  <span class=\"c1\"># url to fetch csv file from</span>\n    <span class=\"nt\">columns</span><span class=\"p\">:</span>                              <span class=\"c1\"># only use specified columns</span>\n      <span class=\"p-Indicator\">-</span> <span class=\"nt\">id</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">identifier</span>                    <span class=\"c1\"># rename original column `identifier` to `id`</span>\n      <span class=\"p-Indicator\">-</span> <span class=\"l-Scalar-Plain\">name</span>\n      <span class=\"p-Indicator\">-</span> <span class=\"l-Scalar-Plain\">date</span>\n    <span class=\"nt\">dt_index</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">date</span>                        <span class=\"c1\"># set date-based index</span>\n  <span class=\"nt\">another_dataset</span><span class=\"p\">:</span>\n    <span class=\"nt\">json_url</span><span class=\"p\">:</span> <span class=\"kt\">!ENV</span> <span class=\"l-Scalar-Plain\">${SECRET_URL}</span>          <span class=\"c1\"># see below for env vars</span>\n    <span class=\"l-Scalar-Plain\">...</span>\n</pre>\n<p>store this as a file, and set the env var <tt>CONFIG</tt> to the path:</p>\n<pre>export CONFIG=./path/to/config.yml\n</pre>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">runpandarun.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">my_dataset</span><span class=\"p\">,</span> <span class=\"n\">another_dataset</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">my_dataset</span><span class=\"o\">.</span><span class=\"n\">get_df</span><span class=\"p\">()</span>\n<span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">'name'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">()</span>\n\n<span class=\"n\">another_dataset</span><span class=\"o\">.</span><span class=\"n\">daily</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">()</span>  <span class=\"c1\"># some handy shorthands for pandas</span>\n</pre>\n<p>Handle data persistence and state of datasets:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">runpandarun.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">my_dataset</span>\n\n<span class=\"c1\"># update data from remote source:</span>\n<span class=\"n\">my_dataset</span> <span class=\"o\">=</span> <span class=\"n\">my_dataset</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># update complete store:</span>\n<span class=\"kn\">from</span> <span class=\"nn\">runpandarun</span> <span class=\"kn\">import</span> <span class=\"n\">Datastore</span>\n\n<span class=\"n\">store</span> <span class=\"o\">=</span> <span class=\"n\">Datastore</span><span class=\"p\">()</span>\n<span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># save a revision</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">my_dataset</span><span class=\"o\">.</span><span class=\"n\">get_df</span><span class=\"p\">()</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">wrangle</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n<span class=\"n\">my_dataset</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"s1\">'wrangled'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># get this revision (in another script)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">my_dataset</span><span class=\"p\">[</span><span class=\"s1\">'wrangled'</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># publish</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">my_dataset</span><span class=\"o\">.</span><span class=\"n\">get_df</span><span class=\"p\">()</span>\n<span class=\"n\">clean</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n<span class=\"n\">my_dataset</span><span class=\"o\">.</span><span class=\"n\">publish</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'cleaned'</span><span class=\"p\">,</span> <span class=\"n\">overwrite</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>Update your datastore from the command-line (for use in cronjobs e.g.)</p>\n<p>Specify config path either via <tt><span class=\"pre\">--config</span></tt> or env var <tt>CONFIG</tt></p>\n<p>Update all:</p>\n<pre>runpandarun update --config /path/to/config.yml\n</pre>\n<p>Only specific datasets and with env var:</p>\n<pre>CONFIG=/path/to/config.yml runpandarun update my_dataset my_other_dataset ...\n</pre>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>Requires python3. Virtualenv use recommended.</p>\n<p>Additional dependencies (<tt>pandas</tt> et. al.) will be installed\nautomatically:</p>\n<pre>pip install runpandarun\n</pre>\n<p>After this, you should be able to execute in your terminal:</p>\n<pre>runpandarun -h\n</pre>\n<p>You should as well be able to import it in your python scripts:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">runpandarun</span> <span class=\"kn\">import</span> <span class=\"n\">Datastore</span>\n\n<span class=\"c1\"># start the party...</span>\n</pre>\n</div>\n<div id=\"config\">\n<h2>Config</h2>\n<p><strong>Easy</strong></p>\n<p>Set an environment variable <tt>CONFIG</tt> pointing to your yaml file.</p>\n<p><tt>runpandarun</tt> will find your config and you are all set (see\n<a href=\"#quickstart\" rel=\"nofollow\">quickstart</a>)</p>\n<p><strong>Manually</strong></p>\n<p>Of course you can initialize the config manually:</p>\n<ul>\n<li>from a file</li>\n<li>as yaml string</li>\n<li>as a python dict</li>\n</ul>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">runpandarun</span> <span class=\"kn\">import</span> <span class=\"n\">Datastore</span>\n\n<span class=\"n\">store</span> <span class=\"o\">=</span> <span class=\"n\">Datastore</span><span class=\"p\">(</span><span class=\"s1\">'./path/to/config.yml'</span><span class=\"p\">)</span>\n<span class=\"n\">store</span> <span class=\"o\">=</span> <span class=\"n\">Datastore</span><span class=\"p\">(</span><span class=\"n\">config_dict</span><span class=\"p\">)</span>\n<span class=\"n\">store</span> <span class=\"o\">=</span> <span class=\"n\">Datastore</span><span class=\"p\">(</span><span class=\"s2\">\"\"\"\n    datasets:\n      my_dataset:\n      ...\n\"\"\"</span><span class=\"p\">)</span>\n</pre>\n<p>To quickly test your config for a dataset named <tt>my_dataset</tt>, you can\nuse the command-line (this will print the generated csv to stdout):</p>\n<pre>CONFIG=config.yml runpandarun print my_dataset\n</pre>\n<div id=\"examples\">\n<h3>examples</h3>\n<p>See the yaml files in <a href=\"./example/\" rel=\"nofollow\">./example/</a></p>\n</div>\n<div id=\"top-level-options\">\n<h3>top-level options</h3>\n<pre><span class=\"nt\">storage</span><span class=\"p\">:</span>\n  <span class=\"nt\">data_root</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">./path/</span>                    <span class=\"c1\"># absolute or relative path where to store the files</span>\n<span class=\"nt\">publish</span><span class=\"p\">:</span>\n  <span class=\"nt\">handlers</span><span class=\"p\">:</span>\n    <span class=\"nt\">filesystem</span><span class=\"p\">:</span>\n      <span class=\"nt\">public_root</span><span class=\"p\">:</span> <span class=\"kt\">!ENV</span> <span class=\"l-Scalar-Plain\">${PUBLIC_ROOT}</span>  <span class=\"c1\"># where to store published data, e.g. a path to a webserver root via env var</span>\n      <span class=\"nt\">enabled</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">true</span>\n    <span class=\"nt\">gcloud</span><span class=\"p\">:</span>\n      <span class=\"nt\">bucket</span><span class=\"p\">:</span> <span class=\"kt\">!ENV</span> <span class=\"l-Scalar-Plain\">${GOOGLE_BUCKET}</span>     <span class=\"c1\"># or in a google cloud storage bucket...</span>\n      <span class=\"nt\">enabled</span><span class=\"p\">:</span> <span class=\"kt\">!ENV</span> <span class=\"l-Scalar-Plain\">${GOOGLE_PUBLISH}</span>   <span class=\"c1\"># enable or disable a publish handler based on environment</span>\n<span class=\"nt\">combine</span><span class=\"p\">:</span>\n  <span class=\"p-Indicator\">-</span> <span class=\"l-Scalar-Plain\">dataset1</span>                            <span class=\"c1\"># keys of defined datasets for quick merging</span>\n  <span class=\"p-Indicator\">-</span> <span class=\"l-Scalar-Plain\">dataset2</span>\n<span class=\"nt\">datasets</span><span class=\"p\">:</span>                               <span class=\"c1\"># definition for datasets</span>\n  <span class=\"nt\">dataset1</span><span class=\"p\">:</span>\n    <span class=\"nt\">csv_url</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">...</span>\n</pre>\n</div>\n<div id=\"dataset-options\">\n<h3>dataset options</h3>\n<p><strong>Source link</strong></p>\n<ul>\n<li><em>required</em></li>\n<li>any of:</li>\n</ul>\n<pre><span class=\"nt\">csv_url</span><span class=\"p\">:</span>            <span class=\"c1\"># url to remote csv, the response must be the direct csv content</span>\n                    <span class=\"c1\"># this can also be a Google Spreadsheet \"published to the web\" in csv format</span>\n<span class=\"nt\">csv_local</span><span class=\"p\">:</span>          <span class=\"c1\"># absolute or relative path to a file on disk</span>\n<span class=\"nt\">json_url</span><span class=\"p\">:</span>           <span class=\"c1\"># url to remote json, the response should be text or application/json</span>\n<span class=\"nt\">json_local</span><span class=\"p\">:</span>         <span class=\"c1\"># absolute or relative path to a file on disk</span>\n</pre>\n<p><strong>Request params</strong></p>\n<ul>\n<li><em>optional</em></li>\n<li>for each source url, you can pass <tt>params</tt> and <tt>headers</tt> that\nwill feed into\n<tt>`requests.get()</tt> &lt;<a href=\"https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request\" rel=\"nofollow\">https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request</a>&gt;`__</li>\n</ul>\n<pre><span class=\"nt\">csv_url</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">https://example.org</span>\n<span class=\"nt\">request</span><span class=\"p\">:</span>\n  <span class=\"nt\">params</span><span class=\"p\">:</span>\n    <span class=\"nt\">format</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">csv</span>\n  <span class=\"nt\">header</span><span class=\"p\">:</span>\n    <span class=\"s\">\"api-key\"</span><span class=\"p-Indicator\">:</span> <span class=\"l-Scalar-Plain\">123abc</span>\n</pre>\n<p><strong>Incremental</strong></p>\n<ul>\n<li><em>optional</em></li>\n<li>instead of versioning the downloaded datasets and only use the latest\none, this flag allows to combine all the downloaded data over time to\none dataset. Use case example: A publisher releases updated data each\nday under the same url</li>\n</ul>\n<pre><span class=\"nt\">incremental</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">true</span>\n</pre>\n<p><strong>Columns</strong></p>\n<ul>\n<li><em>optional</em></li>\n<li>specify list of subset columns to use</li>\n</ul>\n<pre><span class=\"nt\">columns</span><span class=\"p\">:</span>\n  <span class=\"p-Indicator\">-</span> <span class=\"l-Scalar-Plain\">column1</span>\n  <span class=\"p-Indicator\">-</span> <span class=\"nt\">column2</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">origColumnName</span>     <span class=\"c1\"># optional renaming mapping (rename `origColumnName` to `column2`)</span>\n</pre>\n<p><strong>Index</strong></p>\n<ul>\n<li><em>optional</em></li>\n<li>specify which column (after renaming was applied) should be the index</li>\n<li>default: <tt>id</tt></li>\n</ul>\n<pre><span class=\"nt\">index</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">person_id</span>                <span class=\"c1\"># set column `person_id` as index</span>\n</pre>\n<pre><span class=\"nt\">dt_index</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">event_date</span>            <span class=\"c1\"># specify a date/time-based index instead</span>\n</pre>\n<pre><span class=\"nt\">dt_index</span><span class=\"p\">:</span>\n  <span class=\"nt\">column</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">event_date</span>\n  <span class=\"nt\">format</span><span class=\"p\">:</span> <span class=\"s\">\"%d.%m.%Y\"</span>\n</pre>\n</div>\n<div id=\"operations\">\n<h3>Operations</h3>\n<ul>\n<li><em>optional</em></li>\n</ul>\n<p>Apply <cite>any valid operation that is a function attribute of\n``pandas.DataFrame`</cite> &lt;<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\" rel=\"nofollow\">https://pandas.pydata.org/pandas-docs/stable/reference/frame.html</a>&gt;`__\n(like <tt>drop_duplicates</tt>, <tt>sort_values</tt>, <tt>fillna</tt> \u2026) in the given\norder with optional function arguments that will be passed to the call.</p>\n<p>Default operations: <tt>['drop_duplicates', 'sort_index']</tt></p>\n<p>Disable:</p>\n<pre><span class=\"nn\">...</span>\n<span class=\"nt\">ops</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">false</span>\n</pre>\n<p>Here are examples:</p>\n<p><strong>Sort</strong></p>\n<p><tt>`pandas.DataFrame.sort_values()</tt> &lt;<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\" rel=\"nofollow\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html</a>&gt;`__</p>\n<pre><span class=\"nn\">...</span>\n<span class=\"nt\">ops</span><span class=\"p\">:</span>\n  <span class=\"nt\">sort_values</span><span class=\"p\">:</span>                    <span class=\"c1\"># pass parameters for pandas function `sort_values`</span>\n    <span class=\"nt\">by</span><span class=\"p\">:</span>\n      <span class=\"p-Indicator\">-</span> <span class=\"l-Scalar-Plain\">column1</span>\n      <span class=\"p-Indicator\">-</span> <span class=\"l-Scalar-Plain\">column2</span>\n    <span class=\"nt\">ascending</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">false</span>\n</pre>\n<p><strong>De-duplicate</strong></p>\n<p><tt>`pandas.DataFrame.drop_duplicates()</tt> &lt;<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\" rel=\"nofollow\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html</a>&gt;`__\n- when using a subset, use in conjunction with <tt>sort_values</tt> to make\nsure to keep the right records</p>\n<pre><span class=\"nn\">...</span>\n<span class=\"nt\">ops</span><span class=\"p\">:</span>\n  <span class=\"nt\">drop_duplicates</span><span class=\"p\">:</span>              <span class=\"c1\"># pass parameters for pandas function `drop_duplicates`</span>\n    <span class=\"nt\">subset</span><span class=\"p\">:</span>\n      <span class=\"p-Indicator\">-</span> <span class=\"l-Scalar-Plain\">column1</span>\n      <span class=\"p-Indicator\">-</span> <span class=\"l-Scalar-Plain\">column2</span>\n    <span class=\"nt\">keep</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">last</span>\n</pre>\n</div>\n<div id=\"combining\">\n<h3>combining</h3>\n<p>A quick top-level option for easy combining datasets from different\nsources.</p>\n<p>This happens via\n<tt>`pandas.concat</tt> &lt;<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html</a>&gt;`__\nand decides if it should concat <em>long</em> or <em>wide</em> (aka\n<tt><span class=\"pre\">pandas.concat(...,</span> axis=1)</tt>)</p>\n<ul>\n<li><strong>long</strong>: if index name and column names accross the specified\ndatasets in config <tt>combine</tt> are the same</li>\n<li><strong>wide</strong>: if index is the same accross the specified datasets in\nconfig <tt>combine</tt></li>\n</ul>\n<p>TODO: <em>more to come\u2026 (aka merging)</em></p>\n</div>\n<div id=\"env-vars\">\n<h3>env vars</h3>\n<p>For api keys or other secrets, you can put environment variables into\nthe config:</p>\n<pre><span class=\"nt\">storage</span><span class=\"p\">:</span>\n  <span class=\"nt\">data_root</span><span class=\"p\">:</span> <span class=\"kt\">!ENV</span> <span class=\"s\">'${DATA_ROOT}/data/'</span>\n<span class=\"nt\">datasets</span><span class=\"p\">:</span>\n  <span class=\"nt\">google_places</span><span class=\"p\">:</span>\n    <span class=\"nt\">json_url</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">https://maps.googleapis.com/maps/api/place/findplacefromtext/json</span>\n    <span class=\"nt\">request</span><span class=\"p\">:</span>\n      <span class=\"nt\">params</span><span class=\"p\">:</span>\n        <span class=\"nt\">key</span><span class=\"p\">:</span> <span class=\"kt\">!ENV</span> <span class=\"l-Scalar-Plain\">${GOOGLE_APY_KEY}</span>\n    <span class=\"l-Scalar-Plain\">...</span>\n</pre>\n</div>\n</div>\n<div id=\"usage-in-your-scripts\">\n<h2>Usage in your scripts</h2>\n<p>Once set up, you can start moving the data warehousing out of your\nanalysis scripts and focus on the analysis itself\u2026</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">runpandarun</span> <span class=\"kn\">import</span> <span class=\"n\">Datastore</span>\n\n<span class=\"n\">store</span> <span class=\"o\">=</span> <span class=\"n\">Datastore</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># all your datasets become direct attributes of the store:</span>\n<span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">my_dataset</span>\n\n<span class=\"c1\"># all your datasets have their computed (according to your config) `pandas.DataFrame` as attribute:</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">my_dataset</span><span class=\"o\">.</span><span class=\"n\">get_df</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># get combined df (if specified in the config)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">combined</span>\n</pre>\n<div id=\"resampling\">\n<h3>resampling</h3>\n<p>some time-based shorthands (if you have a <tt>dt_index: true</tt> in your\nconfig) based on\n<tt>`pandas.DataFrame.resample</tt> &lt;<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\" rel=\"nofollow\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html</a>&gt;`__</p>\n<p>The resulting <tt>pandas.DataFrame</tt> will only have columns with numeric\ndata in it.</p>\n<pre><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">my_time_based_dataset</span>\n\n<span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">daily</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n<span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">()</span>\n\n<span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">yearly</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">cumsum</span><span class=\"p\">()</span>\n<span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">()</span>\n</pre>\n<p>Available time aggregations: - minutely - hourly - daily - weekly -\nmonthly - yearly</p>\n<p>Available aggregation methods: - sum - mean - max - min - count</p>\n<p>For more advanced resampling, just work on your dataframes directly\u2026</p>\n</div>\n</div>\n<div id=\"publish\">\n<h2>Publish</h2>\n<p>After the workflow is done, you can publish some (or all) results.</p>\n<pre><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">my_dataset</span>\n<span class=\"n\">df1</span> <span class=\"o\">=</span> <span class=\"n\">do_something_with</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">get_df</span><span class=\"p\">())</span>\n<span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">publish</span><span class=\"p\">(</span><span class=\"n\">df1</span><span class=\"p\">,</span> <span class=\"n\">overwrite</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">include_source</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">do_something_else_with</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">get_df</span><span class=\"p\">())</span>\n<span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">publish</span><span class=\"p\">(</span><span class=\"n\">df2</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'filtered_for_europe'</span><span class=\"p\">,</span> <span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"s1\">'json'</span><span class=\"p\">)</span>\n</pre>\n<p>For behaviour reasons the <tt>overwrite</tt>-flag must be set explicitly\n(during function execution or in config, see below), otherwise it will\nraise if a public file already exists. To avoid overwriting, set a\ndifferent name.</p>\n<p>The <tt>publish()</tt> parameters can be set in the config as well, either\nglobally or per dataset, specified for each handler (currently\n<tt>filesystem</tt> or <tt>gcloud</tt>). Dataset-specific settings overwrite\nglobal ones for the storage handler.</p>\n<pre><span class=\"nt\">publish</span><span class=\"p\">:</span>\n  <span class=\"nt\">overwrite</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">true</span>               <span class=\"c1\"># global option for all handlers: always overwrite existing files</span>\n  <span class=\"nt\">with_timestamp</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">true</span>          <span class=\"c1\"># include current timestamp in filename</span>\n  <span class=\"nt\">handlers</span><span class=\"p\">:</span>\n    <span class=\"nt\">filesystem</span><span class=\"p\">:</span>\n      <span class=\"nt\">public_root</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">/path/to/a/dir/a/webserver/can/serve/</span>\n      <span class=\"nt\">include_source</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">true</span>\n    <span class=\"nt\">gcloud</span><span class=\"p\">:</span>\n      <span class=\"nt\">bucket</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">my-bucket-name</span>\n      <span class=\"nt\">include_source</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">false</span>\n<span class=\"nn\">...</span>\n<span class=\"nt\">datasets</span><span class=\"p\">:</span>\n  <span class=\"nt\">my_dataset</span><span class=\"p\">:</span>\n    <span class=\"l-Scalar-Plain\">...</span>\n    <span class=\"l-Scalar-Plain\">publish</span><span class=\"p-Indicator\">:</span>\n      <span class=\"nt\">gcloud</span><span class=\"p\">:</span>\n        <span class=\"nt\">bucket</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">another-bucket</span>\n        <span class=\"nt\">include_source</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">true</span>\n        <span class=\"nt\">format</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">json</span>\n        <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l-Scalar-Plain\">something</span>\n</pre>\n<p><strong>TODO</strong>: currently only storing to a filesystem or google cloud storage\nimplemented.</p>\n<p>But features could be: - goolge spreadsheet - ftp - s3 - \u2026</p>\n</div>\n<div id=\"revisions\">\n<h2>Revisions</h2>\n<p>At any time between reading data in and publishing you can store and get\nrevisions of a dataset. This is usually a <tt>pd.DataFrame</tt> in an\nintermediate state, e.g.\u00a0after date enriching but before analysis.</p>\n<p>This feature can be used in an automated processing workflow consisting\nof multiple notebooks to share DataFrames between each other. The\nunderlying storage mechanism is\n<a href=\"https://docs.python.org/3/library/pickle.html\" rel=\"nofollow\">pickle</a> to make sure\na DataFrame revision behaves as expected. This comes with the downside\nthat pickle\u2019s are not safe to share between different systems, but to\nre-create them in another environment, that\u2019s what a reproducible\nworkflow is for, right?</p>\n<p><strong>store a revision</strong></p>\n<pre><span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">my_dataset</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">get_df</span><span class=\"p\">()</span>\n<span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">revisions</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s1\">'tansformed'</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n</pre>\n<p><strong>load a revision</strong></p>\n<pre><span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">my_dataset</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">ds</span><span class=\"p\">[</span><span class=\"s1\">'transformed'</span><span class=\"p\">]</span>\n</pre>\n<p><strong>show available revisions</strong></p>\n<pre><span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">my_dataset</span>\n<span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">revisions</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</pre>\n<p><strong>iterate through revisions</strong></p>\n<pre><span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">my_dataset</span>\n<span class=\"k\">for</span> <span class=\"n\">df</span> <span class=\"ow\">in</span> <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">revisions</span><span class=\"p\">:</span>\n  <span class=\"n\">do_something</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n</pre>\n<p><em>Pro tip</em> you can go crazy and use this mechanism to store &amp; retrieve\n<em>any</em> object that is serializable via\n<a href=\"https://docs.python.org/3/library/pickle.html\" rel=\"nofollow\">pickle</a></p>\n</div>\n<div id=\"cli\">\n<h2>cli</h2>\n<pre>usage: runpandarun <span class=\"o\">[</span>-h<span class=\"o\">]</span> <span class=\"o\">[</span>--loglevel LOGLEVEL<span class=\"o\">]</span> <span class=\"o\">{</span>update,print,publish<span class=\"o\">}</span> ...\n\npositional arguments:\n  <span class=\"o\">{</span>update,print,publish<span class=\"o\">}</span>\n                        commands help: run <span class=\"sb\">`</span>runpandarun &lt;command&gt; -h<span class=\"sb\">`</span>\n\noptional arguments:\n  -h, --help            show this <span class=\"nb\">help</span> message and <span class=\"nb\">exit</span>\n  --loglevel LOGLEVEL\n</pre>\n</div>\n<div id=\"developement\">\n<h2>developement</h2>\n<p>Install testing requirements:</p>\n<pre>make install\n</pre>\n<p>Test:</p>\n<pre>make test\n</pre>\n</div>\n\n          </div>"}, "last_serial": 7126330, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "d253909cea237e20d3c35864cdfe0bbf", "sha256": "4b0ea22dcabb6bd67255d8435aa7905149ef2d4e04f6624133438c21f70849c0"}, "downloads": -1, "filename": "runpandarun-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d253909cea237e20d3c35864cdfe0bbf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23537, "upload_time": "2020-04-08T13:08:56", "upload_time_iso_8601": "2020-04-08T13:08:56.898684Z", "url": "https://files.pythonhosted.org/packages/4c/01/a99647d7daba8533b4f5bda816b0510b3727ba4494c60b8603ea55709ef3/runpandarun-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0fa20a462949c92c0ef05a75de59a2e1", "sha256": "306be875987cc6aea3b2966f9d5e7e0ed289bd35fb7bafda1dfe7527550e2ea7"}, "downloads": -1, "filename": "runpandarun-0.1.tar.gz", "has_sig": false, "md5_digest": "0fa20a462949c92c0ef05a75de59a2e1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24007, "upload_time": "2020-04-08T13:08:58", "upload_time_iso_8601": "2020-04-08T13:08:58.090645Z", "url": "https://files.pythonhosted.org/packages/60/ee/9febb43e43f6f1bd1be9ec5974770d450d650b725e0f5bcb8206f3b8a253/runpandarun-0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "53070febbd9aaf4d87b9dc41c6e43e24", "sha256": "d8dd4e3095f4f9b46284f5919431c77b97efef2ffed3be558dea3ea000ca23c6"}, "downloads": -1, "filename": "runpandarun-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "53070febbd9aaf4d87b9dc41c6e43e24", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23568, "upload_time": "2020-04-08T15:11:15", "upload_time_iso_8601": "2020-04-08T15:11:15.674686Z", "url": "https://files.pythonhosted.org/packages/db/e1/2195795eae23f6936ee9b41fca883ce71af32fd729a9e0aa11668c1e1a7a/runpandarun-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e884009af21259a5a18ad3f2aeb3c4ed", "sha256": "6eaf74944b40d0afa72416314647056348a843e58a478fff47968aecee179ef7"}, "downloads": -1, "filename": "runpandarun-0.1.1.tar.gz", "has_sig": false, "md5_digest": "e884009af21259a5a18ad3f2aeb3c4ed", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24030, "upload_time": "2020-04-08T15:11:16", "upload_time_iso_8601": "2020-04-08T15:11:16.968351Z", "url": "https://files.pythonhosted.org/packages/8b/5f/2cf13bdd8e346d44bf482a3bef482109a57f52ad102a3602e2f3cf30a018/runpandarun-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "324e2ed7faea912680030d1ec5bac1e3", "sha256": "ae3c3020491cb28a7ee47199349ce96c3ddc155859c100bca2e262a0fdf96003"}, "downloads": -1, "filename": "runpandarun-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "324e2ed7faea912680030d1ec5bac1e3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 24778, "upload_time": "2020-04-17T15:26:35", "upload_time_iso_8601": "2020-04-17T15:26:35.727120Z", "url": "https://files.pythonhosted.org/packages/bd/43/6e78e510e8f7eeb745b39a467c96bc3290d88c905dd22b7a5c2814fa8e77/runpandarun-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d72ac9613fbc3de27628d875b5732d30", "sha256": "4d7089f83db8c103408702a07bcdcc59f4c30767e4aaeb3384edde655ffe633e"}, "downloads": -1, "filename": "runpandarun-0.1.2.tar.gz", "has_sig": false, "md5_digest": "d72ac9613fbc3de27628d875b5732d30", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24898, "upload_time": "2020-04-17T15:26:37", "upload_time_iso_8601": "2020-04-17T15:26:37.333635Z", "url": "https://files.pythonhosted.org/packages/d4/8a/68d78c9628ed19ae0e2c099d1d22759874b15f252ae36cc17451d7563cfb/runpandarun-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "f8bf7fad96d91ede600bddbbdb5ed833", "sha256": "1e2b40dd18d8b1f49e70291024d72e7535b5ae7638f7c047d0ec31cf695f8cb3"}, "downloads": -1, "filename": "runpandarun-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "f8bf7fad96d91ede600bddbbdb5ed833", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 25166, "upload_time": "2020-04-29T05:30:08", "upload_time_iso_8601": "2020-04-29T05:30:08.551853Z", "url": "https://files.pythonhosted.org/packages/c4/e3/42fe1403d59c1810d51cac4595c5fc6208650700dae8599c6ba3f8f8c2da/runpandarun-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fab9ea73b82ed24c14a4cadb16eaf852", "sha256": "b1f05eb81ca4e9d085862f7d5b950389b3267c0838f3933cf065e6d315216584"}, "downloads": -1, "filename": "runpandarun-0.1.3.tar.gz", "has_sig": false, "md5_digest": "fab9ea73b82ed24c14a4cadb16eaf852", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25222, "upload_time": "2020-04-29T05:30:09", "upload_time_iso_8601": "2020-04-29T05:30:09.771542Z", "url": "https://files.pythonhosted.org/packages/fe/ed/7de93b327b064fffac2ca9d6f7ec153018521e587399ca16844c1ea251c3/runpandarun-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "b13595e1ac9453d0a1effd7ca4db0023", "sha256": "4ff3fb11a7ee63b9a5081d656d295c125c2ed5e3432c889b26fd1a8d0c99f388"}, "downloads": -1, "filename": "runpandarun-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "b13595e1ac9453d0a1effd7ca4db0023", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 25283, "upload_time": "2020-04-29T06:14:18", "upload_time_iso_8601": "2020-04-29T06:14:18.450510Z", "url": "https://files.pythonhosted.org/packages/07/b4/40983b1b3a401eeee6fcb727426685cc59bf1ff13dd7c14302dfacfcfe21/runpandarun-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "70178d62e5bf1668b34b5c4e65fb47fc", "sha256": "e52f82289e481e0caf1db24299178d7ef97817e6af7c67184d22df2f6c332225"}, "downloads": -1, "filename": "runpandarun-0.1.4.tar.gz", "has_sig": false, "md5_digest": "70178d62e5bf1668b34b5c4e65fb47fc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25314, "upload_time": "2020-04-29T06:14:19", "upload_time_iso_8601": "2020-04-29T06:14:19.843419Z", "url": "https://files.pythonhosted.org/packages/b1/ca/7c1b2294c5ee56c3744ebed1c6933ed88805b1ca5ae46cb5f685a44bd454/runpandarun-0.1.4.tar.gz", "yanked": false}], "0.1rc1": [{"comment_text": "", "digests": {"md5": "ed48a8d3b3f5a26cb5d6ce54826a9132", "sha256": "d2a51461520fc6c1c380557a3825fe8590a00e7512b4f9f29b4a17538483b0d6"}, "downloads": -1, "filename": "runpandarun-0.1rc1-py3-none-any.whl", "has_sig": false, "md5_digest": "ed48a8d3b3f5a26cb5d6ce54826a9132", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18726, "upload_time": "2020-04-08T11:48:30", "upload_time_iso_8601": "2020-04-08T11:48:30.822924Z", "url": "https://files.pythonhosted.org/packages/06/de/6b7dc5af3279fd0a2c229cefab98cfea12bec01dba7e937c5980ba306743/runpandarun-0.1rc1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ab871a9fc189cbfa88e60de685148c76", "sha256": "a4839453756ee0ca3999d936b800e1d1281e9c4ffc9f79b448ee1a382db30bc2"}, "downloads": -1, "filename": "runpandarun-0.1rc1.tar.gz", "has_sig": false, "md5_digest": "ab871a9fc189cbfa88e60de685148c76", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20857, "upload_time": "2020-04-08T11:48:32", "upload_time_iso_8601": "2020-04-08T11:48:32.892223Z", "url": "https://files.pythonhosted.org/packages/09/7e/2eceb4a68563e58b5525b51d73e855e240691e55eaba2b83558039c139dd/runpandarun-0.1rc1.tar.gz", "yanked": false}], "0.1rc3": [{"comment_text": "", "digests": {"md5": "0e87aad4f18cf7cf670385a1be303273", "sha256": "25631f7121b0dd5d35b649e687d7ddd3c763c48d88665bedd2593c084184bdc7"}, "downloads": -1, "filename": "runpandarun-0.1rc3-py3-none-any.whl", "has_sig": false, "md5_digest": "0e87aad4f18cf7cf670385a1be303273", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18727, "upload_time": "2020-04-08T12:24:58", "upload_time_iso_8601": "2020-04-08T12:24:58.728844Z", "url": "https://files.pythonhosted.org/packages/7f/d1/27e38c25675f29a465a7bed1aed196b02627a4f18fc444067f0420fb6b90/runpandarun-0.1rc3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "690e2ba1e0abd984b24f2c7447d07e94", "sha256": "c85da8abe3dfb5cf81ec7cbea665b0db7d7a187d81304ad9a29f628afae3c80c"}, "downloads": -1, "filename": "runpandarun-0.1rc3.tar.gz", "has_sig": false, "md5_digest": "690e2ba1e0abd984b24f2c7447d07e94", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20851, "upload_time": "2020-04-08T12:24:59", "upload_time_iso_8601": "2020-04-08T12:24:59.912869Z", "url": "https://files.pythonhosted.org/packages/a8/fd/5efd4e39bd6d2780e2140a6231d08dce7048fc9c49c8cdab1e793485522f/runpandarun-0.1rc3.tar.gz", "yanked": false}], "0.1rc4": [{"comment_text": "", "digests": {"md5": "ed44d759908919e58bb534de1d516500", "sha256": "9d56ad59dcd7914070e851f7bdb4c3d4586017f446bb9852bc9eeb4bc0f071c4"}, "downloads": -1, "filename": "runpandarun-0.1rc4-py3-none-any.whl", "has_sig": false, "md5_digest": "ed44d759908919e58bb534de1d516500", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23583, "upload_time": "2020-04-08T13:04:09", "upload_time_iso_8601": "2020-04-08T13:04:09.809835Z", "url": "https://files.pythonhosted.org/packages/b5/5a/509399713d327d6ff4a36786895af1c36bd480734e3eba7fb4170abae2c9/runpandarun-0.1rc4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a84d6f97cd162242ea466418845051d8", "sha256": "7cd783c1219b293be221a4217c0cc6011268f3b2e9c2753edbfa87eafb082af4"}, "downloads": -1, "filename": "runpandarun-0.1rc4.tar.gz", "has_sig": false, "md5_digest": "a84d6f97cd162242ea466418845051d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24034, "upload_time": "2020-04-08T13:04:11", "upload_time_iso_8601": "2020-04-08T13:04:11.550023Z", "url": "https://files.pythonhosted.org/packages/f5/9e/4199d1cca22c9774a4aff40b3921e64e4da7977e3a7ed411afdd62f83232/runpandarun-0.1rc4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b13595e1ac9453d0a1effd7ca4db0023", "sha256": "4ff3fb11a7ee63b9a5081d656d295c125c2ed5e3432c889b26fd1a8d0c99f388"}, "downloads": -1, "filename": "runpandarun-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "b13595e1ac9453d0a1effd7ca4db0023", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 25283, "upload_time": "2020-04-29T06:14:18", "upload_time_iso_8601": "2020-04-29T06:14:18.450510Z", "url": "https://files.pythonhosted.org/packages/07/b4/40983b1b3a401eeee6fcb727426685cc59bf1ff13dd7c14302dfacfcfe21/runpandarun-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "70178d62e5bf1668b34b5c4e65fb47fc", "sha256": "e52f82289e481e0caf1db24299178d7ef97817e6af7c67184d22df2f6c332225"}, "downloads": -1, "filename": "runpandarun-0.1.4.tar.gz", "has_sig": false, "md5_digest": "70178d62e5bf1668b34b5c4e65fb47fc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25314, "upload_time": "2020-04-29T06:14:19", "upload_time_iso_8601": "2020-04-29T06:14:19.843419Z", "url": "https://files.pythonhosted.org/packages/b1/ca/7c1b2294c5ee56c3744ebed1c6933ed88805b1ca5ae46cb5f685a44bd454/runpandarun-0.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:59:58 2020"}