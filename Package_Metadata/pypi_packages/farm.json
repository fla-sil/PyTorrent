{"info": {"author": "Malte Pietsch, Timo Moeller, Branden Chan, Tanay Soni, Huggingface Team Authors, Google AI Language Team Authors, Open AI team Authors", "author_email": "malte.pietsch@deepset.ai", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": ".. image:: https://github.com/deepset-ai/FARM/blob/master/docs/img/farm_logo_text_right_wide.png?raw=true\n    :width: 269\n    :height: 109\n    :align: left\n    :alt: FARM LOGO\n\n\n(**F**\\ ramework for **A**\\ dapting **R**\\ epresentation **M**\\ odels)\n\n.. image:: https://dev.azure.com/deepset/FARM/_apis/build/status/deepset-ai.FARM?branchName=master\n\t:target: https://dev.azure.com/deepset/FARM/_build\n\t:alt: Build\n\n.. image:: https://img.shields.io/github/release/deepset-ai/farm\n\t:target: https://github.com/deepset-ai/FARM/releases\n\t:alt: Release\n\n.. image:: https://img.shields.io/github/license/deepset-ai/farm\n\t:target: https://github.com/deepset-ai/FARM/blob/master/LICENSE\n\t:alt: License\n\n.. image:: https://img.shields.io/github/last-commit/deepset-ai/farm\n\t:target: https://github.com/deepset-ai/FARM/commits/master\n\t:alt: Last Commit\n\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square\n\t:target: https://github.com/ambv/black\n\t:alt: Last Commit\n\n.. image:: https://pepy.tech/badge/farm\n\t:target: https://pepy.tech/project/farm\n\t:alt: Downloads\n\nWhat is it?\n############\nFARM makes **Transfer Learning** with BERT & Co **simple, fast and enterprise-ready**.\nIt's built upon `transformers <https://github.com/huggingface/pytorch-transformers>`_ and provides additional features to simplify the life of developers:\nParallelized preprocessing, highly modular design, multi-task learning, experiment tracking, easy debugging and close integration with AWS SageMaker.\n\nWith FARM you can build **fast proof-of-concepts** for tasks like text classification, NER or question answering and **transfer them easily into production**.\n\n\n- `What is it? <https://github.com/deepset-ai/FARM#what-is-it>`_\n- `Core Features <https://github.com/deepset-ai/FARM#core-features>`_\n- `Resources <https://github.com/deepset-ai/FARM#resources>`_\n- `Installation <https://github.com/deepset-ai/FARM#installation>`_\n- `Basic Usage <https://github.com/deepset-ai/FARM#basic-usage>`_\n- `Advanced Usage <https://github.com/deepset-ai/FARM#advanced-usage>`_\n- `Core Concepts <https://github.com/deepset-ai/FARM#core-concepts>`_\n- `FAQ <https://github.com/deepset-ai/FARM#faq>`_\n- `Upcoming features <https://github.com/deepset-ai/FARM#upcoming-features>`_\n\n\nCore features\n##############\n- **Easy fine-tuning of language models** to your task and domain language\n- **Speed**: AMP optimizers (~35% faster) and parallel preprocessing (16 CPU cores => ~16x faster)\n- **Modular design** of language models and prediction heads\n- Switch between heads or combine them for **multitask learning**\n- **Full Compatibility** with HuggingFace Transformers' models and model hub\n- **Smooth upgrading** to newer language models\n- Integration of **custom datasets** via Processor class\n- Powerful **experiment tracking** & execution\n- **Checkpointing & Caching** to resume training and reduce costs with spot instances\n- Simple **deployment** and **visualization** to showcase your model\n\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| Task                         |      BERT         |  RoBERTa          |  XLNet            |  ALBERT           |  DistilBERT       |  XLMRoBERTa       |\n+==============================+===================+===================+===================+===================+===================+===================+\n| Text classification          | x                 |  x                |  x                |  x                |  x                |  x                |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| NER                          | x                 |  x                |  x                |  x                |  x                |  x                |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| Question Answering           | x                 |  x                |  x                |  x                |  x                |  x                |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| Language Model Fine-tuning   | x                 |                   |                   |                   |                   |                   |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| Text Regression              | x                 |  x                |  x                |  x                |  x                |  x                |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| Multilabel Text classif.     | x                 |  x                |  x                |  x                |  x                |  x                |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| Extracting embeddings        | x                 |  x                |  x                |  x                |  x                |  x                |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| LM from scratch (beta)       | x                 |                   |                   |                   |                   |                   |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| Text Pair Classification     | x                 |  x                |  x                |  x                |  x                |  x                |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n| Passage Ranking              | x                 |  x                |  x                |  x                |  x                |  x                |\n+------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n\n****NEW**** Interested in doing Question Answering at scale? Checkout `Haystack <https://github.com/deepset-ai/haystack>`_!\n\nResources\n##########\n**Docs**\n\n`Online documentation <https://farm.deepset.ai>`_\n\n**Tutorials**\n\n- Tutorial 1 (Overview of building blocks): `Jupyter notebook 1 <https://github.com/deepset-ai/FARM/blob/master/tutorials/1_farm_building_blocks.ipynb>`_  or `Colab 1 <https://colab.research.google.com/drive/130_7dgVC3VdLBPhiEkGULHmqSlflhmVM>`_\n- Tutorial 2 (How to use custom datasets): `Jupyter notebook 2 <https://github.com/deepset-ai/FARM/blob/master/tutorials/2_Build_a_processor_for_your_own_dataset.ipynb>`_  or `Colab 2 <https://colab.research.google.com/drive/1Ce_wWu-fsy_g16jaGioe8M5mAFdLN1Yx>`_\n- Tutorial 3 (How to train and showcase your own QA model): `Colab 3 <https://colab.research.google.com/drive/1tqOJyMw3L5I3eXHLO846eq1fA7O9U2s8>`_\n- Example scripts for each task: `FARM/examples/ <https://github.com/deepset-ai/FARM/tree/master/examples>`_\n\n**Demo**\n\nCheckout https://demos.deepset.ai to play around with some models\n\n**More**\n\n- `Intro to Transfer Learning (Blog) <https://medium.com/voice-tech-podcast/https-medium-com-deepset-ai-transfer-learning-entering-a-new-era-in-nlp-db523d9e667b>`_\n- `Intro to Transfer Learning & FARM (Video) <https://www.youtube.com/watch?v=hoDgtvE-u9E&feature=youtu.be>`_\n- `Question Answering Systems Explained (Blog)  <https://medium.com/deepset-ai/modern-question-answering-systems-explained-4d0913744097>`_\n- `GermanBERT (Blog)  <https://deepset.ai/german-bert>`_\n- `XLM-Roberta: The alternative for non-english NLP (Blog)  <https://towardsdatascience.com/xlm-roberta-the-multilingual-alternative-for-non-english-nlp-cf0b889ccbbf>`_\n\nInstallation\n#############\nRecommended (because of active development)::\n\n    git clone https://github.com/deepset-ai/FARM.git\n    cd FARM\n    pip install -r requirements.txt\n    pip install --editable .\n\nIf problems occur, please do a git pull. The --editable flag will update changes immediately.\n\nFrom PyPi::\n\n    pip install farm\n\nBasic Usage\n############\n1. Train a downstream model\n****************************\nFARM offers two modes for model training:\n\n**Option 1: Run experiment(s) from config**\n\n.. image:: https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/code_snippet_experiment.png\n\n*Use cases:* Training your first model, hyperparameter optimization, evaluating a language model on multiple down-stream tasks.\n\n**Option 2: Stick together your own building blocks**\n\n.. image:: https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/code_snippet_building_blocks.png\n\n*Usecases:* Custom datasets, language models, prediction heads ...\n\nMetrics and parameters of your model training get automatically logged via MLflow. We provide a `public MLflow server <https://public-mlflow.deepset.ai/>`_ for testing and learning purposes. Check it out to see your own experiment results! Just be aware: We will start deleting all experiments on a regular schedule to ensure decent server performance for everybody!\n\n2. Run Inference\n*******************************\nUse a `public model  <https://huggingface.co/models>`__  or your own to get predictions:\n\n.. image:: https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/code_snippet_inference.png\n\n\n3. Showcase your models (API + UI)\n**********************************\n\n* Run :code:`docker-compose up`\n* Open http://localhost:3000 in your browser\n\n.. image:: https://github.com/deepset-ai/FARM/blob/master/docs/img/inference-api-screen.png?raw=true\n    :alt: FARM Inferennce UI\n\nOne docker container exposes a REST API (localhost:5000) and another one runs a simple demo UI (localhost:3000).\nYou can use both of them individually and mount your own models. Check out the docs for details.\n\nAdvanced Usage\n##############\nOnce you got started with FARM, there's plenty of options to customize your pipeline and boost your models.\nLet's highlight a few of them ...\n\n1. Optimizers & Learning rate schedules\n****************************************\nWhile FARM provides decent defaults for both, you can easily configure many other optimizers & LR schedules:\n\n- any optimizer from PyTorch, Apex or Transformers\n- any learning rate schedule from PyTorch or Transformers\n\nYou can configure them by passing a dict to :code:`initialize_optimizer()` (see `example <https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_custom_optimizer.py>`__).\n\n\n2. Early Stopping\n******************\nWith early stopping, the run stops once a chosen metric is not improving any further and you take the best model up to this point.\nThis helps prevent overfitting on small datasets and reduces training time if your model doesn't improve any further (see `example <https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_with_earlystopping.py>`__).\n\n3. Imbalanced classes\n*********************\nIf you do classification on imbalanced classes, consider using class weights. They change the loss function to down-weight frequent classes.\nYou can set them when you init a prediction head::\n\n    prediction_head = TextClassificationHead(\n    class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"),\n    num_labels=len(label_list))`\n\n\n4. Cross Validation\n*******************\nGet more reliable eval metrics on small datasets (see `example <https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_crossvalidation.py>`__)\n\n\n5. Caching & Checkpointing\n***************************\nSave time if you run similar pipelines (e.g. only experimenting with model params): Store your preprocessed dataset & load it next time from cache::\n\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, caching=True)\n\nStart & stop training by saving checkpoints of the trainer::\n\n    trainer = Trainer.create_or_load_checkpoint(\n                ...\n                checkpoint_on_sigterm=True,\n                checkpoint_every=200,\n                checkpoint_root_dir=Path(\u201c/opt/ml/checkpoints/training\u201d),\n                resume_from_checkpoint=\u201clatest\u201d)\n\nThe checkpoints include the state of everything that matters (model, optimizer, lr_schedule ...) to resume training.\nThis is particularly useful, if your training crashes (e.g. because your are using spot cloud instances).\nYou can either save checkpoints every X steps or when a SIGTERM signal is received.\n\n6. Training on AWS SageMaker (incl. Spot Instances)\n***************************************************\nWe are currently working a lot on simplifying large scale training and deployment. As a first step, we are adding support for training on AWS SageMaker. The interesting part\nhere is the option to use Managed Spot Instances and save about 70% on costs compared to the regular EC2 instances. This is particularly relevant for training models from scratch, which we\nintroduce in a basic version in this release and will improve over the next weeks.\nSee this `tutorial <https://github.com/deepset-ai/FARM/blob/master/tutorials/sagemaker/3_train_with_sagemaker.ipynb>`__ to get started with using SageMaker for training on down-stream tasks.\n\nCore concepts\n#########################\nModel\n************\nAdaptiveModel = Language Model + Prediction Head(s)\nWith this modular approach you can easily add prediction heads (multitask learning) and re-use them for different types of language models.\n`(Learn more) <https://farm.deepset.ai/modeling.html>`__\n\n\n.. image:: https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/adaptive_model_no_bg_small.jpg\n\n\nData Processing\n********************\nCustom Datasets can be loaded by customizing the Processor. It converts \"raw data\" into PyTorch Datasets.\nMuch of the heavy lifting is then handled behind the scenes to make it fast & simple to debug.\n`(Learn more) <https://farm.deepset.ai/data_handling.html>`__\n\n.. image:: https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/data_silo_no_bg_small.jpg\n\nFAQ\n####\n**1. What language model shall I use for non-english NLP?**\nIf you\u2019re working with German, French, Chinese, Japanese or Finnish you might be interested in trying out the pretrained BERT models in your language. You can see a list `here <https://huggingface.co/models>`__ of the available models hosted by our friends over at HuggingFace which can be directly accessed through FARM. If your language isn\u2019t one of those (or even if it is), we\u2019d encourage you to try out XLM-Roberta (https://arxiv.org/pdf/1911.02116.pdf)\nwhich supports 100 different languages and shows surprisingly strong performance compared to single language models.\n\n**2. Why do you have separate prediction heads?**\nPredictionHeads are needed in order to adapt the general language understanding capabilities of the language model to a specific task.\nFor example, the predictions of NER and document classification require very different output formats.\nHaving separate PredictionHead classes means that it is a) very easy to re-use prediction heads on top of different language models\nand b) it simplifies multitask-learning. The latter allows you e.g. to add proxy tasks that facilitate learning of your \"true objective\".\nExample: You want to classify documents into classes and know that some document tags (e.g. author) already provide helpful information for this task. It might help to add additional tasks for classifying these meta tags.\n\n**3. When is adaptation of a language model to a domain corpus useful?**\nMostly when your domain language differs a lot to the one that the original model was trained on.\nExample: Your corpus is from the aerospace industry and contains a lot of engineering terminology.\nThis is very different to Wikipedia text on in terms of vocab and semantics.\nWe found that this can boost performance especially if your down-stream tasks are using rather small domain datasets.\nIn contrast, if you have huge downstream datasets, the model can often adapt to the domain \"on-the-fly\" during downstream training.\n\n**4. How can I adapt a language model to a domain corpus?**\nThere are two main methods: you can extend the vocabulary by :code:`Tokenizer.add_tokens([\"term_a\", \"term_b\"...])` or fine-tune your model on a domain text corpus (see `example <https://github.com/deepset-ai/FARM/blob/master/examples/lm_finetuning.py>`__).\n\n**5. How can I convert from / to HuggingFace's models?**\nWe support conversion in both directions (see `example <https://github.com/deepset-ai/FARM/blob/master/examples/conversion_huggingface_models.py>`__)\nYou can also load any language model from HuggingFace's model hub by just specifying the name, e.g. :code:`LanguageModel.load(\"deepset/bert-base-cased-squad2\")`\n\n**6. How can you scale Question Answering to larger collections of documents?**\nIt's currently most common to put a fast \"retriever\" in front of the QA model.\nCheckout `haystack <https://github.com/deepset-ai/haystack/>`__ for such an implementation and more features you need to really run QA in production.\n\n**7. How can you tailor Question Answering to your own domain?**\nWe attained high performance by training a model first on public datasets (e.g. SQuAD, Natural Questions ...) and then fine-tuning it on a few custom QA labels from the domain.\nEven ~2000 domain labels can give you the essential performance boost you need.\nCheckout `haystack <https://github.com/deepset-ai/haystack/>`__ for more details and a QA labeling tool.\n\n**8. My GPU runs out of memory. How can I train with decent batch sizes?**\nUse gradient accumulation! It combines multiple batches before applying backprop. In FARM, just set the param :code:`grad_acc_steps` in :code:`initialize_optimizer()` and :code:`Trainer()` to the number of batches you want to combine (i.e. :code:`grad_acc_steps=2` and :code:`batch_size=16` results in an effective batch size of 32).\n\nUpcoming features\n###################\n- Full AWS SageMaker support (incl. Spot instances)\n- Support for more Question Answering styles and datasets\n- Additional visualizations and statistics to explore and debug your model\n- Enabling large scale deployment for production\n- Simpler benchmark models (fasttext, word2vec ...)\n\n\nAcknowledgements\n###################\n- FARM is built upon parts of the great `Transformers <https://github.com/huggingface/pytorch-transformers>`_  repository from HuggingFace. It utilizes their implementations of models and tokenizers.\n- FARM is a community effort! Essential pieces of it have been implemented by our FARMers out there. Thanks to all contributors!\n- The original BERT model and `paper <https://arxiv.org/abs/1810.04805>`_  was published by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n\nCitation\n###################\nAs of now there is no published paper on FARM. If you want to use or cite our framework, please include\nthe link to this repository. If you are working with the German Bert model, you can link our\n`blog post <https://deepset.ai/german-bert>`_ describing its training details and performance.", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "https://github.com/deepset-ai/FARM/archive/0.4.3.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://gitlab.com/deepset-ai/ml/lm/farm", "keywords": "BERT NLP deep learning language-model transformer qa question-answering transfer-learning", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "farm", "package_url": "https://pypi.org/project/farm/", "platform": "", "project_url": "https://pypi.org/project/farm/", "project_urls": {"Download": "https://github.com/deepset-ai/FARM/archive/0.4.3.tar.gz", "Homepage": "https://gitlab.com/deepset-ai/ml/lm/farm"}, "release_url": "https://pypi.org/project/farm/0.4.3/", "requires_dist": null, "requires_python": ">=3.5.0", "summary": "Toolkit for finetuning and evaluating transformer based language models", "version": "0.4.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <img alt=\"FARM LOGO\" class=\"align-left\" height=\"109\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6a4adf7128839c396c7fcf2a3b5d425448b242a3/68747470733a2f2f6769746875622e636f6d2f646565707365742d61692f4641524d2f626c6f622f6d61737465722f646f63732f696d672f6661726d5f6c6f676f5f746578745f72696768745f776964652e706e673f7261773d74727565\" width=\"269\">\n<p>(<strong>F</strong>ramework for <strong>A</strong>dapting <strong>R</strong>epresentation <strong>M</strong>odels)</p>\n<a href=\"https://dev.azure.com/deepset/FARM/_build\" rel=\"nofollow\"><img alt=\"Build\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bbb6834a331cce51a730cc4c6f56c3862830560e/68747470733a2f2f6465762e617a7572652e636f6d2f646565707365742f4641524d2f5f617069732f6275696c642f7374617475732f646565707365742d61692e4641524d3f6272616e63684e616d653d6d6173746572\"></a>\n<a href=\"https://github.com/deepset-ai/FARM/releases\" rel=\"nofollow\"><img alt=\"Release\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7d02c89a7285bcb86cea6c1f51e0351c4af4a8ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f646565707365742d61692f6661726d\"></a>\n<a href=\"https://github.com/deepset-ai/FARM/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9173a91d935f30cd35a75d0ab4c53b58343f4f07/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f646565707365742d61692f6661726d\"></a>\n<a href=\"https://github.com/deepset-ai/FARM/commits/master\" rel=\"nofollow\"><img alt=\"Last Commit\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/57a4f9de514fba09e1a74c4405196b6134a938cb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f646565707365742d61692f6661726d\"></a>\n<a href=\"https://github.com/ambv/black\" rel=\"nofollow\"><img alt=\"Last Commit\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1c326c58e924b9f3508f32a8ac6b3ee91f40b090/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e7376673f7374796c653d666c61742d737175617265\"></a>\n<a href=\"https://pepy.tech/project/farm\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e96e3097d579543649c581d7d120f66b576920a7/68747470733a2f2f706570792e746563682f62616467652f6661726d\"></a>\n<div id=\"what-is-it\">\n<h2>What is it?</h2>\n<p>FARM makes <strong>Transfer Learning</strong> with BERT &amp; Co <strong>simple, fast and enterprise-ready</strong>.\nIt\u2019s built upon <a href=\"https://github.com/huggingface/pytorch-transformers\" rel=\"nofollow\">transformers</a> and provides additional features to simplify the life of developers:\nParallelized preprocessing, highly modular design, multi-task learning, experiment tracking, easy debugging and close integration with AWS SageMaker.</p>\n<p>With FARM you can build <strong>fast proof-of-concepts</strong> for tasks like text classification, NER or question answering and <strong>transfer them easily into production</strong>.</p>\n<ul>\n<li><a href=\"https://github.com/deepset-ai/FARM#what-is-it\" rel=\"nofollow\">What is it?</a></li>\n<li><a href=\"https://github.com/deepset-ai/FARM#core-features\" rel=\"nofollow\">Core Features</a></li>\n<li><a href=\"https://github.com/deepset-ai/FARM#resources\" rel=\"nofollow\">Resources</a></li>\n<li><a href=\"https://github.com/deepset-ai/FARM#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"https://github.com/deepset-ai/FARM#basic-usage\" rel=\"nofollow\">Basic Usage</a></li>\n<li><a href=\"https://github.com/deepset-ai/FARM#advanced-usage\" rel=\"nofollow\">Advanced Usage</a></li>\n<li><a href=\"https://github.com/deepset-ai/FARM#core-concepts\" rel=\"nofollow\">Core Concepts</a></li>\n<li><a href=\"https://github.com/deepset-ai/FARM#faq\" rel=\"nofollow\">FAQ</a></li>\n<li><a href=\"https://github.com/deepset-ai/FARM#upcoming-features\" rel=\"nofollow\">Upcoming features</a></li>\n</ul>\n</div>\n<div id=\"id2\">\n<h2>Core features</h2>\n<ul>\n<li><strong>Easy fine-tuning of language models</strong> to your task and domain language</li>\n<li><strong>Speed</strong>: AMP optimizers (~35% faster) and parallel preprocessing (16 CPU cores =&gt; ~16x faster)</li>\n<li><strong>Modular design</strong> of language models and prediction heads</li>\n<li>Switch between heads or combine them for <strong>multitask learning</strong></li>\n<li><strong>Full Compatibility</strong> with HuggingFace Transformers\u2019 models and model hub</li>\n<li><strong>Smooth upgrading</strong> to newer language models</li>\n<li>Integration of <strong>custom datasets</strong> via Processor class</li>\n<li>Powerful <strong>experiment tracking</strong> &amp; execution</li>\n<li><strong>Checkpointing &amp; Caching</strong> to resume training and reduce costs with spot instances</li>\n<li>Simple <strong>deployment</strong> and <strong>visualization</strong> to showcase your model</li>\n</ul>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n<col>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>Task</th>\n<th>BERT</th>\n<th>RoBERTa</th>\n<th>XLNet</th>\n<th>ALBERT</th>\n<th>DistilBERT</th>\n<th>XLMRoBERTa</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>Text classification</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n</tr>\n<tr><td>NER</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n</tr>\n<tr><td>Question Answering</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n</tr>\n<tr><td>Language Model Fine-tuning</td>\n<td>x</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n</tr>\n<tr><td>Text Regression</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n</tr>\n<tr><td>Multilabel Text classif.</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n</tr>\n<tr><td>Extracting embeddings</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n</tr>\n<tr><td>LM from scratch (beta)</td>\n<td>x</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n</tr>\n<tr><td>Text Pair Classification</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n</tr>\n<tr><td>Passage Ranking</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n<td>x</td>\n</tr>\n</tbody>\n</table>\n<p><strong>**NEW**</strong> Interested in doing Question Answering at scale? Checkout <a href=\"https://github.com/deepset-ai/haystack\" rel=\"nofollow\">Haystack</a>!</p>\n</div>\n<div id=\"id3\">\n<h2>Resources</h2>\n<p><strong>Docs</strong></p>\n<p><a href=\"https://farm.deepset.ai\" rel=\"nofollow\">Online documentation</a></p>\n<p><strong>Tutorials</strong></p>\n<ul>\n<li>Tutorial 1 (Overview of building blocks): <a href=\"https://github.com/deepset-ai/FARM/blob/master/tutorials/1_farm_building_blocks.ipynb\" rel=\"nofollow\">Jupyter notebook 1</a>  or <a href=\"https://colab.research.google.com/drive/130_7dgVC3VdLBPhiEkGULHmqSlflhmVM\" rel=\"nofollow\">Colab 1</a></li>\n<li>Tutorial 2 (How to use custom datasets): <a href=\"https://github.com/deepset-ai/FARM/blob/master/tutorials/2_Build_a_processor_for_your_own_dataset.ipynb\" rel=\"nofollow\">Jupyter notebook 2</a>  or <a href=\"https://colab.research.google.com/drive/1Ce_wWu-fsy_g16jaGioe8M5mAFdLN1Yx\" rel=\"nofollow\">Colab 2</a></li>\n<li>Tutorial 3 (How to train and showcase your own QA model): <a href=\"https://colab.research.google.com/drive/1tqOJyMw3L5I3eXHLO846eq1fA7O9U2s8\" rel=\"nofollow\">Colab 3</a></li>\n<li>Example scripts for each task: <a href=\"https://github.com/deepset-ai/FARM/tree/master/examples\" rel=\"nofollow\">FARM/examples/</a></li>\n</ul>\n<p><strong>Demo</strong></p>\n<p>Checkout <a href=\"https://demos.deepset.ai\" rel=\"nofollow\">https://demos.deepset.ai</a> to play around with some models</p>\n<p><strong>More</strong></p>\n<ul>\n<li><a href=\"https://medium.com/voice-tech-podcast/https-medium-com-deepset-ai-transfer-learning-entering-a-new-era-in-nlp-db523d9e667b\" rel=\"nofollow\">Intro to Transfer Learning (Blog)</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=hoDgtvE-u9E&amp;feature=youtu.be\" rel=\"nofollow\">Intro to Transfer Learning &amp; FARM (Video)</a></li>\n<li><a href=\"https://medium.com/deepset-ai/modern-question-answering-systems-explained-4d0913744097\" rel=\"nofollow\">Question Answering Systems Explained (Blog)</a></li>\n<li><a href=\"https://deepset.ai/german-bert\" rel=\"nofollow\">GermanBERT (Blog)</a></li>\n<li><a href=\"https://towardsdatascience.com/xlm-roberta-the-multilingual-alternative-for-non-english-nlp-cf0b889ccbbf\" rel=\"nofollow\">XLM-Roberta: The alternative for non-english NLP (Blog)</a></li>\n</ul>\n</div>\n<div id=\"id4\">\n<h2>Installation</h2>\n<p>Recommended (because of active development):</p>\n<pre>git clone https://github.com/deepset-ai/FARM.git\ncd FARM\npip install -r requirements.txt\npip install --editable .\n</pre>\n<p>If problems occur, please do a git pull. The \u2013editable flag will update changes immediately.</p>\n<p>From PyPi:</p>\n<pre>pip install farm\n</pre>\n</div>\n<div id=\"id5\">\n<h2>Basic Usage</h2>\n<div id=\"train-a-downstream-model\">\n<h3>1. Train a downstream model</h3>\n<p>FARM offers two modes for model training:</p>\n<p><strong>Option 1: Run experiment(s) from config</strong></p>\n<img alt=\"https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/code_snippet_experiment.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e78352dc854611e8ccdd23855d945cedabf680af/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f646565707365742d61692f4641524d2f6d61737465722f646f63732f696d672f636f64655f736e69707065745f6578706572696d656e742e706e67\">\n<p><em>Use cases:</em> Training your first model, hyperparameter optimization, evaluating a language model on multiple down-stream tasks.</p>\n<p><strong>Option 2: Stick together your own building blocks</strong></p>\n<img alt=\"https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/code_snippet_building_blocks.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1b863222d5864ba20a541ad99967184884ccda9a/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f646565707365742d61692f4641524d2f6d61737465722f646f63732f696d672f636f64655f736e69707065745f6275696c64696e675f626c6f636b732e706e67\">\n<p><em>Usecases:</em> Custom datasets, language models, prediction heads \u2026</p>\n<p>Metrics and parameters of your model training get automatically logged via MLflow. We provide a <a href=\"https://public-mlflow.deepset.ai/\" rel=\"nofollow\">public MLflow server</a> for testing and learning purposes. Check it out to see your own experiment results! Just be aware: We will start deleting all experiments on a regular schedule to ensure decent server performance for everybody!</p>\n</div>\n<div id=\"run-inference\">\n<h3>2. Run Inference</h3>\n<p>Use a <a href=\"https://huggingface.co/models\" rel=\"nofollow\">public model</a>  or your own to get predictions:</p>\n<img alt=\"https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/code_snippet_inference.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8eb913c3d5016516048f8b3a6fbfb217dc9994d9/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f646565707365742d61692f4641524d2f6d61737465722f646f63732f696d672f636f64655f736e69707065745f696e666572656e63652e706e67\">\n</div>\n<div id=\"showcase-your-models-api-ui\">\n<h3>3. Showcase your models (API + UI)</h3>\n<ul>\n<li>Run <code>docker-compose up</code></li>\n<li>Open <a href=\"http://localhost:3000\" rel=\"nofollow\">http://localhost:3000</a> in your browser</li>\n</ul>\n<img alt=\"FARM Inferennce UI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/939fafba4fd64ffe2ac9a6b45b0b1274aa57ec33/68747470733a2f2f6769746875622e636f6d2f646565707365742d61692f4641524d2f626c6f622f6d61737465722f646f63732f696d672f696e666572656e63652d6170692d73637265656e2e706e673f7261773d74727565\">\n<p>One docker container exposes a REST API (localhost:5000) and another one runs a simple demo UI (localhost:3000).\nYou can use both of them individually and mount your own models. Check out the docs for details.</p>\n</div>\n</div>\n<div id=\"id6\">\n<h2>Advanced Usage</h2>\n<p>Once you got started with FARM, there\u2019s plenty of options to customize your pipeline and boost your models.\nLet\u2019s highlight a few of them \u2026</p>\n<div id=\"optimizers-learning-rate-schedules\">\n<h3>1. Optimizers &amp; Learning rate schedules</h3>\n<p>While FARM provides decent defaults for both, you can easily configure many other optimizers &amp; LR schedules:</p>\n<ul>\n<li>any optimizer from PyTorch, Apex or Transformers</li>\n<li>any learning rate schedule from PyTorch or Transformers</li>\n</ul>\n<p>You can configure them by passing a dict to <code>initialize_optimizer()</code> (see <a href=\"https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_custom_optimizer.py\" rel=\"nofollow\">example</a>).</p>\n</div>\n<div id=\"early-stopping\">\n<h3>2. Early Stopping</h3>\n<p>With early stopping, the run stops once a chosen metric is not improving any further and you take the best model up to this point.\nThis helps prevent overfitting on small datasets and reduces training time if your model doesn\u2019t improve any further (see <a href=\"https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_with_earlystopping.py\" rel=\"nofollow\">example</a>).</p>\n</div>\n<div id=\"imbalanced-classes\">\n<h3>3. Imbalanced classes</h3>\n<p>If you do classification on imbalanced classes, consider using class weights. They change the loss function to down-weight frequent classes.\nYou can set them when you init a prediction head:</p>\n<pre>prediction_head = TextClassificationHead(\nclass_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"),\nnum_labels=len(label_list))`\n</pre>\n</div>\n<div id=\"cross-validation\">\n<h3>4. Cross Validation</h3>\n<p>Get more reliable eval metrics on small datasets (see <a href=\"https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_crossvalidation.py\" rel=\"nofollow\">example</a>)</p>\n</div>\n<div id=\"caching-checkpointing\">\n<h3>5. Caching &amp; Checkpointing</h3>\n<p>Save time if you run similar pipelines (e.g. only experimenting with model params): Store your preprocessed dataset &amp; load it next time from cache:</p>\n<pre>data_silo = DataSilo(processor=processor, batch_size=batch_size, caching=True)\n</pre>\n<p>Start &amp; stop training by saving checkpoints of the trainer:</p>\n<pre>trainer = Trainer.create_or_load_checkpoint(\n            ...\n            checkpoint_on_sigterm=True,\n            checkpoint_every=200,\n            checkpoint_root_dir=Path(\u201c/opt/ml/checkpoints/training\u201d),\n            resume_from_checkpoint=\u201clatest\u201d)\n</pre>\n<p>The checkpoints include the state of everything that matters (model, optimizer, lr_schedule \u2026) to resume training.\nThis is particularly useful, if your training crashes (e.g. because your are using spot cloud instances).\nYou can either save checkpoints every X steps or when a SIGTERM signal is received.</p>\n</div>\n<div id=\"training-on-aws-sagemaker-incl-spot-instances\">\n<h3>6. Training on AWS SageMaker (incl. Spot Instances)</h3>\n<p>We are currently working a lot on simplifying large scale training and deployment. As a first step, we are adding support for training on AWS SageMaker. The interesting part\nhere is the option to use Managed Spot Instances and save about 70% on costs compared to the regular EC2 instances. This is particularly relevant for training models from scratch, which we\nintroduce in a basic version in this release and will improve over the next weeks.\nSee this <a href=\"https://github.com/deepset-ai/FARM/blob/master/tutorials/sagemaker/3_train_with_sagemaker.ipynb\" rel=\"nofollow\">tutorial</a> to get started with using SageMaker for training on down-stream tasks.</p>\n</div>\n</div>\n<div id=\"id7\">\n<h2>Core concepts</h2>\n<div id=\"model\">\n<h3>Model</h3>\n<p>AdaptiveModel = Language Model + Prediction Head(s)\nWith this modular approach you can easily add prediction heads (multitask learning) and re-use them for different types of language models.\n<a href=\"https://farm.deepset.ai/modeling.html\" rel=\"nofollow\">(Learn more)</a></p>\n<img alt=\"https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/adaptive_model_no_bg_small.jpg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0e338602af6eee6aac61316e8755a346f8d699cd/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f646565707365742d61692f4641524d2f6d61737465722f646f63732f696d672f61646170746976655f6d6f64656c5f6e6f5f62675f736d616c6c2e6a7067\">\n</div>\n<div id=\"data-processing\">\n<h3>Data Processing</h3>\n<p>Custom Datasets can be loaded by customizing the Processor. It converts \u201craw data\u201d into PyTorch Datasets.\nMuch of the heavy lifting is then handled behind the scenes to make it fast &amp; simple to debug.\n<a href=\"https://farm.deepset.ai/data_handling.html\" rel=\"nofollow\">(Learn more)</a></p>\n<img alt=\"https://raw.githubusercontent.com/deepset-ai/FARM/master/docs/img/data_silo_no_bg_small.jpg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6d4be9e19940fc0666057a1ce74125acea0f98e4/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f646565707365742d61692f4641524d2f6d61737465722f646f63732f696d672f646174615f73696c6f5f6e6f5f62675f736d616c6c2e6a7067\">\n</div>\n</div>\n<div id=\"id8\">\n<h2>FAQ</h2>\n<p><strong>1. What language model shall I use for non-english NLP?</strong>\nIf you\u2019re working with German, French, Chinese, Japanese or Finnish you might be interested in trying out the pretrained BERT models in your language. You can see a list <a href=\"https://huggingface.co/models\" rel=\"nofollow\">here</a> of the available models hosted by our friends over at HuggingFace which can be directly accessed through FARM. If your language isn\u2019t one of those (or even if it is), we\u2019d encourage you to try out XLM-Roberta (<a href=\"https://arxiv.org/pdf/1911.02116.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1911.02116.pdf</a>)\nwhich supports 100 different languages and shows surprisingly strong performance compared to single language models.</p>\n<p><strong>2. Why do you have separate prediction heads?</strong>\nPredictionHeads are needed in order to adapt the general language understanding capabilities of the language model to a specific task.\nFor example, the predictions of NER and document classification require very different output formats.\nHaving separate PredictionHead classes means that it is a) very easy to re-use prediction heads on top of different language models\nand b) it simplifies multitask-learning. The latter allows you e.g. to add proxy tasks that facilitate learning of your \u201ctrue objective\u201d.\nExample: You want to classify documents into classes and know that some document tags (e.g. author) already provide helpful information for this task. It might help to add additional tasks for classifying these meta tags.</p>\n<p><strong>3. When is adaptation of a language model to a domain corpus useful?</strong>\nMostly when your domain language differs a lot to the one that the original model was trained on.\nExample: Your corpus is from the aerospace industry and contains a lot of engineering terminology.\nThis is very different to Wikipedia text on in terms of vocab and semantics.\nWe found that this can boost performance especially if your down-stream tasks are using rather small domain datasets.\nIn contrast, if you have huge downstream datasets, the model can often adapt to the domain \u201con-the-fly\u201d during downstream training.</p>\n<p><strong>4. How can I adapt a language model to a domain corpus?</strong>\nThere are two main methods: you can extend the vocabulary by <code>Tokenizer.add_tokens([\"term_a\", \"term_b\"...])</code> or fine-tune your model on a domain text corpus (see <a href=\"https://github.com/deepset-ai/FARM/blob/master/examples/lm_finetuning.py\" rel=\"nofollow\">example</a>).</p>\n<p><strong>5. How can I convert from / to HuggingFace\u2019s models?</strong>\nWe support conversion in both directions (see <a href=\"https://github.com/deepset-ai/FARM/blob/master/examples/conversion_huggingface_models.py\" rel=\"nofollow\">example</a>)\nYou can also load any language model from HuggingFace\u2019s model hub by just specifying the name, e.g. <code>LanguageModel.load(\"deepset/bert-base-cased-squad2\")</code></p>\n<p><strong>6. How can you scale Question Answering to larger collections of documents?</strong>\nIt\u2019s currently most common to put a fast \u201cretriever\u201d in front of the QA model.\nCheckout <a href=\"https://github.com/deepset-ai/haystack/\" rel=\"nofollow\">haystack</a> for such an implementation and more features you need to really run QA in production.</p>\n<p><strong>7. How can you tailor Question Answering to your own domain?</strong>\nWe attained high performance by training a model first on public datasets (e.g. SQuAD, Natural Questions \u2026) and then fine-tuning it on a few custom QA labels from the domain.\nEven ~2000 domain labels can give you the essential performance boost you need.\nCheckout <a href=\"https://github.com/deepset-ai/haystack/\" rel=\"nofollow\">haystack</a> for more details and a QA labeling tool.</p>\n<p><strong>8. My GPU runs out of memory. How can I train with decent batch sizes?</strong>\nUse gradient accumulation! It combines multiple batches before applying backprop. In FARM, just set the param <code>grad_acc_steps</code> in <code>initialize_optimizer()</code> and <code>Trainer()</code> to the number of batches you want to combine (i.e. <code>grad_acc_steps=2</code> and <code>batch_size=16</code> results in an effective batch size of 32).</p>\n</div>\n<div id=\"id9\">\n<h2>Upcoming features</h2>\n<ul>\n<li>Full AWS SageMaker support (incl. Spot instances)</li>\n<li>Support for more Question Answering styles and datasets</li>\n<li>Additional visualizations and statistics to explore and debug your model</li>\n<li>Enabling large scale deployment for production</li>\n<li>Simpler benchmark models (fasttext, word2vec \u2026)</li>\n</ul>\n</div>\n<div id=\"acknowledgements\">\n<h2>Acknowledgements</h2>\n<ul>\n<li>FARM is built upon parts of the great <a href=\"https://github.com/huggingface/pytorch-transformers\" rel=\"nofollow\">Transformers</a>  repository from HuggingFace. It utilizes their implementations of models and tokenizers.</li>\n<li>FARM is a community effort! Essential pieces of it have been implemented by our FARMers out there. Thanks to all contributors!</li>\n<li>The original BERT model and <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">paper</a>  was published by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.</li>\n</ul>\n</div>\n<div id=\"citation\">\n<h2>Citation</h2>\n<p>As of now there is no published paper on FARM. If you want to use or cite our framework, please include\nthe link to this repository. If you are working with the German Bert model, you can link our\n<a href=\"https://deepset.ai/german-bert\" rel=\"nofollow\">blog post</a> describing its training details and performance.</p>\n</div>\n\n          </div>"}, "last_serial": 7129938, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "1fde71163072e01967007beb36afd6a9", "sha256": "4c0c82c643737f59af85fb97e56c7634c8b52c1296741f96d7a95bc5c0085d66"}, "downloads": -1, "filename": "farm-0.1.2.tar.gz", "has_sig": false, "md5_digest": "1fde71163072e01967007beb36afd6a9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 57851, "upload_time": "2019-07-29T14:27:39", "upload_time_iso_8601": "2019-07-29T14:27:39.454335Z", "url": "https://files.pythonhosted.org/packages/65/8d/ee5d2149f9b3b6eb2df6b034acca646e668cbc61ec470bb6facd9675b812/farm-0.1.2.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "7442b08b4fd4d89d56fb744d1bda1b8e", "sha256": "3a044615b12982cfb78222f5f367c2321d641684dcb8de04b7bd31e3654a4b87"}, "downloads": -1, "filename": "farm-0.2.0.tar.gz", "has_sig": false, "md5_digest": "7442b08b4fd4d89d56fb744d1bda1b8e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 68894, "upload_time": "2019-08-19T09:12:39", "upload_time_iso_8601": "2019-08-19T09:12:39.792731Z", "url": "https://files.pythonhosted.org/packages/9a/b3/b35c26aea084d7998d403061c4c139c7f024952dec39854da8e1e2435cdf/farm-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "299ef13e2998bb7f756c36c1b7c74e0a", "sha256": "434331fa7c09c5cda908fe58caa3f9fea11d2ae31819cb58bbf914e4a21006cf"}, "downloads": -1, "filename": "farm-0.2.1.tar.gz", "has_sig": false, "md5_digest": "299ef13e2998bb7f756c36c1b7c74e0a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 78278, "upload_time": "2019-10-10T10:25:59", "upload_time_iso_8601": "2019-10-10T10:25:59.174783Z", "url": "https://files.pythonhosted.org/packages/39/18/2ff409ade228b56812da0098ce829caab15f2daece9a2787918f09f1413a/farm-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "0aaab1071e23719c0181bd743fce3073", "sha256": "ad0e016003ae054f85447e3554ab3bd3e670467ae3aefe47b297a01c4440e866"}, "downloads": -1, "filename": "farm-0.2.2.tar.gz", "has_sig": false, "md5_digest": "0aaab1071e23719c0181bd743fce3073", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 80248, "upload_time": "2019-10-14T19:14:01", "upload_time_iso_8601": "2019-10-14T19:14:01.843460Z", "url": "https://files.pythonhosted.org/packages/04/dd/c9646cad37b940cfaf96457408c671773f669e2f9e768390dcfb7f51a1ea/farm-0.2.2.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "2db2326cbb433213dc37cac14e6be28d", "sha256": "5efe3d2422045333781112e757c947143c100855eb945556e356968f8a85a707"}, "downloads": -1, "filename": "farm-0.3.0.tar.gz", "has_sig": false, "md5_digest": "2db2326cbb433213dc37cac14e6be28d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 83279, "upload_time": "2019-10-28T13:52:07", "upload_time_iso_8601": "2019-10-28T13:52:07.350570Z", "url": "https://files.pythonhosted.org/packages/4e/3b/0cfe70f2875ba82d29320b935a7831eb8e2cc4a00c1f696181fbfaac563e/farm-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "0a0667f4d2bd24ffd2c9ff6af065cc7f", "sha256": "d70a0a42c35e5965a69852c987aeed7fb3f940e12679fba646cd5a16571e700a"}, "downloads": -1, "filename": "farm-0.3.1.tar.gz", "has_sig": false, "md5_digest": "0a0667f4d2bd24ffd2c9ff6af065cc7f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 85544, "upload_time": "2019-11-04T13:49:15", "upload_time_iso_8601": "2019-11-04T13:49:15.358947Z", "url": "https://files.pythonhosted.org/packages/a4/eb/5abcd91898ddccc32168c05e7767885fccfcc1daf095ccf9433cf8d03bd8/farm-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "1345cd5273aad3913704a8dd45bc4142", "sha256": "78c898905c5a4a669a86db1b12d24ddf95142a886601fca7ae80d31f4ac9b828"}, "downloads": -1, "filename": "farm-0.3.2.tar.gz", "has_sig": false, "md5_digest": "1345cd5273aad3913704a8dd45bc4142", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 94551, "upload_time": "2019-11-28T10:11:03", "upload_time_iso_8601": "2019-11-28T10:11:03.404236Z", "url": "https://files.pythonhosted.org/packages/b9/cd/e7e42bf91ba506fe1193094cbaa96969730981bb248942756e077ce10832/farm-0.3.2.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "d467e4b6e95f719b97d66a3e8cc94bc6", "sha256": "325cf2ef98050906dad6ff35b2da5babf50fea1b8ec685308956cef4b3e57882"}, "downloads": -1, "filename": "farm-0.4.1.tar.gz", "has_sig": false, "md5_digest": "d467e4b6e95f719b97d66a3e8cc94bc6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 120115, "upload_time": "2020-02-03T10:13:56", "upload_time_iso_8601": "2020-02-03T10:13:56.364359Z", "url": "https://files.pythonhosted.org/packages/d1/86/5ad3bb2311a2401ad11bf958b1d8645940432f6539feb0ac3198cad1d432/farm-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "bc5085d2ae149caa37337010e0be57c6", "sha256": "23cad49455b1528c0ed32b84d11b24e50f9801e194e03d839aa8b737fb4b391b"}, "downloads": -1, "filename": "farm-0.4.2.tar.gz", "has_sig": false, "md5_digest": "bc5085d2ae149caa37337010e0be57c6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 135347, "upload_time": "2020-04-02T12:19:55", "upload_time_iso_8601": "2020-04-02T12:19:55.798029Z", "url": "https://files.pythonhosted.org/packages/5e/52/d9bcda51ec5a87d4066ededed48df13c0074180c2178439037bbc67c7fbb/farm-0.4.2.tar.gz", "yanked": false}], "0.4.3": [{"comment_text": "", "digests": {"md5": "cad070175cca5b1bf7c63c4517ea519a", "sha256": "c0a567c38c13b9b1794d210b687a65975f6625b6180d561dc40016170e0ee6c0"}, "downloads": -1, "filename": "farm-0.4.3.tar.gz", "has_sig": false, "md5_digest": "cad070175cca5b1bf7c63c4517ea519a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 153722, "upload_time": "2020-04-29T15:52:42", "upload_time_iso_8601": "2020-04-29T15:52:42.177910Z", "url": "https://files.pythonhosted.org/packages/0e/a9/b1f1ff65af01d5cd1d6df698e0c142ab3164afb1189b7cecd8075fee853b/farm-0.4.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cad070175cca5b1bf7c63c4517ea519a", "sha256": "c0a567c38c13b9b1794d210b687a65975f6625b6180d561dc40016170e0ee6c0"}, "downloads": -1, "filename": "farm-0.4.3.tar.gz", "has_sig": false, "md5_digest": "cad070175cca5b1bf7c63c4517ea519a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 153722, "upload_time": "2020-04-29T15:52:42", "upload_time_iso_8601": "2020-04-29T15:52:42.177910Z", "url": "https://files.pythonhosted.org/packages/0e/a9/b1f1ff65af01d5cd1d6df698e0c142ab3164afb1189b7cecd8075fee853b/farm-0.4.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:43 2020"}