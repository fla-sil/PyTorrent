{"info": {"author": "ThoughtWorks Arts", "author_email": "info@thoughtworksarts.io", "bugtrack_url": null, "classifiers": ["Operating System :: MacOS :: MacOS X", "Programming Language :: Python :: 3.6"], "description": "# EmoPy\nEmoPy is a python toolkit with deep neural net classes which accurately predict emotions given images of people's faces.\n\n![Labeled FER Images](readme_docs/labeled_images.png \"Labeled Facial Expression Images\")  \n*Figure from [@Chen2014FacialER]*\n\nThe aim of this project is to make accurate [Facial Expression Recognition (FER)](https://en.wikipedia.org/wiki/Emotion_recognition) models free, open, easy to use, and easy to integrate into different projects.\n\nThe developers of EmoPy have written two guides you may find useful:\n* [Recognizing human facial expressions with machine learning](https://www.thoughtworks.com/insights/blog/recognizing-human-facial-expressions-machine-learning)\n* [EmoPy: a machine learning toolkit for emotional expression](https://www.thoughtworks.com/insights/blog/emopy-machine-learning-toolkit-emotional-expression)\n\nWe aim to expand our development community, and we are open to suggestions and contributions. Please [contact us](mailto:aperez@thoughtworks.com) to discuss.\n\n## Overview\n\nEmoPy includes several modules that are plugged together to build a trained FER prediction model.\n\n- `fermodel.py`\n- `neuralnets.py`\n- `dataset.py`\n- `data_loader.py`\n- `csv_data_loader.py`\n- `directory_data_loader.py`\n- `data_generator.py`\n\nThe `fermodel.py` module uses pretrained models for FER prediction, making it the easiest entry point to get a trained model up and running quickly.\n\nEach of the modules contains one class, except for `neuralnets.py`, which has one interface and four subclasses. Each of these subclasses implements a different neural net architecture using the Keras framework with Tensorflow backend, allowing you to experiment and see which one performs best for your needs.\n\nThe [EmoPy documentation](https://emopy.readthedocs.io/) contains detailed information on the classes and their interactions. Also, an overview of the different neural nets included in this project is included below.\n\n## Datasets\n\nTry out the system using your own dataset or a small dataset we have provided in the [examples/image_data](examples/image_data) subdirectory. The sample datasets we provide will not yield good results due to their small size, but they serve as a great way to get started.\n\nPredictions ideally perform well on a diversity of datasets, illumination conditions, and subsets of the standard 7 emotion labels (happiness, anger, fear, surprise, disgust, sadness, calm/neutral) seen in FER research. Some good example public datasets are the [Extended Cohn-Kanade](http://www.consortium.ri.cmu.edu/ckagree/) and [FER+](https://github.com/Microsoft/FERPlus).\n\n## Environment Setup\n\nEmoPy runs using Python 3.6, theoretically on any Python-compatible OS. We tested EmoPy using Python 3.6.6 on OSX. You can install [Python 3.6.6](https://www.python.org/downloads/release/python-366/) from the Python website.\n\nPlease note that this is not the most current version of Python, but the TensorFlow package doesn't work with Python 3.7 yet, so EmoPy cannot run with Python 3.7.\n\nPython is compatible with multiple operating systems. If you would like to use EmoPy on another OS, please convert these instructions to match your target environment. Let us know how you get on, and we will try to support you and share you results.\n\n\n\nIf you do not have Homebrew installed run this command to install:\n\n```\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n```\n\nGraphViz is required for visualisation functions.\n\n```\nbrew install graphviz\n```\n\nThe next step is to set up a virtual environment using virtualenv. Install virtualenv with sudo.\n```\nsudo pip install virtualenv\n```\n\nCreate and activate the virtual environment. Run:\n```\npython3.6 -m venv venv\n```\nwhere the second `venv` is the name of your virtual environment. To activate, run from the same directory: \n```\nsource venv/bin/activate\n```\nYour terminal command line should now be prefixed with ```(venv)```.\n\n(To deactivate the virtual environment run ```deactivate``` in the command line. You'll know it has been deactivated when the prefix ```(venv)``` disappears.)\n\n## Installation\n\n\n### From PyPi\nOnce the virtual environment is activated, you may install EmoPy using\n```\npip install EmoPy\n```\n\n### From the source\n\nClone the directory and open it in your terminal.\n\n```\ngit clone https://github.com/thoughtworksarts/EmoPy.git\ncd EmoPy\n```\n\nInstall the remaining dependencies using pip.\n\n```\npip install -r requirements.txt\n```\n\nNow you're ready to go!\n\n\n## Running the examples\n\nYou can find example code to run each of the current neural net classes in [examples](examples). You may either download the example directory to a location of your choice on your machine, or find the example directory included in the installation.\n\nIf you choose to use the installed package, you can find the examples directory by starting in the virtual environment directory you created and typing:\n```\ncd lib/python3.6/site-packages/EmoPy/examples\n```\n\n\nThe best place to start is the [FERModel example](examples/fermodel_example.py). Here is a listing of that code:\n\n```python\nfrom EmoPy.src.fermodel import FERModel\nfrom pkg_resources import resource_filename\n\ntarget_emotions = ['calm', 'anger', 'happiness']\nmodel = FERModel(target_emotions, verbose=True)\n\nprint('Predicting on happy image...')\nmodel.predict(resource_filename('EmoPy.examples','image_data/sample_happy_image.png'))\n\nprint('Predicting on disgust image...')\nmodel.predict(resource_filename('EmoPy.examples','image_data/sample_disgust_image.png'))\n\nprint('Predicting on anger image...')\nmodel.predict(resource_filename('EmoPy.examples','image_data/sample_anger_image2.png'))\n```\n\nThe code above loads a pre-trained model and then predicts an emotion on a sample image. As you can see, all you have to supply with this example is a set of target emotions and a sample image.\n\nOnce you have completed the installation, you can run this example from the examples folder by running the example script.\n\n```\npython fermodel_example.py\n```\n\nThe first thing the example does is load and initialize the model. Next it prints out emotion probabilities for each sample image its given. It should look like this:\n\n![FERModel Training Output](readme_docs/sample-fermodel-predictions.png \"FERModel Training Output\")\n\nTo train your own neural net, use one of our FER neural net classes to get started. You can try the convolutional_model.py example:\n\n```\npython convolutional_example.py\n```\n\nThe example first initializes the model. A summary of the model architecture will be printed out. This includes a list of all the neural net layers and the shape of their output. Our models are built using the Keras framework, which offers this visualization function.\n\n![Convolutional Example Output Part 1](readme_docs/convolutional_example_output1.png \"Convolutional Example Output Part 1\")\n\nYou will see the training and validation accuracies of the model being updated as it is trained on each sample image. The validation accuracy will be very low since we are only using three images for training and validation. It should look something like this:\n\n![Convolutional Example Output Part 2](readme_docs/convolutional_example_output2.png \"Convolutional Example Output Part 2\")\n\n## Comparison of neural network models\n\n#### ConvolutionalNN\n\nConvolutional Neural Networks ([CNNs](https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8)) are currently considered the go-to neural networks for Image Classification, because they pick up on patterns in small parts of an image, such as the curve of an eyebrow. EmoPy's ConvolutionalNN is trained on still images.\n\n#### TimeDelayConvNN\n\nThe Time-Delayed 3D-Convolutional Neural Network model is inspired by the work described in [this paper](http://ieeexplore.ieee.org/document/7090979/?part=1) written by Dr. Hongying Meng of Brunel University, London. It uses temporal information as part of its training samples. Instead of using still images as training samples, it uses past images from a series for additional context. One training sample will contain *n* number of images from a series and its emotion label will be that of the most recent image. The idea is to capture the progression of a facial expression leading up to a peak emotion.\n\n![Facial Expression Image Sequence](readme_docs/progression-example.png \"Facial expression image sequence\")  \nFacial expression image sequence in Cohn-Kanade database from [@Jia2014]\n\n#### ConvolutionalLstmNN\n\nThe Convolutional Long Short Term Memory neural net is a convolutional and recurrent neural network hybrid. Convolutional NNs  use kernels, or filters, to find patterns in smaller parts of an image. Recurrent NNs ([RNNs](https://deeplearning4j.org/lstm.html#recurrent)) take into account previous training examples, similar to the Time-Delay Neural Network, for context. This model is able to both extract local data from images and use temporal context.\n\nThe Time-Delay model and this model differ in how they use temporal context. The former only takes context from within video clips of a single face as shown in the figure above. The ConvolutionLstmNN is given still images that have no relation to each other. It looks for pattern differences between past image samples and the current sample as well as their labels. It isn\u2019t necessary to have a progression of the same face, simply different faces to compare.\n\n![7 Standard Facial Expressions](readme_docs/seven-expression-examples.jpg \"7 Standard Facial Expressions\")  \nFigure from [@vanGent2016]\n\n#### TransferLearningNN\n\nThis model uses a technique known as [Transfer Learning](https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/), where pre-trained deep neural net models are used as starting points. The pre-trained models it uses are trained on images to classify objects. The model then retrains the pre-trained models using facial expression images with emotion classifications rather than object classifications. It adds a couple top layers to the original model to match the number of target emotions we want to classify and reruns the training algorithm with a set of facial expression images. It only uses still images, no temporal context.\n\n## Performance\n\nCurrently the ConvolutionalLstmNN model is performing best when classifying 7 emotions with a validation accuracy of 47.5%. The table below shows accuracy values of this model and the TransferLearningNN model when trained on all seven standard emotions and on a subset of three emotions (fear, happiness, neutral). They were trained on 5,000 images from the [FER+](https://github.com/Microsoft/FERPlus) dataset.\n\n| Neural Net Model    | 7 emotions        |                     | 3 emotions        |                     |\n|---------------------|-------------------|---------------------|-------------------|---------------------|\n|                     | Training Accuracy | Validation Accuracy | Training Accuracy | Validation Accuracy |\n| ConvolutionalLstmNN | 0.6187            | 0.4751              | 0.9148            | 0.6267              |\n| TransferLearningNN  | 0.5358            | 0.2933              | 0.7393            | 0.4840              |\n\nBoth models are overfitting, meaning that training accuracies are much higher than validation accuracies. This means that the models are doing a really good job of recognizing and classifying patterns in the training images, but are overgeneralizing. They are less accurate when predicting emotions for new images.\n\nIf you would like to experiment with different parameters using our neural net classes, we recommend you use [FloydHub](https://www.floydhub.com/about), a platform for training and deploying deep learning models in the cloud. Let us know how your models are doing! The goal is to optimize the performance and generalizability of all the FERPython models.\n\n## Guiding Principles\n\nThese are the principals we use to guide development and contributions to the project:\n\n- __FER for Good__. FER applications have the potential to be used for malicious purposes. We want to build EmoPy with a community that champions integrity, transparency, and awareness and hope to instill these values throughout development while maintaining an accessible, quality toolkit.\n\n- __User Friendliness.__ EmoPy prioritizes user experience and is designed to be as easy as possible to get an FER prediction model up and running by minimizing the total user requirements for basic use cases.\n\n- __Experimentation to Maximize Performance__. Optimal performance in FER prediction is a primary goal. The deep neural net classes are designed to easily modify training parameters, image pre-processing options, and feature extraction methods in the hopes that experimentation in the open-source community will lead to high-performing FER prediction.\n\n- __Modularity.__ EmoPy contains four base modules (`fermodel`, `neuralnets`, `imageprocessor`, and `featureextractor`) that can be easily used together with minimal restrictions.\n\n## Contributing\n\n1. Fork it!\n2. Create your feature branch: `git checkout -b my-new-feature`\n3. Commit your changes: `git commit -am 'Add some feature'`\n4. Push to the branch: `git push origin my-new-feature`\n5. Submit a pull request :D\n\nThis is a new library that has a lot of room for growth. Check out the list of open issues that we need help addressing!\n\n[@Chen2014FacialER]: https://www.semanticscholar.org/paper/Facial-Expression-Recognition-Based-on-Facial-Comp-Chen-Chen/677ebde61ba3936b805357e27fce06c44513a455 \"Facial Expression Recognition Based on Facial Components Detection and HOG Features\"\n\n[@Jia2014]: https://www.researchgate.net/figure/Fig-2-Facial-expression-image-sequence-in-Cohn-Kanade-database_257627744_fig1 \"Head and facial gestures synthesis using PAD model for an expressive talking avatar\"\n\n[@vanGent2016]: http://www.paulvangent.com/2016/04/01/emotion-recognition-with-python-opencv-and-a-face-dataset/ \"Emotion Recognition With Python, OpenCV and a Face Dataset. A tech blog about fun things with Python and embedded electronics.\"\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/thoughtworksarts/EmoPy", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "EmoPy", "package_url": "https://pypi.org/project/EmoPy/", "platform": "", "project_url": "https://pypi.org/project/EmoPy/", "project_urls": {"Homepage": "https://github.com/thoughtworksarts/EmoPy"}, "release_url": "https://pypi.org/project/EmoPy/0.0.5/", "requires_dist": ["keras (>=2.2.0)", "lasagne", "pytest", "matplotlib (>2.1.0)", "numpy (<=1.14.5,>=1.13.3)", "scikit-image (>=0.13.1)", "scikit-learn (>=0.19.1)", "scikit-neuralnetwork (>=0.7)", "scipy (>=0.19.1)", "tensorflow (>=1.10.1)", "opencv-python", "h5py", "pydot", "graphviz"], "requires_python": ">=3.6.3,<3.7", "summary": "A deep neural net toolkit for emotion analysis via Facial Expression Recognition (FER)", "version": "0.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>EmoPy</h1>\n<p>EmoPy is a python toolkit with deep neural net classes which accurately predict emotions given images of people's faces.</p>\n<p><img alt=\"Labeled FER Images\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/57d614cefd96a497f578a1e5d045e2e9965c73c5/726561646d655f646f63732f6c6162656c65645f696d616765732e706e67\"><br>\n<em>Figure from <a href=\"https://www.semanticscholar.org/paper/Facial-Expression-Recognition-Based-on-Facial-Comp-Chen-Chen/677ebde61ba3936b805357e27fce06c44513a455\" rel=\"nofollow\" title=\"Facial Expression Recognition Based on Facial Components Detection and HOG Features\">@Chen2014FacialER</a></em></p>\n<p>The aim of this project is to make accurate <a href=\"https://en.wikipedia.org/wiki/Emotion_recognition\" rel=\"nofollow\">Facial Expression Recognition (FER)</a> models free, open, easy to use, and easy to integrate into different projects.</p>\n<p>The developers of EmoPy have written two guides you may find useful:</p>\n<ul>\n<li><a href=\"https://www.thoughtworks.com/insights/blog/recognizing-human-facial-expressions-machine-learning\" rel=\"nofollow\">Recognizing human facial expressions with machine learning</a></li>\n<li><a href=\"https://www.thoughtworks.com/insights/blog/emopy-machine-learning-toolkit-emotional-expression\" rel=\"nofollow\">EmoPy: a machine learning toolkit for emotional expression</a></li>\n</ul>\n<p>We aim to expand our development community, and we are open to suggestions and contributions. Please <a href=\"mailto:aperez@thoughtworks.com\">contact us</a> to discuss.</p>\n<h2>Overview</h2>\n<p>EmoPy includes several modules that are plugged together to build a trained FER prediction model.</p>\n<ul>\n<li><code>fermodel.py</code></li>\n<li><code>neuralnets.py</code></li>\n<li><code>dataset.py</code></li>\n<li><code>data_loader.py</code></li>\n<li><code>csv_data_loader.py</code></li>\n<li><code>directory_data_loader.py</code></li>\n<li><code>data_generator.py</code></li>\n</ul>\n<p>The <code>fermodel.py</code> module uses pretrained models for FER prediction, making it the easiest entry point to get a trained model up and running quickly.</p>\n<p>Each of the modules contains one class, except for <code>neuralnets.py</code>, which has one interface and four subclasses. Each of these subclasses implements a different neural net architecture using the Keras framework with Tensorflow backend, allowing you to experiment and see which one performs best for your needs.</p>\n<p>The <a href=\"https://emopy.readthedocs.io/\" rel=\"nofollow\">EmoPy documentation</a> contains detailed information on the classes and their interactions. Also, an overview of the different neural nets included in this project is included below.</p>\n<h2>Datasets</h2>\n<p>Try out the system using your own dataset or a small dataset we have provided in the <a href=\"examples/image_data\" rel=\"nofollow\">examples/image_data</a> subdirectory. The sample datasets we provide will not yield good results due to their small size, but they serve as a great way to get started.</p>\n<p>Predictions ideally perform well on a diversity of datasets, illumination conditions, and subsets of the standard 7 emotion labels (happiness, anger, fear, surprise, disgust, sadness, calm/neutral) seen in FER research. Some good example public datasets are the <a href=\"http://www.consortium.ri.cmu.edu/ckagree/\" rel=\"nofollow\">Extended Cohn-Kanade</a> and <a href=\"https://github.com/Microsoft/FERPlus\" rel=\"nofollow\">FER+</a>.</p>\n<h2>Environment Setup</h2>\n<p>EmoPy runs using Python 3.6, theoretically on any Python-compatible OS. We tested EmoPy using Python 3.6.6 on OSX. You can install <a href=\"https://www.python.org/downloads/release/python-366/\" rel=\"nofollow\">Python 3.6.6</a> from the Python website.</p>\n<p>Please note that this is not the most current version of Python, but the TensorFlow package doesn't work with Python 3.7 yet, so EmoPy cannot run with Python 3.7.</p>\n<p>Python is compatible with multiple operating systems. If you would like to use EmoPy on another OS, please convert these instructions to match your target environment. Let us know how you get on, and we will try to support you and share you results.</p>\n<p>If you do not have Homebrew installed run this command to install:</p>\n<pre><code>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n</code></pre>\n<p>GraphViz is required for visualisation functions.</p>\n<pre><code>brew install graphviz\n</code></pre>\n<p>The next step is to set up a virtual environment using virtualenv. Install virtualenv with sudo.</p>\n<pre><code>sudo pip install virtualenv\n</code></pre>\n<p>Create and activate the virtual environment. Run:</p>\n<pre><code>python3.6 -m venv venv\n</code></pre>\n<p>where the second <code>venv</code> is the name of your virtual environment. To activate, run from the same directory:</p>\n<pre><code>source venv/bin/activate\n</code></pre>\n<p>Your terminal command line should now be prefixed with <code>(venv)</code>.</p>\n<p>(To deactivate the virtual environment run <code>deactivate</code> in the command line. You'll know it has been deactivated when the prefix <code>(venv)</code> disappears.)</p>\n<h2>Installation</h2>\n<h3>From PyPi</h3>\n<p>Once the virtual environment is activated, you may install EmoPy using</p>\n<pre><code>pip install EmoPy\n</code></pre>\n<h3>From the source</h3>\n<p>Clone the directory and open it in your terminal.</p>\n<pre><code>git clone https://github.com/thoughtworksarts/EmoPy.git\ncd EmoPy\n</code></pre>\n<p>Install the remaining dependencies using pip.</p>\n<pre><code>pip install -r requirements.txt\n</code></pre>\n<p>Now you're ready to go!</p>\n<h2>Running the examples</h2>\n<p>You can find example code to run each of the current neural net classes in <a href=\"examples\" rel=\"nofollow\">examples</a>. You may either download the example directory to a location of your choice on your machine, or find the example directory included in the installation.</p>\n<p>If you choose to use the installed package, you can find the examples directory by starting in the virtual environment directory you created and typing:</p>\n<pre><code>cd lib/python3.6/site-packages/EmoPy/examples\n</code></pre>\n<p>The best place to start is the <a href=\"examples/fermodel_example.py\" rel=\"nofollow\">FERModel example</a>. Here is a listing of that code:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">EmoPy.src.fermodel</span> <span class=\"kn\">import</span> <span class=\"n\">FERModel</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pkg_resources</span> <span class=\"kn\">import</span> <span class=\"n\">resource_filename</span>\n\n<span class=\"n\">target_emotions</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'calm'</span><span class=\"p\">,</span> <span class=\"s1\">'anger'</span><span class=\"p\">,</span> <span class=\"s1\">'happiness'</span><span class=\"p\">]</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FERModel</span><span class=\"p\">(</span><span class=\"n\">target_emotions</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'Predicting on happy image...'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">resource_filename</span><span class=\"p\">(</span><span class=\"s1\">'EmoPy.examples'</span><span class=\"p\">,</span><span class=\"s1\">'image_data/sample_happy_image.png'</span><span class=\"p\">))</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'Predicting on disgust image...'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">resource_filename</span><span class=\"p\">(</span><span class=\"s1\">'EmoPy.examples'</span><span class=\"p\">,</span><span class=\"s1\">'image_data/sample_disgust_image.png'</span><span class=\"p\">))</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'Predicting on anger image...'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">resource_filename</span><span class=\"p\">(</span><span class=\"s1\">'EmoPy.examples'</span><span class=\"p\">,</span><span class=\"s1\">'image_data/sample_anger_image2.png'</span><span class=\"p\">))</span>\n</pre>\n<p>The code above loads a pre-trained model and then predicts an emotion on a sample image. As you can see, all you have to supply with this example is a set of target emotions and a sample image.</p>\n<p>Once you have completed the installation, you can run this example from the examples folder by running the example script.</p>\n<pre><code>python fermodel_example.py\n</code></pre>\n<p>The first thing the example does is load and initialize the model. Next it prints out emotion probabilities for each sample image its given. It should look like this:</p>\n<p><img alt=\"FERModel Training Output\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bc757e69ef82892d3cd3b6e95f80b03066e10f25/726561646d655f646f63732f73616d706c652d6665726d6f64656c2d70726564696374696f6e732e706e67\"></p>\n<p>To train your own neural net, use one of our FER neural net classes to get started. You can try the convolutional_model.py example:</p>\n<pre><code>python convolutional_example.py\n</code></pre>\n<p>The example first initializes the model. A summary of the model architecture will be printed out. This includes a list of all the neural net layers and the shape of their output. Our models are built using the Keras framework, which offers this visualization function.</p>\n<p><img alt=\"Convolutional Example Output Part 1\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2e5e2d0342e259fd6a13eed5b465680f7007a2f2/726561646d655f646f63732f636f6e766f6c7574696f6e616c5f6578616d706c655f6f7574707574312e706e67\"></p>\n<p>You will see the training and validation accuracies of the model being updated as it is trained on each sample image. The validation accuracy will be very low since we are only using three images for training and validation. It should look something like this:</p>\n<p><img alt=\"Convolutional Example Output Part 2\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e0dc46b56b62e3db0568f85f450782772c658336/726561646d655f646f63732f636f6e766f6c7574696f6e616c5f6578616d706c655f6f7574707574322e706e67\"></p>\n<h2>Comparison of neural network models</h2>\n<h4>ConvolutionalNN</h4>\n<p>Convolutional Neural Networks (<a href=\"https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8\" rel=\"nofollow\">CNNs</a>) are currently considered the go-to neural networks for Image Classification, because they pick up on patterns in small parts of an image, such as the curve of an eyebrow. EmoPy's ConvolutionalNN is trained on still images.</p>\n<h4>TimeDelayConvNN</h4>\n<p>The Time-Delayed 3D-Convolutional Neural Network model is inspired by the work described in <a href=\"http://ieeexplore.ieee.org/document/7090979/?part=1\" rel=\"nofollow\">this paper</a> written by Dr. Hongying Meng of Brunel University, London. It uses temporal information as part of its training samples. Instead of using still images as training samples, it uses past images from a series for additional context. One training sample will contain <em>n</em> number of images from a series and its emotion label will be that of the most recent image. The idea is to capture the progression of a facial expression leading up to a peak emotion.</p>\n<p><img alt=\"Facial Expression Image Sequence\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5e75e45d9267da26a5b78330b13e94cc1011c26d/726561646d655f646f63732f70726f6772657373696f6e2d6578616d706c652e706e67\"><br>\nFacial expression image sequence in Cohn-Kanade database from <a href=\"https://www.researchgate.net/figure/Fig-2-Facial-expression-image-sequence-in-Cohn-Kanade-database_257627744_fig1\" rel=\"nofollow\" title=\"Head and facial gestures synthesis using PAD model for an expressive talking avatar\">@Jia2014</a></p>\n<h4>ConvolutionalLstmNN</h4>\n<p>The Convolutional Long Short Term Memory neural net is a convolutional and recurrent neural network hybrid. Convolutional NNs  use kernels, or filters, to find patterns in smaller parts of an image. Recurrent NNs (<a href=\"https://deeplearning4j.org/lstm.html#recurrent\" rel=\"nofollow\">RNNs</a>) take into account previous training examples, similar to the Time-Delay Neural Network, for context. This model is able to both extract local data from images and use temporal context.</p>\n<p>The Time-Delay model and this model differ in how they use temporal context. The former only takes context from within video clips of a single face as shown in the figure above. The ConvolutionLstmNN is given still images that have no relation to each other. It looks for pattern differences between past image samples and the current sample as well as their labels. It isn\u2019t necessary to have a progression of the same face, simply different faces to compare.</p>\n<p><img alt=\"7 Standard Facial Expressions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9e7efa0402b92359a160cc4b1e96ed5cedfb1621/726561646d655f646f63732f736576656e2d65787072657373696f6e2d6578616d706c65732e6a7067\"><br>\nFigure from <a href=\"http://www.paulvangent.com/2016/04/01/emotion-recognition-with-python-opencv-and-a-face-dataset/\" rel=\"nofollow\" title=\"Emotion Recognition With Python, OpenCV and a Face Dataset. A tech blog about fun things with Python and embedded electronics.\">@vanGent2016</a></p>\n<h4>TransferLearningNN</h4>\n<p>This model uses a technique known as <a href=\"https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/\" rel=\"nofollow\">Transfer Learning</a>, where pre-trained deep neural net models are used as starting points. The pre-trained models it uses are trained on images to classify objects. The model then retrains the pre-trained models using facial expression images with emotion classifications rather than object classifications. It adds a couple top layers to the original model to match the number of target emotions we want to classify and reruns the training algorithm with a set of facial expression images. It only uses still images, no temporal context.</p>\n<h2>Performance</h2>\n<p>Currently the ConvolutionalLstmNN model is performing best when classifying 7 emotions with a validation accuracy of 47.5%. The table below shows accuracy values of this model and the TransferLearningNN model when trained on all seven standard emotions and on a subset of three emotions (fear, happiness, neutral). They were trained on 5,000 images from the <a href=\"https://github.com/Microsoft/FERPlus\" rel=\"nofollow\">FER+</a> dataset.</p>\n<table>\n<thead>\n<tr>\n<th>Neural Net Model</th>\n<th>7 emotions</th>\n<th></th>\n<th>3 emotions</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>Training Accuracy</td>\n<td>Validation Accuracy</td>\n<td>Training Accuracy</td>\n<td>Validation Accuracy</td>\n</tr>\n<tr>\n<td>ConvolutionalLstmNN</td>\n<td>0.6187</td>\n<td>0.4751</td>\n<td>0.9148</td>\n<td>0.6267</td>\n</tr>\n<tr>\n<td>TransferLearningNN</td>\n<td>0.5358</td>\n<td>0.2933</td>\n<td>0.7393</td>\n<td>0.4840</td>\n</tr></tbody></table>\n<p>Both models are overfitting, meaning that training accuracies are much higher than validation accuracies. This means that the models are doing a really good job of recognizing and classifying patterns in the training images, but are overgeneralizing. They are less accurate when predicting emotions for new images.</p>\n<p>If you would like to experiment with different parameters using our neural net classes, we recommend you use <a href=\"https://www.floydhub.com/about\" rel=\"nofollow\">FloydHub</a>, a platform for training and deploying deep learning models in the cloud. Let us know how your models are doing! The goal is to optimize the performance and generalizability of all the FERPython models.</p>\n<h2>Guiding Principles</h2>\n<p>These are the principals we use to guide development and contributions to the project:</p>\n<ul>\n<li>\n<p><strong>FER for Good</strong>. FER applications have the potential to be used for malicious purposes. We want to build EmoPy with a community that champions integrity, transparency, and awareness and hope to instill these values throughout development while maintaining an accessible, quality toolkit.</p>\n</li>\n<li>\n<p><strong>User Friendliness.</strong> EmoPy prioritizes user experience and is designed to be as easy as possible to get an FER prediction model up and running by minimizing the total user requirements for basic use cases.</p>\n</li>\n<li>\n<p><strong>Experimentation to Maximize Performance</strong>. Optimal performance in FER prediction is a primary goal. The deep neural net classes are designed to easily modify training parameters, image pre-processing options, and feature extraction methods in the hopes that experimentation in the open-source community will lead to high-performing FER prediction.</p>\n</li>\n<li>\n<p><strong>Modularity.</strong> EmoPy contains four base modules (<code>fermodel</code>, <code>neuralnets</code>, <code>imageprocessor</code>, and <code>featureextractor</code>) that can be easily used together with minimal restrictions.</p>\n</li>\n</ul>\n<h2>Contributing</h2>\n<ol>\n<li>Fork it!</li>\n<li>Create your feature branch: <code>git checkout -b my-new-feature</code></li>\n<li>Commit your changes: <code>git commit -am 'Add some feature'</code></li>\n<li>Push to the branch: <code>git push origin my-new-feature</code></li>\n<li>Submit a pull request :D</li>\n</ol>\n<p>This is a new library that has a lot of room for growth. Check out the list of open issues that we need help addressing!</p>\n\n          </div>"}, "last_serial": 4387938, "releases": {"0.0.3": [{"comment_text": "", "digests": {"md5": "ed0bd3fed4289be3ebba9aaeb9f40a8b", "sha256": "55c077c2b681bee02032e73a065479348425c4246514554f7a6bdb7fad9d6412"}, "downloads": -1, "filename": "EmoPy-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "ed0bd3fed4289be3ebba9aaeb9f40a8b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.3,<3.7", "size": 15669511, "upload_time": "2018-10-17T16:29:02", "upload_time_iso_8601": "2018-10-17T16:29:02.983269Z", "url": "https://files.pythonhosted.org/packages/7b/3a/6b9171f5be5c5ca7db7bd46d50be3590a661e96705c2b35e167e58ed4816/EmoPy-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e85aa9910d10adf94addd5f2683cbf10", "sha256": "6a7a1c54da3bdfef3e2d155ccc0290c8f462305961bd98757e215422cf6a7e13"}, "downloads": -1, "filename": "EmoPy-0.0.3.tar.gz", "has_sig": false, "md5_digest": "e85aa9910d10adf94addd5f2683cbf10", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.3,<3.7", "size": 15545848, "upload_time": "2018-10-17T16:29:08", "upload_time_iso_8601": "2018-10-17T16:29:08.833428Z", "url": "https://files.pythonhosted.org/packages/41/e8/1a08a639c6a578f21d45fe685d1b7e96ab5876312968db1a07b02bf87572/EmoPy-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "6c6b78c54c82052cf8be591c2325b7c2", "sha256": "e67acf899ca72e4ed627e26e843e4a9d28a6150ad1e9cd763b8dbe76c6c61197"}, "downloads": -1, "filename": "EmoPy-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "6c6b78c54c82052cf8be591c2325b7c2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.3,<3.7", "size": 15669601, "upload_time": "2018-10-17T18:41:20", "upload_time_iso_8601": "2018-10-17T18:41:20.989533Z", "url": "https://files.pythonhosted.org/packages/a7/59/842087f8011d6743e6db4a1933a8a60eb7a86beeb16268af723ca86aebd6/EmoPy-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1b1bf70b77907c1e1778a41899269549", "sha256": "0d3345603de7581a737999b162fb2979bc21af9e17e47bda3c6262d31faad3ed"}, "downloads": -1, "filename": "EmoPy-0.0.4.tar.gz", "has_sig": false, "md5_digest": "1b1bf70b77907c1e1778a41899269549", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.3,<3.7", "size": 15546145, "upload_time": "2018-10-17T18:42:17", "upload_time_iso_8601": "2018-10-17T18:42:17.563646Z", "url": "https://files.pythonhosted.org/packages/9e/28/b1d11bf2c6409582d31fcf742410b9d274bfa4122ff311bf26e2cd214151/EmoPy-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "d543f7c777622f5fde0504860108f60a", "sha256": "be72da13b68ad7db97d778a56f7b66174b461977d0239a1bd8975366ff425d28"}, "downloads": -1, "filename": "EmoPy-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "d543f7c777622f5fde0504860108f60a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.3,<3.7", "size": 15669595, "upload_time": "2018-10-17T22:15:48", "upload_time_iso_8601": "2018-10-17T22:15:48.403877Z", "url": "https://files.pythonhosted.org/packages/53/36/3c33149c382fb549c29a255cdbeca59c15bb1fa39d8a321836acb3a4c056/EmoPy-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fb8238ee4cdd8dccc3633c0f5a9869f0", "sha256": "422e204b374942fbf7b71c6fe882235a660d27e1bf1ef81d2a930a8f46064fa7"}, "downloads": -1, "filename": "EmoPy-0.0.5.tar.gz", "has_sig": false, "md5_digest": "fb8238ee4cdd8dccc3633c0f5a9869f0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.3,<3.7", "size": 15546097, "upload_time": "2018-10-17T22:17:11", "upload_time_iso_8601": "2018-10-17T22:17:11.748679Z", "url": "https://files.pythonhosted.org/packages/26/f9/c4744b02be585bdf753547efbbf9b24b51f07e10256f710ab419feba5d59/EmoPy-0.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d543f7c777622f5fde0504860108f60a", "sha256": "be72da13b68ad7db97d778a56f7b66174b461977d0239a1bd8975366ff425d28"}, "downloads": -1, "filename": "EmoPy-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "d543f7c777622f5fde0504860108f60a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.3,<3.7", "size": 15669595, "upload_time": "2018-10-17T22:15:48", "upload_time_iso_8601": "2018-10-17T22:15:48.403877Z", "url": "https://files.pythonhosted.org/packages/53/36/3c33149c382fb549c29a255cdbeca59c15bb1fa39d8a321836acb3a4c056/EmoPy-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fb8238ee4cdd8dccc3633c0f5a9869f0", "sha256": "422e204b374942fbf7b71c6fe882235a660d27e1bf1ef81d2a930a8f46064fa7"}, "downloads": -1, "filename": "EmoPy-0.0.5.tar.gz", "has_sig": false, "md5_digest": "fb8238ee4cdd8dccc3633c0f5a9869f0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.3,<3.7", "size": 15546097, "upload_time": "2018-10-17T22:17:11", "upload_time_iso_8601": "2018-10-17T22:17:11.748679Z", "url": "https://files.pythonhosted.org/packages/26/f9/c4744b02be585bdf753547efbbf9b24b51f07e10256f710ab419feba5d59/EmoPy-0.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:46:33 2020"}