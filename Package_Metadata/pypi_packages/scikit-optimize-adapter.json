{"info": {"author": "Jay Kim", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "\n\nScikit-Optimize-Adapter\n=======================\n\n\nScikit-Optimize-Adapter (Adapter: \"A DAsk Parallel TunER\") is an efficient light weight library built on top of Scikit-Optimize and Dask that lets the user do Bayesian optimization hyperparameter tuning with different schemes of parallelized cross-validations.\n\n\nInstall\n-------\n\n::\n\n\tpip install --index-url https://test.pypi.org/simple/ --no-deps scikit-optimize-adapter --upgrade\n\n\nGetting started\n---------------\n\nLet's start with the below dummy training data:\n\n.. code:: python\n\n\timport pandas as pd\n\timport numpy as np\n\n\tdata = np.arange(30*4).reshape(30, 4)\n\tdf = pd.DataFrame(data=data, columns=['target', 'f1', 'f2', 'f3'])\n\n\tfeatures = ['f1', 'f2', 'f3']\n\ttarget = 'target'\n\n\tK = 5\n\n\torderby=None\n\tnum_partition=None\n\twindow_size=None\n\n\tfrom skopt.space import Space, Categorical, Integer, Real, Dimension\n\n\tspace  = [Real(0.5, 10),      # learning rate       (learn_rate)\n\t          Real(0, 1),         # gamma               (min_split_improvement)\n\t          Integer(3, 4),      # max_depth           (max_depth)\n\t          Integer(11, 13),    # n_estimators        (ntrees)\n\t          Integer(2, 4),      # min_child_weight    (min_rows)\n\t          Real(0, 1),         # colsample_bytree    (col_sample_rate_per_tree)\n\t          Real(0, 1)]         # subsample           (sample_rate)\n\n\nAdapter is shipped with XGBoost regressor and classifier, but you can pass in a callable estimator of your design if you wish to customize it. \n\n.. code:: python\n\n\tfrom adapter import Adapter\n\n\tadapt = Adapter(df, features, target, K, groupby=None, \n                cross_validation_scheme='random_shuffle',\n                search_method=\"bayesian_optimization\",\n                estimator=\"xgboost_regression\")  # \"xgboost_regression\" or \"xgboost_classification\" or callable estimator (more on this later)\n\nTry copying the link to the web browser to check out the dask dashboard: ``http://127.0.0.1:8789/status``.\n\nYou can visualize the Dask delayed computation graph:\n\n.. code:: python\n\n\tdelayed_graph = adapt.construct_delayed_graph(num_iter=3, search_space=space)  # we will set n_iter to 3 to make visualizing manageable. \n\tdelayed_graph.visualize()\n\n.. figure:: https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/graph.png\n\nLet's run the code. ``num_initial`` is the number of random initial searches and ``num_iter`` is the total number of search steps taken, including the ``num_initial`` step counts. (Example: ``num_initial=5, num_iter=15`` means 5 random search and 10 Bayesian search)\n\n.. code:: python\n\n\tres = adapt.run(num_initial=5, num_iter=15, search_space=space)\n\nWhile it runs, checkout the dashboard again, and click on the ``Graph`` tab. You will see the above computation graph being worked on in real time!\n\n.. figure:: https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/daskdashboard.png\n\n\nNow you can retrieve the results:\n\n.. code:: python\n\n\tadapt.plot_improvements()  # to show the improvements \n\toptimal_params = adapt.get_optimal_params()  # which you can use to train your final model\n\n.. figure:: https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/improvement.png\n\nIf you are running this in a local machine, you must take responsibility of removing the temporary directory:\n\n.. code:: python\n\n\tadapt.cleanup()\n\n\nCross-validation schemes\n------------------------\n\nThere are 5 different cross-validation schemes supported by the adapter:\n\n* ``random_shuffle``: create K cross-validation folds from randomly shuffled rows\n\t- Default mode for most regression tasks .\n* ``ordered``: create K cross-validation folds after sorting the train data by a certain column\n\t- Used for regression tasks where data has time series nature with high temporal auto-correlation.\n\t- Must supply ``orderby`` argument.\n* ``binary_classification``: create K cross-validation folds where positive/negative label proportion is preserved\n\t- Used for classification task.\n\t- This mode will preserve the positive and negative label proportions in each fold.\n* ``stratified_sampling``: create K cross-validation folds such that the skew distribution of response is preserved \n\t- Used for regression task where the continuous response variable is highly skewed.\n\t- This mode will preserve the skew distribution of the response values by sampling from stratification.\n\t- Must supply ``num_partition`` argument.\n* ``expanding_window``: mainly for time series modeling \n\t- Refer to:\n\n\nTuning for multiple models in parallel\n--------------------------------------\n\nAgain, let's take a look at a specific example data:\n\n.. code:: python\n\n\timport pandas as pd\n\timport numpy as np\n\n\tgroup_col = np.asarray([1]*10 + [2]*10 + [3]*10 + [4]*10 + [5]*10 + [6]*10).reshape(-1, 1)  # this time we have a column specifying group\n\tdata = np.arange(60*4).reshape(60, 4)\n\tdata = np.hstack((data, group_col))\n\tdf = pd.DataFrame(data=data, columns=['target', 'f1', 'f2', 'f3', 'groups'])\n\n\tfeatures = ['f1', 'f2', 'f3']\n\ttarget = 'target'\n\n\tK = 5\n\n\torderby=None\n\tnum_partition=None\n\twindow_size=None\n\n\tfrom skopt.space import Space, Categorical, Integer, Real, Dimension\n\n\tspace  = [Real(0.5, 10),      # learning rate       (learn_rate)\n\t          Real(0, 1),         # gamma               (min_split_improvement)\n\t          Integer(3, 4),      # max_depth           (max_depth)\n\t          Integer(11, 13),    # n_estimators        (ntrees)\n\t          Integer(2, 4),      # min_child_weight    (min_rows)\n\t          Real(0, 1),         # colsample_bytree    (col_sample_rate_per_tree)\n\t          Real(0, 1)]         # subsample           (sample_rate)\n\nWe can tune the models for each group by passing by ``groupby`` argument. \n\n.. code:: python\n\n\tfrom adapter import Adapter\n\n\tadapt = Adapter(df, features, target, K, groupby='groups', \n                cross_validation_scheme='random_shuffle',\n                search_method=\"bayesian_optimization\",\n                estimator=\"xgboost_regression\")  \n\nRun the adapter the same way:\n\n.. code:: python\n\n\tres = adapt.run(num_initial=5, num_iter=15, search_space=space)\n\nYou can visualize the Dask delayed computation graph:\n\n.. figure:: https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/multigraph_dashboard.png\n\n\nPassing in an arbitrary callable estimator\n------------------------------------------\n\nYou can pass in an arbitrary callable estimator as long as it implements the standard scikit-learn estimator API: \n\n.. code:: python\n\n\tfrom abc import ABCMeta, abstractmethod\n\n\tclass BaseEstimator(object, metaclass=ABCMeta):\n\t    \"\"\"\n\t    Base class for all Algorithm classes.\n\t    \"\"\"\n\n\t    def __init__(self, **kwargs):\n\t        pass\n\n\t    @abstractmethod\n\t    def fit(self, X, y, params):\n\t        pass\n\n\t    @abstractmethod\n\t    def score(self, X, y):\n\t        pass\n\n\t    @abstractmethod\n\t    def predict(self, X):\n\t        pass \n\nFor example, we can even do something like:\n\n.. code:: python\n\n\tfrom adapter import BaseEstimator  # import BaseEstimator!\n\n\tclass DummyEstimator(BaseEstimator):\n\n\t    def __init__(self):\n\t        pass\n\n\t    def fit(self, train_X, train_y, params):\n\t        a = len(train_X)/10.\n\n\t        for i in range(int(a*5000000)):\n\t            i + 1\n\n\t        print(len(train_X), len(train_y))\n\n\t    def score(self, validation_X, validation_y):\n\n\t        print(len(validation_X), len(validation_y))\n\n\t        return 1.5\n\n\t    def predict(self, test_X):\n\n\t        return len(test_X)\n\n\tmy_estimator = DummyEstimator()\n\nThen you can use it with the Adapter:\n\n.. code:: python\n\n\tfrom adapter import Adapter\n\n\tadapt = Adapter(df, features, target, K, groupby='groups', \n                cross_validation_scheme='random_shuffle',\n                search_method=\"bayesian_optimization\",\n                estimator=my_estimator)  # your own estimator\n\n\nTuning multiple models with highly skewed training data sizes\n-------------------------------------------------------------\n\nWhen the data size for each group is highly skewed, a suboptimal resource allocation can occur. In this case, it is more advantageous to throttle the feeding of delayed graphs to the Dask client by using multiple thread instances. Let's again look at an example case:\n\n.. code:: python\n\n\timport pandas as pd\n\timport numpy as np\n\timport time\n\n\tgroup_col = np.asarray([1]*100 + [2]*2 + [3]*2 + [4]*2 + [5]*2 + [6]*2 + [7]*2 + [8]*2 + [9]*2 + [16]*2 + [26]*2 + [17]*2 + [18]*2 + [19]*2 + [116]*2 + [126]*2).reshape(-1, 1)\n\tdata = np.arange(130*4).reshape(130, 4)\n\tdata = np.hstack((data, group_col))\n\tdf = pd.DataFrame(data=data, columns=['target', 'f1', 'f2', 'f3', 'groups'])\n\n\tfeatures = ['f1', 'f2', 'f3']\n\tgroupby = 'groups'\n\ttarget = 'target'\n\n\tK = 5\n\n\tfrom adapter import BaseEstimator  # import BaseEstimator!\n\n\tclass DummyEstimator(BaseEstimator):\n\n\t    def __init__(self):\n\t        pass\n\n\t    def fit(self, train_X, train_y, params):\n\t        a = len(train_X)/10.\n\n\t        for i in range(int(a*5000000)):\n\t            i + 1\n\n\t        print(len(train_X), len(train_y))\n\n\t    def score(self, validation_X, validation_y):\n\n\t        print(len(validation_X), len(validation_y))\n\n\t        return 1.5\n\n\t    def predict(self, test_X):\n\n\t        return len(test_X)\n\n\tmy_estimator = DummyEstimator()\n\n\torderby=None\n\tnum_partition=None\n\twindow_size=None\n\n\tfrom skopt.space import Space, Categorical, Integer, Real, Dimension\n\n\tspace  = [Real(0.5, 10),      # learning rate       (learn_rate)\n\t          Real(0, 1),         # gamma               (min_split_improvement)\n\t          Integer(3, 4),      # max_depth           (max_depth)\n\t          Integer(11, 13),    # n_estimators        (ntrees)\n\t          Integer(2, 4),      # min_child_weight    (min_rows)\n\t          Real(0, 1),         # colsample_bytree    (col_sample_rate_per_tree)\n\t          Real(0, 1)]         # subsample           (sample_rate)\n\nIn such a case, we use ``run_with_threads`` method call, where we pass an additional argument of ``num_threads``:\n\n.. code:: python\n\n\tfrom adapter import Adapter\n\n\tadapt = Adapter(df, features, target, K, groupby='groups',\n\t        cross_validation_scheme='random_shuffle',\n\t        search_method=\"bayesian_optimization\",\n\t        estimator=my_estimator)  # your own estimator\n\n\tres = adapt.run(num_initial=5, num_iter=15, search_space=space, num_threads=2)  # num_threads\n\nYou can check fromt the Dask dashboard that only two delayed computation graphs are worked on at the same time, achieving a dynamic resource allocation in effect:\n\n.. figure:: https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/twograph_dashboard.png\n\n\n\nTodo:\n\n1. rest of the cross validation schemes\n2. testing hard thresholded submit process (and testing speed without it)\n3. supervised encodings\n4. add unit tests\n5. continuous integration set up\n6. random search method\n7. multi GPU environment\n8. documentations\n9. ~~getting the results of the optimization~~\n10. ~~visualization of optimizations~~\n11. early stop criterion using callbacks\n12. ~~beta readme.rst for install and tutorial~~\n13. full readme.rst for install and tutorial\n14. periodic training \n15. bayesian warm start training\n16. dependency managements\n17. active per worker threadpool managements\n\n\n\n\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "DSB 3-clause", "maintainer": "", "maintainer_email": "", "name": "scikit-optimize-adapter", "package_url": "https://pypi.org/project/scikit-optimize-adapter/", "platform": "", "project_url": "https://pypi.org/project/scikit-optimize-adapter/", "project_urls": null, "release_url": "https://pypi.org/project/scikit-optimize-adapter/0.0b1/", "requires_dist": ["scikit-optimize (>=0.7.4)", "dask", "distributed"], "requires_python": "", "summary": "Dask parallelized bayesian optimization toolbox", "version": "0.0b1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Scikit-Optimize-Adapter (Adapter: \u201cA DAsk Parallel TunER\u201d) is an efficient light weight library built on top of Scikit-Optimize and Dask that lets the user do Bayesian optimization hyperparameter tuning with different schemes of parallelized cross-validations.</p>\n<div id=\"install\">\n<h2>Install</h2>\n<pre>pip install --index-url https://test.pypi.org/simple/ --no-deps scikit-optimize-adapter --upgrade\n</pre>\n</div>\n<div id=\"getting-started\">\n<h2>Getting started</h2>\n<p>Let\u2019s start with the below dummy training data:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">30</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'target'</span><span class=\"p\">,</span> <span class=\"s1\">'f1'</span><span class=\"p\">,</span> <span class=\"s1\">'f2'</span><span class=\"p\">,</span> <span class=\"s1\">'f3'</span><span class=\"p\">])</span>\n\n<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'f1'</span><span class=\"p\">,</span> <span class=\"s1\">'f2'</span><span class=\"p\">,</span> <span class=\"s1\">'f3'</span><span class=\"p\">]</span>\n<span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"s1\">'target'</span>\n\n<span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n\n<span class=\"n\">orderby</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n<span class=\"n\">num_partition</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n<span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">skopt.space</span> <span class=\"kn\">import</span> <span class=\"n\">Space</span><span class=\"p\">,</span> <span class=\"n\">Categorical</span><span class=\"p\">,</span> <span class=\"n\">Integer</span><span class=\"p\">,</span> <span class=\"n\">Real</span><span class=\"p\">,</span> <span class=\"n\">Dimension</span>\n\n<span class=\"n\">space</span>  <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>      <span class=\"c1\"># learning rate       (learn_rate)</span>\n          <span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>         <span class=\"c1\"># gamma               (min_split_improvement)</span>\n          <span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span>      <span class=\"c1\"># max_depth           (max_depth)</span>\n          <span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"mi\">13</span><span class=\"p\">),</span>    <span class=\"c1\"># n_estimators        (ntrees)</span>\n          <span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span>      <span class=\"c1\"># min_child_weight    (min_rows)</span>\n          <span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>         <span class=\"c1\"># colsample_bytree    (col_sample_rate_per_tree)</span>\n          <span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)]</span>         <span class=\"c1\"># subsample           (sample_rate)</span>\n</pre>\n<p>Adapter is shipped with XGBoost regressor and classifier, but you can pass in a callable estimator of your design if you wish to customize it.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adapter</span> <span class=\"kn\">import</span> <span class=\"n\">Adapter</span>\n\n<span class=\"n\">adapt</span> <span class=\"o\">=</span> <span class=\"n\">Adapter</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">groupby</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">cross_validation_scheme</span><span class=\"o\">=</span><span class=\"s1\">'random_shuffle'</span><span class=\"p\">,</span>\n        <span class=\"n\">search_method</span><span class=\"o\">=</span><span class=\"s2\">\"bayesian_optimization\"</span><span class=\"p\">,</span>\n        <span class=\"n\">estimator</span><span class=\"o\">=</span><span class=\"s2\">\"xgboost_regression\"</span><span class=\"p\">)</span>  <span class=\"c1\"># \"xgboost_regression\" or \"xgboost_classification\" or callable estimator (more on this later)</span>\n</pre>\n<p>Try copying the link to the web browser to check out the dask dashboard: <tt><span class=\"pre\">http://127.0.0.1:8789/status</span></tt>.</p>\n<p>You can visualize the Dask delayed computation graph:</p>\n<pre><span class=\"n\">delayed_graph</span> <span class=\"o\">=</span> <span class=\"n\">adapt</span><span class=\"o\">.</span><span class=\"n\">construct_delayed_graph</span><span class=\"p\">(</span><span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">search_space</span><span class=\"o\">=</span><span class=\"n\">space</span><span class=\"p\">)</span>  <span class=\"c1\"># we will set n_iter to 3 to make visualizing manageable.</span>\n<span class=\"n\">delayed_graph</span><span class=\"o\">.</span><span class=\"n\">visualize</span><span class=\"p\">()</span>\n</pre>\n<div>\n<img alt=\"https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/graph.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e3be61b4aab809c4a849be271e935b1470a7b4e2/68747470733a2f2f6769746875622e636f6d2f6d6f7a6a6179303631392f7363696b69742d6f7074696d697a652d616461707465722f626c6f622f6d61737465722f6d656469612f67726170682e706e67\">\n</div>\n<p>Let\u2019s run the code. <tt>num_initial</tt> is the number of random initial searches and <tt>num_iter</tt> is the total number of search steps taken, including the <tt>num_initial</tt> step counts. (Example: <tt>num_initial=5, num_iter=15</tt> means 5 random search and 10 Bayesian search)</p>\n<pre><span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">adapt</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">num_initial</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">search_space</span><span class=\"o\">=</span><span class=\"n\">space</span><span class=\"p\">)</span>\n</pre>\n<p>While it runs, checkout the dashboard again, and click on the <tt>Graph</tt> tab. You will see the above computation graph being worked on in real time!</p>\n<div>\n<img alt=\"https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/daskdashboard.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/add0ff51028b4b136bdc25ac0c2d6432e5082a85/68747470733a2f2f6769746875622e636f6d2f6d6f7a6a6179303631392f7363696b69742d6f7074696d697a652d616461707465722f626c6f622f6d61737465722f6d656469612f6461736b64617368626f6172642e706e67\">\n</div>\n<p>Now you can retrieve the results:</p>\n<pre><span class=\"n\">adapt</span><span class=\"o\">.</span><span class=\"n\">plot_improvements</span><span class=\"p\">()</span>  <span class=\"c1\"># to show the improvements</span>\n<span class=\"n\">optimal_params</span> <span class=\"o\">=</span> <span class=\"n\">adapt</span><span class=\"o\">.</span><span class=\"n\">get_optimal_params</span><span class=\"p\">()</span>  <span class=\"c1\"># which you can use to train your final model</span>\n</pre>\n<div>\n<img alt=\"https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/improvement.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b0c06a761cc00368f68ca7e2d6b6e0f86e414c0b/68747470733a2f2f6769746875622e636f6d2f6d6f7a6a6179303631392f7363696b69742d6f7074696d697a652d616461707465722f626c6f622f6d61737465722f6d656469612f696d70726f76656d656e742e706e67\">\n</div>\n<p>If you are running this in a local machine, you must take responsibility of removing the temporary directory:</p>\n<pre><span class=\"n\">adapt</span><span class=\"o\">.</span><span class=\"n\">cleanup</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"cross-validation-schemes\">\n<h2>Cross-validation schemes</h2>\n<p>There are 5 different cross-validation schemes supported by the adapter:</p>\n<ul>\n<li><dl>\n<dt><tt>random_shuffle</tt>: create K cross-validation folds from randomly shuffled rows</dt>\n<dd><ul>\n<li>Default mode for most regression tasks .</li>\n</ul>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt><tt>ordered</tt>: create K cross-validation folds after sorting the train data by a certain column</dt>\n<dd><ul>\n<li>Used for regression tasks where data has time series nature with high temporal auto-correlation.</li>\n<li>Must supply <tt>orderby</tt> argument.</li>\n</ul>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt><tt>binary_classification</tt>: create K cross-validation folds where positive/negative label proportion is preserved</dt>\n<dd><ul>\n<li>Used for classification task.</li>\n<li>This mode will preserve the positive and negative label proportions in each fold.</li>\n</ul>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt><tt>stratified_sampling</tt>: create K cross-validation folds such that the skew distribution of response is preserved</dt>\n<dd><ul>\n<li>Used for regression task where the continuous response variable is highly skewed.</li>\n<li>This mode will preserve the skew distribution of the response values by sampling from stratification.</li>\n<li>Must supply <tt>num_partition</tt> argument.</li>\n</ul>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt><tt>expanding_window</tt>: mainly for time series modeling</dt>\n<dd><ul>\n<li>Refer to:</li>\n</ul>\n</dd>\n</dl>\n</li>\n</ul>\n</div>\n<div id=\"tuning-for-multiple-models-in-parallel\">\n<h2>Tuning for multiple models in parallel</h2>\n<p>Again, let\u2019s take a look at a specific example data:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"n\">group_col</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">10</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">10</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">10</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">10</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">10</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">10</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># this time we have a column specifying group</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">60</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"mi\">60</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">((</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">group_col</span><span class=\"p\">))</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'target'</span><span class=\"p\">,</span> <span class=\"s1\">'f1'</span><span class=\"p\">,</span> <span class=\"s1\">'f2'</span><span class=\"p\">,</span> <span class=\"s1\">'f3'</span><span class=\"p\">,</span> <span class=\"s1\">'groups'</span><span class=\"p\">])</span>\n\n<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'f1'</span><span class=\"p\">,</span> <span class=\"s1\">'f2'</span><span class=\"p\">,</span> <span class=\"s1\">'f3'</span><span class=\"p\">]</span>\n<span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"s1\">'target'</span>\n\n<span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n\n<span class=\"n\">orderby</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n<span class=\"n\">num_partition</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n<span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">skopt.space</span> <span class=\"kn\">import</span> <span class=\"n\">Space</span><span class=\"p\">,</span> <span class=\"n\">Categorical</span><span class=\"p\">,</span> <span class=\"n\">Integer</span><span class=\"p\">,</span> <span class=\"n\">Real</span><span class=\"p\">,</span> <span class=\"n\">Dimension</span>\n\n<span class=\"n\">space</span>  <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>      <span class=\"c1\"># learning rate       (learn_rate)</span>\n          <span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>         <span class=\"c1\"># gamma               (min_split_improvement)</span>\n          <span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span>      <span class=\"c1\"># max_depth           (max_depth)</span>\n          <span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"mi\">13</span><span class=\"p\">),</span>    <span class=\"c1\"># n_estimators        (ntrees)</span>\n          <span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span>      <span class=\"c1\"># min_child_weight    (min_rows)</span>\n          <span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>         <span class=\"c1\"># colsample_bytree    (col_sample_rate_per_tree)</span>\n          <span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)]</span>         <span class=\"c1\"># subsample           (sample_rate)</span>\n</pre>\n<p>We can tune the models for each group by passing by <tt>groupby</tt> argument.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adapter</span> <span class=\"kn\">import</span> <span class=\"n\">Adapter</span>\n\n<span class=\"n\">adapt</span> <span class=\"o\">=</span> <span class=\"n\">Adapter</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">groupby</span><span class=\"o\">=</span><span class=\"s1\">'groups'</span><span class=\"p\">,</span>\n        <span class=\"n\">cross_validation_scheme</span><span class=\"o\">=</span><span class=\"s1\">'random_shuffle'</span><span class=\"p\">,</span>\n        <span class=\"n\">search_method</span><span class=\"o\">=</span><span class=\"s2\">\"bayesian_optimization\"</span><span class=\"p\">,</span>\n        <span class=\"n\">estimator</span><span class=\"o\">=</span><span class=\"s2\">\"xgboost_regression\"</span><span class=\"p\">)</span>\n</pre>\n<p>Run the adapter the same way:</p>\n<pre><span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">adapt</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">num_initial</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">search_space</span><span class=\"o\">=</span><span class=\"n\">space</span><span class=\"p\">)</span>\n</pre>\n<p>You can visualize the Dask delayed computation graph:</p>\n<div>\n<img alt=\"https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/multigraph_dashboard.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8e1708935d4d5ebaec5e147f5a082c59b768dbad/68747470733a2f2f6769746875622e636f6d2f6d6f7a6a6179303631392f7363696b69742d6f7074696d697a652d616461707465722f626c6f622f6d61737465722f6d656469612f6d756c746967726170685f64617368626f6172642e706e67\">\n</div>\n</div>\n<div id=\"passing-in-an-arbitrary-callable-estimator\">\n<h2>Passing in an arbitrary callable estimator</h2>\n<p>You can pass in an arbitrary callable estimator as long as it implements the standard scikit-learn estimator API:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">abc</span> <span class=\"kn\">import</span> <span class=\"n\">ABCMeta</span><span class=\"p\">,</span> <span class=\"n\">abstractmethod</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">BaseEstimator</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">,</span> <span class=\"n\">metaclass</span><span class=\"o\">=</span><span class=\"n\">ABCMeta</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"\n    Base class for all Algorithm classes.\n    \"\"\"</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"nd\">@abstractmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"nd\">@abstractmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">score</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"nd\">@abstractmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n</pre>\n<p>For example, we can even do something like:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adapter</span> <span class=\"kn\">import</span> <span class=\"n\">BaseEstimator</span>  <span class=\"c1\"># import BaseEstimator!</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">DummyEstimator</span><span class=\"p\">(</span><span class=\"n\">BaseEstimator</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">train_X</span><span class=\"p\">,</span> <span class=\"n\">train_y</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">):</span>\n        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_X</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mf\">10.</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">*</span><span class=\"mi\">5000000</span><span class=\"p\">)):</span>\n            <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_X</span><span class=\"p\">),</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_y</span><span class=\"p\">))</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">score</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">validation_X</span><span class=\"p\">,</span> <span class=\"n\">validation_y</span><span class=\"p\">):</span>\n\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">validation_X</span><span class=\"p\">),</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">validation_y</span><span class=\"p\">))</span>\n\n        <span class=\"k\">return</span> <span class=\"mf\">1.5</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">test_X</span><span class=\"p\">):</span>\n\n        <span class=\"k\">return</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_X</span><span class=\"p\">)</span>\n\n<span class=\"n\">my_estimator</span> <span class=\"o\">=</span> <span class=\"n\">DummyEstimator</span><span class=\"p\">()</span>\n</pre>\n<p>Then you can use it with the Adapter:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adapter</span> <span class=\"kn\">import</span> <span class=\"n\">Adapter</span>\n\n<span class=\"n\">adapt</span> <span class=\"o\">=</span> <span class=\"n\">Adapter</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">groupby</span><span class=\"o\">=</span><span class=\"s1\">'groups'</span><span class=\"p\">,</span>\n        <span class=\"n\">cross_validation_scheme</span><span class=\"o\">=</span><span class=\"s1\">'random_shuffle'</span><span class=\"p\">,</span>\n        <span class=\"n\">search_method</span><span class=\"o\">=</span><span class=\"s2\">\"bayesian_optimization\"</span><span class=\"p\">,</span>\n        <span class=\"n\">estimator</span><span class=\"o\">=</span><span class=\"n\">my_estimator</span><span class=\"p\">)</span>  <span class=\"c1\"># your own estimator</span>\n</pre>\n</div>\n<div id=\"tuning-multiple-models-with-highly-skewed-training-data-sizes\">\n<h2>Tuning multiple models with highly skewed training data sizes</h2>\n<p>When the data size for each group is highly skewed, a suboptimal resource allocation can occur. In this case, it is more advantageous to throttle the feeding of delayed graphs to the Dask client by using multiple thread instances. Let\u2019s again look at an example case:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n\n<span class=\"n\">group_col</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">100</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">26</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">17</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">19</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">116</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"mi\">126</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">130</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"mi\">130</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">((</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">group_col</span><span class=\"p\">))</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'target'</span><span class=\"p\">,</span> <span class=\"s1\">'f1'</span><span class=\"p\">,</span> <span class=\"s1\">'f2'</span><span class=\"p\">,</span> <span class=\"s1\">'f3'</span><span class=\"p\">,</span> <span class=\"s1\">'groups'</span><span class=\"p\">])</span>\n\n<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'f1'</span><span class=\"p\">,</span> <span class=\"s1\">'f2'</span><span class=\"p\">,</span> <span class=\"s1\">'f3'</span><span class=\"p\">]</span>\n<span class=\"n\">groupby</span> <span class=\"o\">=</span> <span class=\"s1\">'groups'</span>\n<span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"s1\">'target'</span>\n\n<span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">adapter</span> <span class=\"kn\">import</span> <span class=\"n\">BaseEstimator</span>  <span class=\"c1\"># import BaseEstimator!</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">DummyEstimator</span><span class=\"p\">(</span><span class=\"n\">BaseEstimator</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">train_X</span><span class=\"p\">,</span> <span class=\"n\">train_y</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">):</span>\n        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_X</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mf\">10.</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">*</span><span class=\"mi\">5000000</span><span class=\"p\">)):</span>\n            <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_X</span><span class=\"p\">),</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_y</span><span class=\"p\">))</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">score</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">validation_X</span><span class=\"p\">,</span> <span class=\"n\">validation_y</span><span class=\"p\">):</span>\n\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">validation_X</span><span class=\"p\">),</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">validation_y</span><span class=\"p\">))</span>\n\n        <span class=\"k\">return</span> <span class=\"mf\">1.5</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">test_X</span><span class=\"p\">):</span>\n\n        <span class=\"k\">return</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_X</span><span class=\"p\">)</span>\n\n<span class=\"n\">my_estimator</span> <span class=\"o\">=</span> <span class=\"n\">DummyEstimator</span><span class=\"p\">()</span>\n\n<span class=\"n\">orderby</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n<span class=\"n\">num_partition</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n<span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"kc\">None</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">skopt.space</span> <span class=\"kn\">import</span> <span class=\"n\">Space</span><span class=\"p\">,</span> <span class=\"n\">Categorical</span><span class=\"p\">,</span> <span class=\"n\">Integer</span><span class=\"p\">,</span> <span class=\"n\">Real</span><span class=\"p\">,</span> <span class=\"n\">Dimension</span>\n\n<span class=\"n\">space</span>  <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>      <span class=\"c1\"># learning rate       (learn_rate)</span>\n          <span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>         <span class=\"c1\"># gamma               (min_split_improvement)</span>\n          <span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span>      <span class=\"c1\"># max_depth           (max_depth)</span>\n          <span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"mi\">13</span><span class=\"p\">),</span>    <span class=\"c1\"># n_estimators        (ntrees)</span>\n          <span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span>      <span class=\"c1\"># min_child_weight    (min_rows)</span>\n          <span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>         <span class=\"c1\"># colsample_bytree    (col_sample_rate_per_tree)</span>\n          <span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)]</span>         <span class=\"c1\"># subsample           (sample_rate)</span>\n</pre>\n<p>In such a case, we use <tt>run_with_threads</tt> method call, where we pass an additional argument of <tt>num_threads</tt>:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">adapter</span> <span class=\"kn\">import</span> <span class=\"n\">Adapter</span>\n\n<span class=\"n\">adapt</span> <span class=\"o\">=</span> <span class=\"n\">Adapter</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">groupby</span><span class=\"o\">=</span><span class=\"s1\">'groups'</span><span class=\"p\">,</span>\n        <span class=\"n\">cross_validation_scheme</span><span class=\"o\">=</span><span class=\"s1\">'random_shuffle'</span><span class=\"p\">,</span>\n        <span class=\"n\">search_method</span><span class=\"o\">=</span><span class=\"s2\">\"bayesian_optimization\"</span><span class=\"p\">,</span>\n        <span class=\"n\">estimator</span><span class=\"o\">=</span><span class=\"n\">my_estimator</span><span class=\"p\">)</span>  <span class=\"c1\"># your own estimator</span>\n\n<span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">adapt</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">num_initial</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_iter</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">search_space</span><span class=\"o\">=</span><span class=\"n\">space</span><span class=\"p\">,</span> <span class=\"n\">num_threads</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># num_threads</span>\n</pre>\n<p>You can check fromt the Dask dashboard that only two delayed computation graphs are worked on at the same time, achieving a dynamic resource allocation in effect:</p>\n<div>\n<img alt=\"https://github.com/mozjay0619/scikit-optimize-adapter/blob/master/media/twograph_dashboard.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fd03ff434e1703bb2c5ff3dfa3f40890f1b795cf/68747470733a2f2f6769746875622e636f6d2f6d6f7a6a6179303631392f7363696b69742d6f7074696d697a652d616461707465722f626c6f622f6d61737465722f6d656469612f74776f67726170685f64617368626f6172642e706e67\">\n</div>\n<p>Todo:</p>\n<ol>\n<li>rest of the cross validation schemes</li>\n<li>testing hard thresholded submit process (and testing speed without it)</li>\n<li>supervised encodings</li>\n<li>add unit tests</li>\n<li>continuous integration set up</li>\n<li>random search method</li>\n<li>multi GPU environment</li>\n<li>documentations</li>\n<li>~~getting the results of the optimization~~</li>\n<li>~~visualization of optimizations~~</li>\n<li>early stop criterion using callbacks</li>\n<li>~~beta readme.rst for install and tutorial~~</li>\n<li>full readme.rst for install and tutorial</li>\n<li>periodic training</li>\n<li>bayesian warm start training</li>\n<li>dependency managements</li>\n<li>active per worker threadpool managements</li>\n</ol>\n</div>\n\n          </div>"}, "last_serial": 6803565, "releases": {"0.0b0": [{"comment_text": "", "digests": {"md5": "b8623a60e819a095cf653c0ba0d2b296", "sha256": "ba1c6d898068a6224144048396f2d2707e98f7b3db8c06ceb904f566150a8213"}, "downloads": -1, "filename": "scikit_optimize_adapter-0.0b0-py3-none-any.whl", "has_sig": false, "md5_digest": "b8623a60e819a095cf653c0ba0d2b296", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 16981, "upload_time": "2020-03-10T02:55:42", "upload_time_iso_8601": "2020-03-10T02:55:42.828964Z", "url": "https://files.pythonhosted.org/packages/b7/2a/93ba5b51cffb2e3c5299ec20825c4458d006935955657e4e36a64c21593c/scikit_optimize_adapter-0.0b0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ef638fdc8aa7990a15fe7fe12786ae35", "sha256": "603067d77b9085b4efb3bffb9a8842d20037c0add9989a6df667ba030a326f5b"}, "downloads": -1, "filename": "scikit-optimize-adapter-0.0b0.tar.gz", "has_sig": false, "md5_digest": "ef638fdc8aa7990a15fe7fe12786ae35", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13622, "upload_time": "2020-03-10T02:55:45", "upload_time_iso_8601": "2020-03-10T02:55:45.037342Z", "url": "https://files.pythonhosted.org/packages/18/f0/f6c99440de47ace27f047d408c763ec1fb99593567cd6077067fdf31e28b/scikit-optimize-adapter-0.0b0.tar.gz", "yanked": false}], "0.0b1": [{"comment_text": "", "digests": {"md5": "2ceb84c3bfb28201a18c2bbf9f44f2af", "sha256": "63c326ef19f5e7d224a4788a6163652310e50b04d5f0f66236fb3de9290cddd9"}, "downloads": -1, "filename": "scikit_optimize_adapter-0.0b1-py3-none-any.whl", "has_sig": false, "md5_digest": "2ceb84c3bfb28201a18c2bbf9f44f2af", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18695, "upload_time": "2020-03-13T04:36:10", "upload_time_iso_8601": "2020-03-13T04:36:10.974443Z", "url": "https://files.pythonhosted.org/packages/9a/15/716fd396be237cf66e778f320cbedc607c4555a51375065428897cda33a8/scikit_optimize_adapter-0.0b1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "70869bd0cc34167e6a73d73b5bc10a79", "sha256": "52295d72b378e60ff94ce350332adb2bedbf6335ad8779e2258c21eb81345837"}, "downloads": -1, "filename": "scikit-optimize-adapter-0.0b1.tar.gz", "has_sig": false, "md5_digest": "70869bd0cc34167e6a73d73b5bc10a79", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17052, "upload_time": "2020-03-13T04:36:12", "upload_time_iso_8601": "2020-03-13T04:36:12.568699Z", "url": "https://files.pythonhosted.org/packages/ab/1a/c2cd40e6d34273d8d3ce702d2b8e7bc98e9bf3cb366f89497834548fb42c/scikit-optimize-adapter-0.0b1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2ceb84c3bfb28201a18c2bbf9f44f2af", "sha256": "63c326ef19f5e7d224a4788a6163652310e50b04d5f0f66236fb3de9290cddd9"}, "downloads": -1, "filename": "scikit_optimize_adapter-0.0b1-py3-none-any.whl", "has_sig": false, "md5_digest": "2ceb84c3bfb28201a18c2bbf9f44f2af", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18695, "upload_time": "2020-03-13T04:36:10", "upload_time_iso_8601": "2020-03-13T04:36:10.974443Z", "url": "https://files.pythonhosted.org/packages/9a/15/716fd396be237cf66e778f320cbedc607c4555a51375065428897cda33a8/scikit_optimize_adapter-0.0b1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "70869bd0cc34167e6a73d73b5bc10a79", "sha256": "52295d72b378e60ff94ce350332adb2bedbf6335ad8779e2258c21eb81345837"}, "downloads": -1, "filename": "scikit-optimize-adapter-0.0b1.tar.gz", "has_sig": false, "md5_digest": "70869bd0cc34167e6a73d73b5bc10a79", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17052, "upload_time": "2020-03-13T04:36:12", "upload_time_iso_8601": "2020-03-13T04:36:12.568699Z", "url": "https://files.pythonhosted.org/packages/ab/1a/c2cd40e6d34273d8d3ce702d2b8e7bc98e9bf3cb366f89497834548fb42c/scikit-optimize-adapter-0.0b1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:57:29 2020"}