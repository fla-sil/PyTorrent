{"info": {"author": "Google", "author_email": "neural-tangents-dev@google.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Operating System :: MacOS", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Scientific/Engineering", "Topic :: Software Development"], "description": "# Neural Tangents [[arXiv](https://arxiv.org/abs/1912.02803)]\n[**Quickstart**](#colab-notebooks)\n| [**Install guide**](#installation)\n| [**Reference docs**](https://neural-tangents.readthedocs.io/en/latest/)\n\n[![PyPI](https://img.shields.io/pypi/v/neural-tangents)](https://pypi.org/project/neural-tangents/) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/neural-tangents)](https://pypi.org/project/neural-tangents/)\n[![Build Status](https://travis-ci.org/google/neural-tangents.svg?branch=master)](https://travis-ci.org/google/neural-tangents)\n[![Readthedocs](https://readthedocs.org/projects/neural-tangents/badge/?version=latest)](https://neural-tangents.readthedocs.io/en/latest/?badge=latest)\n[![PyPI - License](https://img.shields.io/pypi/l/neural_tangents)](https://github.com/google/neural-tangents/blob/master/LICENSE)\n\n**News:**\n\n* Neural Tangents just got faster! >4X speedup in computing analytic\nkernels for CNN architectures with pooling, starting from version 0.2.1. See our\n[Performance](#performance).\n\n* We will be at [ICLR 2020](https://iclr.cc/), stay tuned for our live session\ntime slots.\n\n## Overview\n\nNeural Tangents is a high-level neural network API for specifying complex, hierarchical, neural networks of both finite and _infinite_ width. Neural Tangents allows researchers to define, train, and evaluate infinite networks as easily as finite ones.\n\nInfinite (in width or channel count) neural networks are Gaussian Processes (GPs) with a kernel function determined by their architecture (see [References](#references) for details and nuances of this correspondence).\n\nNeural Tangents allows you to construct a neural network model with the usual building blocks like convolutions, pooling, residual connections, nonlinearities etc. and obtain not only the finite model, but also the kernel function of the respective GP.\n\nThe library is written in python using [JAX](https://github.com/google/jax) and leveraging [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/index.md) to run out-of-the-box on CPU, GPU, or TPU. Kernel computation is highly optimized for speed and memory efficiency, and can be automatically distributed over multiple accelerators with near-perfect scaling.\n\nNeural Tangents is a work in progress.\nWe happily welcome contributions!\n\n\n\n\n## Contents\n* [Colab Notebooks](#colab-notebooks)\n* [Installation](#installation)\n* [5-Minute intro](#5-minute-intro)\n* [Package description](#package-description)\n* [Technical gotchas](#technical-gotchas)\n* [Training dynamics of wide but finite networks](#training-dynamics-of-wide-but-finite-networks)\n* [Performance](#performance)\n* [Papers](#papers)\n* [Citation](#citation)\n* [References](#references)\n\n## Colab Notebooks\n\nAn easy way to get started with Neural Tangents is by playing around with the following interactive notebooks in Colaboratory. They demo the major features of Neural Tangents and show how it can be used in research.\n\n- [Neural Tangents Cookbook](https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb)\n- [Weight Space Linearization](https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/weight_space_linearization.ipynb)\n- [Function Space Linearization](https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/function_space_linearization.ipynb)\n- [Neural Network Phase Diagram](https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/phase_diagram.ipynb)\n\n## Installation\n\nTo use GPU, first follow [JAX's](https://www.github.com/google/jax/) GPU installation instructions. Otherwise, install JAX on CPU by running\n\n```\npip install jaxlib jax --upgrade\n```\n\nOnce JAX is installed install Neural Tangents by running\n\n```\npip install neural-tangents\n```\nor, to use the bleeding-edge version from GitHub source,\n\n```\ngit clone https://github.com/google/neural-tangents\npip install -e neural-tangents\n```\n\nYou can now run the examples (using [`tensorflow_datasets`](https://github.com/tensorflow/datasets)) by calling:\n\n```\n# Note that Tensorflow does not work with Python 3.8\n# https://github.com/tensorflow/tensorflow/issues/33374\npip install tensorflow \"tensorflow-datasets>=2.0.0\"\n\npython neural-tangents/examples/infinite_fcn.py\npython neural-tangents/examples/weight_space.py\npython neural-tangents/examples/function_space.py\n```\n\nFinally, you can run tests by calling:\n\n```\n# NOTE: a few tests will fail without\n# pip install tensorflow \"tensorflow-datasets>=2.0.0\"\n\nfor f in neural-tangents/neural_tangents/tests/*.py; do python $f; done\n```\n\n## 5-Minute intro\n\n<b>See this [Colab](https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb) for a detailed tutorial. Below is a very quick introduction.</b>\n\nOur library closely follows JAX's API for specifying neural networks,  [`stax`](https://github.com/google/jax/blob/master/jax/experimental/stax.py). In `stax` a network is defined by a pair of functions `(init_fn, apply_fn)` initializing the trainable parameters and computing the outputs of the network respectively. Below is an example of defining a 3-layer network and computing it's outputs `y` given inputs `x`.\n\n```python\nfrom jax import random\nfrom jax.experimental import stax\n\ninit_fn, apply_fn = stax.serial(\n    stax.Dense(512), stax.Relu,\n    stax.Dense(512), stax.Relu,\n    stax.Dense(1)\n)\n\nkey = random.PRNGKey(1)\nx = random.normal(key, (10, 100))\n_, params = init_fn(key, input_shape=x.shape)\n\ny = apply_fn(params, x)  # (10, 1) np.ndarray outputs of the neural network\n```\n\nNeural Tangents is designed to serve as a drop-in replacement for `stax`, extending the `(init_fn, apply_fn)` tuple to a triple `(init_fn, apply_fn, kernel_fn)`, where `kernel_fn` is the kernel function of the infinite network (GP) of the given architecture. Below is an example of computing the covariances of the GP between two batches of inputs `x1` and `x2`.\n\n```python\nfrom jax import random\nfrom neural_tangents import stax\n\ninit_fn, apply_fn, kernel_fn = stax.serial(\n    stax.Dense(512), stax.Relu(),\n    stax.Dense(512), stax.Relu(),\n    stax.Dense(1)\n)\n\nkey1, key2 = random.split(random.PRNGKey(1))\nx1 = random.normal(key1, (10, 100))\nx2 = random.normal(key2, (20, 100))\n\nkernel = kernel_fn(x1, x2, 'nngp')\n```\nNote that `kernel_fn` can compute _two_ covariance matrices corresponding to the Neural Network Gaussian Process (NNGP) and Neural Tangent (NT) kernels respectively. The NNGP kernel corresponds to the _Bayesian_ infinite neural network [[1-5]](#5-deep-neural-networks-as-gaussian-processes-iclr-2018-jaehoon-lee-yasaman-bahri-roman-novak-samuel-s-schoenholz-jeffrey-pennington-jascha-sohl-dickstein). The NTK corresponds to the _(continuous) gradient descent trained_ infinite network [[10]](#10-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-neurips-2018-arthur-jacot-franck-gabriel-cl\u00e9ment-hongler). In the above example, we compute the NNGP kernel but we could compute the NTK or both:\n\n```python\n# Get kernel of a single type\nnngp = kernel_fn(x1, x2, 'nngp') # (10, 20) np.ndarray\nntk = kernel_fn(x1, x2, 'ntk') # (10, 20) np.ndarray\n\n# Get kernels as a namedtuple\nboth = kernel_fn(x1, x2, ('nngp', 'ntk'))\nboth.nngp == nngp  # True\nboth.ntk == ntk  # True\n\n# Unpack the kernels namedtuple\nnngp, ntk = kernel_fn(x1, x2, ('nngp', 'ntk'))\n```\n\nAdditionally, if no third-argument is specified then the `kernel_fn` will return a `Kernel` namedtuple that contains additional metadata. This can be useful for composing applications of `kernel_fn` as follows:\n\n```python\nkernel = kernel_fn(x1, x2)\nkernel = kernel_fn(kernel)\nprint(kernel.nngp)\n```\n\nDoing inference with infinite networks trained on MSE loss reduces to classical GP inference, for which we also provide convenient tools:\n\n```python\nimport neural_tangents as nt\n\nx_train, x_test = x1, x2\ny_train = random.uniform(key1, shape=(10, 1))  # training targets\n\ny_test_nngp = nt.predict.gp_inference(kernel_fn, x_train, y_train, x_test,\n                                      get='nngp')\n# (20, 1) np.ndarray test predictions of an infinite Bayesian network\n\ny_test_ntk = nt.predict.gp_inference(kernel_fn, x_train, y_train, x_test,\n                                     get='ntk')\n# (20, 1) np.ndarray test predictions of an infinite continuous\n# gradient descent trained network at convergence (t = inf)\n```\n\n\n### Infinitely WideResnet\n\nWe can define a more compex, (infinitely) Wide Residual Network [[14]](#14-wide-residual-networks-bmvc-2018-sergey-zagoruyko-nikos-komodakis) using the same `nt.stax` building blocks:\n\n```python\nfrom neural_tangents import stax\n\ndef WideResnetBlock(channels, strides=(1, 1), channel_mismatch=False):\n  Main = stax.serial(\n      stax.Relu(), stax.Conv(channels, (3, 3), strides, padding='SAME'),\n      stax.Relu(), stax.Conv(channels, (3, 3), padding='SAME'))\n  Shortcut = stax.Identity() if not channel_mismatch else stax.Conv(\n      channels, (3, 3), strides, padding='SAME')\n  return stax.serial(stax.FanOut(2),\n                     stax.parallel(Main, Shortcut),\n                     stax.FanInSum())\n\ndef WideResnetGroup(n, channels, strides=(1, 1)):\n  blocks = []\n  blocks += [WideResnetBlock(channels, strides, channel_mismatch=True)]\n  for _ in range(n - 1):\n    blocks += [WideResnetBlock(channels, (1, 1))]\n  return stax.serial(*blocks)\n\ndef WideResnet(block_size, k, num_classes):\n  return stax.serial(\n      stax.Conv(16, (3, 3), padding='SAME'),\n      WideResnetGroup(block_size, int(16 * k)),\n      WideResnetGroup(block_size, int(32 * k), (2, 2)),\n      WideResnetGroup(block_size, int(64 * k), (2, 2)),\n      stax.AvgPool((8, 8)),\n      stax.Flatten(),\n      stax.Dense(num_classes, 1., 0.))\n\ninit_fn, apply_fn, kernel_fn = WideResnet(block_size=4, k=1, num_classes=10)\n```\n\n\n## Package description\n\nThe `neural_tangents` (`nt`) package contains the following modules and methods:\n\n* `stax` - primitives to construct neural networks like `Conv`, `Relu`, `serial`, `parallel` etc.\n\n* `predict` - predictions with infinite networks:\n\n  * `predict.gp_inference` - either fully Bayesian inference (`get='nngp'`) or inference with a network trained to full convergence (infinite time) on MSE loss using continuous gradient descent (`get='ntk'`).\n\n  * `predict.gradient_descent_mse` - inference with a network trained on MSE loss with continuous gradient descent for an arbitrary finite time.\n\n  * `predict.gradient_descent` - inference with a network trained on arbitrary loss with continuous gradient descent for an arbitrary finite time (using an ODE solver).\n\n  * `predict.momentum` - inference with a network trained on arbitrary loss with continuous momentum gradient descent for an arbitrary finite time (using an ODE solver).\n\n* `monte_carlo_kernel_fn` - compute a Monte Carlo kernel estimate  of _any_ `(init_fn, apply_fn)`, not necessarily specified `nt.stax`, enabling the kernel computation of infinite networks without closed-form expressions.\n\n* Tools to investigate training dynamics of _wide but finite_ neural networks, like `linearize`, `taylor_expand`, `empirical_kernel_fn` and more. See [Training dynamics of wide but finite networks](#training-dynamics-of-wide-but-finite-networks) for details.\n\n\n## Technical gotchas\n\n\n### 64-bit precision\nTo enable 64-bit precision, set the respective JAX flag _before_ importing `neural_tangents` (see the JAX [guide](https://colab.research.google.com/github/google/jax/blob/master/notebooks/Common_Gotchas_in_JAX.ipynb#scrollTo=YTktlwTTMgFl)), for example:\n\n```python\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport neural_tangents as nt  # 64-bit precision enabled\n```\n\n\n### [`nt.stax`](https://github.com/google/neural-tangents/blob/master/neural_tangents/stax.py) vs [`jax.experimental.stax`](https://github.com/google/jax/blob/master/jax/experimental/stax.py)\nWe remark the following differences between our library and the JAX one.\n\n* All `nt.stax` layers are instantiated with a function call, i.e. `nt.stax.Relu()` vs `jax.experimental.stax.Relu`.\n* All layers with trainable parameters use the _NTK parameterization_ by default (see [[10]](#10-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-neurips-2018-arthur-jacot-franck-gabriel-cl\u00e9ment-hongler), Remark 1). However, Dense and Conv layers also support the _standard parameterization_ via a `parameterization` keyword argument (see [[15]](#15-on-the-infinite-width-limit-of-neural-networks-with-a-standard-parameterization-arxiv-2020-jascha-sohl-dickstein-roman-novak-samuel-s-schoenholz-jaehoon-lee)).\n* `nt.stax` and `jax.experimental.stax` may have different layers and options available (for example `nt.stax` layers support `CIRCULAR` padding, have `LayerNorm`, but no `BatchNorm`.).\n\n\n### CPU and TPU performance\n\nFor CNNs w/ pooling, our CPU and TPU performance is suboptimal due to low core\nutilization (10-20%, looks like an XLA:CPU issue), and excessive padding\nrespectively. We will look into improving performance, but recommend NVIDIA GPUs\nin the meantime. See [Performance](#performance).\n\n\n## Training dynamics of wide but finite networks\n\nThe kernel of an infinite network `kernel_fn(x1, x2).ntk` combined with  `nt.predict.gradient_descent_mse` together allow to analytically track the outputs of an infinitely wide neural network trained on MSE loss througout training. Here we discuss the implications for _wide but finite_ neural networks and present tools to study their evolution in _weight space_ (trainable parameters of the network) and _function space_ (outputs of the network).\n\n### Weight space\n\nContinuous gradient descent in an infinite network has been shown in [[11]](#11-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent-neurips-2019-jaehoon-lee-lechao-xiao-samuel-s-schoenholz-yasaman-bahri-roman-novak-jascha-sohl-dickstein-jeffrey-pennington) to correspond to training a _linear_ (in trainable parameters) model, which makes linearized neural networks an important subject of study for understanding the behavior of parameters in wide models.\n\nFor this, we provide two convenient methods:\n\n* `nt.linearize`, and\n* `nt.taylor_expand`,\n\nwhich allow to linearize or get an arbitrary-order Taylor expansion of any function `apply_fn(params, x)` around some initial parameters `params_0` as `apply_fn_lin = nt.linearize(apply_fn, params_0)`.\n\nOne can use `apply_fn_lin(params, x)` exactly as you would any other function\n(including as an input to JAX optimizers). This makes it easy to compare the\ntraining trajectory of neural networks with that of its linearization.\nPrevious theory and experiments have examined the linearization of neural\nnetworks from inputs to logits or pre-activations, rather than from inputs to\npost-activations which are substantially more nonlinear.\n\n#### Example:\n\n```python\nimport jax.numpy as np\nimport neural_tangents as nt\n\ndef apply_fn(params, x):\n  W, b = params\n  return np.dot(x, W) + b\n\nW_0 = np.array([[1., 0.], [0., 1.]])\nb_0 = np.zeros((2,))\n\napply_fn_lin = nt.linearize(apply_fn, (W_0, b_0))\nW = np.array([[1.5, 0.2], [0.1, 0.9]])\nb = b_0 + 0.2\n\nx = np.array([[0.3, 0.2], [0.4, 0.5], [1.2, 0.2]])\nlogits = apply_fn_lin((W, b), x)  # (3, 2) np.ndarray\n```\n\n### Function space:\n\nOutputs of a linearized model evolve identically to those of an infinite one [[11]](#11-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent-neurips-2019-jaehoon-lee-lechao-xiao-samuel-s-schoenholz-yasaman-bahri-roman-novak-jascha-sohl-dickstein-jeffrey-pennington) but with a different kernel - specifically, the Neural Tangent Kernel [[10]](#10-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-neurips-2018-arthur-jacot-franck-gabriel-cl\u00e9ment-hongler) evaluated on the specific `apply_fn` of the finite network given specific `params_0` that the network is initialized with. For this we provide the `nt.empirical_kernel_fn` function that accepts any `apply_fn` and returns a `kernel_fn(x1, x2, params)` that allows to compute the empirical NTK and NNGP kernels on specific `params`.\n\n#### Example:\n\n```python\nimport jax.random as random\nimport jax.numpy as np\nimport neural_tangents as nt\n\ndef apply_fn(params, x):\n  W, b = params\n  return np.dot(x, W) + b\n\nW_0 = np.array([[1., 0.], [0., 1.]])\nb_0 = np.zeros((2,))\nparams = (W_0, b_0)\n\nkey1, key2 = random.split(random.PRNGKey(1), 2)\nx_train = random.normal(key1, (3, 2))\nx_test = random.normal(key2, (4, 2))\ny_train = random.uniform(key1, shape=(3, 2))\n\nkernel_fn = nt.empirical_kernel_fn(apply_fn)\nntk_train_train = kernel_fn(x_train, x_train, params, 'ntk')\nntk_test_train = kernel_fn(x_test, x_train, params, 'ntk')\nmse_predictor = nt.predict.gradient_descent_mse(\n    ntk_train_train, y_train, ntk_test_train)\n\nt = 5.\ny_train_0 = apply_fn(params, x_train)\ny_test_0 = apply_fn(params, x_test)\ny_train_t, y_test_t = mse_predictor(t, y_train_0, y_test_0)\n# (3, 2) and (4, 2) np.ndarray train and test outputs after `t` units of time\n# training with continuous gradient descent\n```\n\n### What to Expect\n\nThe success or failure of the linear approximation is highly architecture\ndependent. However, some rules of thumb that we've observed are:\n\n1. Convergence as the network size increases.\n\n   * For fully-connected networks one generally observes very strong\n     agreement by the time the layer-width is 512 (RMSE of about 0.05 at the\n     end of training).\n\n   * For convolutional networks one generally observes reasonable agreement\n     agreement by the time the number of channels is 512.\n\n2. Convergence at small learning rates.\n\nWith a new model it is therefore advisable to start with a very large model on\na small dataset using a small learning rate.\n\n\n## Performance\n\nIn the table below we measure time to compute a single NTK\nentry in a 21-layer CNN (`3x3` filters, no strides, `SAME` padding, `ReLU`)\nfollowed by `stax.GlobalAvgPool` on inputs of shape `3x32x32`. Precisely:\n\n```python\nlayers = []\nfor _ in range(21):\n  layers += [stax.Conv(1, (3, 3), (1, 1), 'SAME'), stax.Relu()]\nlayers += [stax.GlobalAvgPool()]\n_, _, kernel_fn = stax.serial(*layers)\n```\n\n\n| Platform                    | Precision | Milliseconds / NTK entry | Max batch size (`NxN`) |\n|-----------------------------|-----------|--------------------------|------------------------|\n| CPU, >56 cores, >700 Gb RAM | 32        |  112.90                  | >= 128                 |\n| CPU, >56 cores, >700 Gb RAM | 64        |  258.55                  |    95 (fastest - 72)   |\n| TPU v2                      | 32/16     |  3.2550                  |    16                  |\n| TPU v3                      | 32/16     |  2.3022                  |    24                  |\n| NVIDIA P100                 | 32        |  5.9433                  |    26                  |\n| NVIDIA P100                 | 64        |  11.349                  |    18                  |\n| NVIDIA V100                 | 32        |  2.7001                  |    26                  |\n| NVIDIA V100                 | 64        |  6.2058                  |    18                  |\n\n\n\nTested using version 0.2.1. All GPU results are per single accelerator.\nNote that runtime is proportional to the depth of your network.\nIf your performance differs significantly,\nplease [file a bug](https://github.com/google/neural-tangents/issues/new)!\n\n\n## Papers\n\nNeural Tangents has been used in the following papers:\n\n* [The large learning rate phase of deep learning: the catapult mechanism.](https://arxiv.org/abs/2003.02218) \\\nAitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, Guy Gur-Ari\n\n* [Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks.\n](https://arxiv.org/abs/2002.02561) \\\nBlake Bordelon, Abdulkadir Canatar, Cengiz Pehlevan\n\n* [Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width.](https://arxiv.org/abs/2002.04010) \\\n   Yu Bai, Ben Krause, Huan Wang, Caiming Xiong, Richard Socher\n\n* [On the Infinite Width Limit of Neural Networks with a Standard Parameterization.](https://arxiv.org/pdf/2001.07301.pdf) \\\nJascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, Jaehoon Lee\n\n* [Disentangling Trainability and Generalization in Deep Learning.](https://arxiv.org/abs/1912.13053) \\\nLechao Xiao, Jeffrey Pennington, Samuel S. Schoenholz\n\n* [Information in Infinite Ensembles of Infinitely-Wide Neural Networks.](https://arxiv.org/abs/1911.09189) \\\nRavid Shwartz-Ziv, Alexander A. Alemi\n\n* [Training Dynamics of Deep Networks using Stochastic Gradient Descent via Neural Tangent Kernel.](https://arxiv.org/abs/1905.13654) \\\nSoufiane Hayou, Arnaud Doucet, Judith Rousseau\n\n* [Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient\nDescent.](https://arxiv.org/abs/1902.06720) \\\nJaehoon Lee*, Lechao Xiao*, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha\nSohl-Dickstein, Jeffrey Pennington\n\nPlease let us know if you make use of the code in a publication and we'll add it\nto the list!\n\n\n## Citation\n\nIf you use the code in a publication, please cite our ICLR 2020 paper:\n\n```\n@inproceedings{neuraltangents2020,\n    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\n    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\n    booktitle={International Conference on Learning Representations},\n    year={2020},\n    url={https://github.com/google/neural-tangents}\n}\n```\n\n\n\n## References\n\n##### [1] [Priors for Infinite Networks.](https://www.cs.toronto.edu/~radford/pin.abstract.html) Radford M. Neal\n\n##### [2] [Exponential expressivity in deep neural networks through transient chaos.](https://arxiv.org/abs/1606.05340) *NeurIPS 2016.* Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, Surya Ganguli\n\n##### [3] [Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity.](http://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity) *NeurIPS 2016.* Amit Daniely, Roy Frostig, Yoram Singer\n\n##### [4] [Deep Information Propagation.](https://arxiv.org/abs/1611.01232) *ICLR 2017.* Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, Jascha Sohl-Dickstein\n\n##### [5] [Deep Neural Networks as Gaussian Processes.](https://arxiv.org/abs/1806.07572) *ICLR 2018.* Jaehoon Lee*, Yasaman Bahri*, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein\n\n##### [6] [Gaussian Process Behaviour in Wide Deep Neural Networks.](https://arxiv.org/abs/1804.11271) *ICLR 2018.* Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, Zoubin Ghahramani\n\n##### [7] [Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks.](https://arxiv.org/abs/1806.05393) *ICML 2018.* Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, Jeffrey Pennington\n\n##### [8] [Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes.](https://arxiv.org/abs/1810.05148) *ICLR 2019.* Roman Novak*, Lechao Xiao*, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein\n\n##### [9] [Deep Convolutional Networks as shallow Gaussian Processes.](https://arxiv.org/abs/1808.05587) *ICLR 2019.* Adri\u00e0 Garriga-Alonso, Carl Edward Rasmussen, Laurence Aitchison\n\n##### [10] [Neural Tangent Kernel: Convergence and Generalization in Neural Networks.](https://arxiv.org/abs/1806.07572) *NeurIPS 2018.* Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler\n\n##### [11] [Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent.](https://arxiv.org/abs/1902.06720) *NeurIPS 2019.* Jaehoon Lee*, Lechao Xiao*, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington\n\n##### [12] [Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation.](https://arxiv.org/abs/1902.04760) *arXiv 2019.* Greg Yang\n\n##### [13] [Mean Field Residual Networks: On the Edge of Chaos.](https://arxiv.org/abs/1712.08969) *NeurIPS 2017.* Greg Yang, Samuel S. Schoenholz\n\n##### [14] [Wide Residual Networks.](https://arxiv.org/abs/1605.07146) *BMVC 2018.* Sergey Zagoruyko, Nikos Komodakis\n\n##### [15] [On the Infinite Width Limit of Neural Networks with a Standard Parameterization.](https://arxiv.org/pdf/2001.07301.pdf) *arXiv 2020.* Jascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, Jaehoon Lee\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://pypi.org/project/neural-tangents/", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/google/neural-tangents", "keywords": "", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "neural-tangents", "package_url": "https://pypi.org/project/neural-tangents/", "platform": "", "project_url": "https://pypi.org/project/neural-tangents/", "project_urls": {"Bug Tracker": "https://github.com/google/neural-tangents/issues", "Documentation": "https://arxiv.org/abs/1912.02803", "Download": "https://pypi.org/project/neural-tangents/", "Homepage": "https://github.com/google/neural-tangents", "Source Code": "https://github.com/google/neural-tangents"}, "release_url": "https://pypi.org/project/neural-tangents/0.2.1/", "requires_dist": ["jax (>=0.1.58)", "frozendict", "dataclasses"], "requires_python": ">=3.6", "summary": "Fast and Easy Infinite Neural Networks in Python", "version": "0.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Neural Tangents [<a href=\"https://arxiv.org/abs/1912.02803\" rel=\"nofollow\">arXiv</a>]</h1>\n<p><a href=\"#colab-notebooks\" rel=\"nofollow\"><strong>Quickstart</strong></a>\n| <a href=\"#installation\" rel=\"nofollow\"><strong>Install guide</strong></a>\n| <a href=\"https://neural-tangents.readthedocs.io/en/latest/\" rel=\"nofollow\"><strong>Reference docs</strong></a></p>\n<p><a href=\"https://pypi.org/project/neural-tangents/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e7b168cea02b942972708190f2fa115d4a593744/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6e657572616c2d74616e67656e7473\"></a> <a href=\"https://pypi.org/project/neural-tangents/\" rel=\"nofollow\"><img alt=\"PyPI - Python Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/598fc9b6d82747859ab8706f15ef59d369213d4c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6e657572616c2d74616e67656e7473\"></a>\n<a href=\"https://travis-ci.org/google/neural-tangents\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e13ce61ef12d51533f0d0ba10c9bee20f51d1a6e/68747470733a2f2f7472617669732d63692e6f72672f676f6f676c652f6e657572616c2d74616e67656e74732e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://neural-tangents.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Readthedocs\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/95f2f280a0df7c8f15972495376c710ee8f3ba36/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6e657572616c2d74616e67656e74732f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<a href=\"https://github.com/google/neural-tangents/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"PyPI - License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/44f87468f2102e111d7634bfdeb016b8d31c6249/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6e657572616c5f74616e67656e7473\"></a></p>\n<p><strong>News:</strong></p>\n<ul>\n<li>\n<p>Neural Tangents just got faster! &gt;4X speedup in computing analytic\nkernels for CNN architectures with pooling, starting from version 0.2.1. See our\n<a href=\"#performance\" rel=\"nofollow\">Performance</a>.</p>\n</li>\n<li>\n<p>We will be at <a href=\"https://iclr.cc/\" rel=\"nofollow\">ICLR 2020</a>, stay tuned for our live session\ntime slots.</p>\n</li>\n</ul>\n<h2>Overview</h2>\n<p>Neural Tangents is a high-level neural network API for specifying complex, hierarchical, neural networks of both finite and <em>infinite</em> width. Neural Tangents allows researchers to define, train, and evaluate infinite networks as easily as finite ones.</p>\n<p>Infinite (in width or channel count) neural networks are Gaussian Processes (GPs) with a kernel function determined by their architecture (see <a href=\"#references\" rel=\"nofollow\">References</a> for details and nuances of this correspondence).</p>\n<p>Neural Tangents allows you to construct a neural network model with the usual building blocks like convolutions, pooling, residual connections, nonlinearities etc. and obtain not only the finite model, but also the kernel function of the respective GP.</p>\n<p>The library is written in python using <a href=\"https://github.com/google/jax\" rel=\"nofollow\">JAX</a> and leveraging <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/index.md\" rel=\"nofollow\">XLA</a> to run out-of-the-box on CPU, GPU, or TPU. Kernel computation is highly optimized for speed and memory efficiency, and can be automatically distributed over multiple accelerators with near-perfect scaling.</p>\n<p>Neural Tangents is a work in progress.\nWe happily welcome contributions!</p>\n<h2>Contents</h2>\n<ul>\n<li><a href=\"#colab-notebooks\" rel=\"nofollow\">Colab Notebooks</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#5-minute-intro\" rel=\"nofollow\">5-Minute intro</a></li>\n<li><a href=\"#package-description\" rel=\"nofollow\">Package description</a></li>\n<li><a href=\"#technical-gotchas\" rel=\"nofollow\">Technical gotchas</a></li>\n<li><a href=\"#training-dynamics-of-wide-but-finite-networks\" rel=\"nofollow\">Training dynamics of wide but finite networks</a></li>\n<li><a href=\"#performance\" rel=\"nofollow\">Performance</a></li>\n<li><a href=\"#papers\" rel=\"nofollow\">Papers</a></li>\n<li><a href=\"#citation\" rel=\"nofollow\">Citation</a></li>\n<li><a href=\"#references\" rel=\"nofollow\">References</a></li>\n</ul>\n<h2>Colab Notebooks</h2>\n<p>An easy way to get started with Neural Tangents is by playing around with the following interactive notebooks in Colaboratory. They demo the major features of Neural Tangents and show how it can be used in research.</p>\n<ul>\n<li><a href=\"https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\" rel=\"nofollow\">Neural Tangents Cookbook</a></li>\n<li><a href=\"https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/weight_space_linearization.ipynb\" rel=\"nofollow\">Weight Space Linearization</a></li>\n<li><a href=\"https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/function_space_linearization.ipynb\" rel=\"nofollow\">Function Space Linearization</a></li>\n<li><a href=\"https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/phase_diagram.ipynb\" rel=\"nofollow\">Neural Network Phase Diagram</a></li>\n</ul>\n<h2>Installation</h2>\n<p>To use GPU, first follow <a href=\"https://www.github.com/google/jax/\" rel=\"nofollow\">JAX's</a> GPU installation instructions. Otherwise, install JAX on CPU by running</p>\n<pre><code>pip install jaxlib jax --upgrade\n</code></pre>\n<p>Once JAX is installed install Neural Tangents by running</p>\n<pre><code>pip install neural-tangents\n</code></pre>\n<p>or, to use the bleeding-edge version from GitHub source,</p>\n<pre><code>git clone https://github.com/google/neural-tangents\npip install -e neural-tangents\n</code></pre>\n<p>You can now run the examples (using <a href=\"https://github.com/tensorflow/datasets\" rel=\"nofollow\"><code>tensorflow_datasets</code></a>) by calling:</p>\n<pre><code># Note that Tensorflow does not work with Python 3.8\n# https://github.com/tensorflow/tensorflow/issues/33374\npip install tensorflow \"tensorflow-datasets&gt;=2.0.0\"\n\npython neural-tangents/examples/infinite_fcn.py\npython neural-tangents/examples/weight_space.py\npython neural-tangents/examples/function_space.py\n</code></pre>\n<p>Finally, you can run tests by calling:</p>\n<pre><code># NOTE: a few tests will fail without\n# pip install tensorflow \"tensorflow-datasets&gt;=2.0.0\"\n\nfor f in neural-tangents/neural_tangents/tests/*.py; do python $f; done\n</code></pre>\n<h2>5-Minute intro</h2>\n<p><b>See this <a href=\"https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\" rel=\"nofollow\">Colab</a> for a detailed tutorial. Below is a very quick introduction.</b></p>\n<p>Our library closely follows JAX's API for specifying neural networks,  <a href=\"https://github.com/google/jax/blob/master/jax/experimental/stax.py\" rel=\"nofollow\"><code>stax</code></a>. In <code>stax</code> a network is defined by a pair of functions <code>(init_fn, apply_fn)</code> initializing the trainable parameters and computing the outputs of the network respectively. Below is an example of defining a 3-layer network and computing it's outputs <code>y</code> given inputs <code>x</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">jax</span> <span class=\"kn\">import</span> <span class=\"n\">random</span>\n<span class=\"kn\">from</span> <span class=\"nn\">jax.experimental</span> <span class=\"kn\">import</span> <span class=\"n\">stax</span>\n\n<span class=\"n\">init_fn</span><span class=\"p\">,</span> <span class=\"n\">apply_fn</span> <span class=\"o\">=</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Relu</span><span class=\"p\">,</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Relu</span><span class=\"p\">,</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">key</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">PRNGKey</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">))</span>\n<span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">init_fn</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">apply_fn</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">)</span>  <span class=\"c1\"># (10, 1) np.ndarray outputs of the neural network</span>\n</pre>\n<p>Neural Tangents is designed to serve as a drop-in replacement for <code>stax</code>, extending the <code>(init_fn, apply_fn)</code> tuple to a triple <code>(init_fn, apply_fn, kernel_fn)</code>, where <code>kernel_fn</code> is the kernel function of the infinite network (GP) of the given architecture. Below is an example of computing the covariances of the GP between two batches of inputs <code>x1</code> and <code>x2</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">jax</span> <span class=\"kn\">import</span> <span class=\"n\">random</span>\n<span class=\"kn\">from</span> <span class=\"nn\">neural_tangents</span> <span class=\"kn\">import</span> <span class=\"n\">stax</span>\n\n<span class=\"n\">init_fn</span><span class=\"p\">,</span> <span class=\"n\">apply_fn</span><span class=\"p\">,</span> <span class=\"n\">kernel_fn</span> <span class=\"o\">=</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Relu</span><span class=\"p\">(),</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Relu</span><span class=\"p\">(),</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">key1</span><span class=\"p\">,</span> <span class=\"n\">key2</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">PRNGKey</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">x1</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">key1</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">))</span>\n<span class=\"n\">x2</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">key2</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">))</span>\n\n<span class=\"n\">kernel</span> <span class=\"o\">=</span> <span class=\"n\">kernel_fn</span><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span><span class=\"p\">,</span> <span class=\"s1\">'nngp'</span><span class=\"p\">)</span>\n</pre>\n<p>Note that <code>kernel_fn</code> can compute <em>two</em> covariance matrices corresponding to the Neural Network Gaussian Process (NNGP) and Neural Tangent (NT) kernels respectively. The NNGP kernel corresponds to the <em>Bayesian</em> infinite neural network <a href=\"#5-deep-neural-networks-as-gaussian-processes-iclr-2018-jaehoon-lee-yasaman-bahri-roman-novak-samuel-s-schoenholz-jeffrey-pennington-jascha-sohl-dickstein\" rel=\"nofollow\">[1-5]</a>. The NTK corresponds to the <em>(continuous) gradient descent trained</em> infinite network <a href=\"#10-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-neurips-2018-arthur-jacot-franck-gabriel-cl%C3%A9ment-hongler\" rel=\"nofollow\">[10]</a>. In the above example, we compute the NNGP kernel but we could compute the NTK or both:</p>\n<pre><span class=\"c1\"># Get kernel of a single type</span>\n<span class=\"n\">nngp</span> <span class=\"o\">=</span> <span class=\"n\">kernel_fn</span><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span><span class=\"p\">,</span> <span class=\"s1\">'nngp'</span><span class=\"p\">)</span> <span class=\"c1\"># (10, 20) np.ndarray</span>\n<span class=\"n\">ntk</span> <span class=\"o\">=</span> <span class=\"n\">kernel_fn</span><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span><span class=\"p\">,</span> <span class=\"s1\">'ntk'</span><span class=\"p\">)</span> <span class=\"c1\"># (10, 20) np.ndarray</span>\n\n<span class=\"c1\"># Get kernels as a namedtuple</span>\n<span class=\"n\">both</span> <span class=\"o\">=</span> <span class=\"n\">kernel_fn</span><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s1\">'nngp'</span><span class=\"p\">,</span> <span class=\"s1\">'ntk'</span><span class=\"p\">))</span>\n<span class=\"n\">both</span><span class=\"o\">.</span><span class=\"n\">nngp</span> <span class=\"o\">==</span> <span class=\"n\">nngp</span>  <span class=\"c1\"># True</span>\n<span class=\"n\">both</span><span class=\"o\">.</span><span class=\"n\">ntk</span> <span class=\"o\">==</span> <span class=\"n\">ntk</span>  <span class=\"c1\"># True</span>\n\n<span class=\"c1\"># Unpack the kernels namedtuple</span>\n<span class=\"n\">nngp</span><span class=\"p\">,</span> <span class=\"n\">ntk</span> <span class=\"o\">=</span> <span class=\"n\">kernel_fn</span><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s1\">'nngp'</span><span class=\"p\">,</span> <span class=\"s1\">'ntk'</span><span class=\"p\">))</span>\n</pre>\n<p>Additionally, if no third-argument is specified then the <code>kernel_fn</code> will return a <code>Kernel</code> namedtuple that contains additional metadata. This can be useful for composing applications of <code>kernel_fn</code> as follows:</p>\n<pre><span class=\"n\">kernel</span> <span class=\"o\">=</span> <span class=\"n\">kernel_fn</span><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span><span class=\"p\">)</span>\n<span class=\"n\">kernel</span> <span class=\"o\">=</span> <span class=\"n\">kernel_fn</span><span class=\"p\">(</span><span class=\"n\">kernel</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">kernel</span><span class=\"o\">.</span><span class=\"n\">nngp</span><span class=\"p\">)</span>\n</pre>\n<p>Doing inference with infinite networks trained on MSE loss reduces to classical GP inference, for which we also provide convenient tools:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">neural_tangents</span> <span class=\"k\">as</span> <span class=\"nn\">nt</span>\n\n<span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span>\n<span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">uniform</span><span class=\"p\">(</span><span class=\"n\">key1</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>  <span class=\"c1\"># training targets</span>\n\n<span class=\"n\">y_test_nngp</span> <span class=\"o\">=</span> <span class=\"n\">nt</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"o\">.</span><span class=\"n\">gp_inference</span><span class=\"p\">(</span><span class=\"n\">kernel_fn</span><span class=\"p\">,</span> <span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span><span class=\"p\">,</span>\n                                      <span class=\"n\">get</span><span class=\"o\">=</span><span class=\"s1\">'nngp'</span><span class=\"p\">)</span>\n<span class=\"c1\"># (20, 1) np.ndarray test predictions of an infinite Bayesian network</span>\n\n<span class=\"n\">y_test_ntk</span> <span class=\"o\">=</span> <span class=\"n\">nt</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"o\">.</span><span class=\"n\">gp_inference</span><span class=\"p\">(</span><span class=\"n\">kernel_fn</span><span class=\"p\">,</span> <span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span><span class=\"p\">,</span>\n                                     <span class=\"n\">get</span><span class=\"o\">=</span><span class=\"s1\">'ntk'</span><span class=\"p\">)</span>\n<span class=\"c1\"># (20, 1) np.ndarray test predictions of an infinite continuous</span>\n<span class=\"c1\"># gradient descent trained network at convergence (t = inf)</span>\n</pre>\n<h3>Infinitely WideResnet</h3>\n<p>We can define a more compex, (infinitely) Wide Residual Network <a href=\"#14-wide-residual-networks-bmvc-2018-sergey-zagoruyko-nikos-komodakis\" rel=\"nofollow\">[14]</a> using the same <code>nt.stax</code> building blocks:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">neural_tangents</span> <span class=\"kn\">import</span> <span class=\"n\">stax</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">WideResnetBlock</span><span class=\"p\">(</span><span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">channel_mismatch</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">):</span>\n  <span class=\"n\">Main</span> <span class=\"o\">=</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span>\n      <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Relu</span><span class=\"p\">(),</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">'SAME'</span><span class=\"p\">),</span>\n      <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Relu</span><span class=\"p\">(),</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">'SAME'</span><span class=\"p\">))</span>\n  <span class=\"n\">Shortcut</span> <span class=\"o\">=</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Identity</span><span class=\"p\">()</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">channel_mismatch</span> <span class=\"k\">else</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span>\n      <span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">'SAME'</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span><span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">FanOut</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n                     <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">parallel</span><span class=\"p\">(</span><span class=\"n\">Main</span><span class=\"p\">,</span> <span class=\"n\">Shortcut</span><span class=\"p\">),</span>\n                     <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">FanInSum</span><span class=\"p\">())</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">WideResnetGroup</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)):</span>\n  <span class=\"n\">blocks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n  <span class=\"n\">blocks</span> <span class=\"o\">+=</span> <span class=\"p\">[</span><span class=\"n\">WideResnetBlock</span><span class=\"p\">(</span><span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"n\">strides</span><span class=\"p\">,</span> <span class=\"n\">channel_mismatch</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)]</span>\n  <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n    <span class=\"n\">blocks</span> <span class=\"o\">+=</span> <span class=\"p\">[</span><span class=\"n\">WideResnetBlock</span><span class=\"p\">(</span><span class=\"n\">channels</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))]</span>\n  <span class=\"k\">return</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">blocks</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">WideResnet</span><span class=\"p\">(</span><span class=\"n\">block_size</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">):</span>\n  <span class=\"k\">return</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span>\n      <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">'SAME'</span><span class=\"p\">),</span>\n      <span class=\"n\">WideResnetGroup</span><span class=\"p\">(</span><span class=\"n\">block_size</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mi\">16</span> <span class=\"o\">*</span> <span class=\"n\">k</span><span class=\"p\">)),</span>\n      <span class=\"n\">WideResnetGroup</span><span class=\"p\">(</span><span class=\"n\">block_size</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mi\">32</span> <span class=\"o\">*</span> <span class=\"n\">k</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)),</span>\n      <span class=\"n\">WideResnetGroup</span><span class=\"p\">(</span><span class=\"n\">block_size</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mi\">64</span> <span class=\"o\">*</span> <span class=\"n\">k</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)),</span>\n      <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">AvgPool</span><span class=\"p\">((</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">)),</span>\n      <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Flatten</span><span class=\"p\">(),</span>\n      <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">))</span>\n\n<span class=\"n\">init_fn</span><span class=\"p\">,</span> <span class=\"n\">apply_fn</span><span class=\"p\">,</span> <span class=\"n\">kernel_fn</span> <span class=\"o\">=</span> <span class=\"n\">WideResnet</span><span class=\"p\">(</span><span class=\"n\">block_size</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<h2>Package description</h2>\n<p>The <code>neural_tangents</code> (<code>nt</code>) package contains the following modules and methods:</p>\n<ul>\n<li>\n<p><code>stax</code> - primitives to construct neural networks like <code>Conv</code>, <code>Relu</code>, <code>serial</code>, <code>parallel</code> etc.</p>\n</li>\n<li>\n<p><code>predict</code> - predictions with infinite networks:</p>\n<ul>\n<li>\n<p><code>predict.gp_inference</code> - either fully Bayesian inference (<code>get='nngp'</code>) or inference with a network trained to full convergence (infinite time) on MSE loss using continuous gradient descent (<code>get='ntk'</code>).</p>\n</li>\n<li>\n<p><code>predict.gradient_descent_mse</code> - inference with a network trained on MSE loss with continuous gradient descent for an arbitrary finite time.</p>\n</li>\n<li>\n<p><code>predict.gradient_descent</code> - inference with a network trained on arbitrary loss with continuous gradient descent for an arbitrary finite time (using an ODE solver).</p>\n</li>\n<li>\n<p><code>predict.momentum</code> - inference with a network trained on arbitrary loss with continuous momentum gradient descent for an arbitrary finite time (using an ODE solver).</p>\n</li>\n</ul>\n</li>\n<li>\n<p><code>monte_carlo_kernel_fn</code> - compute a Monte Carlo kernel estimate  of <em>any</em> <code>(init_fn, apply_fn)</code>, not necessarily specified <code>nt.stax</code>, enabling the kernel computation of infinite networks without closed-form expressions.</p>\n</li>\n<li>\n<p>Tools to investigate training dynamics of <em>wide but finite</em> neural networks, like <code>linearize</code>, <code>taylor_expand</code>, <code>empirical_kernel_fn</code> and more. See <a href=\"#training-dynamics-of-wide-but-finite-networks\" rel=\"nofollow\">Training dynamics of wide but finite networks</a> for details.</p>\n</li>\n</ul>\n<h2>Technical gotchas</h2>\n<h3>64-bit precision</h3>\n<p>To enable 64-bit precision, set the respective JAX flag <em>before</em> importing <code>neural_tangents</code> (see the JAX <a href=\"https://colab.research.google.com/github/google/jax/blob/master/notebooks/Common_Gotchas_in_JAX.ipynb#scrollTo=YTktlwTTMgFl\" rel=\"nofollow\">guide</a>), for example:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">jax.config</span> <span class=\"kn\">import</span> <span class=\"n\">config</span>\n<span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">(</span><span class=\"s2\">\"jax_enable_x64\"</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"kn\">import</span> <span class=\"nn\">neural_tangents</span> <span class=\"k\">as</span> <span class=\"nn\">nt</span>  <span class=\"c1\"># 64-bit precision enabled</span>\n</pre>\n<h3><a href=\"https://github.com/google/neural-tangents/blob/master/neural_tangents/stax.py\" rel=\"nofollow\"><code>nt.stax</code></a> vs <a href=\"https://github.com/google/jax/blob/master/jax/experimental/stax.py\" rel=\"nofollow\"><code>jax.experimental.stax</code></a></h3>\n<p>We remark the following differences between our library and the JAX one.</p>\n<ul>\n<li>All <code>nt.stax</code> layers are instantiated with a function call, i.e. <code>nt.stax.Relu()</code> vs <code>jax.experimental.stax.Relu</code>.</li>\n<li>All layers with trainable parameters use the <em>NTK parameterization</em> by default (see <a href=\"#10-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-neurips-2018-arthur-jacot-franck-gabriel-cl%C3%A9ment-hongler\" rel=\"nofollow\">[10]</a>, Remark 1). However, Dense and Conv layers also support the <em>standard parameterization</em> via a <code>parameterization</code> keyword argument (see <a href=\"#15-on-the-infinite-width-limit-of-neural-networks-with-a-standard-parameterization-arxiv-2020-jascha-sohl-dickstein-roman-novak-samuel-s-schoenholz-jaehoon-lee\" rel=\"nofollow\">[15]</a>).</li>\n<li><code>nt.stax</code> and <code>jax.experimental.stax</code> may have different layers and options available (for example <code>nt.stax</code> layers support <code>CIRCULAR</code> padding, have <code>LayerNorm</code>, but no <code>BatchNorm</code>.).</li>\n</ul>\n<h3>CPU and TPU performance</h3>\n<p>For CNNs w/ pooling, our CPU and TPU performance is suboptimal due to low core\nutilization (10-20%, looks like an XLA:CPU issue), and excessive padding\nrespectively. We will look into improving performance, but recommend NVIDIA GPUs\nin the meantime. See <a href=\"#performance\" rel=\"nofollow\">Performance</a>.</p>\n<h2>Training dynamics of wide but finite networks</h2>\n<p>The kernel of an infinite network <code>kernel_fn(x1, x2).ntk</code> combined with  <code>nt.predict.gradient_descent_mse</code> together allow to analytically track the outputs of an infinitely wide neural network trained on MSE loss througout training. Here we discuss the implications for <em>wide but finite</em> neural networks and present tools to study their evolution in <em>weight space</em> (trainable parameters of the network) and <em>function space</em> (outputs of the network).</p>\n<h3>Weight space</h3>\n<p>Continuous gradient descent in an infinite network has been shown in <a href=\"#11-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent-neurips-2019-jaehoon-lee-lechao-xiao-samuel-s-schoenholz-yasaman-bahri-roman-novak-jascha-sohl-dickstein-jeffrey-pennington\" rel=\"nofollow\">[11]</a> to correspond to training a <em>linear</em> (in trainable parameters) model, which makes linearized neural networks an important subject of study for understanding the behavior of parameters in wide models.</p>\n<p>For this, we provide two convenient methods:</p>\n<ul>\n<li><code>nt.linearize</code>, and</li>\n<li><code>nt.taylor_expand</code>,</li>\n</ul>\n<p>which allow to linearize or get an arbitrary-order Taylor expansion of any function <code>apply_fn(params, x)</code> around some initial parameters <code>params_0</code> as <code>apply_fn_lin = nt.linearize(apply_fn, params_0)</code>.</p>\n<p>One can use <code>apply_fn_lin(params, x)</code> exactly as you would any other function\n(including as an input to JAX optimizers). This makes it easy to compare the\ntraining trajectory of neural networks with that of its linearization.\nPrevious theory and experiments have examined the linearization of neural\nnetworks from inputs to logits or pre-activations, rather than from inputs to\npost-activations which are substantially more nonlinear.</p>\n<h4>Example:</h4>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">jax.numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">neural_tangents</span> <span class=\"k\">as</span> <span class=\"nn\">nt</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">apply_fn</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n  <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">params</span>\n  <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n\n<span class=\"n\">W_0</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">]])</span>\n<span class=\"n\">b_0</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">2</span><span class=\"p\">,))</span>\n\n<span class=\"n\">apply_fn_lin</span> <span class=\"o\">=</span> <span class=\"n\">nt</span><span class=\"o\">.</span><span class=\"n\">linearize</span><span class=\"p\">(</span><span class=\"n\">apply_fn</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">W_0</span><span class=\"p\">,</span> <span class=\"n\">b_0</span><span class=\"p\">))</span>\n<span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.9</span><span class=\"p\">]])</span>\n<span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">b_0</span> <span class=\"o\">+</span> <span class=\"mf\">0.2</span>\n\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">1.2</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">]])</span>\n<span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">apply_fn_lin</span><span class=\"p\">((</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"n\">x</span><span class=\"p\">)</span>  <span class=\"c1\"># (3, 2) np.ndarray</span>\n</pre>\n<h3>Function space:</h3>\n<p>Outputs of a linearized model evolve identically to those of an infinite one <a href=\"#11-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent-neurips-2019-jaehoon-lee-lechao-xiao-samuel-s-schoenholz-yasaman-bahri-roman-novak-jascha-sohl-dickstein-jeffrey-pennington\" rel=\"nofollow\">[11]</a> but with a different kernel - specifically, the Neural Tangent Kernel <a href=\"#10-neural-tangent-kernel-convergence-and-generalization-in-neural-networks-neurips-2018-arthur-jacot-franck-gabriel-cl%C3%A9ment-hongler\" rel=\"nofollow\">[10]</a> evaluated on the specific <code>apply_fn</code> of the finite network given specific <code>params_0</code> that the network is initialized with. For this we provide the <code>nt.empirical_kernel_fn</code> function that accepts any <code>apply_fn</code> and returns a <code>kernel_fn(x1, x2, params)</code> that allows to compute the empirical NTK and NNGP kernels on specific <code>params</code>.</p>\n<h4>Example:</h4>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">jax.random</span> <span class=\"k\">as</span> <span class=\"nn\">random</span>\n<span class=\"kn\">import</span> <span class=\"nn\">jax.numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">neural_tangents</span> <span class=\"k\">as</span> <span class=\"nn\">nt</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">apply_fn</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n  <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">params</span>\n  <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n\n<span class=\"n\">W_0</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">]])</span>\n<span class=\"n\">b_0</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">2</span><span class=\"p\">,))</span>\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">W_0</span><span class=\"p\">,</span> <span class=\"n\">b_0</span><span class=\"p\">)</span>\n\n<span class=\"n\">key1</span><span class=\"p\">,</span> <span class=\"n\">key2</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">PRNGKey</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">x_train</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">key1</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">key2</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">uniform</span><span class=\"p\">(</span><span class=\"n\">key1</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n\n<span class=\"n\">kernel_fn</span> <span class=\"o\">=</span> <span class=\"n\">nt</span><span class=\"o\">.</span><span class=\"n\">empirical_kernel_fn</span><span class=\"p\">(</span><span class=\"n\">apply_fn</span><span class=\"p\">)</span>\n<span class=\"n\">ntk_train_train</span> <span class=\"o\">=</span> <span class=\"n\">kernel_fn</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"s1\">'ntk'</span><span class=\"p\">)</span>\n<span class=\"n\">ntk_test_train</span> <span class=\"o\">=</span> <span class=\"n\">kernel_fn</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"s1\">'ntk'</span><span class=\"p\">)</span>\n<span class=\"n\">mse_predictor</span> <span class=\"o\">=</span> <span class=\"n\">nt</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"o\">.</span><span class=\"n\">gradient_descent_mse</span><span class=\"p\">(</span>\n    <span class=\"n\">ntk_train_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">ntk_test_train</span><span class=\"p\">)</span>\n\n<span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"mf\">5.</span>\n<span class=\"n\">y_train_0</span> <span class=\"o\">=</span> <span class=\"n\">apply_fn</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x_train</span><span class=\"p\">)</span>\n<span class=\"n\">y_test_0</span> <span class=\"o\">=</span> <span class=\"n\">apply_fn</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x_test</span><span class=\"p\">)</span>\n<span class=\"n\">y_train_t</span><span class=\"p\">,</span> <span class=\"n\">y_test_t</span> <span class=\"o\">=</span> <span class=\"n\">mse_predictor</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"n\">y_train_0</span><span class=\"p\">,</span> <span class=\"n\">y_test_0</span><span class=\"p\">)</span>\n<span class=\"c1\"># (3, 2) and (4, 2) np.ndarray train and test outputs after `t` units of time</span>\n<span class=\"c1\"># training with continuous gradient descent</span>\n</pre>\n<h3>What to Expect</h3>\n<p>The success or failure of the linear approximation is highly architecture\ndependent. However, some rules of thumb that we've observed are:</p>\n<ol>\n<li>\n<p>Convergence as the network size increases.</p>\n<ul>\n<li>\n<p>For fully-connected networks one generally observes very strong\nagreement by the time the layer-width is 512 (RMSE of about 0.05 at the\nend of training).</p>\n</li>\n<li>\n<p>For convolutional networks one generally observes reasonable agreement\nagreement by the time the number of channels is 512.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Convergence at small learning rates.</p>\n</li>\n</ol>\n<p>With a new model it is therefore advisable to start with a very large model on\na small dataset using a small learning rate.</p>\n<h2>Performance</h2>\n<p>In the table below we measure time to compute a single NTK\nentry in a 21-layer CNN (<code>3x3</code> filters, no strides, <code>SAME</code> padding, <code>ReLU</code>)\nfollowed by <code>stax.GlobalAvgPool</code> on inputs of shape <code>3x32x32</code>. Precisely:</p>\n<pre><span class=\"n\">layers</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">21</span><span class=\"p\">):</span>\n  <span class=\"n\">layers</span> <span class=\"o\">+=</span> <span class=\"p\">[</span><span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"s1\">'SAME'</span><span class=\"p\">),</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Relu</span><span class=\"p\">()]</span>\n<span class=\"n\">layers</span> <span class=\"o\">+=</span> <span class=\"p\">[</span><span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">GlobalAvgPool</span><span class=\"p\">()]</span>\n<span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">kernel_fn</span> <span class=\"o\">=</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">layers</span><span class=\"p\">)</span>\n</pre>\n<table>\n<thead>\n<tr>\n<th>Platform</th>\n<th>Precision</th>\n<th>Milliseconds / NTK entry</th>\n<th>Max batch size (<code>NxN</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CPU, &gt;56 cores, &gt;700 Gb RAM</td>\n<td>32</td>\n<td>112.90</td>\n<td>&gt;= 128</td>\n</tr>\n<tr>\n<td>CPU, &gt;56 cores, &gt;700 Gb RAM</td>\n<td>64</td>\n<td>258.55</td>\n<td>95 (fastest - 72)</td>\n</tr>\n<tr>\n<td>TPU v2</td>\n<td>32/16</td>\n<td>3.2550</td>\n<td>16</td>\n</tr>\n<tr>\n<td>TPU v3</td>\n<td>32/16</td>\n<td>2.3022</td>\n<td>24</td>\n</tr>\n<tr>\n<td>NVIDIA P100</td>\n<td>32</td>\n<td>5.9433</td>\n<td>26</td>\n</tr>\n<tr>\n<td>NVIDIA P100</td>\n<td>64</td>\n<td>11.349</td>\n<td>18</td>\n</tr>\n<tr>\n<td>NVIDIA V100</td>\n<td>32</td>\n<td>2.7001</td>\n<td>26</td>\n</tr>\n<tr>\n<td>NVIDIA V100</td>\n<td>64</td>\n<td>6.2058</td>\n<td>18</td>\n</tr></tbody></table>\n<p>Tested using version 0.2.1. All GPU results are per single accelerator.\nNote that runtime is proportional to the depth of your network.\nIf your performance differs significantly,\nplease <a href=\"https://github.com/google/neural-tangents/issues/new\" rel=\"nofollow\">file a bug</a>!</p>\n<h2>Papers</h2>\n<p>Neural Tangents has been used in the following papers:</p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/2003.02218\" rel=\"nofollow\">The large learning rate phase of deep learning: the catapult mechanism.</a> <br>\nAitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, Guy Gur-Ari</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2002.02561\" rel=\"nofollow\">Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks.\n</a> <br>\nBlake Bordelon, Abdulkadir Canatar, Cengiz Pehlevan</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2002.04010\" rel=\"nofollow\">Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width.</a> <br>\nYu Bai, Ben Krause, Huan Wang, Caiming Xiong, Richard Socher</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/pdf/2001.07301.pdf\" rel=\"nofollow\">On the Infinite Width Limit of Neural Networks with a Standard Parameterization.</a> <br>\nJascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, Jaehoon Lee</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/1912.13053\" rel=\"nofollow\">Disentangling Trainability and Generalization in Deep Learning.</a> <br>\nLechao Xiao, Jeffrey Pennington, Samuel S. Schoenholz</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/1911.09189\" rel=\"nofollow\">Information in Infinite Ensembles of Infinitely-Wide Neural Networks.</a> <br>\nRavid Shwartz-Ziv, Alexander A. Alemi</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/1905.13654\" rel=\"nofollow\">Training Dynamics of Deep Networks using Stochastic Gradient Descent via Neural Tangent Kernel.</a> <br>\nSoufiane Hayou, Arnaud Doucet, Judith Rousseau</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/1902.06720\" rel=\"nofollow\">Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient\nDescent.</a> <br>\nJaehoon Lee*, Lechao Xiao*, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha\nSohl-Dickstein, Jeffrey Pennington</p>\n</li>\n</ul>\n<p>Please let us know if you make use of the code in a publication and we'll add it\nto the list!</p>\n<h2>Citation</h2>\n<p>If you use the code in a publication, please cite our ICLR 2020 paper:</p>\n<pre><code>@inproceedings{neuraltangents2020,\n    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\n    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\n    booktitle={International Conference on Learning Representations},\n    year={2020},\n    url={https://github.com/google/neural-tangents}\n}\n</code></pre>\n<h2>References</h2>\n<h5>[1] <a href=\"https://www.cs.toronto.edu/%7Eradford/pin.abstract.html\" rel=\"nofollow\">Priors for Infinite Networks.</a> Radford M. Neal</h5>\n<h5>[2] <a href=\"https://arxiv.org/abs/1606.05340\" rel=\"nofollow\">Exponential expressivity in deep neural networks through transient chaos.</a> <em>NeurIPS 2016.</em> Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, Surya Ganguli</h5>\n<h5>[3] <a href=\"http://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity\" rel=\"nofollow\">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity.</a> <em>NeurIPS 2016.</em> Amit Daniely, Roy Frostig, Yoram Singer</h5>\n<h5>[4] <a href=\"https://arxiv.org/abs/1611.01232\" rel=\"nofollow\">Deep Information Propagation.</a> <em>ICLR 2017.</em> Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, Jascha Sohl-Dickstein</h5>\n<h5>[5] <a href=\"https://arxiv.org/abs/1806.07572\" rel=\"nofollow\">Deep Neural Networks as Gaussian Processes.</a> <em>ICLR 2018.</em> Jaehoon Lee*, Yasaman Bahri*, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein</h5>\n<h5>[6] <a href=\"https://arxiv.org/abs/1804.11271\" rel=\"nofollow\">Gaussian Process Behaviour in Wide Deep Neural Networks.</a> <em>ICLR 2018.</em> Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, Zoubin Ghahramani</h5>\n<h5>[7] <a href=\"https://arxiv.org/abs/1806.05393\" rel=\"nofollow\">Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks.</a> <em>ICML 2018.</em> Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, Jeffrey Pennington</h5>\n<h5>[8] <a href=\"https://arxiv.org/abs/1810.05148\" rel=\"nofollow\">Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes.</a> <em>ICLR 2019.</em> Roman Novak*, Lechao Xiao*, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein</h5>\n<h5>[9] <a href=\"https://arxiv.org/abs/1808.05587\" rel=\"nofollow\">Deep Convolutional Networks as shallow Gaussian Processes.</a> <em>ICLR 2019.</em> Adri\u00e0 Garriga-Alonso, Carl Edward Rasmussen, Laurence Aitchison</h5>\n<h5>[10] <a href=\"https://arxiv.org/abs/1806.07572\" rel=\"nofollow\">Neural Tangent Kernel: Convergence and Generalization in Neural Networks.</a> <em>NeurIPS 2018.</em> Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler</h5>\n<h5>[11] <a href=\"https://arxiv.org/abs/1902.06720\" rel=\"nofollow\">Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent.</a> <em>NeurIPS 2019.</em> Jaehoon Lee*, Lechao Xiao*, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington</h5>\n<h5>[12] <a href=\"https://arxiv.org/abs/1902.04760\" rel=\"nofollow\">Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation.</a> <em>arXiv 2019.</em> Greg Yang</h5>\n<h5>[13] <a href=\"https://arxiv.org/abs/1712.08969\" rel=\"nofollow\">Mean Field Residual Networks: On the Edge of Chaos.</a> <em>NeurIPS 2017.</em> Greg Yang, Samuel S. Schoenholz</h5>\n<h5>[14] <a href=\"https://arxiv.org/abs/1605.07146\" rel=\"nofollow\">Wide Residual Networks.</a> <em>BMVC 2018.</em> Sergey Zagoruyko, Nikos Komodakis</h5>\n<h5>[15] <a href=\"https://arxiv.org/pdf/2001.07301.pdf\" rel=\"nofollow\">On the Infinite Width Limit of Neural Networks with a Standard Parameterization.</a> <em>arXiv 2020.</em> Jascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, Jaehoon Lee</h5>\n\n          </div>"}, "last_serial": 7044956, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "f6268513d7d097337b8b911dbdfa7961", "sha256": "8cf2a4bec406f9bdccffa490fa118a4f64be5218468eb1a1b1e51d3e9c1d86ec"}, "downloads": -1, "filename": "neural_tangents-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "f6268513d7d097337b8b911dbdfa7961", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=2.7", "size": 61203, "upload_time": "2019-11-11T22:43:56", "upload_time_iso_8601": "2019-11-11T22:43:56.814906Z", "url": "https://files.pythonhosted.org/packages/42/ee/5b6a5637f2e5ea512d48109a3faf18c2a2661b49721969ce4380703bccb0/neural_tangents-0.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b2caae7b01a9014a4d779fe4d204ee97", "sha256": "a442727abfec978f5ee3e22b80149565ce6ee4f26f23b4d9746f97816424239e"}, "downloads": -1, "filename": "neural-tangents-0.1.1.tar.gz", "has_sig": false, "md5_digest": "b2caae7b01a9014a4d779fe4d204ee97", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 52712, "upload_time": "2019-11-11T22:43:58", "upload_time_iso_8601": "2019-11-11T22:43:58.734784Z", "url": "https://files.pythonhosted.org/packages/26/85/443469e67dcce9e598b054ab7bf9e3d7ff483da9c1a53a3041f06c3582fd/neural-tangents-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "128ef9034444c3c900a9da069d96141b", "sha256": "7b53e2b9db4110ebc6f1b337b068aeea6a966d66f6d657bd88e37fc9d45bc022"}, "downloads": -1, "filename": "neural_tangents-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "128ef9034444c3c900a9da069d96141b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=2.7", "size": 61325, "upload_time": "2019-11-18T21:55:37", "upload_time_iso_8601": "2019-11-18T21:55:37.788276Z", "url": "https://files.pythonhosted.org/packages/f6/6a/89d3708d4e9d3df6b2058cd573eeeeeb87564108a0b8d5f648bb66c01f3f/neural_tangents-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a1288646f3c94d237d20510bafe0e5a6", "sha256": "fa62c90cc6fc4ce3a8e53e3819472623d016189c7fa1c9ff1508905cbca716d2"}, "downloads": -1, "filename": "neural-tangents-0.1.2.tar.gz", "has_sig": false, "md5_digest": "a1288646f3c94d237d20510bafe0e5a6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 53230, "upload_time": "2019-11-18T21:55:39", "upload_time_iso_8601": "2019-11-18T21:55:39.873861Z", "url": "https://files.pythonhosted.org/packages/0e/3a/9acedf41a6fd250f8fa528498b37fa7256c8fe0fa03e80e60b278a6c52b8/neural-tangents-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "4fdf0d0e5e2761f59c9eca5263477696", "sha256": "456902eb301203a4937d16c908dbbb2233f079c502420193249fcf910f0e5f12"}, "downloads": -1, "filename": "neural_tangents-0.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "4fdf0d0e5e2761f59c9eca5263477696", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=2.7", "size": 64035, "upload_time": "2019-11-27T22:54:41", "upload_time_iso_8601": "2019-11-27T22:54:41.643568Z", "url": "https://files.pythonhosted.org/packages/d9/8a/a8ca4b6033896ad31bdced24d7188a9be4af7014e0cb722ef5d31c8c45b0/neural_tangents-0.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "677f49ad126f97cc8808b9a5c44930b9", "sha256": "52d5302c65207d0f46e061c5e41c371fba00afaf27240e1807eafe4ae1a4b567"}, "downloads": -1, "filename": "neural-tangents-0.1.3.tar.gz", "has_sig": false, "md5_digest": "677f49ad126f97cc8808b9a5c44930b9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 56491, "upload_time": "2019-11-27T22:54:43", "upload_time_iso_8601": "2019-11-27T22:54:43.677884Z", "url": "https://files.pythonhosted.org/packages/7d/24/8f8d49df59e63b067e7289182a276b0dcaf1a102e114270db32cdce78255/neural-tangents-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "e3a2bf0d4d47f784492bd7208fe27889", "sha256": "a296861a472e212dd5f1349fbea5bcfc82b6696c5c8fc57002fce389cffb39ee"}, "downloads": -1, "filename": "neural_tangents-0.1.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "e3a2bf0d4d47f784492bd7208fe27889", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=2.7", "size": 64226, "upload_time": "2019-12-07T08:50:23", "upload_time_iso_8601": "2019-12-07T08:50:23.197200Z", "url": "https://files.pythonhosted.org/packages/7e/ae/766c8e2647dc07ec5bd25970ffd2698738e9911ee78ba0d2a5be3f2c602b/neural_tangents-0.1.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a6773238d55955f09a1813f0ebbded3c", "sha256": "37bbd9af57a5b737f50b90ce77dbbddaadf34c74894362484c9a21fd675c28ac"}, "downloads": -1, "filename": "neural-tangents-0.1.4.tar.gz", "has_sig": false, "md5_digest": "a6773238d55955f09a1813f0ebbded3c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 57134, "upload_time": "2019-12-07T08:50:25", "upload_time_iso_8601": "2019-12-07T08:50:25.211697Z", "url": "https://files.pythonhosted.org/packages/c2/9d/63507594fbae2056478618f01d10f5cd726b548b64fc1652fb3aa3b237e8/neural-tangents-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "245696a7c0444a13dd10c73a2afe4f33", "sha256": "21d28285866291b316cf94b123c9248ed38cbab9154c6fd55a7c13376dbeb450"}, "downloads": -1, "filename": "neural_tangents-0.1.5-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "245696a7c0444a13dd10c73a2afe4f33", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 65955, "upload_time": "2020-01-16T23:06:30", "upload_time_iso_8601": "2020-01-16T23:06:30.488962Z", "url": "https://files.pythonhosted.org/packages/fb/0d/3503e22fbcafbcd22df9b46cf5afe815f50461dbc8bc3d2633876768a77a/neural_tangents-0.1.5-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fcbee8887060b6ab215e6b1ecb422d1c", "sha256": "9574d5dd771877044e50ad2bf631688a0b48ef2cc2547defbd610dde7fad0cd6"}, "downloads": -1, "filename": "neural-tangents-0.1.5.tar.gz", "has_sig": false, "md5_digest": "fcbee8887060b6ab215e6b1ecb422d1c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 58876, "upload_time": "2020-01-16T23:06:32", "upload_time_iso_8601": "2020-01-16T23:06:32.557537Z", "url": "https://files.pythonhosted.org/packages/05/8d/5ce84fe079996c06dc6c20cdf5ccb9155e4b4e0dae184cc327991479c45e/neural-tangents-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "b5665c5210c4a28357f2289892e26801", "sha256": "e6b6ae5f2eee4cfae0c50e5a7827743241b44fd21001d7d59720dbc8beb62f41"}, "downloads": -1, "filename": "neural_tangents-0.1.6-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "b5665c5210c4a28357f2289892e26801", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 68550, "upload_time": "2020-01-24T01:01:47", "upload_time_iso_8601": "2020-01-24T01:01:47.685255Z", "url": "https://files.pythonhosted.org/packages/59/d5/d4d64090468bac0b1755f206d78a70e31626353bd720e8a6c03217ec1683/neural_tangents-0.1.6-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f727eead00800fcc993ca953f16255ce", "sha256": "863771c3047ecd4b4c7d31ec885fe06e2f9188e3ecec803d050cdf2964081893"}, "downloads": -1, "filename": "neural-tangents-0.1.6.tar.gz", "has_sig": false, "md5_digest": "f727eead00800fcc993ca953f16255ce", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 61587, "upload_time": "2020-01-24T01:01:49", "upload_time_iso_8601": "2020-01-24T01:01:49.647226Z", "url": "https://files.pythonhosted.org/packages/05/11/3174fd5844d58e8c7f03c417408cec796976bbdfe377f231c7c3b349ff5f/neural-tangents-0.1.6.tar.gz", "yanked": false}], "0.1.7": [{"comment_text": "", "digests": {"md5": "29115fcf51ce10d7f9bb4b1f7630cb3e", "sha256": "c2136f2dfaf70a922a2cd5d7f2c4cf603fe3ac5045d77f3be00129839a44e921"}, "downloads": -1, "filename": "neural_tangents-0.1.7-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "29115fcf51ce10d7f9bb4b1f7630cb3e", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 69707, "upload_time": "2020-01-28T17:37:47", "upload_time_iso_8601": "2020-01-28T17:37:47.419527Z", "url": "https://files.pythonhosted.org/packages/b9/e4/2356105f1ac542381e5ab82d6cf1c2c0c5f241dcfdafbd416f4b39275d0f/neural_tangents-0.1.7-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7c2f9d9c6c0b1e77f29cbe9252bc98a6", "sha256": "9dd55ad8a034a44caa5d633876fa749ae8502c121dd57a70e70815388394f4a6"}, "downloads": -1, "filename": "neural-tangents-0.1.7.tar.gz", "has_sig": false, "md5_digest": "7c2f9d9c6c0b1e77f29cbe9252bc98a6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 62278, "upload_time": "2020-01-28T17:37:49", "upload_time_iso_8601": "2020-01-28T17:37:49.036658Z", "url": "https://files.pythonhosted.org/packages/10/ce/2865a275f3abd9c581cdf33b4b15178ea08d6b20240f09a0c5f888fb6a66/neural-tangents-0.1.7.tar.gz", "yanked": false}], "0.1.8": [{"comment_text": "", "digests": {"md5": "11dc40a6ee8f6a433b6d55954a403d6c", "sha256": "6b8ec23ef817c6a619fc2c39705e5940caf794d59209bef7fa5e5e01deed9bba"}, "downloads": -1, "filename": "neural_tangents-0.1.8-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "11dc40a6ee8f6a433b6d55954a403d6c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6", "size": 70451, "upload_time": "2020-03-13T17:54:49", "upload_time_iso_8601": "2020-03-13T17:54:49.794504Z", "url": "https://files.pythonhosted.org/packages/53/a5/644a69efb535aef773853f8b9b61667de0a316f813ff717fcad9ae361a9d/neural_tangents-0.1.8-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d1860d1d59a6e5ad01b9c244a576529e", "sha256": "f1ba2c748ccd0f3a82bf260ea0bf33905d7b4dd0e6944c0c45aa3b9c1111177f"}, "downloads": -1, "filename": "neural-tangents-0.1.8.tar.gz", "has_sig": false, "md5_digest": "d1860d1d59a6e5ad01b9c244a576529e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 63536, "upload_time": "2020-03-13T17:54:51", "upload_time_iso_8601": "2020-03-13T17:54:51.715164Z", "url": "https://files.pythonhosted.org/packages/c7/d1/34810d46d0a9bc9119e6dde2ae47dcb35173adab19924cf4e3579453c877/neural-tangents-0.1.8.tar.gz", "yanked": false}], "0.1.9": [{"comment_text": "", "digests": {"md5": "cce202b8163de2c5bd5e4f6ac8c7f8bc", "sha256": "ed43009d6b159308aaa468da4f957845ed1cdc0fb02d4de3662f6c3349887d6f"}, "downloads": -1, "filename": "neural_tangents-0.1.9-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "cce202b8163de2c5bd5e4f6ac8c7f8bc", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6", "size": 77686, "upload_time": "2020-04-07T22:13:38", "upload_time_iso_8601": "2020-04-07T22:13:38.781786Z", "url": "https://files.pythonhosted.org/packages/69/e3/c191dd23f6a15199902157557b3ac59427673c1f5f0bc06580dca8003fe5/neural_tangents-0.1.9-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "63485ff5a077bd84406799fdcb9d27e1", "sha256": "5a0d057a24599024d29e59ccb5486facd407ea1422ae4ef1ecdf3093bd86825c"}, "downloads": -1, "filename": "neural-tangents-0.1.9.tar.gz", "has_sig": false, "md5_digest": "63485ff5a077bd84406799fdcb9d27e1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 69600, "upload_time": "2020-04-07T22:13:40", "upload_time_iso_8601": "2020-04-07T22:13:40.848429Z", "url": "https://files.pythonhosted.org/packages/9d/2a/a08814f0f06e8f7d98eeec741b1100adbef5f321e55dcd688ead4d54742e/neural-tangents-0.1.9.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "6704103198741f3402fce7076dccd72f", "sha256": "fa3fd17a8cf6e42653e807b2fe4de7ef864f6c854acf825b39f755947f61337a"}, "downloads": -1, "filename": "neural_tangents-0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "6704103198741f3402fce7076dccd72f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6", "size": 77677, "upload_time": "2020-04-15T01:49:10", "upload_time_iso_8601": "2020-04-15T01:49:10.572511Z", "url": "https://files.pythonhosted.org/packages/6b/48/7d95ef800663ec2925252a6984212efda4dd834ccff7ae1d66e69cb34d49/neural_tangents-0.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "185ef7f81d690be64dd52632d1d071bc", "sha256": "4c458b97919d7a534fedb110a684c721188462b12fd0d74737850748104bc500"}, "downloads": -1, "filename": "neural-tangents-0.2.tar.gz", "has_sig": false, "md5_digest": "185ef7f81d690be64dd52632d1d071bc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 69665, "upload_time": "2020-04-15T01:49:12", "upload_time_iso_8601": "2020-04-15T01:49:12.397016Z", "url": "https://files.pythonhosted.org/packages/f6/1f/9966e6a66cd7a88f30ab80f1500fc286d1306f80597e11657f50e843c3e4/neural-tangents-0.2.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "89542aa11971d067acc8e71af91263c0", "sha256": "7e3b21329cd8f317fccce9e2ea05aeb8246089c304869cb9a3efa89e6762f792"}, "downloads": -1, "filename": "neural_tangents-0.2.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "89542aa11971d067acc8e71af91263c0", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6", "size": 78682, "upload_time": "2020-04-18T00:09:43", "upload_time_iso_8601": "2020-04-18T00:09:43.581937Z", "url": "https://files.pythonhosted.org/packages/e7/7b/7ac2ff967a5bd14353ad49d67dbbf3a7ed2a9f2ac449c065bb6bdfe47e5a/neural_tangents-0.2.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e8836411599aa44de567571edaba2d24", "sha256": "ec4b5471fee8049e8b307b6ce31550c8e1f126bbf1baa16d02f292d30e624f29"}, "downloads": -1, "filename": "neural-tangents-0.2.1.tar.gz", "has_sig": false, "md5_digest": "e8836411599aa44de567571edaba2d24", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 71382, "upload_time": "2020-04-18T00:09:45", "upload_time_iso_8601": "2020-04-18T00:09:45.415097Z", "url": "https://files.pythonhosted.org/packages/ae/8f/c9121a37b880ab2d18fce29e15fe018266ce79c241c37aca4cb8767d7c25/neural-tangents-0.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "89542aa11971d067acc8e71af91263c0", "sha256": "7e3b21329cd8f317fccce9e2ea05aeb8246089c304869cb9a3efa89e6762f792"}, "downloads": -1, "filename": "neural_tangents-0.2.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "89542aa11971d067acc8e71af91263c0", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6", "size": 78682, "upload_time": "2020-04-18T00:09:43", "upload_time_iso_8601": "2020-04-18T00:09:43.581937Z", "url": "https://files.pythonhosted.org/packages/e7/7b/7ac2ff967a5bd14353ad49d67dbbf3a7ed2a9f2ac449c065bb6bdfe47e5a/neural_tangents-0.2.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e8836411599aa44de567571edaba2d24", "sha256": "ec4b5471fee8049e8b307b6ce31550c8e1f126bbf1baa16d02f292d30e624f29"}, "downloads": -1, "filename": "neural-tangents-0.2.1.tar.gz", "has_sig": false, "md5_digest": "e8836411599aa44de567571edaba2d24", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 71382, "upload_time": "2020-04-18T00:09:45", "upload_time_iso_8601": "2020-04-18T00:09:45.415097Z", "url": "https://files.pythonhosted.org/packages/ae/8f/c9121a37b880ab2d18fce29e15fe018266ce79c241c37aca4cb8767d7c25/neural-tangents-0.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:47 2020"}