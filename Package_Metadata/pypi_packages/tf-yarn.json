{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Libraries"], "description": "# tf-yarn\u1d5d\n\ntf-yarn is a Python library we have built at Criteo for training TensorFlow models on a Hadoop/YARN cluster. An introducing blog post can be found [here](https://medium.com/criteo-labs/train-tensorflow-models-on-yarn-in-just-a-few-lines-of-code-ba0f354f38e3).\n\nIt supports running on one worker or on multiple workers with different distribution strategies, it can run on CPUs or GPUs and also runs with the recently added standalone client mode, and this with just a few lines of code.\n\nIts API provides an easy entry point for working with Estimators. Keras is currently supported via the [model_to_estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator) conversion function, and low-level distributed TensorFlow via standalone client mode API. Please refer to the [examples](https://github.com/criteo/tf-yarn/tree/master/examples) for some code samples.\n\n[MLflow](https://www.mlflow.org/docs/latest/quickstart.html) is supported for all kind of trainings (one worker/distributed).\nMore infos [here](https://github.com/criteo/tf-yarn/blob/master/docs/MLflow.md).\n\n[Tensorboard](https://github.com/criteo/tf-yarn/blob/master/docs/Tensorboard.md) can be spawned in a separate container during learnings.\n\nTwo alternatives to TensorFlow's distribution strategies are available:\n[Horovod with gloo](https://github.com/criteo/tf-yarn/blob/master/docs/HorovodWithGloo.md) and [tf-collective-all-reduce](https://github.com/criteo/tf-collective-all-reduce)\n\n![tf-yarn](https://github.com/criteo/tf-yarn/blob/master/skein.png?raw=true)\n\n## Installation\n\n### Install with Pip\n\n```bash\n$ pip install tf-yarn\n```\n\n### Install from source\n\n```bash\n$ git clone https://github.com/criteo/tf-yarn\n$ cd tf-yarn\n$ pip install .\n```\n\n### Prerequisites\n\ntf-yarn only supports Python \u22653.6.\n\nMake sure to have Tensorflow working with HDFS by setting up all the environment variables as described [here](https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/hadoop.md).\n\nYou can run the `check_hadoop_env` script to check that your setup is OK (it has been installed by tf_yarn):\n\n```\n$ check_hadoop_env\n# You should see something like\n# INFO:tf_yarn.bin.check_hadoop_env:results will be written in /home/.../shared/Dev/tf-yarn/check_hadoop_env.log\n# INFO:tf_yarn.bin.check_hadoop_env:check_env: True\n# INFO:tf_yarn.bin.check_hadoop_env:write dummy file to hdfs hdfs://root/tmp/a1df7b99-fa47-4a86-b5f3-9bc09019190f/hello_tf_yarn.txt\n# INFO:tf_yarn.bin.check_hadoop_env:check_local_hadoop_tensorflow: True\n# INFO:root:Launching remote check\n# ...\n# INFO:tf_yarn.bin.check_hadoop_env:remote_check: True\n# INFO:tf_yarn.bin.check_hadoop_env:Hadoop setup: OK\n```\n\n## tf-yarn API's\n\ntf-yarn comes with two API's to launch a training \u2014 run_on_yarn and standalone_client_mode.\n\n### run_on_yarn\n\nThe only abstraction tf-yarn adds on top of the ones already present in\nTensorFlow is `experiment_fn`. It is a function returning a triple of one `Estimator` and two specs -- `TrainSpec` and `EvalSpec`.\n\nHere is a stripped down `experiment_fn` from one of the provided [examples][linear_classifier_example] to give you an idea of how it might look:\n\n```python\nfrom tf_yarn import Experiment\n\ndef experiment_fn():\n  # ...\n  estimator = tf.estimator.DNNClassifier(...)\n  return Experiment(\n    estimator,\n    tf.estimator.TrainSpec(train_input_fn, max_steps=...),\n    tf.estimator.EvalSpec(eval_input_fn)\n )\n```\n\nAn experiment can be scheduled on YARN using the run_on_yarn function which takes three required arguments:\n\n- `pyenv_zip_path` which contains the tf-yarn modules and dependencies like TensorFlow to be shipped to the cluster. pyenv_zip_path can be generated easily with a helper function based on the current installed virtual environment;\n- `experiment_fn` as described above;\n- `task_specs` dictionary specifying how much resources to allocate for each of the distributed TensorFlow task type.\n\nThe example uses the [Wine Quality][wine-quality] dataset from UCI ML repository. With just under 5000 training instances available, there is no need for multi-node training, meaning that a chief complemented by an evaluator would manage just fine. Note that each task will be executed in its own YARN container.\n\n```python\nfrom tf_yarn import TaskSpec, run_on_yarn\nimport cluster_pack\n\npyenv_zip_path, _ = cluster_pack.upload_env()\nrun_on_yarn(\n    pyenv_zip_path,\n    experiment_fn,\n    task_specs={\n        \"chief\": TaskSpec(memory=\"2 GiB\", vcores=4),\n        \"evaluator\": TaskSpec(memory=\"2 GiB\", vcores=1),\n        \"tensorboard\": TaskSpec(memory=\"2 GiB\", vcores=1)\n    }\n)\n```\n\nThe final bit is to forward the `winequality.py` module to the YARN containers,\nin order for the tasks to be able to import them:\n\n```python\nrun_on_yarn(\n    ...,\n    files={\n        os.path.basename(winequality.__file__): winequality.__file__,\n    }\n)\n```\n\nUnder the hood, the experiment function is shipped to each container, evaluated and then passed to the `train_and_evaluate` function.\n\n```python\nexperiment = experiment_fn()\ntf.estimator.train_and_evaluate(\n  experiment.estimator,\n  experiment.train_spec,\n  experiment.eval_spec\n)\n```\n\n[linear_classifier_example]: https://github.com/criteo/tf-yarn/blob/master/examples/linear_classifier_example.py\n[wine-quality]: https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n\n### standalone_client_mode\n\n[Standalone client mode](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/distribute/README.md?#standalone-client-mode) keeps most of the previous concepts. Instead of calling `train_and_evaluate` on each worker, one just spawns the TensorFlow server on each worker and then locally runs `train_and_evaluate` on the client. TensorFlow will take care of sending the graph to each worker. This removes the burden of having to ship manually the experiment function to the containers.\n\nHere is the previous example in Standalone client mode:\n\n```python\nfrom tensorflow.contrib.distribute import DistributeConfig, ParameterServerStrategy\nfrom tf_yarn import standalone_client_mode, TaskSpec\n\nwith standalone_client_mode(\n     task_specs={\n       \"worker\": TaskSpec(memory=4 * 2**10, vcores=4, instances=2),\n       \"ps\": TaskSpec(memory=4 * 2**10, vcores=4, instances=1)}\n) as cluster_spec:\n    distrib_config = DistributeConfig(\n      train_distribute=ParameterServerStrategy(),\n      remote_cluster=cluster_spec\n    )\n    estimator = tf.estimator.DNNClassifier(\n      ...\n      config=tf.estimator.RunConfig(\n        experimental_distribute=distrib_config\n      )\n    )\n\n    tf.estimator.train_and_evaluate(\n      estimator,\n      tf.estimator.TrainSpec(...),\n      tf.estimator.EvalSpec(...))\n```\n\n`standalone_client_mode`  takes care of creating the `ClusterSpec` as described before. We activate `ParameterServerStrategy` in the `RunConfig` and then call `train_and_evaluate`.\n\nIn addition to training estimators, Standalone client mode also gives access to TensorFlow\u2019s low-level API. Have a look at the [examples](https://github.com/criteo/tf-yarn/blob/master/examples/session_run_example.py) for more information.\n\n## Distributed TensorFlow\n\nThe following is a brief summary of the core distributed TensorFlow concepts relevant to training [estimators](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate) with the ParameterServerStrategy, as it is the distribution strategy activated by default when training Estimators on multiple nodes.\n\nDistributed TensorFlow operates in terms of tasks.\nA task has a type which defines its purpose in the distributed TensorFlow cluster:\n- `worker` tasks headed by the `chief` doing model training\n- `chief` task additionally handling checkpoints, saving/restoring the model, etc.\n- `ps`  tasks (aka parameter servers) storing the model itself. These tasks typically do not compute anything.\nTheir sole purpose is serving the model variables\n- `evaluator` task periodically evaluating the model from the saved checkpoint\n\nThe types of tasks can depend on the distribution strategy, for example, ps tasks are only used by ParameterServerStrategy.\nThe following picture presents an example of a cluster setup with 2 workers, 1 chief, 1 ps and 1 evaluator.\n\n```\n+-----------+              +---------+   +----------+   +----------+\n| evaluator |        +-----+ chief:0 |   | worker:0 |   | worker:1 |\n+-----+-----+        |     +----^----+   +-----^----+   +-----^----+\n      ^              |          |            |              |\n      |              v          |            |              |\n      |        +-----+---+      |            |              |\n      |        | model   |   +--v---+        |              |\n      +--------+ exports |   | ps:0 <--------+--------------+\n               +---------+   +------+\n```\n\nThe cluster is defined by a ClusterSpec, a mapping from task types to their associated network addresses. For instance, for the above example, it looks like that:\n\n```\n{\n  \"chief\": [\"chief.example.com:2125\"],\n  \"worker\": [\"worker0.example.com:6784\",\n             \"worker1.example.com:6475\"],\n  \"ps\": [\"ps0.example.com:7419\"],\n  \"evaluator\": [\"evaluator.example.com:8347\"]\n}\n```\nStarting a task in the cluster requires a ClusterSpec. This means that the spec should be fully known before starting any of the tasks.\n\nOnce the cluster is known, we need to export the ClusterSpec through the [TF_CONFIG](https://cloud.google.com/ml-engine/docs/tensorflow/distributed-training-details) environment variable and start the TensorFlow server on each container.\n\nThen we can run the [train-and-evaluate](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate) function on each container.\nWe just launch the same function as in local training mode, TensorFlow will automatically detect that we have set up a ClusterSpec and start a distributed learning.\n\nYou can find more information about distributed Tensorflow [here](https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md) and about distributed training Estimators [here](https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate).\n\n## Training with multiple workers\n\nActivating the previous example in tf-yarn is just changing the cluster_spec by adding the additional `worker` and `ps` instances: \n\n```python\nrun_on_yarn(\n    ...,\n    task_specs={\n        \"chief\": TaskSpec(memory=\"2 GiB\", vcores=4),\n        \"worker\": TaskSpec(memory=\"2 GiB\", vcores=4, instances=2),\n        \"ps\": TaskSpec(memory=\"2 GiB\", vcores=8),\n        \"evaluator\": TaskSpec(memory=\"2 GiB\", vcores=1),\n        \"tensorboard\": TaskSpec(memory=\"2 GiB\", vcores=1)\n    }\n)\n```\n\n## Configuring the Python interpreter and packages\n\ntf-yarn uses [cluster-pack](https://github.com/criteo/cluster-pack) to to ship an isolated virtual environment to the containers.\n(You should have installed the dependencies from `requirements.txt` into your virtual environment first `pip install -r requirements.txt`)\nThis works if you use Anaconda and also with [Virtual Environments](https://docs.python.org/3/tutorial/venv.html).\n\nBy default the generated package is a [pex][pex] package. cluster-pack will generate the pex package, upload it to hdfs and you can start tf_yarn by providing the hdfs path.\n\n```python\nimport cluster_pack\npyenv_zip_path, env_name = cluster_pack.upload_env()\nrun_on_yarn(\n    pyenv_zip_path=pyenv_zip_path\n)\n```\n\nIf you hosting evironment is Anaconda `upload_env` the packaging module will use [conda-pack][conda-pack] to create the package.\n\nYou can also directly use the command line tools provided by [conda-pack][conda-pack] and [pex][pex] to generate the packages.\n\nFor pex you can run this command in the root directory to create the package (it includes all requirements from setup.py)\n```\npex . -o myarchive.pex\n```\n\nYou can then run tf-yarn with your generated package:\n\n```python\nrun_on_yarn(\n    pyenv_zip_path=\"myarchive.pex\"\n)\n```\n\n[conda-pack]: https://conda.github.io/conda-pack/\n[pex]: https://pex.readthedocs.io/en/stable/\n\n## Running on GPU\n\nYARN does not have first-class support for GPU resources. A common workaround is\nto use [node labels][node-labels] where CPU-only nodes are unlabelled, while\nthe GPU ones have a label. Furthermore, in this setting GPU nodes are\ntypically bound to a separate queue which is different from the default one.\n\nCurrently, tf-yarn assumes that the GPU label is ``\"gpu\"``. There are no\nassumptions on the name of the queue with GPU nodes, however, for the sake of\nexample we wil use the name ``\"ml-gpu\"``.\n\nThe default behaviour of `run_on_yarn` is to run on CPU-only nodes. In order\nto run on the GPU ones:\n\n1. Set the `queue` argument.\n2. Set `TaskSpec.label` to `NodeLabel.GPU` for relevant task types.\n   A good rule of a thumb is to run compute heavy `\"chief\"` and `\"worker\"`\n   tasks on GPU, while keeping `\"ps\"` and `\"evaluator\"` on CPU.\n3. Generate two python environements: one with Tensorflow for CPUs and one\n   with Tensorflow for GPUs. You need to provide a custom path in archive_on_hdfs\n   as the default one is already use by the CPU pyenv_zip_path\n\n```python\nimport getpass\nimport cluster_pack\nfrom tf_yarn import NodeLabel\n\n\npyenv_zip_path_cpu, _ = cluster_pack.upload_env()\npyenv_zip_path_gpu, _ = cluster_pack.upload_env(\n    archive_on_hdfs=f\"{cluster_pack.get_default_fs()}/user/{getpass.getuser()}/envs/tf_yarn_gpu_env.pex\",\n    additional_packages={\"tensorflow-gpu\": \"2.0.0a0\"},\n    ignored_packages={\"tensorflow\"}\n)\nrun_on_yarn(\n    {NodeLabel.CPU: pyenv_zip_path_cpu, NodeLabel.GPU: pyenv_zip_path_gpu}\n    experiment_fn,\n    task_specs={\n        \"chief\": TaskSpec(memory=\"2 GiB\", vcores=4, label=NodeLabel.GPU),\n        \"evaluator\": TaskSpec(memory=\"1 GiB\", vcores=1)\n    },\n    queue=\"ml-gpu\"\n)\n```\n\n[node-labels]: https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/NodeLabel.html\n\n## Accessing HDFS in the presence of [federation][federation]\n\n`skein` the library underlying `tf_yarn` automatically acquires a delegation token\nfor ``fs.defaultFS`` on security-enabled clusters. This should be enough for most\nuse-cases. However, if your experiment needs to access data on namenodes other than\nthe default one, you have to explicitly list them in the `file_systems` argument\nto `run_on_yarn`. This would instruct `skein` to acquire a delegation token for\nthese namenodes in addition to ``fs.defaultFS``:\n\n```python\nrun_on_yarn(\n    ...,\n    file_systems=[\"hdfs://preprod\"]\n)\n```\n\nDepending on the cluster configuration, you might need to point libhdfs to a\ndifferent configuration folder. For instance:\n\n```python\nrun_on_yarn(\n    ...,\n    env={\"HADOOP_CONF_DIR\": \"/etc/hadoop/conf.all\"}\n)\n```\n\n[federation]: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/criteo/tf-yarn", "keywords": "tensorflow yarn", "license": "", "maintainer": "Criteo", "maintainer_email": "github@criteo.com", "name": "tf-yarn", "package_url": "https://pypi.org/project/tf-yarn/", "platform": "", "project_url": "https://pypi.org/project/tf-yarn/", "project_urls": {"Homepage": "https://github.com/criteo/tf-yarn"}, "release_url": "https://pypi.org/project/tf-yarn/0.4.15/", "requires_dist": ["cluster-pack (==0.0.5)", "skein (==0.8.0)", "cloudpickle (==1.0.0)", "tensorflow (<1.15,>=1.12.2)"], "requires_python": ">=3.6", "summary": "Distributed TensorFlow on a YARN cluster", "version": "0.4.15", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>tf-yarn\u1d5d</h1>\n<p>tf-yarn is a Python library we have built at Criteo for training TensorFlow models on a Hadoop/YARN cluster. An introducing blog post can be found <a href=\"https://medium.com/criteo-labs/train-tensorflow-models-on-yarn-in-just-a-few-lines-of-code-ba0f354f38e3\" rel=\"nofollow\">here</a>.</p>\n<p>It supports running on one worker or on multiple workers with different distribution strategies, it can run on CPUs or GPUs and also runs with the recently added standalone client mode, and this with just a few lines of code.</p>\n<p>Its API provides an easy entry point for working with Estimators. Keras is currently supported via the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator\" rel=\"nofollow\">model_to_estimator</a> conversion function, and low-level distributed TensorFlow via standalone client mode API. Please refer to the <a href=\"https://github.com/criteo/tf-yarn/tree/master/examples\" rel=\"nofollow\">examples</a> for some code samples.</p>\n<p><a href=\"https://www.mlflow.org/docs/latest/quickstart.html\" rel=\"nofollow\">MLflow</a> is supported for all kind of trainings (one worker/distributed).\nMore infos <a href=\"https://github.com/criteo/tf-yarn/blob/master/docs/MLflow.md\" rel=\"nofollow\">here</a>.</p>\n<p><a href=\"https://github.com/criteo/tf-yarn/blob/master/docs/Tensorboard.md\" rel=\"nofollow\">Tensorboard</a> can be spawned in a separate container during learnings.</p>\n<p>Two alternatives to TensorFlow's distribution strategies are available:\n<a href=\"https://github.com/criteo/tf-yarn/blob/master/docs/HorovodWithGloo.md\" rel=\"nofollow\">Horovod with gloo</a> and <a href=\"https://github.com/criteo/tf-collective-all-reduce\" rel=\"nofollow\">tf-collective-all-reduce</a></p>\n<p><img alt=\"tf-yarn\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/450065655d384b7b2d4d3016bfb7ac505c8db887/68747470733a2f2f6769746875622e636f6d2f63726974656f2f74662d7961726e2f626c6f622f6d61737465722f736b65696e2e706e673f7261773d74727565\"></p>\n<h2>Installation</h2>\n<h3>Install with Pip</h3>\n<pre>$ pip install tf-yarn\n</pre>\n<h3>Install from source</h3>\n<pre>$ git clone https://github.com/criteo/tf-yarn\n$ <span class=\"nb\">cd</span> tf-yarn\n$ pip install .\n</pre>\n<h3>Prerequisites</h3>\n<p>tf-yarn only supports Python \u22653.6.</p>\n<p>Make sure to have Tensorflow working with HDFS by setting up all the environment variables as described <a href=\"https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/hadoop.md\" rel=\"nofollow\">here</a>.</p>\n<p>You can run the <code>check_hadoop_env</code> script to check that your setup is OK (it has been installed by tf_yarn):</p>\n<pre><code>$ check_hadoop_env\n# You should see something like\n# INFO:tf_yarn.bin.check_hadoop_env:results will be written in /home/.../shared/Dev/tf-yarn/check_hadoop_env.log\n# INFO:tf_yarn.bin.check_hadoop_env:check_env: True\n# INFO:tf_yarn.bin.check_hadoop_env:write dummy file to hdfs hdfs://root/tmp/a1df7b99-fa47-4a86-b5f3-9bc09019190f/hello_tf_yarn.txt\n# INFO:tf_yarn.bin.check_hadoop_env:check_local_hadoop_tensorflow: True\n# INFO:root:Launching remote check\n# ...\n# INFO:tf_yarn.bin.check_hadoop_env:remote_check: True\n# INFO:tf_yarn.bin.check_hadoop_env:Hadoop setup: OK\n</code></pre>\n<h2>tf-yarn API's</h2>\n<p>tf-yarn comes with two API's to launch a training \u2014 run_on_yarn and standalone_client_mode.</p>\n<h3>run_on_yarn</h3>\n<p>The only abstraction tf-yarn adds on top of the ones already present in\nTensorFlow is <code>experiment_fn</code>. It is a function returning a triple of one <code>Estimator</code> and two specs -- <code>TrainSpec</code> and <code>EvalSpec</code>.</p>\n<p>Here is a stripped down <code>experiment_fn</code> from one of the provided <a href=\"https://github.com/criteo/tf-yarn/blob/master/examples/linear_classifier_example.py\" rel=\"nofollow\">examples</a> to give you an idea of how it might look:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tf_yarn</span> <span class=\"kn\">import</span> <span class=\"n\">Experiment</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">experiment_fn</span><span class=\"p\">():</span>\n  <span class=\"c1\"># ...</span>\n  <span class=\"n\">estimator</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">DNNClassifier</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">Experiment</span><span class=\"p\">(</span>\n    <span class=\"n\">estimator</span><span class=\"p\">,</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">TrainSpec</span><span class=\"p\">(</span><span class=\"n\">train_input_fn</span><span class=\"p\">,</span> <span class=\"n\">max_steps</span><span class=\"o\">=...</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">EvalSpec</span><span class=\"p\">(</span><span class=\"n\">eval_input_fn</span><span class=\"p\">)</span>\n <span class=\"p\">)</span>\n</pre>\n<p>An experiment can be scheduled on YARN using the run_on_yarn function which takes three required arguments:</p>\n<ul>\n<li><code>pyenv_zip_path</code> which contains the tf-yarn modules and dependencies like TensorFlow to be shipped to the cluster. pyenv_zip_path can be generated easily with a helper function based on the current installed virtual environment;</li>\n<li><code>experiment_fn</code> as described above;</li>\n<li><code>task_specs</code> dictionary specifying how much resources to allocate for each of the distributed TensorFlow task type.</li>\n</ul>\n<p>The example uses the <a href=\"https://archive.ics.uci.edu/ml/datasets/Wine+Quality\" rel=\"nofollow\">Wine Quality</a> dataset from UCI ML repository. With just under 5000 training instances available, there is no need for multi-node training, meaning that a chief complemented by an evaluator would manage just fine. Note that each task will be executed in its own YARN container.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tf_yarn</span> <span class=\"kn\">import</span> <span class=\"n\">TaskSpec</span><span class=\"p\">,</span> <span class=\"n\">run_on_yarn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">cluster_pack</span>\n\n<span class=\"n\">pyenv_zip_path</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">cluster_pack</span><span class=\"o\">.</span><span class=\"n\">upload_env</span><span class=\"p\">()</span>\n<span class=\"n\">run_on_yarn</span><span class=\"p\">(</span>\n    <span class=\"n\">pyenv_zip_path</span><span class=\"p\">,</span>\n    <span class=\"n\">experiment_fn</span><span class=\"p\">,</span>\n    <span class=\"n\">task_specs</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s2\">\"chief\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"2 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n        <span class=\"s2\">\"evaluator\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"2 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"s2\">\"tensorboard\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"2 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n</pre>\n<p>The final bit is to forward the <code>winequality.py</code> module to the YARN containers,\nin order for the tasks to be able to import them:</p>\n<pre><span class=\"n\">run_on_yarn</span><span class=\"p\">(</span>\n    <span class=\"o\">...</span><span class=\"p\">,</span>\n    <span class=\"n\">files</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">basename</span><span class=\"p\">(</span><span class=\"n\">winequality</span><span class=\"o\">.</span><span class=\"vm\">__file__</span><span class=\"p\">):</span> <span class=\"n\">winequality</span><span class=\"o\">.</span><span class=\"vm\">__file__</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n</pre>\n<p>Under the hood, the experiment function is shipped to each container, evaluated and then passed to the <code>train_and_evaluate</code> function.</p>\n<pre><span class=\"n\">experiment</span> <span class=\"o\">=</span> <span class=\"n\">experiment_fn</span><span class=\"p\">()</span>\n<span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">train_and_evaluate</span><span class=\"p\">(</span>\n  <span class=\"n\">experiment</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"p\">,</span>\n  <span class=\"n\">experiment</span><span class=\"o\">.</span><span class=\"n\">train_spec</span><span class=\"p\">,</span>\n  <span class=\"n\">experiment</span><span class=\"o\">.</span><span class=\"n\">eval_spec</span>\n<span class=\"p\">)</span>\n</pre>\n<h3>standalone_client_mode</h3>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/distribute/README.md?#standalone-client-mode\" rel=\"nofollow\">Standalone client mode</a> keeps most of the previous concepts. Instead of calling <code>train_and_evaluate</code> on each worker, one just spawns the TensorFlow server on each worker and then locally runs <code>train_and_evaluate</code> on the client. TensorFlow will take care of sending the graph to each worker. This removes the burden of having to ship manually the experiment function to the containers.</p>\n<p>Here is the previous example in Standalone client mode:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tensorflow.contrib.distribute</span> <span class=\"kn\">import</span> <span class=\"n\">DistributeConfig</span><span class=\"p\">,</span> <span class=\"n\">ParameterServerStrategy</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tf_yarn</span> <span class=\"kn\">import</span> <span class=\"n\">standalone_client_mode</span><span class=\"p\">,</span> <span class=\"n\">TaskSpec</span>\n\n<span class=\"k\">with</span> <span class=\"n\">standalone_client_mode</span><span class=\"p\">(</span>\n     <span class=\"n\">task_specs</span><span class=\"o\">=</span><span class=\"p\">{</span>\n       <span class=\"s2\">\"worker\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"mi\">4</span> <span class=\"o\">*</span> <span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">instances</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n       <span class=\"s2\">\"ps\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"mi\">4</span> <span class=\"o\">*</span> <span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">instances</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)}</span>\n<span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">cluster_spec</span><span class=\"p\">:</span>\n    <span class=\"n\">distrib_config</span> <span class=\"o\">=</span> <span class=\"n\">DistributeConfig</span><span class=\"p\">(</span>\n      <span class=\"n\">train_distribute</span><span class=\"o\">=</span><span class=\"n\">ParameterServerStrategy</span><span class=\"p\">(),</span>\n      <span class=\"n\">remote_cluster</span><span class=\"o\">=</span><span class=\"n\">cluster_spec</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">estimator</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">DNNClassifier</span><span class=\"p\">(</span>\n      <span class=\"o\">...</span>\n      <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">RunConfig</span><span class=\"p\">(</span>\n        <span class=\"n\">experimental_distribute</span><span class=\"o\">=</span><span class=\"n\">distrib_config</span>\n      <span class=\"p\">)</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">train_and_evaluate</span><span class=\"p\">(</span>\n      <span class=\"n\">estimator</span><span class=\"p\">,</span>\n      <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">TrainSpec</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">),</span>\n      <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">EvalSpec</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">))</span>\n</pre>\n<p><code>standalone_client_mode</code>  takes care of creating the <code>ClusterSpec</code> as described before. We activate <code>ParameterServerStrategy</code> in the <code>RunConfig</code> and then call <code>train_and_evaluate</code>.</p>\n<p>In addition to training estimators, Standalone client mode also gives access to TensorFlow\u2019s low-level API. Have a look at the <a href=\"https://github.com/criteo/tf-yarn/blob/master/examples/session_run_example.py\" rel=\"nofollow\">examples</a> for more information.</p>\n<h2>Distributed TensorFlow</h2>\n<p>The following is a brief summary of the core distributed TensorFlow concepts relevant to training <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate\" rel=\"nofollow\">estimators</a> with the ParameterServerStrategy, as it is the distribution strategy activated by default when training Estimators on multiple nodes.</p>\n<p>Distributed TensorFlow operates in terms of tasks.\nA task has a type which defines its purpose in the distributed TensorFlow cluster:</p>\n<ul>\n<li><code>worker</code> tasks headed by the <code>chief</code> doing model training</li>\n<li><code>chief</code> task additionally handling checkpoints, saving/restoring the model, etc.</li>\n<li><code>ps</code>  tasks (aka parameter servers) storing the model itself. These tasks typically do not compute anything.\nTheir sole purpose is serving the model variables</li>\n<li><code>evaluator</code> task periodically evaluating the model from the saved checkpoint</li>\n</ul>\n<p>The types of tasks can depend on the distribution strategy, for example, ps tasks are only used by ParameterServerStrategy.\nThe following picture presents an example of a cluster setup with 2 workers, 1 chief, 1 ps and 1 evaluator.</p>\n<pre><code>+-----------+              +---------+   +----------+   +----------+\n| evaluator |        +-----+ chief:0 |   | worker:0 |   | worker:1 |\n+-----+-----+        |     +----^----+   +-----^----+   +-----^----+\n      ^              |          |            |              |\n      |              v          |            |              |\n      |        +-----+---+      |            |              |\n      |        | model   |   +--v---+        |              |\n      +--------+ exports |   | ps:0 &lt;--------+--------------+\n               +---------+   +------+\n</code></pre>\n<p>The cluster is defined by a ClusterSpec, a mapping from task types to their associated network addresses. For instance, for the above example, it looks like that:</p>\n<pre><code>{\n  \"chief\": [\"chief.example.com:2125\"],\n  \"worker\": [\"worker0.example.com:6784\",\n             \"worker1.example.com:6475\"],\n  \"ps\": [\"ps0.example.com:7419\"],\n  \"evaluator\": [\"evaluator.example.com:8347\"]\n}\n</code></pre>\n<p>Starting a task in the cluster requires a ClusterSpec. This means that the spec should be fully known before starting any of the tasks.</p>\n<p>Once the cluster is known, we need to export the ClusterSpec through the <a href=\"https://cloud.google.com/ml-engine/docs/tensorflow/distributed-training-details\" rel=\"nofollow\">TF_CONFIG</a> environment variable and start the TensorFlow server on each container.</p>\n<p>Then we can run the <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate\" rel=\"nofollow\">train-and-evaluate</a> function on each container.\nWe just launch the same function as in local training mode, TensorFlow will automatically detect that we have set up a ClusterSpec and start a distributed learning.</p>\n<p>You can find more information about distributed Tensorflow <a href=\"https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md\" rel=\"nofollow\">here</a> and about distributed training Estimators <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate\" rel=\"nofollow\">here</a>.</p>\n<h2>Training with multiple workers</h2>\n<p>Activating the previous example in tf-yarn is just changing the cluster_spec by adding the additional <code>worker</code> and <code>ps</code> instances:</p>\n<pre><span class=\"n\">run_on_yarn</span><span class=\"p\">(</span>\n    <span class=\"o\">...</span><span class=\"p\">,</span>\n    <span class=\"n\">task_specs</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s2\">\"chief\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"2 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n        <span class=\"s2\">\"worker\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"2 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">instances</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n        <span class=\"s2\">\"ps\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"2 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">),</span>\n        <span class=\"s2\">\"evaluator\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"2 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"s2\">\"tensorboard\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"2 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n</pre>\n<h2>Configuring the Python interpreter and packages</h2>\n<p>tf-yarn uses <a href=\"https://github.com/criteo/cluster-pack\" rel=\"nofollow\">cluster-pack</a> to to ship an isolated virtual environment to the containers.\n(You should have installed the dependencies from <code>requirements.txt</code> into your virtual environment first <code>pip install -r requirements.txt</code>)\nThis works if you use Anaconda and also with <a href=\"https://docs.python.org/3/tutorial/venv.html\" rel=\"nofollow\">Virtual Environments</a>.</p>\n<p>By default the generated package is a <a href=\"https://pex.readthedocs.io/en/stable/\" rel=\"nofollow\">pex</a> package. cluster-pack will generate the pex package, upload it to hdfs and you can start tf_yarn by providing the hdfs path.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">cluster_pack</span>\n<span class=\"n\">pyenv_zip_path</span><span class=\"p\">,</span> <span class=\"n\">env_name</span> <span class=\"o\">=</span> <span class=\"n\">cluster_pack</span><span class=\"o\">.</span><span class=\"n\">upload_env</span><span class=\"p\">()</span>\n<span class=\"n\">run_on_yarn</span><span class=\"p\">(</span>\n    <span class=\"n\">pyenv_zip_path</span><span class=\"o\">=</span><span class=\"n\">pyenv_zip_path</span>\n<span class=\"p\">)</span>\n</pre>\n<p>If you hosting evironment is Anaconda <code>upload_env</code> the packaging module will use <a href=\"https://conda.github.io/conda-pack/\" rel=\"nofollow\">conda-pack</a> to create the package.</p>\n<p>You can also directly use the command line tools provided by <a href=\"https://conda.github.io/conda-pack/\" rel=\"nofollow\">conda-pack</a> and <a href=\"https://pex.readthedocs.io/en/stable/\" rel=\"nofollow\">pex</a> to generate the packages.</p>\n<p>For pex you can run this command in the root directory to create the package (it includes all requirements from setup.py)</p>\n<pre><code>pex . -o myarchive.pex\n</code></pre>\n<p>You can then run tf-yarn with your generated package:</p>\n<pre><span class=\"n\">run_on_yarn</span><span class=\"p\">(</span>\n    <span class=\"n\">pyenv_zip_path</span><span class=\"o\">=</span><span class=\"s2\">\"myarchive.pex\"</span>\n<span class=\"p\">)</span>\n</pre>\n<h2>Running on GPU</h2>\n<p>YARN does not have first-class support for GPU resources. A common workaround is\nto use <a href=\"https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/NodeLabel.html\" rel=\"nofollow\">node labels</a> where CPU-only nodes are unlabelled, while\nthe GPU ones have a label. Furthermore, in this setting GPU nodes are\ntypically bound to a separate queue which is different from the default one.</p>\n<p>Currently, tf-yarn assumes that the GPU label is <code>\"gpu\"</code>. There are no\nassumptions on the name of the queue with GPU nodes, however, for the sake of\nexample we wil use the name <code>\"ml-gpu\"</code>.</p>\n<p>The default behaviour of <code>run_on_yarn</code> is to run on CPU-only nodes. In order\nto run on the GPU ones:</p>\n<ol>\n<li>Set the <code>queue</code> argument.</li>\n<li>Set <code>TaskSpec.label</code> to <code>NodeLabel.GPU</code> for relevant task types.\nA good rule of a thumb is to run compute heavy <code>\"chief\"</code> and <code>\"worker\"</code>\ntasks on GPU, while keeping <code>\"ps\"</code> and <code>\"evaluator\"</code> on CPU.</li>\n<li>Generate two python environements: one with Tensorflow for CPUs and one\nwith Tensorflow for GPUs. You need to provide a custom path in archive_on_hdfs\nas the default one is already use by the CPU pyenv_zip_path</li>\n</ol>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">getpass</span>\n<span class=\"kn\">import</span> <span class=\"nn\">cluster_pack</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tf_yarn</span> <span class=\"kn\">import</span> <span class=\"n\">NodeLabel</span>\n\n\n<span class=\"n\">pyenv_zip_path_cpu</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">cluster_pack</span><span class=\"o\">.</span><span class=\"n\">upload_env</span><span class=\"p\">()</span>\n<span class=\"n\">pyenv_zip_path_gpu</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">cluster_pack</span><span class=\"o\">.</span><span class=\"n\">upload_env</span><span class=\"p\">(</span>\n    <span class=\"n\">archive_on_hdfs</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">cluster_pack</span><span class=\"o\">.</span><span class=\"n\">get_default_fs</span><span class=\"p\">()</span><span class=\"si\">}</span><span class=\"s2\">/user/</span><span class=\"si\">{</span><span class=\"n\">getpass</span><span class=\"o\">.</span><span class=\"n\">getuser</span><span class=\"p\">()</span><span class=\"si\">}</span><span class=\"s2\">/envs/tf_yarn_gpu_env.pex\"</span><span class=\"p\">,</span>\n    <span class=\"n\">additional_packages</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"tensorflow-gpu\"</span><span class=\"p\">:</span> <span class=\"s2\">\"2.0.0a0\"</span><span class=\"p\">},</span>\n    <span class=\"n\">ignored_packages</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"tensorflow\"</span><span class=\"p\">}</span>\n<span class=\"p\">)</span>\n<span class=\"n\">run_on_yarn</span><span class=\"p\">(</span>\n    <span class=\"p\">{</span><span class=\"n\">NodeLabel</span><span class=\"o\">.</span><span class=\"n\">CPU</span><span class=\"p\">:</span> <span class=\"n\">pyenv_zip_path_cpu</span><span class=\"p\">,</span> <span class=\"n\">NodeLabel</span><span class=\"o\">.</span><span class=\"n\">GPU</span><span class=\"p\">:</span> <span class=\"n\">pyenv_zip_path_gpu</span><span class=\"p\">}</span>\n    <span class=\"n\">experiment_fn</span><span class=\"p\">,</span>\n    <span class=\"n\">task_specs</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s2\">\"chief\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"2 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"n\">NodeLabel</span><span class=\"o\">.</span><span class=\"n\">GPU</span><span class=\"p\">),</span>\n        <span class=\"s2\">\"evaluator\"</span><span class=\"p\">:</span> <span class=\"n\">TaskSpec</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s2\">\"1 GiB\"</span><span class=\"p\">,</span> <span class=\"n\">vcores</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"p\">},</span>\n    <span class=\"n\">queue</span><span class=\"o\">=</span><span class=\"s2\">\"ml-gpu\"</span>\n<span class=\"p\">)</span>\n</pre>\n<h2>Accessing HDFS in the presence of <a href=\"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html\" rel=\"nofollow\">federation</a></h2>\n<p><code>skein</code> the library underlying <code>tf_yarn</code> automatically acquires a delegation token\nfor <code>fs.defaultFS</code> on security-enabled clusters. This should be enough for most\nuse-cases. However, if your experiment needs to access data on namenodes other than\nthe default one, you have to explicitly list them in the <code>file_systems</code> argument\nto <code>run_on_yarn</code>. This would instruct <code>skein</code> to acquire a delegation token for\nthese namenodes in addition to <code>fs.defaultFS</code>:</p>\n<pre><span class=\"n\">run_on_yarn</span><span class=\"p\">(</span>\n    <span class=\"o\">...</span><span class=\"p\">,</span>\n    <span class=\"n\">file_systems</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"hdfs://preprod\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n</pre>\n<p>Depending on the cluster configuration, you might need to point libhdfs to a\ndifferent configuration folder. For instance:</p>\n<pre><span class=\"n\">run_on_yarn</span><span class=\"p\">(</span>\n    <span class=\"o\">...</span><span class=\"p\">,</span>\n    <span class=\"n\">env</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"HADOOP_CONF_DIR\"</span><span class=\"p\">:</span> <span class=\"s2\">\"/etc/hadoop/conf.all\"</span><span class=\"p\">}</span>\n<span class=\"p\">)</span>\n</pre>\n\n          </div>"}, "last_serial": 6688534, "releases": {"0.2.2": [{"comment_text": "", "digests": {"md5": "0c73047a959d8d070930e39d5574c4eb", "sha256": "fb89c67c663895e83f51e87ef966ff305aa292e0c81a29ffad3a656ac05d28a8"}, "downloads": -1, "filename": "tf_yarn-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0c73047a959d8d070930e39d5574c4eb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 29280, "upload_time": "2019-04-12T13:15:02", "upload_time_iso_8601": "2019-04-12T13:15:02.930934Z", "url": "https://files.pythonhosted.org/packages/83/0d/ef28400d27d7bca870b09f05a64f1b3d91d2e5f3e13dd038dc3af33d6be1/tf_yarn-0.2.2-py3-none-any.whl", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "1e1dedc7b9d0d578a01357dc3260e605", "sha256": "ebdefe6341c17bb8956059236acb1a5e03489bb76cfae96cce8507bee1117e90"}, "downloads": -1, "filename": "tf_yarn-0.3.2-py3-none-any.whl", "has_sig": false, "md5_digest": "1e1dedc7b9d0d578a01357dc3260e605", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 33019, "upload_time": "2019-05-22T14:46:55", "upload_time_iso_8601": "2019-05-22T14:46:55.913007Z", "url": "https://files.pythonhosted.org/packages/45/8b/43b5bd9b34504b11f1368fc036f19b9d2799b39a9c41a79d099b63147ee0/tf_yarn-0.3.2-py3-none-any.whl", "yanked": false}], "0.4.14": [{"comment_text": "", "digests": {"md5": "22405295448b43bdc052534d51407bfa", "sha256": "76c4aa55b5365c6bc219e218cee1df2abe616b8ee732f0c3638dea23bf8fa15d"}, "downloads": -1, "filename": "tf_yarn-0.4.14-py3-none-any.whl", "has_sig": false, "md5_digest": "22405295448b43bdc052534d51407bfa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 46013, "upload_time": "2019-12-13T15:51:58", "upload_time_iso_8601": "2019-12-13T15:51:58.869401Z", "url": "https://files.pythonhosted.org/packages/a3/21/a2d7b1f425ba35f440c5ff8970e542a6cc68ee90787b18347bcdb2063fda/tf_yarn-0.4.14-py3-none-any.whl", "yanked": false}], "0.4.15": [{"comment_text": "", "digests": {"md5": "98fccce38264ea83fe1797b231d1a41e", "sha256": "a0e7a3161d4bd4cc391660ab0e33afb9679b17288001c63cc24884c92891cc73"}, "downloads": -1, "filename": "tf_yarn-0.4.15-py3-none-any.whl", "has_sig": false, "md5_digest": "98fccce38264ea83fe1797b231d1a41e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 41151, "upload_time": "2020-01-08T10:07:50", "upload_time_iso_8601": "2020-01-08T10:07:50.670441Z", "url": "https://files.pythonhosted.org/packages/2b/ee/860487b5754e4daaca0be93c698fc76614658ba6ae519f9260a64b1cb023/tf_yarn-0.4.15-py3-none-any.whl", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "96614bfc17c101828c082fc90c228c6f", "sha256": "3de1b2f362710a3f428f92eafdcc68f1dde96beb078d5d9f42f0f65df8490197"}, "downloads": -1, "filename": "tf_yarn-0.4.2-py3-none-any.whl", "has_sig": false, "md5_digest": "96614bfc17c101828c082fc90c228c6f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 34750, "upload_time": "2019-07-08T16:09:03", "upload_time_iso_8601": "2019-07-08T16:09:03.333158Z", "url": "https://files.pythonhosted.org/packages/61/96/17b4ee7a51eb01591fcd9d233aef97dfce0d2b6b0665214a83ec140b2a1d/tf_yarn-0.4.2-py3-none-any.whl", "yanked": false}], "0.4.3": [{"comment_text": "", "digests": {"md5": "ebc49efb41171897d206461a460d9207", "sha256": "ae304741eb9ebfdb649fe6527c2a666bda342a653b34091f1024a3529d7e9173"}, "downloads": -1, "filename": "tf_yarn-0.4.3-py3-none-any.whl", "has_sig": false, "md5_digest": "ebc49efb41171897d206461a460d9207", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 37030, "upload_time": "2019-07-11T17:24:33", "upload_time_iso_8601": "2019-07-11T17:24:33.925254Z", "url": "https://files.pythonhosted.org/packages/19/1c/870c39da390788a0c791f738fe86172d58d3210535fd477f6c9952751271/tf_yarn-0.4.3-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "98fccce38264ea83fe1797b231d1a41e", "sha256": "a0e7a3161d4bd4cc391660ab0e33afb9679b17288001c63cc24884c92891cc73"}, "downloads": -1, "filename": "tf_yarn-0.4.15-py3-none-any.whl", "has_sig": false, "md5_digest": "98fccce38264ea83fe1797b231d1a41e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 41151, "upload_time": "2020-01-08T10:07:50", "upload_time_iso_8601": "2020-01-08T10:07:50.670441Z", "url": "https://files.pythonhosted.org/packages/2b/ee/860487b5754e4daaca0be93c698fc76614658ba6ae519f9260a64b1cb023/tf_yarn-0.4.15-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 02:54:19 2020"}