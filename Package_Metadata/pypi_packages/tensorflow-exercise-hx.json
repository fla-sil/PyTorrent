{"info": {"author": "hx", "author_email": "huangsayn@163.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python"], "description": "# \u8bf4\u660e\u6587\u6863\n`iris.py`\u662f\u9e22\u5c3e\u82b1\u9884\u6d4b\u6a21\u578b  \n`cali_house.py`\u662f\u52a0\u5dde\u623f\u4ef7\u9884\u6d4b\u6a21\u578b  \n\n## \u9e22\u5c3e\u82b1\u9884\u6d4b\u6a21\u578b\n### 1\u3001\u6570\u636e\u5904\u7406\n\u6570\u636e\u96c6\u5305\u542b150\u4e2a\u6570\u636e\u96c6(\u5176\u4e2d120\u4e2a\u662f\u8bad\u7ec3\u96c6`iris_training.csv`\uff0c30\u4e2a\u662f\u6d4b\u8bd5\u96c6`iris_test.csv`)\uff0c\u5206\u4e3a3\u7c7b\uff08Setosa\uff0cVersicolour\uff0cVirginica\uff09\uff0c\u6bcf\u7c7b50\u4e2a\u6570\u636e\uff0c\u6bcf\u4e2a\u6570\u636e\u5305\u542b4\u4e2a\u5c5e\u6027\uff1a\u82b1\u843c\u957f\u5ea6\uff0c\u82b1\u843c\u5bbd\u5ea6\uff0c\u82b1\u74e3\u957f\u5ea6\uff0c\u82b1\u74e3\u5bbd\u5ea6\u3002\n\n    120,4,setosa,versicolor,virginica  \n    6.4,2.8,5.6,2.2,2  \n    5.0,2.3,3.3,1.0,1  \n    4.9,2.5,4.5,1.7,2  \n     .   .   .   .  . \n     .   .   .   .  . \n     .   .   .   .  . \n    4.4,2.9,1.4,0.2,0\n    4.8,3.0,1.4,0.1,0\n    5.5,2.4,3.7,1.0,1\n    \n\u7531\u4e8e\u6807\u7b7e\u662f\u9e22\u5c3e\u82b1\u7684\u7c7b\u522b\uff0c\u56e0\u6b64\u5c06\u6807\u7b7e\u8f6c\u6362\u6210\u72ec\u70ed\u7f16\u7801[1, 0, 0], [0, 1, 0], [0, 0, 1]  \n\n### 2\u3001\u5efa\u7acb\u6a21\u578b\n\u91c7\u7528tensorflow\u5efa\u7acb\u4e00\u4e2a\u7b80\u5355\u7684\u7ebf\u6027\u6a21\u578b\uff1a\n\n    W = tf.Variable(tf.zeros([4, 3]))\n    b = tf.Variable(tf.zeros([3]) + 0.01)\n    output = tf.nn.softmax(tf.matmul(xs, W) + b)\n    \n\u8f93\u5165\u901a\u8fc7\u4e00\u5c42\u7f51\u7edc\u540e\u76f4\u63a5\u63a5\u5165\u4e00\u4e2a`softmax`\u51fd\u6570\u540e\u8f93\u51fa  \n\u635f\u5931\u51fd\u6570\u4e3a\u4ea4\u53c9\u71b5\uff1a\n\n    loss = -tf.reduce_sum(ys * tf.log(output + 1e-10))\n    \n\u91c7\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u6700\u5c0f\u5316`loss`\uff0c\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a`0.001`\uff1a\n\n    train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n    \n### 3\u3001\u6a21\u578b\u8bad\u7ec3\n\u6a21\u578b\u5efa\u7acb\u597d\u540e\uff0c\u901a\u8fc7`tf.global_variables_initializer()`\u5bf9\u53d8\u91cf\u8fdb\u884c\u521d\u59cb\u5316  \n\u6a21\u578b\u603b\u5171\u8bad\u7ec31000\u6b21\uff0c\u6bcf100\u6b21\u8f93\u51fa`loss`\u67e5\u770b\u8bad\u7ec3\u8fc7\u7a0b\n\n    for i in range(1000):\n        sess.run(train_step, feed_dict={xs: x_train, ys: y_train})\n    if i % 100 == 0:\n        print('Loss\uff08train set\uff09:%.2f' % (sess.run(loss, feed_dict={xs: x_train, ys: y_train})))\n        \n\n        \n### 4\u3001\u9e22\u5c3e\u82b1\u79cd\u7c7b\u9884\u6d4b\n\u6a21\u578b\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u5373\u53ef\u5c06\u6d4b\u8bd5\u96c6\u8f93\u5165\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002\u7531\u4e8e\u9884\u6d4b\u7ed3\u679c\u662f\u72ec\u70ed\u7f16\u7801\uff0c\u6240\u4ee5\u51c6\u786e\u7387\u8ba1\u7b97\u4f7f\u7528`tf.argmax()`\u51fd\u6570\u6765\u5b9e\u73b0\u3002\u8fd4\u56de\u503c\u662f\u9884\u6d4b\u7ed3\u679c\u4e2d\u6700\u5927\u503c\u7684\u7d22\u5f15\uff0c\u7531\u4e8e\u72ec\u70ed\u7f16\u7801\u7684\u6027\u8d28\uff0c\u8fd4\u56de\u7684\u7d22\u5f15\u503c\u5373\u4e3a\u7c7b\u522b\u3002\n\u7136\u540e\u4f7f\u7528`tf.equal()`\u5224\u65ad\u662f\u5426\u4e0e\u5b9e\u9645\u7c7b\u522b\u4e00\u81f4\uff08\u8fd4\u56de\u503c\u4e3abool\u578b\uff09\u3002\u6240\u4ee5\u9700\u8981\u901a\u8fc7\u4e00\u4e2a`tf.cast()`\u51fd\u6570\u6765\u8f6c\u6362\u4e3a[0, 1]\u503c\uff0c\u6700\u540e\u53d6\u5e73\u5747\u503c\u6c42\u51fa\u51c6\u786e\u7387\u3002\n\n    access = tf.equal(tf.argmax(output, 1), tf.argmax(ys, 1))\n    accuracy = tf.reduce_mean(tf.cast(access, \"float\"))\n    \n### 5\u3001\u7ed3\u679c\n\u6700\u540e\u6211\u4eec\u5728\u8bad\u7ec3\u96c6\u4e0a\u4ee5\u53ca\u6d4b\u8bd5\u96c6\u90fd\u5f97\u5230\u4e00\u4e2a\u8f83\u6ee1\u610f\u7684\u7ed3\u679c\n\n    --------------------\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b----------------\n    Loss\uff08train set\uff09:125.14\n    Loss\uff08train set\uff09:67.55\n    Loss\uff08train set\uff09:30.55\n    Loss\uff08train set\uff09:23.07\n    Loss\uff08train set\uff09:20.45\n    Loss\uff08train set\uff09:18.60\n    Loss\uff08train set\uff09:17.22\n    Loss\uff08train set\uff09:16.14\n    Loss\uff08train set\uff09:15.28\n    Loss\uff08train set\uff09:14.57\n    --------------------\u8bad\u7ec3\u7ed3\u675f--------------------\n    \n    \n    ********************\u6027\u80fd\u8bc4\u4ef7********************\n    \u8bad\u7ec3\u96c6\u51c6\u786e\u7387\uff1a 0.975\n    \u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\uff1a 0.96666664\n\n\n## \u52a0\u5dde\u623f\u4ef7\u9884\u6d4b\u6a21\u578b\n### 1\u3001\u6570\u636e\u9884\u5904\u7406\n\u52a0\u5dde\u623f\u4ef7\u7684\u6570\u636e\u670920640\u4e2a\u6837\u672c\uff0c\u7279\u5f81\u503c\u67099\u4e2a\uff0cmedian_house_value\u4f5c\u4e3a\u8f93\u51fa\u7ed3\u679c\u3002\u9996\u5148\u4f7f\u7528`describe()`\u6765\u521d\u6b65\u89c2\u5bdf\u6570\u636e\u3002\n\n    import pandas as pd\n\n    features = pd.read_csv('D:/tensorflow_exercise/data/housing.csv')\n    print(features.describe())\n\n    >>>\n             total_bedrooms    population    households  median_income  \n    count        20433           20640         20640        20640  \n    mean       537.870553   1425.476744    499.539680       3.870671   \n    std        421.385070   1132.462122    382.329753       1.899822   \n    min          1.000000      3.000000      1.000000       0.499900   \n    \n\u4ee5\u4e0a\u53ea\u5c55\u793a\u4e86\u90e8\u5206\u7edf\u8ba1\u6570\u636e\uff0c\u4e0d\u8fc7\u53ef\u4ee5\u770b\u5230`total_bedrooms`\u6709\u4e00\u90e8\u5206\u7f3a\u7701\u503c\uff0c\u56e0\u6b64\u6211\u4eec\u5220\u53bb\u6709\u7f3a\u7701\u503c\u7684\u6570\u636e\u3002\u8003\u8651\u5230\u53ef\u80fd\u4f1a\u6709\u91cd\u590d\u7684\u6570\u636e\uff0c\u6240\u4ee5\u8fd8\u9700\u8981\u53bb\u6389\u91cd\u590d\u7684\u6837\u672c\u3002\n\n    nan = features.dropna(subset=['total_bedrooms'], axis=0)  # \u53bb\u9664\u7f3a\u7701\u503c\n    repeat = nan.drop_duplicates()  # \u53bb\u6389\u91cd\u590d\u503c\u6837\u672c\n\n### 2\u3001\u7279\u5f81\u5de5\u7a0b\n\u5728`housing.csv`\u91cc\u9762\uff0c\u524d\u9762\u4e24\u4e2a\u7279\u5f81\u662f\u7ecf\u5ea6(longitude)\u548c\u7eac\u5ea6(latitude)\uff0c\u5c5e\u4e8e\u6570\u503c\u578b\u7279\u5f81\u3002\u79bb\u6563\u7684\u7ecf\u5ea6\u7eac\u5ea6\u5bf9\u4e8e\u623f\u4ef7\u9884\u6d4b\u4f3c\u4e4e\u6ca1\u4ec0\u4e48\u91cd\u8981\u4fe1\u606f\uff0c\u56e0\u6b64\u6211\u4eec\u5bf9\u7ecf\u5ea6\u7eac\u5ea6\u8fdb\u884c\u5206\u7bb1\u5e76\u5408\u5e76\u4e3a\u72ec\u70ed\u7f16\u7801\u3002\n\u6211\u4eec\u53ef\u4ee5\u770b\u770b\u4e4b\u524d\u7684\u7edf\u8ba1\u4fe1\u606f\uff1a\n\n              longitude      latitude  \n    count        20640        20640 \n    mean    -119.569704     35.631861\n    std        2.003532      2.135952\n    min     -124.350000     32.540000\n    max     -114.310000     41.950000  \n    \n\u53ef\u4ee5\u77e5\u9053\u7ecf\u5ea6\u7684\u8303\u56f4\u5927\u6982\u5728(-124.35, -114.31)\u4e4b\u95f4\uff0c\u7eac\u5ea6\u7684\u8303\u56f4\u5927\u6982\u5728(32.54, 41.95)\u4e4b\u95f4\u3002\u6211\u4eec\u4ee51\u00b0\u4e3a\u533a\u95f4\u5206\u522b\u5bf9\u7ecf\u5ea6\u548c\u7eac\u5ea6\u8fdb\u884c\u5206\u7bb1\u3002\n    \n    pd.cut(longitude, range(-125, -112), right=False)\n    pd.cut(latitude, range(31, 43), right=False)\n\u7136\u800c\u5355\u72ec\u8003\u8651\u7ecf\u5ea6\u6216\u662f\u7eac\u5ea6\u90fd\u6ca1\u6709\u592a\u5927\u7684\u610f\u4e49\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u8fd9\u4e24\u4e2a\u7279\u5f81\u4ea4\u53c9\u7ec4\u5408\u6210\u4e00\u4e2a\u7279\u5f81\uff0c\u8fd9\u4e2a\u7279\u5f81\u4ecd\u91c7\u7528\u72ec\u70ed\u7f16\u7801\uff0c\u5b83\u7684\u957f\u5ea6\u4e3a132\u3002\n\n\u5728\u8fd9\u4e9b\u7279\u5f81\u4e2d\uff0c`ocean_proximity`\u662f\u5b57\u7b26\u578b\u7279\u5f81\uff1aNEAR BAY, NEAR OCEAN, ISLAND, INLAND, <1H OCEAN\u3002\u56e0\u6b64\u901a\u8fc7`get_dummies()`\u5c06\u5176\u8f6c\u6362\u4e3a\u72ec\u70ed\u7f16\u7801\u3002\n\n\u518d\u8005\uff0c\u5bf9\u4e8e`total_rooms`\uff0c`population`\u6211\u4eec\u53ef\u4ee5\u7528\u4eba\u5747\u623f\u95f4\u6570`rooms_per_person`\u6765\u8868\u8fbe\u4ed6\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u8fd0\u7b97\u5373\u53ef\u6c42\u51fa\uff1a\n\n    def rooms_per_person(data):  # \u5408\u6210\u65b0\u7279\u5f81\uff1a\u4eba\u5747\u623f\u95f4\u6570 = \u603b\u623f\u95f4\u6570 / \u603b\u4eba\u6570\n        rooms_per_person = data.apply(lambda x: x['total_rooms'] / x['population'], axis=1)  # \u8ba1\u7b97\u7279\u5f81\u503c\n        rooms_per_person[np.abs(rooms_per_person) > 5] = 5  # \u5bf9\u5f02\u5e38\u503c\u8fdb\u884c\u622a\u65ad\u5904\u7406\n        rooms_per_person = rooms_per_person.rename('rooms_per_person')  # \u7279\u5f81\u540d\u79f0\n        return rooms_per_person\n\n\u8ba1\u7b97\u65b0\u7279\u5f81\u4e4b\u540e\u6211\u4eec\u91cd\u65b0\u8fdb\u884c\u7edf\u8ba1\uff0c\u53d1\u73b0\u6709\u4e9b\u7279\u5f81\u503c\u4e0d\u5408\u5e38\u7406\uff0c\u56e0\u6b64\u5bf9\u4e8e\u65b0\u7279\u5f81\u505a\u622a\u65ad\u5904\u7406\uff0c\u628a\u4eba\u5747\u623f\u95f4\u6570\u9650\u5236\u5728(0, 5)\u4e4b\u95f4\u3002\n\n\n### 3\u3001\u7279\u5f81\u503c\u5f52\u4e00\u5316\n\u518d\u6765\u89c2\u5bdf\u6570\u636e\u60c5\u51b5\uff1a\n\n              households    median_income  \n    count       20640             20640 \n    mean      499.539680       3.870671   \n    std       382.329753       1.899822   \n    min         1.000000       0.499900   \n    25%       280.000000       2.563400   \n    50%       409.000000       3.534800   \n    75%       605.000000       4.743250   \n    max       6082.000000      15.000100 \n    \n\u53ef\u4ee5\u770b\u5230\u5bf9\u4e8e\u4e0d\u540c\u7279\u5f81\uff0c\u4ed6\u4eec\u7684\u503c\u53ef\u80fd\u5dee\u522b\u4f1a\u975e\u5e38\u5927\uff0c\u5982\u679c\u76f4\u63a5\u5efa\u7acb\u6a21\u578b\uff0c\u53ef\u80fd\u9020\u6210\u6a21\u578b\u5728\u6570\u503c\u5927\u7684\u7279\u5f81\u4e0a\u6295\u5165\u66f4\u591a\u7cbe\u529b\uff0c\u4f1a\u9020\u6210\u7ed3\u679c\u504f\u62df\u5408\u3002  \n\u56e0\u6b64\u9700\u8981\u5bf9\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff1a\n\n    normalized = \uff08value - mean\uff09 / std\n    \n### 4\u3001\u5efa\u7acb\u6a21\u578b\n\u540c\u6837\u91c7\u7528\u7ebf\u6027\u5c42\uff1a\n\n    W = tf.Variable(tf.zeros([142, 1]))\n    b = tf.Variable(tf.zeros([1]) + 0.01)\n    output = tf.matmul(xs, W) + b\n    \n\u635f\u5931\u51fd\u6570\u7528\u5e73\u65b9\u8bef\u5dee\uff1a\n\n    loss = tf.reduce_mean(tf.square(ys - output))\n   \n\u68af\u5ea6\u4e0b\u964d\u6cd5(learning rate = 0.003)\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\uff1a\n\n     train_step = tf.train.GradientDescentOptimizer(0.003).minimize(loss)\n     \n### 5\u3001\u8bad\u7ec3\u6a21\u578b\n\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\u4e4b\u540e\uff0c\u5f97\u5230\u4e00\u4e2a\u66f4\u4e3a\u7cbe\u70bc\u7684\u6570\u636e\u96c6\uff0c\u5c06\u6570\u636e\u96c6\u968f\u673a\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff1a\n\n    from sklearn.model_selection import train_test_split\n    \n    data = np.array(pd.read_csv('D:/tensorflow_exercise/data/housing_features.csv'))\n    x_train, x_test, y_train, y_test = train_test_split(data[:, :-1], data[:, -1], test_size=0.25, random_state=2018)\n    \n\u5212\u5206\u6bd4\u4f8b\u4e3a3\uff1a1\uff0c\u5f97\u5230\u7684\u8bad\u7ec3\u96c6\u670915325\u4e2a\u6837\u672c\uff0c\u6d4b\u8bd5\u96c6\u67095108\u4e2a\u6837\u672c\u3002  \n\u8bad\u7ec3\u6b21\u657020000\u6b21\uff0c\u6bcf1000\u6b21\u8f93\u51fa\u8bef\u5dee\uff1a\n\n        for i in range(20000):\n            sess.run(train_step, feed_dict={xs: x_train, ys: y_train})\n            if i % 1000 == 0:\n                print('Loss\uff08train set\uff09:%.2f' % (sess.run(loss, feed_dict={xs: x_train, ys: y_train})))\n\n### 6\u3001\u623f\u4ef7\u9884\u6d4b\n\u6700\u7ec8\u7684\u8bad\u7ec3\u7ed3\u679c\uff1a`Loss\uff08train set\uff09:0.32`\n\u5c06\u8bad\u7ec3\u96c6\u8f93\u5165\u6a21\u578b\uff0c\u5f97\u5230\u9884\u6d4b\u7ed3\u679c\uff0c\u901a\u8fc7\u771f\u5b9e-\u9884\u6d4b\u5173\u7cfb\u56fe\u6765\u53cd\u5e94\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5f97\u5230`Loss\uff08test set\uff09:0.31`\n![result](house_value_prediction.PNG)\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://pypi.org/manage/projects/", "keywords": "test python package", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tensorflow-exercise-hx", "package_url": "https://pypi.org/project/tensorflow-exercise-hx/", "platform": "", "project_url": "https://pypi.org/project/tensorflow-exercise-hx/", "project_urls": {"Homepage": "https://pypi.org/manage/projects/"}, "release_url": "https://pypi.org/project/tensorflow-exercise-hx/1.0.1/", "requires_dist": null, "requires_python": "", "summary": "tensorflow\u7ec3\u4e60\uff1a\u9e22\u5c3e\u82b1\u79cd\u7c7b\u9884\u6d4b\uff0c\u52a0\u5dde\u623f\u4ef7\u9884\u6d4b", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            # \u8bf4\u660e\u6587\u6863<br>`iris.py`\u662f\u9e22\u5c3e\u82b1\u9884\u6d4b\u6a21\u578b  <br>`cali_house.py`\u662f\u52a0\u5dde\u623f\u4ef7\u9884\u6d4b\u6a21\u578b  <br><br>## \u9e22\u5c3e\u82b1\u9884\u6d4b\u6a21\u578b<br>### 1\u3001\u6570\u636e\u5904\u7406<br>\u6570\u636e\u96c6\u5305\u542b150\u4e2a\u6570\u636e\u96c6(\u5176\u4e2d120\u4e2a\u662f\u8bad\u7ec3\u96c6`iris_training.csv`\uff0c30\u4e2a\u662f\u6d4b\u8bd5\u96c6`iris_test.csv`)\uff0c\u5206\u4e3a3\u7c7b\uff08Setosa\uff0cVersicolour\uff0cVirginica\uff09\uff0c\u6bcf\u7c7b50\u4e2a\u6570\u636e\uff0c\u6bcf\u4e2a\u6570\u636e\u5305\u542b4\u4e2a\u5c5e\u6027\uff1a\u82b1\u843c\u957f\u5ea6\uff0c\u82b1\u843c\u5bbd\u5ea6\uff0c\u82b1\u74e3\u957f\u5ea6\uff0c\u82b1\u74e3\u5bbd\u5ea6\u3002<br><br>    120,4,setosa,versicolor,virginica  <br>    6.4,2.8,5.6,2.2,2  <br>    5.0,2.3,3.3,1.0,1  <br>    4.9,2.5,4.5,1.7,2  <br>     .   .   .   .  . <br>     .   .   .   .  . <br>     .   .   .   .  . <br>    4.4,2.9,1.4,0.2,0<br>    4.8,3.0,1.4,0.1,0<br>    5.5,2.4,3.7,1.0,1<br>    <br>\u7531\u4e8e\u6807\u7b7e\u662f\u9e22\u5c3e\u82b1\u7684\u7c7b\u522b\uff0c\u56e0\u6b64\u5c06\u6807\u7b7e\u8f6c\u6362\u6210\u72ec\u70ed\u7f16\u7801[1, 0, 0], [0, 1, 0], [0, 0, 1]  <br><br>### 2\u3001\u5efa\u7acb\u6a21\u578b<br>\u91c7\u7528tensorflow\u5efa\u7acb\u4e00\u4e2a\u7b80\u5355\u7684\u7ebf\u6027\u6a21\u578b\uff1a<br><br>    W = tf.Variable(tf.zeros([4, 3]))<br>    b = tf.Variable(tf.zeros([3]) + 0.01)<br>    output = tf.nn.softmax(tf.matmul(xs, W) + b)<br>    <br>\u8f93\u5165\u901a\u8fc7\u4e00\u5c42\u7f51\u7edc\u540e\u76f4\u63a5\u63a5\u5165\u4e00\u4e2a`softmax`\u51fd\u6570\u540e\u8f93\u51fa  <br>\u635f\u5931\u51fd\u6570\u4e3a\u4ea4\u53c9\u71b5\uff1a<br><br>    loss = -tf.reduce_sum(ys * tf.log(output + 1e-10))<br>    <br>\u91c7\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u6700\u5c0f\u5316`loss`\uff0c\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a`0.001`\uff1a<br><br>    train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)<br>    <br>### 3\u3001\u6a21\u578b\u8bad\u7ec3<br>\u6a21\u578b\u5efa\u7acb\u597d\u540e\uff0c\u901a\u8fc7`tf.global_variables_initializer()`\u5bf9\u53d8\u91cf\u8fdb\u884c\u521d\u59cb\u5316  <br>\u6a21\u578b\u603b\u5171\u8bad\u7ec31000\u6b21\uff0c\u6bcf100\u6b21\u8f93\u51fa`loss`\u67e5\u770b\u8bad\u7ec3\u8fc7\u7a0b<br><br>    for i in range(1000):<br>        sess.run(train_step, feed_dict={xs: x_train, ys: y_train})<br>    if i % 100 == 0:<br>        print('Loss\uff08train set\uff09:%.2f' % (sess.run(loss, feed_dict={xs: x_train, ys: y_train})))<br>        <br><br>        <br>### 4\u3001\u9e22\u5c3e\u82b1\u79cd\u7c7b\u9884\u6d4b<br>\u6a21\u578b\u8bad\u7ec3\u5b8c\u6bd5\u4e4b\u540e\uff0c\u5373\u53ef\u5c06\u6d4b\u8bd5\u96c6\u8f93\u5165\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002\u7531\u4e8e\u9884\u6d4b\u7ed3\u679c\u662f\u72ec\u70ed\u7f16\u7801\uff0c\u6240\u4ee5\u51c6\u786e\u7387\u8ba1\u7b97\u4f7f\u7528`tf.argmax()`\u51fd\u6570\u6765\u5b9e\u73b0\u3002\u8fd4\u56de\u503c\u662f\u9884\u6d4b\u7ed3\u679c\u4e2d\u6700\u5927\u503c\u7684\u7d22\u5f15\uff0c\u7531\u4e8e\u72ec\u70ed\u7f16\u7801\u7684\u6027\u8d28\uff0c\u8fd4\u56de\u7684\u7d22\u5f15\u503c\u5373\u4e3a\u7c7b\u522b\u3002<br>\u7136\u540e\u4f7f\u7528`tf.equal()`\u5224\u65ad\u662f\u5426\u4e0e\u5b9e\u9645\u7c7b\u522b\u4e00\u81f4\uff08\u8fd4\u56de\u503c\u4e3abool\u578b\uff09\u3002\u6240\u4ee5\u9700\u8981\u901a\u8fc7\u4e00\u4e2a`tf.cast()`\u51fd\u6570\u6765\u8f6c\u6362\u4e3a[0, 1]\u503c\uff0c\u6700\u540e\u53d6\u5e73\u5747\u503c\u6c42\u51fa\u51c6\u786e\u7387\u3002<br><br>    access = tf.equal(tf.argmax(output, 1), tf.argmax(ys, 1))<br>    accuracy = tf.reduce_mean(tf.cast(access, \"float\"))<br>    <br>### 5\u3001\u7ed3\u679c<br>\u6700\u540e\u6211\u4eec\u5728\u8bad\u7ec3\u96c6\u4e0a\u4ee5\u53ca\u6d4b\u8bd5\u96c6\u90fd\u5f97\u5230\u4e00\u4e2a\u8f83\u6ee1\u610f\u7684\u7ed3\u679c<br><br>    --------------------\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b----------------<br>    Loss\uff08train set\uff09:125.14<br>    Loss\uff08train set\uff09:67.55<br>    Loss\uff08train set\uff09:30.55<br>    Loss\uff08train set\uff09:23.07<br>    Loss\uff08train set\uff09:20.45<br>    Loss\uff08train set\uff09:18.60<br>    Loss\uff08train set\uff09:17.22<br>    Loss\uff08train set\uff09:16.14<br>    Loss\uff08train set\uff09:15.28<br>    Loss\uff08train set\uff09:14.57<br>    --------------------\u8bad\u7ec3\u7ed3\u675f--------------------<br>    <br>    <br>    ********************\u6027\u80fd\u8bc4\u4ef7********************<br>    \u8bad\u7ec3\u96c6\u51c6\u786e\u7387\uff1a 0.975<br>    \u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\uff1a 0.96666664<br><br><br>## \u52a0\u5dde\u623f\u4ef7\u9884\u6d4b\u6a21\u578b<br>### 1\u3001\u6570\u636e\u9884\u5904\u7406<br>\u52a0\u5dde\u623f\u4ef7\u7684\u6570\u636e\u670920640\u4e2a\u6837\u672c\uff0c\u7279\u5f81\u503c\u67099\u4e2a\uff0cmedian_house_value\u4f5c\u4e3a\u8f93\u51fa\u7ed3\u679c\u3002\u9996\u5148\u4f7f\u7528`describe()`\u6765\u521d\u6b65\u89c2\u5bdf\u6570\u636e\u3002<br><br>    import pandas as pd<br><br>    features = pd.read_csv('D:/tensorflow_exercise/data/housing.csv')<br>    print(features.describe())<br><br>    &gt;&gt;&gt;<br>             total_bedrooms    population    households  median_income  <br>    count        20433           20640         20640        20640  <br>    mean       537.870553   1425.476744    499.539680       3.870671   <br>    std        421.385070   1132.462122    382.329753       1.899822   <br>    min          1.000000      3.000000      1.000000       0.499900   <br>    <br>\u4ee5\u4e0a\u53ea\u5c55\u793a\u4e86\u90e8\u5206\u7edf\u8ba1\u6570\u636e\uff0c\u4e0d\u8fc7\u53ef\u4ee5\u770b\u5230`total_bedrooms`\u6709\u4e00\u90e8\u5206\u7f3a\u7701\u503c\uff0c\u56e0\u6b64\u6211\u4eec\u5220\u53bb\u6709\u7f3a\u7701\u503c\u7684\u6570\u636e\u3002\u8003\u8651\u5230\u53ef\u80fd\u4f1a\u6709\u91cd\u590d\u7684\u6570\u636e\uff0c\u6240\u4ee5\u8fd8\u9700\u8981\u53bb\u6389\u91cd\u590d\u7684\u6837\u672c\u3002<br><br>    nan = features.dropna(subset=['total_bedrooms'], axis=0)  # \u53bb\u9664\u7f3a\u7701\u503c<br>    repeat = nan.drop_duplicates()  # \u53bb\u6389\u91cd\u590d\u503c\u6837\u672c<br><br>### 2\u3001\u7279\u5f81\u5de5\u7a0b<br>\u5728`housing.csv`\u91cc\u9762\uff0c\u524d\u9762\u4e24\u4e2a\u7279\u5f81\u662f\u7ecf\u5ea6(longitude)\u548c\u7eac\u5ea6(latitude)\uff0c\u5c5e\u4e8e\u6570\u503c\u578b\u7279\u5f81\u3002\u79bb\u6563\u7684\u7ecf\u5ea6\u7eac\u5ea6\u5bf9\u4e8e\u623f\u4ef7\u9884\u6d4b\u4f3c\u4e4e\u6ca1\u4ec0\u4e48\u91cd\u8981\u4fe1\u606f\uff0c\u56e0\u6b64\u6211\u4eec\u5bf9\u7ecf\u5ea6\u7eac\u5ea6\u8fdb\u884c\u5206\u7bb1\u5e76\u5408\u5e76\u4e3a\u72ec\u70ed\u7f16\u7801\u3002<br>\u6211\u4eec\u53ef\u4ee5\u770b\u770b\u4e4b\u524d\u7684\u7edf\u8ba1\u4fe1\u606f\uff1a<br><br>              longitude      latitude  <br>    count        20640        20640 <br>    mean    -119.569704     35.631861<br>    std        2.003532      2.135952<br>    min     -124.350000     32.540000<br>    max     -114.310000     41.950000  <br>    <br>\u53ef\u4ee5\u77e5\u9053\u7ecf\u5ea6\u7684\u8303\u56f4\u5927\u6982\u5728(-124.35, -114.31)\u4e4b\u95f4\uff0c\u7eac\u5ea6\u7684\u8303\u56f4\u5927\u6982\u5728(32.54, 41.95)\u4e4b\u95f4\u3002\u6211\u4eec\u4ee51\u00b0\u4e3a\u533a\u95f4\u5206\u522b\u5bf9\u7ecf\u5ea6\u548c\u7eac\u5ea6\u8fdb\u884c\u5206\u7bb1\u3002<br>    <br>    pd.cut(longitude, range(-125, -112), right=False)<br>    pd.cut(latitude, range(31, 43), right=False)<br>\u7136\u800c\u5355\u72ec\u8003\u8651\u7ecf\u5ea6\u6216\u662f\u7eac\u5ea6\u90fd\u6ca1\u6709\u592a\u5927\u7684\u610f\u4e49\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u8fd9\u4e24\u4e2a\u7279\u5f81\u4ea4\u53c9\u7ec4\u5408\u6210\u4e00\u4e2a\u7279\u5f81\uff0c\u8fd9\u4e2a\u7279\u5f81\u4ecd\u91c7\u7528\u72ec\u70ed\u7f16\u7801\uff0c\u5b83\u7684\u957f\u5ea6\u4e3a132\u3002<br><br>\u5728\u8fd9\u4e9b\u7279\u5f81\u4e2d\uff0c`ocean_proximity`\u662f\u5b57\u7b26\u578b\u7279\u5f81\uff1aNEAR BAY, NEAR OCEAN, ISLAND, INLAND, &lt;1H OCEAN\u3002\u56e0\u6b64\u901a\u8fc7`get_dummies()`\u5c06\u5176\u8f6c\u6362\u4e3a\u72ec\u70ed\u7f16\u7801\u3002<br><br>\u518d\u8005\uff0c\u5bf9\u4e8e`total_rooms`\uff0c`population`\u6211\u4eec\u53ef\u4ee5\u7528\u4eba\u5747\u623f\u95f4\u6570`rooms_per_person`\u6765\u8868\u8fbe\u4ed6\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u8fd0\u7b97\u5373\u53ef\u6c42\u51fa\uff1a<br><br>    def rooms_per_person(data):  # \u5408\u6210\u65b0\u7279\u5f81\uff1a\u4eba\u5747\u623f\u95f4\u6570 = \u603b\u623f\u95f4\u6570 / \u603b\u4eba\u6570<br>        rooms_per_person = data.apply(lambda x: x['total_rooms'] / x['population'], axis=1)  # \u8ba1\u7b97\u7279\u5f81\u503c<br>        rooms_per_person[np.abs(rooms_per_person) &gt; 5] = 5  # \u5bf9\u5f02\u5e38\u503c\u8fdb\u884c\u622a\u65ad\u5904\u7406<br>        rooms_per_person = rooms_per_person.rename('rooms_per_person')  # \u7279\u5f81\u540d\u79f0<br>        return rooms_per_person<br><br>\u8ba1\u7b97\u65b0\u7279\u5f81\u4e4b\u540e\u6211\u4eec\u91cd\u65b0\u8fdb\u884c\u7edf\u8ba1\uff0c\u53d1\u73b0\u6709\u4e9b\u7279\u5f81\u503c\u4e0d\u5408\u5e38\u7406\uff0c\u56e0\u6b64\u5bf9\u4e8e\u65b0\u7279\u5f81\u505a\u622a\u65ad\u5904\u7406\uff0c\u628a\u4eba\u5747\u623f\u95f4\u6570\u9650\u5236\u5728(0, 5)\u4e4b\u95f4\u3002<br><br><br>### 3\u3001\u7279\u5f81\u503c\u5f52\u4e00\u5316<br>\u518d\u6765\u89c2\u5bdf\u6570\u636e\u60c5\u51b5\uff1a<br><br>              households    median_income  <br>    count       20640             20640 <br>    mean      499.539680       3.870671   <br>    std       382.329753       1.899822   <br>    min         1.000000       0.499900   <br>    25%       280.000000       2.563400   <br>    50%       409.000000       3.534800   <br>    75%       605.000000       4.743250   <br>    max       6082.000000      15.000100 <br>    <br>\u53ef\u4ee5\u770b\u5230\u5bf9\u4e8e\u4e0d\u540c\u7279\u5f81\uff0c\u4ed6\u4eec\u7684\u503c\u53ef\u80fd\u5dee\u522b\u4f1a\u975e\u5e38\u5927\uff0c\u5982\u679c\u76f4\u63a5\u5efa\u7acb\u6a21\u578b\uff0c\u53ef\u80fd\u9020\u6210\u6a21\u578b\u5728\u6570\u503c\u5927\u7684\u7279\u5f81\u4e0a\u6295\u5165\u66f4\u591a\u7cbe\u529b\uff0c\u4f1a\u9020\u6210\u7ed3\u679c\u504f\u62df\u5408\u3002  <br>\u56e0\u6b64\u9700\u8981\u5bf9\u7279\u5f81\u8fdb\u884c\u5f52\u4e00\u5316\uff1a<br><br>    normalized = \uff08value - mean\uff09 / std<br>    <br>### 4\u3001\u5efa\u7acb\u6a21\u578b<br>\u540c\u6837\u91c7\u7528\u7ebf\u6027\u5c42\uff1a<br><br>    W = tf.Variable(tf.zeros([142, 1]))<br>    b = tf.Variable(tf.zeros([1]) + 0.01)<br>    output = tf.matmul(xs, W) + b<br>    <br>\u635f\u5931\u51fd\u6570\u7528\u5e73\u65b9\u8bef\u5dee\uff1a<br><br>    loss = tf.reduce_mean(tf.square(ys - output))<br>   <br>\u68af\u5ea6\u4e0b\u964d\u6cd5(learning rate = 0.003)\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\uff1a<br><br>     train_step = tf.train.GradientDescentOptimizer(0.003).minimize(loss)<br>     <br>### 5\u3001\u8bad\u7ec3\u6a21\u578b<br>\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\u4e4b\u540e\uff0c\u5f97\u5230\u4e00\u4e2a\u66f4\u4e3a\u7cbe\u70bc\u7684\u6570\u636e\u96c6\uff0c\u5c06\u6570\u636e\u96c6\u968f\u673a\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff1a<br><br>    from sklearn.model_selection import train_test_split<br>    <br>    data = np.array(pd.read_csv('D:/tensorflow_exercise/data/housing_features.csv'))<br>    x_train, x_test, y_train, y_test = train_test_split(data[:, :-1], data[:, -1], test_size=0.25, random_state=2018)<br>    <br>\u5212\u5206\u6bd4\u4f8b\u4e3a3\uff1a1\uff0c\u5f97\u5230\u7684\u8bad\u7ec3\u96c6\u670915325\u4e2a\u6837\u672c\uff0c\u6d4b\u8bd5\u96c6\u67095108\u4e2a\u6837\u672c\u3002  <br>\u8bad\u7ec3\u6b21\u657020000\u6b21\uff0c\u6bcf1000\u6b21\u8f93\u51fa\u8bef\u5dee\uff1a<br><br>        for i in range(20000):<br>            sess.run(train_step, feed_dict={xs: x_train, ys: y_train})<br>            if i % 1000 == 0:<br>                print('Loss\uff08train set\uff09:%.2f' % (sess.run(loss, feed_dict={xs: x_train, ys: y_train})))<br><br>### 6\u3001\u623f\u4ef7\u9884\u6d4b<br>\u6700\u7ec8\u7684\u8bad\u7ec3\u7ed3\u679c\uff1a`Loss\uff08train set\uff09:0.32`<br>\u5c06\u8bad\u7ec3\u96c6\u8f93\u5165\u6a21\u578b\uff0c\u5f97\u5230\u9884\u6d4b\u7ed3\u679c\uff0c\u901a\u8fc7\u771f\u5b9e-\u9884\u6d4b\u5173\u7cfb\u56fe\u6765\u53cd\u5e94\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5f97\u5230`Loss\uff08test set\uff09:0.31`<br>![result](house_value_prediction.PNG)<br>\n          </div>"}, "last_serial": 4233624, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "d8728aac188e152605c93b108d9e03ed", "sha256": "7c50b25464c0006c94f573a68007e5747b61f0f110d98380d336fcc0ba3f03ae"}, "downloads": -1, "filename": "tensorflow-exercise-hx-1.0.1.tar.gz", "has_sig": false, "md5_digest": "d8728aac188e152605c93b108d9e03ed", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 408077, "upload_time": "2018-09-03T08:14:56", "upload_time_iso_8601": "2018-09-03T08:14:56.997726Z", "url": "https://files.pythonhosted.org/packages/04/8e/6457a54e1ff771c066827629c477591a743795ebc7e3524e68279e1e03dc/tensorflow-exercise-hx-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d8728aac188e152605c93b108d9e03ed", "sha256": "7c50b25464c0006c94f573a68007e5747b61f0f110d98380d336fcc0ba3f03ae"}, "downloads": -1, "filename": "tensorflow-exercise-hx-1.0.1.tar.gz", "has_sig": false, "md5_digest": "d8728aac188e152605c93b108d9e03ed", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 408077, "upload_time": "2018-09-03T08:14:56", "upload_time_iso_8601": "2018-09-03T08:14:56.997726Z", "url": "https://files.pythonhosted.org/packages/04/8e/6457a54e1ff771c066827629c477591a743795ebc7e3524e68279e1e03dc/tensorflow-exercise-hx-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:22 2020"}