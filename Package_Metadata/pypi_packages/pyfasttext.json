{"info": {"author": "Vincent Rasneur", "author_email": "vrasneur@free.fr", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: POSIX", "Programming Language :: C++", "Programming Language :: Cython", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "pyfasttext\n==========\n\n*Warning!* ``pyfasttext`` **is no longer maintained: use the official Python binding from the fastText repository:** https://github.com/facebookresearch/fastText/tree/master/python\n\nYet another Python binding for\n`fastText <https://github.com/facebookresearch/fastText>`__.\n\nThe binding supports Python 2.6, 2.7 and Python 3. It requires\n`Cython <http://cython.org/>`__.\n\n`Numpy <http://www.numpy.org/>`__ and\n`cysignals <http://cysignals.readthedocs.io/en/latest/>`__ are also\ndependencies, but are optional.\n\n| ``pyfasttext`` has been tested successfully on Linux and Mac OS X.\n| *Warning*: if you want to compile ``pyfasttext`` on Windows, do not\n  compile with the ``cysignals`` module because it does not support this\n  platform.\n\nTable of Contents\n=================\n\n-  `pyfasttext <#pyfasttext>`__\n-  `Table of Contents <#table-of-contents>`__\n\n   -  `Installation <#installation>`__\n\n      -  `Simplest way to install pyfasttext: use\n         pip <#simplest-way-to-install-pyfasttext-use-pip>`__\n\n         -  `Possible compilation error <#possible-compilation-error>`__\n\n      -  `Cloning <#cloning>`__\n      -  `Requirements for Python 2.7 <#requirements-for-python-27>`__\n      -  `Building and installing\n         manually <#building-and-installing-manually>`__\n\n         -  `Building and installing without optional\n            dependencies <#building-and-installing-without-optional-dependencies>`__\n\n   -  `Usage <#usage>`__\n\n      -  `How to load the library? <#how-to-load-the-library>`__\n      -  `How to load an existing\n         model? <#how-to-load-an-existing-model>`__\n      -  `Word representation\n         learning <#word-representation-learning>`__\n\n         -  `Training using Skipgram <#training-using-skipgram>`__\n         -  `Training using CBoW <#training-using-cbow>`__\n\n      -  `Word vectors <#word-vectors>`__\n\n         -  `Word vectors access <#word-vectors-access>`__\n         -  `Vector for a given word <#vector-for-a-given-word>`__\n\n            -  `Numpy ndarray <#numpy-ndarray>`__\n\n         -  `Words for a given vector <#words-for-a-given-vector>`__\n         -  `Get the number of words in the\n            model <#get-the-number-of-words-in-the-model>`__\n         -  `Get all the word vectors in a\n            model <#get-all-the-word-vectors-in-a-model>`__\n\n            -  `Numpy ndarray <#numpy-ndarray-1>`__\n\n         -  `Misc operations with word\n            vectors <#misc-operations-with-word-vectors>`__\n         -  `Word similarity <#word-similarity>`__\n         -  `Most similar words <#most-similar-words>`__\n         -  `Analogies <#analogies>`__\n\n      -  `Text classification <#text-classification>`__\n\n         -  `Supervised learning <#supervised-learning>`__\n         -  `Get all the labels <#get-all-the-labels>`__\n         -  `Get the number of labels <#get-the-number-of-labels>`__\n         -  `Prediction <#prediction>`__\n         -  `Labels and probabilities <#labels-and-probabilities>`__\n\n            -  `Normalized probabilities <#normalized-probabilities>`__\n\n         -  `Labels only <#labels-only>`__\n         -  `Quantization <#quantization>`__\n         -  `Is a model quantized? <#is-a-model-quantized>`__\n\n      -  `Subwords <#subwords>`__\n\n         -  `Get the subwords <#get-the-subwords>`__\n         -  `Get the subword vectors <#get-the-subword-vectors>`__\n\n      -  `Sentence and text vectors <#sentence-and-text-vectors>`__\n\n         -  `Unsupervised models <#unsupervised-models>`__\n         -  `Supervised models <#supervised-models>`__\n\n      -  `Misc utilities <#misc-utilities>`__\n\n         -  `Show the module version <#show-the-module-version>`__\n         -  `Show fastText version <#show-fasttext-version>`__\n         -  `Show the model\n            (hyper)parameters <#show-the-model-hyperparameters>`__\n         -  `Show the model version\n            number <#show-the-model-version-number>`__\n         -  `Extract labels or classes from a\n            dataset <#extract-labels-or-classes-from-a-dataset>`__\n         -  `Extract labels <#extract-labels>`__\n         -  `Extract classes <#extract-classes>`__\n\n      -  `Exceptions <#exceptions>`__\n      -  `Interruptible operations <#interruptible-operations>`__\n\nInstallation\n------------\n\nTo compile ``pyfasttext``, make sure you have the following compiler: \\*\nGCC (``g++``) with C++11 support. \\* LLVM (``clang++``) with (at least)\npartial C++17 support.\n\nSimplest way to install pyfasttext: use pip\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nJust type these lines:\n\n.. code:: bash\n\n    pip install cython\n    pip install pyfasttext\n\nPossible compilation error\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf you have a compilation error, you can try to install ``cysignals``\nmanually:\n\n.. code:: bash\n\n    pip install cysignals\n\nThen, retry to install ``pyfasttext`` with the already mentioned ``pip``\ncommand.\n\nCloning\n~~~~~~~\n\n| ``pyfasttext`` uses git\n  `submodules <https://git-scm.com/book/en/v2/Git-Tools-Submodules>`__.\n| So, you need to add the ``--recursive`` option when you clone the\n  repository.\n\n.. code:: bash\n\n    git clone --recursive https://github.com/vrasneur/pyfasttext.git\n    cd pyfasttext\n\nRequirements for Python 2.7\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n| Python 2.7 support relies on the `future <http://python-future.org>`__\n  module: ``pyfasttext`` needs ``bytes`` objects, which are not\n  available natively in Python2.\n| You can install the ``future`` module with ``pip``.\n\n.. code:: bash\n\n    pip install future\n\nBuilding and installing manually\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFirst, install all the requirements:\n\n.. code:: bash\n\n    pip install -r requirements.txt\n\nThen, build and install with ``setup.py``:\n\n.. code:: bash\n\n    python setup.py install\n\nBuilding and installing without optional dependencies\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n``pyfasttext`` can export word vectors as ``numpy`` ``ndarray``\\ s,\nhowever this feature can be disabled at compile time.\n\nTo compile without ``numpy``, pyfasttext has a ``USE_NUMPY`` environment\nvariable. Set this variable to 0 (or empty), like this:\n\n.. code:: bash\n\n    USE_NUMPY=0 python setup.py install\n\nIf you want to compile without ``cysignals``, likewise, you can set the\n``USE_CYSIGNALS`` environment variable to 0 (or empty).\n\nUsage\n-----\n\nHow to load the library?\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    >>> from pyfasttext import FastText\n\nHow to load an existing model?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    >>> model = FastText('/path/to/model.bin')\n\nor\n\n.. code:: python\n\n    >>> model = FastText()\n    >>> model.load_model('/path/to/model.bin')\n\nWord representation learning\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n| You can use all the options provided by the ``fastText`` binary\n  (``input``, ``output``, ``epoch``, ``lr``, ...).\n| Just use keyword arguments in the training methods of the ``FastText``\n  object.\n\nTraining using Skipgram\n^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code:: python\n\n    >>> model = FastText()\n    >>> model.skipgram(input='data.txt', output='model', epoch=100, lr=0.7)\n\nTraining using CBoW\n^^^^^^^^^^^^^^^^^^^\n\n.. code:: python\n\n    >>> model = FastText()\n    >>> model.cbow(input='data.txt', output='model', epoch=100, lr=0.7)\n\nWord vectors\n~~~~~~~~~~~~\n\nWord vectors access\n^^^^^^^^^^^^^^^^^^^\n\nVector for a given word\n'''''''''''''''''''''''\n\nBy default, a single word vector is returned as a regular Python array\nof floats.\n\n.. code:: python\n\n    >>> model['dog']\n    array('f', [-1.308749794960022, -1.8326224088668823, ...])\n\nNumpy ndarray\n             \n\nThe ``model.get_numpy_vector(word)`` method returns the word vector as a\n``numpy`` ``ndarray``.\n\n.. code:: python\n\n    >>> model.get_numpy_vector('dog')\n    array([-1.30874979, -1.83262241, ...], dtype=float32)\n\nIf you want a normalized vector (*i.e.* the vector divided by its norm),\nthere is an optional boolean parameter named ``normalized``.\n\n.. code:: python\n\n    >>> model.get_numpy_vector('dog', normalized=True)\n    array([-0.07084749, -0.09920666, ...], dtype=float32)\n\nWords for a given vector\n''''''''''''''''''''''''\n\n| The inverse operation of ``model[word]`` or\n  ``model.get_numpy_vector(word)`` is\n  ``model.words_for_vector(vector, k)``.\n| It returns a list of the ``k`` words closest to the provided vector.\n  The default value for ``k`` is 1.\n\n.. code:: python\n\n    >>> king = model.get_numpy_vector('king')\n    >>> man = model.get_numpy_vector('man')\n    >>> woman = model.get_numpy_vector('woman')\n    >>> model.words_for_vector(king + woman - man, k=1)\n    [('queen', 0.77121970653533936)]\n\nGet the number of words in the model\n''''''''''''''''''''''''''''''''''''\n\n.. code:: python\n\n    >>> model.nwords\n    500000\n\nGet all the word vectors in a model\n'''''''''''''''''''''''''''''''''''\n\n.. code:: python\n\n    >>> for word in model.words:\n    ...   print(word, model[word])\n\nNumpy ndarray\n             \n\nIf you want all the word vectors as a big ``numpy`` ``ndarray``, you can\nuse the ``numpy_normalized_vectors`` member. Note that all these vectors\nare *normalized*.\n\n.. code:: python\n\n    >>> model.nwords\n    500000\n    >>> model.numpy_normalized_vectors\n    array([[-0.07549749, -0.09407753, ...],\n           [ 0.00635979, -0.17272158, ...],\n           ..., \n           [-0.01009259,  0.14604086, ...],\n           [ 0.12467574, -0.0609326 , ...]], dtype=float32)\n    >>> model.numpy_normalized_vectors.shape\n    (500000, 100) # (number of words, dimension)\n\nMisc operations with word vectors\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWord similarity\n'''''''''''''''\n\n.. code:: python\n\n    >>> model.similarity('dog', 'cat')\n    0.75596606254577637\n\nMost similar words\n''''''''''''''''''\n\n.. code:: python\n\n    >>> model.nearest_neighbors('dog', k=2)\n    [('dogs', 0.7843924736976624), ('cat', 75596606254577637)]\n\nAnalogies\n'''''''''\n\nThe ``model.most_similar()`` method works similarly as the one in\n`gensim <https://radimrehurek.com/gensim/models/keyedvectors.html>`__.\n\n.. code:: python\n\n    >>> model.most_similar(positive=['woman', 'king'], negative=['man'], k=1)\n    [('queen', 0.77121970653533936)]\n\nText classification\n~~~~~~~~~~~~~~~~~~~\n\nSupervised learning\n^^^^^^^^^^^^^^^^^^^\n\n.. code:: python\n\n    >>> model = FastText()\n    >>> model.supervised(input='/path/to/input.txt', output='/path/to/model', epoch=100, lr=0.7)\n\nGet all the labels\n^^^^^^^^^^^^^^^^^^\n\n.. code:: python\n\n    >>> model.labels\n    ['LABEL1', 'LABEL2', ...]\n\nGet the number of labels\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code:: python\n\n    >>> model.nlabels\n    100\n\nPrediction\n^^^^^^^^^^\n\n| To obtain the ``k`` most likely labels from test sentences, there are\n  multiple ``model.predict_*()`` methods.\n| The default value for ``k`` is 1. If you want to obtain all the\n  possible labels, use ``None`` for ``k``.\n\nLabels and probabilities\n''''''''''''''''''''''''\n\nIf you have a list of strings (or an iterable object), use this:\n\n.. code:: python\n\n    >>> model.predict_proba(['first sentence\\n', 'second sentence\\n'], k=2)\n    [[('LABEL1', 0.99609375), ('LABEL3', 1.953126549381068e-08)], [('LABEL2', 1.0), ('LABEL3', 1.953126549381068e-08)]]\n\nIf you want to test a single string, use this:\n\n.. code:: python\n\n    >>> model.predict_proba_single('first sentence\\n', k=2)\n    [('LABEL1', 0.99609375), ('LABEL3', 1.953126549381068e-08)]\n\n**WARNING**: In order to get the same probabilities as the ``fastText``\nbinary, you have to add a newline (``\\n``) at the end of each string.\n\nIf your test data is stored inside a file, use this:\n\n.. code:: python\n\n    >>> model.predict_proba_file('/path/to/test.txt', k=2)\n    [[('LABEL1', 0.99609375), ('LABEL3', 1.953126549381068e-08)], [('LABEL2', 1.0), ('LABEL3', 1.953126549381068e-08)]]\n\nNormalized probabilities\n                        \n\nFor performance reasons, fastText probabilities often do not sum up to\n1.0.\n\nIf you want normalized probabilities (where the sum is closer to 1.0\nthan the original probabilities), you can use the ``normalized=True``\nparameter in all the methods that output probabilities\n(``model.predict_proba()``, ``model.predict_proba_file()`` and\n``model.predict_proba_single()``).\n\n.. code:: python\n\n    >>> sum(proba for label, proba in model.predict_proba_single('this is a sentence that needs to be classified\\n', k=None))\n    0.9785203068801335\n    >>> sum(proba for label, proba in model.predict_proba_single('this is a sentence that needs to be classified\\n', k=None, normalized=True))\n    0.9999999999999898\n\nLabels only\n'''''''''''\n\nIf you have a list of strings (or an iterable object), use this:\n\n.. code:: python\n\n    >>> model.predict(['first sentence\\n', 'second sentence\\n'], k=2)\n    [['LABEL1', 'LABEL3'], ['LABEL2', 'LABEL3']]\n\nIf you want to test a single string, use this:\n\n.. code:: python\n\n    >>> model.predict_single('first sentence\\n', k=2)\n    ['LABEL1', 'LABEL3']\n\n**WARNING**: In order to get the same probabilities as the ``fastText``\nbinary, you have to add a newline (``\\n``) at the end of each string.\n\nIf your test data is stored inside a file, use this:\n\n.. code:: python\n\n    >>> model.predict_file('/path/to/test.txt', k=2)\n    [['LABEL1', 'LABEL3'], ['LABEL2', 'LABEL3']]\n\nQuantization\n^^^^^^^^^^^^\n\nUse keyword arguments in the ``model.quantize()`` method.\n\n.. code:: python\n\n    >>> model.quantize(input='/path/to/input.txt', output='/path/to/model')\n\nYou can load quantized models using the ``FastText`` constructor or the\n``model.load_model()`` method.\n\nIs a model quantized?\n'''''''''''''''''''''\n\nIf you want to know if a model has been quantized before, use the\n``model.quantized`` attribute.\n\n.. code:: python\n\n    >>> model = FastText('/path/to/model.bin')\n    >>> model.quantized\n    False\n    >>> model = FastText('/path/to/model.ftz')\n    >>> model.quantized\n    True\n\nSubwords\n~~~~~~~~\n\nfastText can use subwords (*i.e.* character ngrams) when doing\nunsupervised or supervised learning.\n\nYou can access the subwords, and their associated vectors, using\n``pyfasttext``.\n\nGet the subwords\n^^^^^^^^^^^^^^^^\n\nfastText's word embeddings can be augmented with subword-level\ninformation. It is possible to retrieve the subwords and their\nassociated vectors from a model using ``pyfasttext``.\n\nTo retrieve all the subwords for a given word, use the\n``model.get_all_subwords(word)`` method.\n\n.. code:: python\n\n    >>> model.args.get('minn'), model.args.get('maxn')\n    (2, 4)\n    >>> model.get_all_subwords('hello') # word + subwords from 2 to 4 characters\n    ['hello', '<h', '<he', '<hel', 'he', 'hel', 'hell', 'el', 'ell', 'ello', 'll', 'llo', 'llo>', 'lo', 'lo>', 'o>']\n\nFor fastText, ``<`` means \"beginning of a word\" and ``>`` means \"end of\na word\".\n\nAs you can see, fastText includes the full word. You can omit it using\nthe ``omit_word=True`` keyword argument.\n\n.. code:: python\n\n    >>> model.get_all_subwords('hello', omit_word=True)\n    ['<h', '<he', '<hel', 'he', 'hel', 'hell', 'el', 'ell', 'ello', 'll', 'llo', 'llo>', 'lo', 'lo>', 'o>']\n\nWhen a model is quantized, fastText may *prune* some subwords. If you\nwant to see only the subwords that are really used when computing a word\nvector, you should use the ``model.get_subwords(word)`` method.\n\n.. code:: python\n\n    >>> model.quantized\n    True\n    >>> model.get_subwords('beautiful')\n    ['eau', 'aut', 'ful', 'ul']\n    >>> model.get_subwords('hello')\n    ['hello'] # fastText will not use any subwords when computing the word vector, only the full word\n\nGet the subword vectors\n^^^^^^^^^^^^^^^^^^^^^^^\n\nTo get the individual vectors given the subwords, use the\n``model.get_numpy_subword_vectors(word)`` method.\n\n.. code:: python\n\n    >>> model.get_numpy_subword_vectors('beautiful') # 4 vectors, so 4 rows\n    array([[ 0.49022141,  0.13586822,  ..., -0.14065443,  0.89617103], # subword \"eau\"\n           [-0.42594951,  0.06260503,  ..., -0.18182631,  0.34219387], # subword \"aut\"\n           [ 0.49958718,  2.93831301,  ..., -1.97498322, -1.16815805], # subword \"ful\"\n           [-0.4368791 , -1.92924356,  ...,  1.62921488, 1.90240896]], dtype=float32) # subword \"ul\"\n\nIn fastText, the final word vector is the average of these individual\nvectors.\n\n.. code:: python\n\n    >>> import numpy as np\n    >>> vec1 = model.get_numpy_vector('beautiful')\n    >>> vecs2 = model.get_numpy_subword_vectors('beautiful')\n    >>> np.allclose(vec1, np.average(vecs2, axis=0))\n    True\n\nSentence and text vectors\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo compute the vector of a sequence of words (*i.e.* a sentence),\nfastText uses two different methods: \\* one for unsupervised models \\*\nanother one for supervised models\n\nWhen fastText computes a word vector, recall that it uses the average of\nthe following vectors: the word itself and its subwords.\n\nUnsupervised models\n^^^^^^^^^^^^^^^^^^^\n\nFor unsupervised models, the representation of a sentence for fastText\nis the average of the normalized word vectors.\n\n| To get the resulting vector as a regular Python array, use the\n  ``model.get_sentence_vector(line)`` method.\n| To get the resulting vector as a ``numpy`` ``ndarray``, use the\n  ``model.get_numpy_sentence_vector(line)`` method.\n\n.. code:: python\n\n    >>> vec = model.get_numpy_sentence_vector('beautiful cats')\n    >>> vec1 = model.get_numpy_vector('beautiful', normalized=True)\n    >>> vec2 = model.get_numpy_vector('cats', normalized=True)\n    >>> np.allclose(vec, np.average([vec1, vec2], axis=0)\n    True\n\nSupervised models\n^^^^^^^^^^^^^^^^^\n\nFor supervised models, fastText uses the regular word vectors, as well\nas vectors computed using word ngrams (*i.e.* shorter sequences of words\nfrom the sentence). When computing the average, these vectors are not\nnormalized.\n\n| To get the resulting vector as a regular Python array, use the\n  ``model.get_text_vector(line)`` method.\n| To get the resulting vector as a ``numpy`` ``ndarray``, use the\n  ``model.get_numpy_text_vector(line)`` method.\n\n.. code:: python\n\n    >>> model.get_numpy_sentence_vector('beautiful cats') # for an unsupervised model\n    array([-0.20266785,  0.3407566 ,  ...,  0.03044436,  0.39055538], dtype=float32)\n    >>> model.get_numpy_text_vector('beautiful cats') # for a supervised model\n    array([-0.20840774,  0.4289546 ,  ..., -0.00457615,  0.52417743], dtype=float32)\n\nMisc utilities\n~~~~~~~~~~~~~~\n\nShow the module version\n^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code:: python\n\n    >>> import pyfasttext\n    >>> pyfasttext.__version__\n    '0.4.3'\n\nShow fastText version\n^^^^^^^^^^^^^^^^^^^^^\n\nAs there is no version number in fastText, we use the latest fastText\ncommit hash (from ``HEAD``) as a substitute.\n\n.. code:: python\n\n    >>> import pyfasttext\n    >>> pyfasttext.__fasttext_version__\n    '431c9e2a9b5149369cc60fb9f5beba58dcf8ca17'\n\nShow the model (hyper)parameters\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code:: python\n\n    >>> model.args\n    {'bucket': 11000000,\n     'cutoff': 0,\n     'dim': 100,\n     'dsub': 2,\n     'epoch': 100,\n    ...\n    }\n\nShow the model version number\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nfastText uses a versioning scheme for its generated models. You can\nretrieve the model version number using the ``model.version`` attribute.\n\n+----------------+------------------------+\n| version number | description            |\n+================+========================+\n| -1             | for really old models  |\n|                | with no version number |\n+----------------+------------------------+\n| 11             | first version number   |\n|                | added by fastText      |\n+----------------+------------------------+\n| 12             | for models generated   |\n|                | after fastText added   |\n|                | support for subwords   |\n|                | in supervised learning |\n+----------------+------------------------+\n\n.. code:: python\n\n    >>> model.version\n    12\n\nExtract labels or classes from a dataset\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nYou can use the ``FastText`` object to extract labels or classes from a\ndataset. The label prefix (which is ``__label__`` by default) is set\nusing the ``label`` parameter in the constructor.\n\nIf you load an existing model, the label prefix will be the one defined\nin the model.\n\n.. code:: python\n\n    >>> model = FastText(label='__my_prefix__')\n\nExtract labels\n''''''''''''''\n\nThere can be multiple labels per line.\n\n.. code:: python\n\n    >>> model.extract_labels('/path/to/dataset1.txt')\n    [['LABEL2', 'LABEL5'], ['LABEL1'], ...]\n\nExtract classes\n'''''''''''''''\n\nThere can be only one class per line.\n\n.. code:: python\n\n    >>> model.extract_classes('/path/to/dataset2.txt')\n    ['LABEL3', 'LABEL1', 'LABEL2', ...]\n\nExceptions\n~~~~~~~~~~\n\nThe ``fastText`` source code directly calls exit() when something wrong\nhappens (*e.g.* a model file does not exist, ...).\n\nInstead of exiting, ``pyfasttext`` raises a Python exception\n(``RuntimeError``).\n\n.. code:: python\n\n    >>> import pyfasttext\n    >>> model = pyfasttext.FastText('/path/to/non-existing_model.bin')\n    Model file cannot be opened for loading!\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"src/pyfasttext.pyx\", line 124, in pyfasttext.FastText.__cinit__ (src/pyfasttext.cpp:1800)\n      File \"src/pyfasttext.pyx\", line 348, in pyfasttext.FastText.load_model (src/pyfasttext.cpp:5947)\n    RuntimeError: fastext tried to exit: 1\n\nInterruptible operations\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n``pyfasttext`` uses ``cysignals`` to make all the computationally\nintensive operations (*e.g.* training) interruptible.\n\nTo easily interrupt such an operation, just type ``Ctrl-C`` in your\nPython shell.\n\n.. code:: python\n\n    >>> model.skipgram(input='/path/to/input.txt', output='/path/to/mymodel')\n    Read 12M words\n    Number of words:  60237\n    Number of labels: 0\n    ... # type Ctrl-C during training\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"src/pyfasttext.pyx\", line 680, in pyfasttext.FastText.skipgram (src/pyfasttext.cpp:11125)\n      File \"src/pyfasttext.pyx\", line 674, in pyfasttext.FastText.train (src/pyfasttext.cpp:11009)\n      File \"src/pyfasttext.pyx\", line 668, in pyfasttext.FastText.train (src/pyfasttext.cpp:10926)\n      File \"src/cysignals/signals.pyx\", line 94, in cysignals.signals.sig_raise_exception (build/src/cysignals/signals.c:1328)\n    KeyboardInterrupt\n    >>> # you can have your shell back!", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/vrasneur/pyfasttext/releases/download/0.4.6/pyfasttext-0.4.6.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/vrasneur/pyfasttext", "keywords": "", "license": "GPLv3", "maintainer": "", "maintainer_email": "", "name": "pyfasttext", "package_url": "https://pypi.org/project/pyfasttext/", "platform": "", "project_url": "https://pypi.org/project/pyfasttext/", "project_urls": {"Download": "https://github.com/vrasneur/pyfasttext/releases/download/0.4.6/pyfasttext-0.4.6.tar.gz", "Homepage": "https://github.com/vrasneur/pyfasttext"}, "release_url": "https://pypi.org/project/pyfasttext/0.4.6/", "requires_dist": null, "requires_python": "", "summary": "Yet another Python binding for fastText", "version": "0.4.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"pyfasttext\">\n<h2>pyfasttext</h2>\n<p><em>Warning!</em> <tt>pyfasttext</tt> <strong>is no longer maintained: use the official Python binding from the fastText repository:</strong> <a href=\"https://github.com/facebookresearch/fastText/tree/master/python\" rel=\"nofollow\">https://github.com/facebookresearch/fastText/tree/master/python</a></p>\n<p>Yet another Python binding for\n<a href=\"https://github.com/facebookresearch/fastText\" rel=\"nofollow\">fastText</a>.</p>\n<p>The binding supports Python 2.6, 2.7 and Python 3. It requires\n<a href=\"http://cython.org/\" rel=\"nofollow\">Cython</a>.</p>\n<p><a href=\"http://www.numpy.org/\" rel=\"nofollow\">Numpy</a> and\n<a href=\"http://cysignals.readthedocs.io/en/latest/\" rel=\"nofollow\">cysignals</a> are also\ndependencies, but are optional.</p>\n<div>\n<div><tt>pyfasttext</tt> has been tested successfully on Linux and Mac OS X.</div>\n<div><em>Warning</em>: if you want to compile <tt>pyfasttext</tt> on Windows, do not\ncompile with the <tt>cysignals</tt> module because it does not support this\nplatform.</div>\n</div>\n</div>\n<div id=\"table-of-contents\">\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#pyfasttext\" rel=\"nofollow\">pyfasttext</a></li>\n<li><a href=\"#table-of-contents\" rel=\"nofollow\">Table of Contents</a><ul>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a><ul>\n<li><a href=\"#simplest-way-to-install-pyfasttext-use-pip\" rel=\"nofollow\">Simplest way to install pyfasttext: use\npip</a><ul>\n<li><a href=\"#possible-compilation-error\" rel=\"nofollow\">Possible compilation error</a></li>\n</ul>\n</li>\n<li><a href=\"#cloning\" rel=\"nofollow\">Cloning</a></li>\n<li><a href=\"#requirements-for-python-27\" rel=\"nofollow\">Requirements for Python 2.7</a></li>\n<li><a href=\"#building-and-installing-manually\" rel=\"nofollow\">Building and installing\nmanually</a><ul>\n<li><a href=\"#building-and-installing-without-optional-dependencies\" rel=\"nofollow\">Building and installing without optional\ndependencies</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a><ul>\n<li><a href=\"#how-to-load-the-library\" rel=\"nofollow\">How to load the library?</a></li>\n<li><a href=\"#how-to-load-an-existing-model\" rel=\"nofollow\">How to load an existing\nmodel?</a></li>\n<li><a href=\"#word-representation-learning\" rel=\"nofollow\">Word representation\nlearning</a><ul>\n<li><a href=\"#training-using-skipgram\" rel=\"nofollow\">Training using Skipgram</a></li>\n<li><a href=\"#training-using-cbow\" rel=\"nofollow\">Training using CBoW</a></li>\n</ul>\n</li>\n<li><a href=\"#word-vectors\" rel=\"nofollow\">Word vectors</a><ul>\n<li><a href=\"#word-vectors-access\" rel=\"nofollow\">Word vectors access</a></li>\n<li><a href=\"#vector-for-a-given-word\" rel=\"nofollow\">Vector for a given word</a><ul>\n<li><a href=\"#numpy-ndarray\" rel=\"nofollow\">Numpy ndarray</a></li>\n</ul>\n</li>\n<li><a href=\"#words-for-a-given-vector\" rel=\"nofollow\">Words for a given vector</a></li>\n<li><a href=\"#get-the-number-of-words-in-the-model\" rel=\"nofollow\">Get the number of words in the\nmodel</a></li>\n<li><a href=\"#get-all-the-word-vectors-in-a-model\" rel=\"nofollow\">Get all the word vectors in a\nmodel</a><ul>\n<li><a href=\"#numpy-ndarray-1\" rel=\"nofollow\">Numpy ndarray</a></li>\n</ul>\n</li>\n<li><a href=\"#misc-operations-with-word-vectors\" rel=\"nofollow\">Misc operations with word\nvectors</a></li>\n<li><a href=\"#word-similarity\" rel=\"nofollow\">Word similarity</a></li>\n<li><a href=\"#most-similar-words\" rel=\"nofollow\">Most similar words</a></li>\n<li><a href=\"#analogies\" rel=\"nofollow\">Analogies</a></li>\n</ul>\n</li>\n<li><a href=\"#text-classification\" rel=\"nofollow\">Text classification</a><ul>\n<li><a href=\"#supervised-learning\" rel=\"nofollow\">Supervised learning</a></li>\n<li><a href=\"#get-all-the-labels\" rel=\"nofollow\">Get all the labels</a></li>\n<li><a href=\"#get-the-number-of-labels\" rel=\"nofollow\">Get the number of labels</a></li>\n<li><a href=\"#prediction\" rel=\"nofollow\">Prediction</a></li>\n<li><a href=\"#labels-and-probabilities\" rel=\"nofollow\">Labels and probabilities</a><ul>\n<li><a href=\"#normalized-probabilities\" rel=\"nofollow\">Normalized probabilities</a></li>\n</ul>\n</li>\n<li><a href=\"#labels-only\" rel=\"nofollow\">Labels only</a></li>\n<li><a href=\"#quantization\" rel=\"nofollow\">Quantization</a></li>\n<li><a href=\"#is-a-model-quantized\" rel=\"nofollow\">Is a model quantized?</a></li>\n</ul>\n</li>\n<li><a href=\"#subwords\" rel=\"nofollow\">Subwords</a><ul>\n<li><a href=\"#get-the-subwords\" rel=\"nofollow\">Get the subwords</a></li>\n<li><a href=\"#get-the-subword-vectors\" rel=\"nofollow\">Get the subword vectors</a></li>\n</ul>\n</li>\n<li><a href=\"#sentence-and-text-vectors\" rel=\"nofollow\">Sentence and text vectors</a><ul>\n<li><a href=\"#unsupervised-models\" rel=\"nofollow\">Unsupervised models</a></li>\n<li><a href=\"#supervised-models\" rel=\"nofollow\">Supervised models</a></li>\n</ul>\n</li>\n<li><a href=\"#misc-utilities\" rel=\"nofollow\">Misc utilities</a><ul>\n<li><a href=\"#show-the-module-version\" rel=\"nofollow\">Show the module version</a></li>\n<li><a href=\"#show-fasttext-version\" rel=\"nofollow\">Show fastText version</a></li>\n<li><a href=\"#show-the-model-hyperparameters\" rel=\"nofollow\">Show the model\n(hyper)parameters</a></li>\n<li><a href=\"#show-the-model-version-number\" rel=\"nofollow\">Show the model version\nnumber</a></li>\n<li><a href=\"#extract-labels-or-classes-from-a-dataset\" rel=\"nofollow\">Extract labels or classes from a\ndataset</a></li>\n<li><a href=\"#extract-labels\" rel=\"nofollow\">Extract labels</a></li>\n<li><a href=\"#extract-classes\" rel=\"nofollow\">Extract classes</a></li>\n</ul>\n</li>\n<li><a href=\"#exceptions\" rel=\"nofollow\">Exceptions</a></li>\n<li><a href=\"#interruptible-operations\" rel=\"nofollow\">Interruptible operations</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<div id=\"installation\">\n<h3>Installation</h3>\n<p>To compile <tt>pyfasttext</tt>, make sure you have the following compiler: *\nGCC (<tt>g++</tt>) with C++11 support. * LLVM (<tt>clang++</tt>) with (at least)\npartial C++17 support.</p>\n<div id=\"simplest-way-to-install-pyfasttext-use-pip\">\n<h4>Simplest way to install pyfasttext: use pip</h4>\n<p>Just type these lines:</p>\n<pre>pip install cython\npip install pyfasttext\n</pre>\n<div id=\"possible-compilation-error\">\n<h5>Possible compilation error</h5>\n<p>If you have a compilation error, you can try to install <tt>cysignals</tt>\nmanually:</p>\n<pre>pip install cysignals\n</pre>\n<p>Then, retry to install <tt>pyfasttext</tt> with the already mentioned <tt>pip</tt>\ncommand.</p>\n</div>\n</div>\n<div id=\"cloning\">\n<h4>Cloning</h4>\n<div>\n<div><tt>pyfasttext</tt> uses git\n<a href=\"https://git-scm.com/book/en/v2/Git-Tools-Submodules\" rel=\"nofollow\">submodules</a>.</div>\n<div>So, you need to add the <tt><span class=\"pre\">--recursive</span></tt> option when you clone the\nrepository.</div>\n</div>\n<pre>git clone --recursive https://github.com/vrasneur/pyfasttext.git\n<span class=\"nb\">cd</span> pyfasttext\n</pre>\n</div>\n<div id=\"requirements-for-python-2-7\">\n<h4>Requirements for Python 2.7</h4>\n<div>\n<div>Python 2.7 support relies on the <a href=\"http://python-future.org\" rel=\"nofollow\">future</a>\nmodule: <tt>pyfasttext</tt> needs <tt>bytes</tt> objects, which are not\navailable natively in Python2.</div>\n<div>You can install the <tt>future</tt> module with <tt>pip</tt>.</div>\n</div>\n<pre>pip install future\n</pre>\n</div>\n<div id=\"building-and-installing-manually\">\n<h4>Building and installing manually</h4>\n<p>First, install all the requirements:</p>\n<pre>pip install -r requirements.txt\n</pre>\n<p>Then, build and install with <tt>setup.py</tt>:</p>\n<pre>python setup.py install\n</pre>\n<div id=\"building-and-installing-without-optional-dependencies\">\n<h5>Building and installing without optional dependencies</h5>\n<p><tt>pyfasttext</tt> can export word vectors as <tt>numpy</tt> <tt>ndarray</tt>s,\nhowever this feature can be disabled at compile time.</p>\n<p>To compile without <tt>numpy</tt>, pyfasttext has a <tt>USE_NUMPY</tt> environment\nvariable. Set this variable to 0 (or empty), like this:</p>\n<pre><span class=\"nv\">USE_NUMPY</span><span class=\"o\">=</span><span class=\"m\">0</span> python setup.py install\n</pre>\n<p>If you want to compile without <tt>cysignals</tt>, likewise, you can set the\n<tt>USE_CYSIGNALS</tt> environment variable to 0 (or empty).</p>\n</div>\n</div>\n</div>\n<div id=\"usage\">\n<h3>Usage</h3>\n<div id=\"how-to-load-the-library\">\n<h4>How to load the library?</h4>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">pyfasttext</span> <span class=\"kn\">import</span> <span class=\"n\">FastText</span>\n</pre>\n</div>\n<div id=\"how-to-load-an-existing-model\">\n<h4>How to load an existing model?</h4>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FastText</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/model.bin'</span><span class=\"p\">)</span>\n</pre>\n<p>or</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FastText</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/model.bin'</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"word-representation-learning\">\n<h4>Word representation learning</h4>\n<div>\n<div>You can use all the options provided by the <tt>fastText</tt> binary\n(<tt>input</tt>, <tt>output</tt>, <tt>epoch</tt>, <tt>lr</tt>, \u2026).</div>\n<div>Just use keyword arguments in the training methods of the <tt>FastText</tt>\nobject.</div>\n</div>\n<div id=\"training-using-skipgram\">\n<h5>Training using Skipgram</h5>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FastText</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">skipgram</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"o\">=</span><span class=\"s1\">'data.txt'</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"o\">=</span><span class=\"s1\">'model'</span><span class=\"p\">,</span> <span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"training-using-cbow\">\n<h5>Training using CBoW</h5>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FastText</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">cbow</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"o\">=</span><span class=\"s1\">'data.txt'</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"o\">=</span><span class=\"s1\">'model'</span><span class=\"p\">,</span> <span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">)</span>\n</pre>\n</div>\n</div>\n<div id=\"word-vectors\">\n<h4>Word vectors</h4>\n<div id=\"word-vectors-access\">\n<h5>Word vectors access</h5>\n<div id=\"vector-for-a-given-word\">\n<h6>Vector for a given word</h6>\n<p>By default, a single word vector is returned as a regular Python array\nof floats.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"p\">[</span><span class=\"s1\">'dog'</span><span class=\"p\">]</span>\n<span class=\"n\">array</span><span class=\"p\">(</span><span class=\"s1\">'f'</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">1.308749794960022</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.8326224088668823</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">])</span>\n</pre>\n<p>Numpy ndarray</p>\n<p>The <tt>model.get_numpy_vector(word)</tt> method returns the word vector as a\n<tt>numpy</tt> <tt>ndarray</tt>.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_vector</span><span class=\"p\">(</span><span class=\"s1\">'dog'</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mf\">1.30874979</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.83262241</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n</pre>\n<p>If you want a normalized vector (<em>i.e.</em> the vector divided by its norm),\nthere is an optional boolean parameter named <tt>normalized</tt>.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_vector</span><span class=\"p\">(</span><span class=\"s1\">'dog'</span><span class=\"p\">,</span> <span class=\"n\">normalized</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mf\">0.07084749</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.09920666</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"words-for-a-given-vector\">\n<h6>Words for a given vector</h6>\n<div>\n<div>The inverse operation of <tt>model[word]</tt> or\n<tt>model.get_numpy_vector(word)</tt> is\n<tt>model.words_for_vector(vector, k)</tt>.</div>\n<div>It returns a list of the <tt>k</tt> words closest to the provided vector.\nThe default value for <tt>k</tt> is 1.</div>\n</div>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">king</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_vector</span><span class=\"p\">(</span><span class=\"s1\">'king'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">man</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_vector</span><span class=\"p\">(</span><span class=\"s1\">'man'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">woman</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_vector</span><span class=\"p\">(</span><span class=\"s1\">'woman'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">words_for_vector</span><span class=\"p\">(</span><span class=\"n\">king</span> <span class=\"o\">+</span> <span class=\"n\">woman</span> <span class=\"o\">-</span> <span class=\"n\">man</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">[(</span><span class=\"s1\">'queen'</span><span class=\"p\">,</span> <span class=\"mf\">0.77121970653533936</span><span class=\"p\">)]</span>\n</pre>\n</div>\n<div id=\"get-the-number-of-words-in-the-model\">\n<h6>Get the number of words in the model</h6>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">nwords</span>\n<span class=\"mi\">500000</span>\n</pre>\n</div>\n<div id=\"get-all-the-word-vectors-in-a-model\">\n<h6>Get all the word vectors in a model</h6>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">words</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>   <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">word</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">])</span>\n</pre>\n<p>Numpy ndarray</p>\n<p>If you want all the word vectors as a big <tt>numpy</tt> <tt>ndarray</tt>, you can\nuse the <tt>numpy_normalized_vectors</tt> member. Note that all these vectors\nare <em>normalized</em>.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">nwords</span>\n<span class=\"mi\">500000</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">numpy_normalized_vectors</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"o\">-</span><span class=\"mf\">0.07549749</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.09407753</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span> <span class=\"mf\">0.00635979</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.17272158</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span>\n       <span class=\"o\">...</span><span class=\"p\">,</span>\n       <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">0.01009259</span><span class=\"p\">,</span>  <span class=\"mf\">0.14604086</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span> <span class=\"mf\">0.12467574</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.0609326</span> <span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">numpy_normalized_vectors</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n<span class=\"p\">(</span><span class=\"mi\">500000</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">)</span> <span class=\"c1\"># (number of words, dimension)</span>\n</pre>\n</div>\n</div>\n<div id=\"misc-operations-with-word-vectors\">\n<h5>Misc operations with word vectors</h5>\n<div id=\"word-similarity\">\n<h6>Word similarity</h6>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">similarity</span><span class=\"p\">(</span><span class=\"s1\">'dog'</span><span class=\"p\">,</span> <span class=\"s1\">'cat'</span><span class=\"p\">)</span>\n<span class=\"mf\">0.75596606254577637</span>\n</pre>\n</div>\n<div id=\"most-similar-words\">\n<h6>Most similar words</h6>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">nearest_neighbors</span><span class=\"p\">(</span><span class=\"s1\">'dog'</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">[(</span><span class=\"s1\">'dogs'</span><span class=\"p\">,</span> <span class=\"mf\">0.7843924736976624</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'cat'</span><span class=\"p\">,</span> <span class=\"mi\">75596606254577637</span><span class=\"p\">)]</span>\n</pre>\n</div>\n<div id=\"analogies\">\n<h6>Analogies</h6>\n<p>The <tt>model.most_similar()</tt> method works similarly as the one in\n<a href=\"https://radimrehurek.com/gensim/models/keyedvectors.html\" rel=\"nofollow\">gensim</a>.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">most_similar</span><span class=\"p\">(</span><span class=\"n\">positive</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'woman'</span><span class=\"p\">,</span> <span class=\"s1\">'king'</span><span class=\"p\">],</span> <span class=\"n\">negative</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'man'</span><span class=\"p\">],</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">[(</span><span class=\"s1\">'queen'</span><span class=\"p\">,</span> <span class=\"mf\">0.77121970653533936</span><span class=\"p\">)]</span>\n</pre>\n</div>\n</div>\n</div>\n<div id=\"text-classification\">\n<h4>Text classification</h4>\n<div id=\"supervised-learning\">\n<h5>Supervised learning</h5>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FastText</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">supervised</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"o\">=</span><span class=\"s1\">'/path/to/input.txt'</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"o\">=</span><span class=\"s1\">'/path/to/model'</span><span class=\"p\">,</span> <span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"get-all-the-labels\">\n<h5>Get all the labels</h5>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">labels</span>\n<span class=\"p\">[</span><span class=\"s1\">'LABEL1'</span><span class=\"p\">,</span> <span class=\"s1\">'LABEL2'</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>\n</pre>\n</div>\n<div id=\"get-the-number-of-labels\">\n<h5>Get the number of labels</h5>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">nlabels</span>\n<span class=\"mi\">100</span>\n</pre>\n</div>\n<div id=\"prediction\">\n<h5>Prediction</h5>\n<div>\n<div>To obtain the <tt>k</tt> most likely labels from test sentences, there are\nmultiple <tt><span class=\"pre\">model.predict_*()</span></tt> methods.</div>\n<div>The default value for <tt>k</tt> is 1. If you want to obtain all the\npossible labels, use <tt>None</tt> for <tt>k</tt>.</div>\n</div>\n<div id=\"labels-and-probabilities\">\n<h6>Labels and probabilities</h6>\n<p>If you have a list of strings (or an iterable object), use this:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_proba</span><span class=\"p\">([</span><span class=\"s1\">'first sentence</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"s1\">'second sentence</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">],</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">[[(</span><span class=\"s1\">'LABEL1'</span><span class=\"p\">,</span> <span class=\"mf\">0.99609375</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'LABEL3'</span><span class=\"p\">,</span> <span class=\"mf\">1.953126549381068e-08</span><span class=\"p\">)],</span> <span class=\"p\">[(</span><span class=\"s1\">'LABEL2'</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'LABEL3'</span><span class=\"p\">,</span> <span class=\"mf\">1.953126549381068e-08</span><span class=\"p\">)]]</span>\n</pre>\n<p>If you want to test a single string, use this:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_proba_single</span><span class=\"p\">(</span><span class=\"s1\">'first sentence</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">[(</span><span class=\"s1\">'LABEL1'</span><span class=\"p\">,</span> <span class=\"mf\">0.99609375</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'LABEL3'</span><span class=\"p\">,</span> <span class=\"mf\">1.953126549381068e-08</span><span class=\"p\">)]</span>\n</pre>\n<p><strong>WARNING</strong>: In order to get the same probabilities as the <tt>fastText</tt>\nbinary, you have to add a newline (<tt>\\n</tt>) at the end of each string.</p>\n<p>If your test data is stored inside a file, use this:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_proba_file</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/test.txt'</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">[[(</span><span class=\"s1\">'LABEL1'</span><span class=\"p\">,</span> <span class=\"mf\">0.99609375</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'LABEL3'</span><span class=\"p\">,</span> <span class=\"mf\">1.953126549381068e-08</span><span class=\"p\">)],</span> <span class=\"p\">[(</span><span class=\"s1\">'LABEL2'</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'LABEL3'</span><span class=\"p\">,</span> <span class=\"mf\">1.953126549381068e-08</span><span class=\"p\">)]]</span>\n</pre>\n<p>Normalized probabilities</p>\n<p>For performance reasons, fastText probabilities often do not sum up to\n1.0.</p>\n<p>If you want normalized probabilities (where the sum is closer to 1.0\nthan the original probabilities), you can use the <tt>normalized=True</tt>\nparameter in all the methods that output probabilities\n(<tt>model.predict_proba()</tt>, <tt>model.predict_proba_file()</tt> and\n<tt>model.predict_proba_single()</tt>).</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">proba</span> <span class=\"k\">for</span> <span class=\"n\">label</span><span class=\"p\">,</span> <span class=\"n\">proba</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_proba_single</span><span class=\"p\">(</span><span class=\"s1\">'this is a sentence that needs to be classified</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">))</span>\n<span class=\"mf\">0.9785203068801335</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">proba</span> <span class=\"k\">for</span> <span class=\"n\">label</span><span class=\"p\">,</span> <span class=\"n\">proba</span> <span class=\"ow\">in</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_proba_single</span><span class=\"p\">(</span><span class=\"s1\">'this is a sentence that needs to be classified</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">normalized</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>\n<span class=\"mf\">0.9999999999999898</span>\n</pre>\n</div>\n<div id=\"labels-only\">\n<h6>Labels only</h6>\n<p>If you have a list of strings (or an iterable object), use this:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"s1\">'first sentence</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"s1\">'second sentence</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">],</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">[[</span><span class=\"s1\">'LABEL1'</span><span class=\"p\">,</span> <span class=\"s1\">'LABEL3'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'LABEL2'</span><span class=\"p\">,</span> <span class=\"s1\">'LABEL3'</span><span class=\"p\">]]</span>\n</pre>\n<p>If you want to test a single string, use this:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_single</span><span class=\"p\">(</span><span class=\"s1\">'first sentence</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'LABEL1'</span><span class=\"p\">,</span> <span class=\"s1\">'LABEL3'</span><span class=\"p\">]</span>\n</pre>\n<p><strong>WARNING</strong>: In order to get the same probabilities as the <tt>fastText</tt>\nbinary, you have to add a newline (<tt>\\n</tt>) at the end of each string.</p>\n<p>If your test data is stored inside a file, use this:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_file</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/test.txt'</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">[[</span><span class=\"s1\">'LABEL1'</span><span class=\"p\">,</span> <span class=\"s1\">'LABEL3'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'LABEL2'</span><span class=\"p\">,</span> <span class=\"s1\">'LABEL3'</span><span class=\"p\">]]</span>\n</pre>\n</div>\n</div>\n<div id=\"quantization\">\n<h5>Quantization</h5>\n<p>Use keyword arguments in the <tt>model.quantize()</tt> method.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">quantize</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"o\">=</span><span class=\"s1\">'/path/to/input.txt'</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"o\">=</span><span class=\"s1\">'/path/to/model'</span><span class=\"p\">)</span>\n</pre>\n<p>You can load quantized models using the <tt>FastText</tt> constructor or the\n<tt>model.load_model()</tt> method.</p>\n<div id=\"is-a-model-quantized\">\n<h6>Is a model quantized?</h6>\n<p>If you want to know if a model has been quantized before, use the\n<tt>model.quantized</tt> attribute.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FastText</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/model.bin'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">quantized</span>\n<span class=\"kc\">False</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FastText</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/model.ftz'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">quantized</span>\n<span class=\"kc\">True</span>\n</pre>\n</div>\n</div>\n</div>\n<div id=\"subwords\">\n<h4>Subwords</h4>\n<p>fastText can use subwords (<em>i.e.</em> character ngrams) when doing\nunsupervised or supervised learning.</p>\n<p>You can access the subwords, and their associated vectors, using\n<tt>pyfasttext</tt>.</p>\n<div id=\"get-the-subwords\">\n<h5>Get the subwords</h5>\n<p>fastText\u2019s word embeddings can be augmented with subword-level\ninformation. It is possible to retrieve the subwords and their\nassociated vectors from a model using <tt>pyfasttext</tt>.</p>\n<p>To retrieve all the subwords for a given word, use the\n<tt>model.get_all_subwords(word)</tt> method.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'minn'</span><span class=\"p\">),</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'maxn'</span><span class=\"p\">)</span>\n<span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_all_subwords</span><span class=\"p\">(</span><span class=\"s1\">'hello'</span><span class=\"p\">)</span> <span class=\"c1\"># word + subwords from 2 to 4 characters</span>\n<span class=\"p\">[</span><span class=\"s1\">'hello'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;h'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;he'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;hel'</span><span class=\"p\">,</span> <span class=\"s1\">'he'</span><span class=\"p\">,</span> <span class=\"s1\">'hel'</span><span class=\"p\">,</span> <span class=\"s1\">'hell'</span><span class=\"p\">,</span> <span class=\"s1\">'el'</span><span class=\"p\">,</span> <span class=\"s1\">'ell'</span><span class=\"p\">,</span> <span class=\"s1\">'ello'</span><span class=\"p\">,</span> <span class=\"s1\">'ll'</span><span class=\"p\">,</span> <span class=\"s1\">'llo'</span><span class=\"p\">,</span> <span class=\"s1\">'llo&gt;'</span><span class=\"p\">,</span> <span class=\"s1\">'lo'</span><span class=\"p\">,</span> <span class=\"s1\">'lo&gt;'</span><span class=\"p\">,</span> <span class=\"s1\">'o&gt;'</span><span class=\"p\">]</span>\n</pre>\n<p>For fastText, <tt>&lt;</tt> means \u201cbeginning of a word\u201d and <tt>&gt;</tt> means \u201cend of\na word\u201d.</p>\n<p>As you can see, fastText includes the full word. You can omit it using\nthe <tt>omit_word=True</tt> keyword argument.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_all_subwords</span><span class=\"p\">(</span><span class=\"s1\">'hello'</span><span class=\"p\">,</span> <span class=\"n\">omit_word</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'&lt;h'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;he'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;hel'</span><span class=\"p\">,</span> <span class=\"s1\">'he'</span><span class=\"p\">,</span> <span class=\"s1\">'hel'</span><span class=\"p\">,</span> <span class=\"s1\">'hell'</span><span class=\"p\">,</span> <span class=\"s1\">'el'</span><span class=\"p\">,</span> <span class=\"s1\">'ell'</span><span class=\"p\">,</span> <span class=\"s1\">'ello'</span><span class=\"p\">,</span> <span class=\"s1\">'ll'</span><span class=\"p\">,</span> <span class=\"s1\">'llo'</span><span class=\"p\">,</span> <span class=\"s1\">'llo&gt;'</span><span class=\"p\">,</span> <span class=\"s1\">'lo'</span><span class=\"p\">,</span> <span class=\"s1\">'lo&gt;'</span><span class=\"p\">,</span> <span class=\"s1\">'o&gt;'</span><span class=\"p\">]</span>\n</pre>\n<p>When a model is quantized, fastText may <em>prune</em> some subwords. If you\nwant to see only the subwords that are really used when computing a word\nvector, you should use the <tt>model.get_subwords(word)</tt> method.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">quantized</span>\n<span class=\"kc\">True</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_subwords</span><span class=\"p\">(</span><span class=\"s1\">'beautiful'</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'eau'</span><span class=\"p\">,</span> <span class=\"s1\">'aut'</span><span class=\"p\">,</span> <span class=\"s1\">'ful'</span><span class=\"p\">,</span> <span class=\"s1\">'ul'</span><span class=\"p\">]</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_subwords</span><span class=\"p\">(</span><span class=\"s1\">'hello'</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'hello'</span><span class=\"p\">]</span> <span class=\"c1\"># fastText will not use any subwords when computing the word vector, only the full word</span>\n</pre>\n</div>\n<div id=\"get-the-subword-vectors\">\n<h5>Get the subword vectors</h5>\n<p>To get the individual vectors given the subwords, use the\n<tt>model.get_numpy_subword_vectors(word)</tt> method.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_subword_vectors</span><span class=\"p\">(</span><span class=\"s1\">'beautiful'</span><span class=\"p\">)</span> <span class=\"c1\"># 4 vectors, so 4 rows</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span> <span class=\"mf\">0.49022141</span><span class=\"p\">,</span>  <span class=\"mf\">0.13586822</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.14065443</span><span class=\"p\">,</span>  <span class=\"mf\">0.89617103</span><span class=\"p\">],</span> <span class=\"c1\"># subword \"eau\"</span>\n       <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">0.42594951</span><span class=\"p\">,</span>  <span class=\"mf\">0.06260503</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.18182631</span><span class=\"p\">,</span>  <span class=\"mf\">0.34219387</span><span class=\"p\">],</span> <span class=\"c1\"># subword \"aut\"</span>\n       <span class=\"p\">[</span> <span class=\"mf\">0.49958718</span><span class=\"p\">,</span>  <span class=\"mf\">2.93831301</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.97498322</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.16815805</span><span class=\"p\">],</span> <span class=\"c1\"># subword \"ful\"</span>\n       <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">0.4368791</span> <span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.92924356</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span>  <span class=\"mf\">1.62921488</span><span class=\"p\">,</span> <span class=\"mf\">1.90240896</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span> <span class=\"c1\"># subword \"ul\"</span>\n</pre>\n<p>In fastText, the final word vector is the average of these individual\nvectors.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">vec1</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_vector</span><span class=\"p\">(</span><span class=\"s1\">'beautiful'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">vecs2</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_subword_vectors</span><span class=\"p\">(</span><span class=\"s1\">'beautiful'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">vec1</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"p\">(</span><span class=\"n\">vecs2</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n<span class=\"kc\">True</span>\n</pre>\n</div>\n</div>\n<div id=\"sentence-and-text-vectors\">\n<h4>Sentence and text vectors</h4>\n<p>To compute the vector of a sequence of words (<em>i.e.</em> a sentence),\nfastText uses two different methods: * one for unsupervised models *\nanother one for supervised models</p>\n<p>When fastText computes a word vector, recall that it uses the average of\nthe following vectors: the word itself and its subwords.</p>\n<div id=\"unsupervised-models\">\n<h5>Unsupervised models</h5>\n<p>For unsupervised models, the representation of a sentence for fastText\nis the average of the normalized word vectors.</p>\n<div>\n<div>To get the resulting vector as a regular Python array, use the\n<tt>model.get_sentence_vector(line)</tt> method.</div>\n<div>To get the resulting vector as a <tt>numpy</tt> <tt>ndarray</tt>, use the\n<tt>model.get_numpy_sentence_vector(line)</tt> method.</div>\n</div>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">vec</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_sentence_vector</span><span class=\"p\">(</span><span class=\"s1\">'beautiful cats'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">vec1</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_vector</span><span class=\"p\">(</span><span class=\"s1\">'beautiful'</span><span class=\"p\">,</span> <span class=\"n\">normalized</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">vec2</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_vector</span><span class=\"p\">(</span><span class=\"s1\">'cats'</span><span class=\"p\">,</span> <span class=\"n\">normalized</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">vec</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"p\">([</span><span class=\"n\">vec1</span><span class=\"p\">,</span> <span class=\"n\">vec2</span><span class=\"p\">],</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"kc\">True</span>\n</pre>\n</div>\n<div id=\"supervised-models\">\n<h5>Supervised models</h5>\n<p>For supervised models, fastText uses the regular word vectors, as well\nas vectors computed using word ngrams (<em>i.e.</em> shorter sequences of words\nfrom the sentence). When computing the average, these vectors are not\nnormalized.</p>\n<div>\n<div>To get the resulting vector as a regular Python array, use the\n<tt>model.get_text_vector(line)</tt> method.</div>\n<div>To get the resulting vector as a <tt>numpy</tt> <tt>ndarray</tt>, use the\n<tt>model.get_numpy_text_vector(line)</tt> method.</div>\n</div>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_sentence_vector</span><span class=\"p\">(</span><span class=\"s1\">'beautiful cats'</span><span class=\"p\">)</span> <span class=\"c1\"># for an unsupervised model</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mf\">0.20266785</span><span class=\"p\">,</span>  <span class=\"mf\">0.3407566</span> <span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span>  <span class=\"mf\">0.03044436</span><span class=\"p\">,</span>  <span class=\"mf\">0.39055538</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_numpy_text_vector</span><span class=\"p\">(</span><span class=\"s1\">'beautiful cats'</span><span class=\"p\">)</span> <span class=\"c1\"># for a supervised model</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mf\">0.20840774</span><span class=\"p\">,</span>  <span class=\"mf\">0.4289546</span> <span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.00457615</span><span class=\"p\">,</span>  <span class=\"mf\">0.52417743</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n</pre>\n</div>\n</div>\n<div id=\"misc-utilities\">\n<h4>Misc utilities</h4>\n<div id=\"show-the-module-version\">\n<h5>Show the module version</h5>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">pyfasttext</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">__version__</span>\n<span class=\"s1\">'0.4.3'</span>\n</pre>\n</div>\n<div id=\"show-fasttext-version\">\n<h5>Show fastText version</h5>\n<p>As there is no version number in fastText, we use the latest fastText\ncommit hash (from <tt>HEAD</tt>) as a substitute.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">pyfasttext</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">__fasttext_version__</span>\n<span class=\"s1\">'431c9e2a9b5149369cc60fb9f5beba58dcf8ca17'</span>\n</pre>\n</div>\n<div id=\"show-the-model-hyper-parameters\">\n<h5>Show the model (hyper)parameters</h5>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">args</span>\n<span class=\"p\">{</span><span class=\"s1\">'bucket'</span><span class=\"p\">:</span> <span class=\"mi\">11000000</span><span class=\"p\">,</span>\n <span class=\"s1\">'cutoff'</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n <span class=\"s1\">'dim'</span><span class=\"p\">:</span> <span class=\"mi\">100</span><span class=\"p\">,</span>\n <span class=\"s1\">'dsub'</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n <span class=\"s1\">'epoch'</span><span class=\"p\">:</span> <span class=\"mi\">100</span><span class=\"p\">,</span>\n<span class=\"o\">...</span>\n<span class=\"p\">}</span>\n</pre>\n</div>\n<div id=\"show-the-model-version-number\">\n<h5>Show the model version number</h5>\n<p>fastText uses a versioning scheme for its generated models. You can\nretrieve the model version number using the <tt>model.version</tt> attribute.</p>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>version number</th>\n<th>description</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>-1</td>\n<td>for really old models\nwith no version number</td>\n</tr>\n<tr><td>11</td>\n<td>first version number\nadded by fastText</td>\n</tr>\n<tr><td>12</td>\n<td>for models generated\nafter fastText added\nsupport for subwords\nin supervised learning</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">version</span>\n<span class=\"mi\">12</span>\n</pre>\n</div>\n<div id=\"extract-labels-or-classes-from-a-dataset\">\n<h5>Extract labels or classes from a dataset</h5>\n<p>You can use the <tt>FastText</tt> object to extract labels or classes from a\ndataset. The label prefix (which is <tt>__label__</tt> by default) is set\nusing the <tt>label</tt> parameter in the constructor.</p>\n<p>If you load an existing model, the label prefix will be the one defined\nin the model.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FastText</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">'__my_prefix__'</span><span class=\"p\">)</span>\n</pre>\n<div id=\"extract-labels\">\n<h6>Extract labels</h6>\n<p>There can be multiple labels per line.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">extract_labels</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/dataset1.txt'</span><span class=\"p\">)</span>\n<span class=\"p\">[[</span><span class=\"s1\">'LABEL2'</span><span class=\"p\">,</span> <span class=\"s1\">'LABEL5'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'LABEL1'</span><span class=\"p\">],</span> <span class=\"o\">...</span><span class=\"p\">]</span>\n</pre>\n</div>\n<div id=\"extract-classes\">\n<h6>Extract classes</h6>\n<p>There can be only one class per line.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">extract_classes</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/dataset2.txt'</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'LABEL3'</span><span class=\"p\">,</span> <span class=\"s1\">'LABEL1'</span><span class=\"p\">,</span> <span class=\"s1\">'LABEL2'</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>\n</pre>\n</div>\n</div>\n</div>\n<div id=\"exceptions\">\n<h4>Exceptions</h4>\n<p>The <tt>fastText</tt> source code directly calls exit() when something wrong\nhappens (<em>e.g.</em> a model file does not exist, \u2026).</p>\n<p>Instead of exiting, <tt>pyfasttext</tt> raises a Python exception\n(<tt>RuntimeError</tt>).</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">pyfasttext</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">FastText</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/non-existing_model.bin'</span><span class=\"p\">)</span>\n<span class=\"n\">Model</span> <span class=\"n\">file</span> <span class=\"n\">cannot</span> <span class=\"n\">be</span> <span class=\"n\">opened</span> <span class=\"k\">for</span> <span class=\"n\">loading</span><span class=\"err\">!</span>\n<span class=\"n\">Traceback</span> <span class=\"p\">(</span><span class=\"n\">most</span> <span class=\"n\">recent</span> <span class=\"n\">call</span> <span class=\"n\">last</span><span class=\"p\">):</span>\n  <span class=\"n\">File</span> <span class=\"s2\">\"&lt;stdin&gt;\"</span><span class=\"p\">,</span> <span class=\"n\">line</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"ow\">in</span> <span class=\"o\">&lt;</span><span class=\"n\">module</span><span class=\"o\">&gt;</span>\n  <span class=\"n\">File</span> <span class=\"s2\">\"src/pyfasttext.pyx\"</span><span class=\"p\">,</span> <span class=\"n\">line</span> <span class=\"mi\">124</span><span class=\"p\">,</span> <span class=\"ow\">in</span> <span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">FastText</span><span class=\"o\">.</span><span class=\"n\">__cinit__</span> <span class=\"p\">(</span><span class=\"n\">src</span><span class=\"o\">/</span><span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">cpp</span><span class=\"p\">:</span><span class=\"mi\">1800</span><span class=\"p\">)</span>\n  <span class=\"n\">File</span> <span class=\"s2\">\"src/pyfasttext.pyx\"</span><span class=\"p\">,</span> <span class=\"n\">line</span> <span class=\"mi\">348</span><span class=\"p\">,</span> <span class=\"ow\">in</span> <span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">FastText</span><span class=\"o\">.</span><span class=\"n\">load_model</span> <span class=\"p\">(</span><span class=\"n\">src</span><span class=\"o\">/</span><span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">cpp</span><span class=\"p\">:</span><span class=\"mi\">5947</span><span class=\"p\">)</span>\n<span class=\"ne\">RuntimeError</span><span class=\"p\">:</span> <span class=\"n\">fastext</span> <span class=\"n\">tried</span> <span class=\"n\">to</span> <span class=\"n\">exit</span><span class=\"p\">:</span> <span class=\"mi\">1</span>\n</pre>\n</div>\n<div id=\"interruptible-operations\">\n<h4>Interruptible operations</h4>\n<p><tt>pyfasttext</tt> uses <tt>cysignals</tt> to make all the computationally\nintensive operations (<em>e.g.</em> training) interruptible.</p>\n<p>To easily interrupt such an operation, just type <tt><span class=\"pre\">Ctrl-C</span></tt> in your\nPython shell.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">skipgram</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"o\">=</span><span class=\"s1\">'/path/to/input.txt'</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"o\">=</span><span class=\"s1\">'/path/to/mymodel'</span><span class=\"p\">)</span>\n<span class=\"n\">Read</span> <span class=\"mi\">12</span><span class=\"n\">M</span> <span class=\"n\">words</span>\n<span class=\"n\">Number</span> <span class=\"n\">of</span> <span class=\"n\">words</span><span class=\"p\">:</span>  <span class=\"mi\">60237</span>\n<span class=\"n\">Number</span> <span class=\"n\">of</span> <span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"mi\">0</span>\n<span class=\"o\">...</span> <span class=\"c1\"># type Ctrl-C during training</span>\n<span class=\"n\">Traceback</span> <span class=\"p\">(</span><span class=\"n\">most</span> <span class=\"n\">recent</span> <span class=\"n\">call</span> <span class=\"n\">last</span><span class=\"p\">):</span>\n  <span class=\"n\">File</span> <span class=\"s2\">\"&lt;stdin&gt;\"</span><span class=\"p\">,</span> <span class=\"n\">line</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"ow\">in</span> <span class=\"o\">&lt;</span><span class=\"n\">module</span><span class=\"o\">&gt;</span>\n  <span class=\"n\">File</span> <span class=\"s2\">\"src/pyfasttext.pyx\"</span><span class=\"p\">,</span> <span class=\"n\">line</span> <span class=\"mi\">680</span><span class=\"p\">,</span> <span class=\"ow\">in</span> <span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">FastText</span><span class=\"o\">.</span><span class=\"n\">skipgram</span> <span class=\"p\">(</span><span class=\"n\">src</span><span class=\"o\">/</span><span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">cpp</span><span class=\"p\">:</span><span class=\"mi\">11125</span><span class=\"p\">)</span>\n  <span class=\"n\">File</span> <span class=\"s2\">\"src/pyfasttext.pyx\"</span><span class=\"p\">,</span> <span class=\"n\">line</span> <span class=\"mi\">674</span><span class=\"p\">,</span> <span class=\"ow\">in</span> <span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">FastText</span><span class=\"o\">.</span><span class=\"n\">train</span> <span class=\"p\">(</span><span class=\"n\">src</span><span class=\"o\">/</span><span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">cpp</span><span class=\"p\">:</span><span class=\"mi\">11009</span><span class=\"p\">)</span>\n  <span class=\"n\">File</span> <span class=\"s2\">\"src/pyfasttext.pyx\"</span><span class=\"p\">,</span> <span class=\"n\">line</span> <span class=\"mi\">668</span><span class=\"p\">,</span> <span class=\"ow\">in</span> <span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">FastText</span><span class=\"o\">.</span><span class=\"n\">train</span> <span class=\"p\">(</span><span class=\"n\">src</span><span class=\"o\">/</span><span class=\"n\">pyfasttext</span><span class=\"o\">.</span><span class=\"n\">cpp</span><span class=\"p\">:</span><span class=\"mi\">10926</span><span class=\"p\">)</span>\n  <span class=\"n\">File</span> <span class=\"s2\">\"src/cysignals/signals.pyx\"</span><span class=\"p\">,</span> <span class=\"n\">line</span> <span class=\"mi\">94</span><span class=\"p\">,</span> <span class=\"ow\">in</span> <span class=\"n\">cysignals</span><span class=\"o\">.</span><span class=\"n\">signals</span><span class=\"o\">.</span><span class=\"n\">sig_raise_exception</span> <span class=\"p\">(</span><span class=\"n\">build</span><span class=\"o\">/</span><span class=\"n\">src</span><span class=\"o\">/</span><span class=\"n\">cysignals</span><span class=\"o\">/</span><span class=\"n\">signals</span><span class=\"o\">.</span><span class=\"n\">c</span><span class=\"p\">:</span><span class=\"mi\">1328</span><span class=\"p\">)</span>\n<span class=\"ne\">KeyboardInterrupt</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># you can have your shell back!</span>\n</pre>\n</div>\n</div>\n</div>\n\n          </div>"}, "last_serial": 4575456, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "8f1a6d23f40824c9f23c05ee3c3b34af", "sha256": "e822820e9a96f89c0dbedd38001a05bfb13b5c0a2a786629e6b3ab8c6deea1dd"}, "downloads": -1, "filename": "pyfasttext-0.1.0.tar.gz", "has_sig": false, "md5_digest": "8f1a6d23f40824c9f23c05ee3c3b34af", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 135287, "upload_time": "2017-08-01T20:22:28", "upload_time_iso_8601": "2017-08-01T20:22:28.493266Z", "url": "https://files.pythonhosted.org/packages/13/28/ac911b206a24585c4c893b62c2d481570f5ec3a9b9ac9b388c58dc8236bc/pyfasttext-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "42c1f5ee9485f1abf7b7f45cca781bf1", "sha256": "98abaab0ff35a839970a1a4a954ba052bce0b2e53e87a959408b846b0e0701f2"}, "downloads": -1, "filename": "pyfasttext-0.2.0.tar.gz", "has_sig": false, "md5_digest": "42c1f5ee9485f1abf7b7f45cca781bf1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 155590, "upload_time": "2017-08-06T20:08:44", "upload_time_iso_8601": "2017-08-06T20:08:44.860737Z", "url": "https://files.pythonhosted.org/packages/54/4a/c3b104bb44b36f06fbeaa0a8d1f04b7254a855b82bf52883d297ad95d647/pyfasttext-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "572c5e77ef8f63595df8177f2a3e13b3", "sha256": "98965f186716836df7ed2af70fa685a95441c55ac6af99bf113b66988ceec8a3"}, "downloads": -1, "filename": "pyfasttext-0.2.1.tar.gz", "has_sig": false, "md5_digest": "572c5e77ef8f63595df8177f2a3e13b3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 157411, "upload_time": "2017-08-16T20:29:13", "upload_time_iso_8601": "2017-08-16T20:29:13.925135Z", "url": "https://files.pythonhosted.org/packages/fe/f3/4f3a38a65788307ff1404f161956f24304dded3fbaccb132af056d6b915d/pyfasttext-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "ccda3aec5242e7a186b24bf4fbee8f94", "sha256": "ce1191ae01869996785921a58aa8859842a768ff9465621c280ee3a2e85d083a"}, "downloads": -1, "filename": "pyfasttext-0.2.2.tar.gz", "has_sig": false, "md5_digest": "ccda3aec5242e7a186b24bf4fbee8f94", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 210596, "upload_time": "2017-08-22T22:59:39", "upload_time_iso_8601": "2017-08-22T22:59:39.772158Z", "url": "https://files.pythonhosted.org/packages/9f/cc/cefb7ec9d6046cb8d861d84d77ee646c06235b5ea5d9732473d379a02af3/pyfasttext-0.2.2.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "92c63fa748b9415cb6d8231ef21fb5b8", "sha256": "a07796aa6d1ca8375e775d5fdfdcb49a575736f0d6b5791edd411545c0718483"}, "downloads": -1, "filename": "pyfasttext-0.3.0.tar.gz", "has_sig": false, "md5_digest": "92c63fa748b9415cb6d8231ef21fb5b8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 207803, "upload_time": "2017-09-02T23:59:21", "upload_time_iso_8601": "2017-09-02T23:59:21.055401Z", "url": "https://files.pythonhosted.org/packages/57/a8/0bc0b7c7639dee4586b755334efd6bba19f1ac59a416794c3198dc32a974/pyfasttext-0.3.0.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "1a227993895a934c1cf744902b686f63", "sha256": "9fc92bc2c56a5e80453285bfdc20b6a99e74a3987301a7e0c82de6151301350c"}, "downloads": -1, "filename": "pyfasttext-0.4.0.tar.gz", "has_sig": false, "md5_digest": "1a227993895a934c1cf744902b686f63", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 213680, "upload_time": "2017-10-04T20:17:49", "upload_time_iso_8601": "2017-10-04T20:17:49.099800Z", "url": "https://files.pythonhosted.org/packages/35/69/cd6891f3a0b35bca1e7ac0f0be442a583c5fec189eceb4c28320f91803e1/pyfasttext-0.4.0.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "b60ce164b621eb2116f94b9766dc67bf", "sha256": "668fd3879439f7a73c754cc122c9bb8ab2bc6ffa8c146c9369b6244b590c6c41"}, "downloads": -1, "filename": "pyfasttext-0.4.1.tar.gz", "has_sig": false, "md5_digest": "b60ce164b621eb2116f94b9766dc67bf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 211879, "upload_time": "2017-10-13T20:33:38", "upload_time_iso_8601": "2017-10-13T20:33:38.813182Z", "url": "https://files.pythonhosted.org/packages/b4/fc/07761e7201fd2c7cf49c09660ecc74765d295c68d6b246f104446b0e6ad3/pyfasttext-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "28e17c7d64ab40de688083d725423717", "sha256": "2ad710bcb71073674dd250b35cecdd5c28a6168b8bf6fff171fb4a9152eafc95"}, "downloads": -1, "filename": "pyfasttext-0.4.2.tar.gz", "has_sig": false, "md5_digest": "28e17c7d64ab40de688083d725423717", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 211980, "upload_time": "2017-10-14T15:13:50", "upload_time_iso_8601": "2017-10-14T15:13:50.905675Z", "url": "https://files.pythonhosted.org/packages/4a/8a/130f4d295aee788ef29dcd93a410563078c91c15960370558b3b64b827e8/pyfasttext-0.4.2.tar.gz", "yanked": false}], "0.4.3": [{"comment_text": "", "digests": {"md5": "7072044c3bf0c1b66a0d9185200494cc", "sha256": "da60cf0c2c7fc50465a0a7cb73671039a4aa775e33504fc9d09cebbfa65ca9a8"}, "downloads": -1, "filename": "pyfasttext-0.4.3.tar.gz", "has_sig": false, "md5_digest": "7072044c3bf0c1b66a0d9185200494cc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 231656, "upload_time": "2017-10-26T20:27:53", "upload_time_iso_8601": "2017-10-26T20:27:53.229558Z", "url": "https://files.pythonhosted.org/packages/6a/64/7a06c6bf18a34f6530d1bb07cb98ab56f6f21edf0129c9ea8baf0d666f4a/pyfasttext-0.4.3.tar.gz", "yanked": false}], "0.4.4": [{"comment_text": "", "digests": {"md5": "72a7d1e09b97ca8cf36bd2981b8c786c", "sha256": "232a17b53ec50373749d2bc51679bddfe76ed154d1f8710f6fcd280347188a4a"}, "downloads": -1, "filename": "pyfasttext-0.4.4.tar.gz", "has_sig": false, "md5_digest": "72a7d1e09b97ca8cf36bd2981b8c786c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 235047, "upload_time": "2017-11-04T16:25:58", "upload_time_iso_8601": "2017-11-04T16:25:58.826843Z", "url": "https://files.pythonhosted.org/packages/61/fc/9f3c2b2eb0406ce298815beacfea3e00622b3d65acc3b90ab751f15c2581/pyfasttext-0.4.4.tar.gz", "yanked": false}], "0.4.5": [{"comment_text": "", "digests": {"md5": "8a8102860ae38c58562d2e7fc1bb14c3", "sha256": "425f49bf6fc740fba4f050d367673de8e433f0d1657dede269b5dbc837983689"}, "downloads": -1, "filename": "pyfasttext-0.4.5.tar.gz", "has_sig": false, "md5_digest": "8a8102860ae38c58562d2e7fc1bb14c3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 232564, "upload_time": "2018-05-06T15:59:38", "upload_time_iso_8601": "2018-05-06T15:59:38.014132Z", "url": "https://files.pythonhosted.org/packages/72/96/741e3502b19d62fcba2b702d06d6707991ea2ba9f1b393e6d36aeae835c5/pyfasttext-0.4.5.tar.gz", "yanked": false}], "0.4.6": [{"comment_text": "", "digests": {"md5": "8ae5f71a8eaa6ca0baf4a1b7f327c403", "sha256": "aeb56569b5b47e958e5eeca3576047d666a495a5cc23010b0a503001d68b3390"}, "downloads": -1, "filename": "pyfasttext-0.4.6.tar.gz", "has_sig": false, "md5_digest": "8ae5f71a8eaa6ca0baf4a1b7f327c403", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 244723, "upload_time": "2018-12-08T15:32:09", "upload_time_iso_8601": "2018-12-08T15:32:09.098274Z", "url": "https://files.pythonhosted.org/packages/f5/ef/90606442481d1e4ab10eba8c2b2c449ceaa70c60e9b8d5898bb7504e3634/pyfasttext-0.4.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8ae5f71a8eaa6ca0baf4a1b7f327c403", "sha256": "aeb56569b5b47e958e5eeca3576047d666a495a5cc23010b0a503001d68b3390"}, "downloads": -1, "filename": "pyfasttext-0.4.6.tar.gz", "has_sig": false, "md5_digest": "8ae5f71a8eaa6ca0baf4a1b7f327c403", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 244723, "upload_time": "2018-12-08T15:32:09", "upload_time_iso_8601": "2018-12-08T15:32:09.098274Z", "url": "https://files.pythonhosted.org/packages/f5/ef/90606442481d1e4ab10eba8c2b2c449ceaa70c60e9b8d5898bb7504e3634/pyfasttext-0.4.6.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:06:00 2020"}