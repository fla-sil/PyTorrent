{"info": {"author": "heron", "author_email": "adept@heronsystems.com", "bugtrack_url": null, "classifiers": [], "description": "![banner](images/banner.png)\n\nadept is a reinforcement learning framework designed to accelerate research \nby providing:\n* a modular interface for using custom networks, agents, and environments\n* baseline reinforcement learning models and algorithms for PyTorch\n* multi-GPU support\n* access to various environments\n* built-in tensorboard logging, model saving, reloading, evaluation, and \nrendering\n* proven hyperparameter defaults\n\nThis code is early-access, expect rough edges. Interfaces subject to change. \nWe're happy to accept feedback and contributions.\n\n### Read More\n* [Installation](#installation)\n* [Quickstart](#quickstart)\n* [Features](#features)\n* [Performance](#performance)\n\n### Documentation\n* [Architecture Overview](docs/api_overview.md)\n* [ModularNetwork Overview](docs/modular_network.md)\n* [Resume training](docs/resume_training.md)\n* Evaluate a model\n* Render environment\n\n### Examples\n* Custom Network ([stub](examples/custom_network_stub.py) | example)\n* Custom SubModule ([stub](examples/custom_submodule_stub.py) | [example](adept/networks/net1d/lstm.py))\n* Custom Agent ([stub](examples/custom_agent_stub.py) | [example](adept/agents/actor_critic.py))\n* Custom Environment ([stub](examples/custom_environment_stub.py) | [example](adept/environments/openai_gym.py))\n\n## Installation\n**Dependencies:**\n* gym\n* PyTorch 1.x\n* Python 3.5+\n* We recommend CUDA 10, pytorch 1.0, python 3.6\n\n**From source:**\n* Follow instructions for [PyTorch](https://pytorch.org/)\n* (Optional) Follow instructions for \n[StarCraft 2](https://github.com/Blizzard/s2client-proto#downloads)\n```bash\ngit clone https://github.com/heronsystems/adeptRL\ncd adeptRL\n# Remove mpi, sc2, profiler if you don't plan on using these features:\npip install .[mpi,sc2,profiler]\n```\n\n**From docker:**\n* [docker instructions](./docker/)\n\n## Quickstart\n**Train an Agent**\nLogs go to `/tmp/adept_logs/` by default. The log directory contains the \ntensorboard file, saved models, and other metadata.\n\n```bash\n# Local Mode (A2C)\n# We recommend 4GB+ GPU memory, 8GB+ RAM, 4+ Cores\npython -m adept.app local --env BeamRiderNoFrameskip-v4\n\n# Distributed Mode (A2C, requires NCCL)\n# We recommend 2+ GPUs, 8GB+ GPU memory, 32GB+ RAM, 4+ Cores\npython -m adept.app distrib --env BeamRiderNoFrameskip-v4\n\n# IMPALA (requires mpi4py and is resource intensive)\n# We recommend 2+ GPUs, 8GB+ GPU memory, 32GB+ RAM, 4+ Cores\npython -m adept.app impala --agent ActorCriticVtrace --env BeamRiderNoFrameskip-v4\n\n# StarCraft 2 (IMPALA not supported yet)\n# Warning: much more resource intensive than Atari\npython -m adept.app local --env CollectMineralShards\n\n# To see a full list of options:\npython -m adept.app -h\npython -m adept.app help <command>\n```\n\n**Use your own Agent, Environment, Network, or SubModule**  \n```python\n\"\"\"\nmy_script.py\n\nTrain an agent on a single GPU.\n\"\"\"\nfrom adept.scripts.local import parse_args, main\nfrom adept.networks import NetworkModule, NetworkRegistry, SubModule1D\nfrom adept.agents import AgentModule, AgentRegistry\nfrom adept.environments import EnvModule, EnvRegistry\n\n\nclass MyAgent(AgentModule):\n    pass  # Implement\n\n\nclass MyEnv(EnvModule):\n    pass  # Implement\n\n\nclass MyNet(NetworkModule):\n    pass  # Implement\n\n\nclass MySubModule1D(SubModule1D):\n    pass  # Implement\n\n\nif __name__ == '__main__':\n    agent_registry = AgentRegistry()\n    agent_registry.register_agent(MyAgent)\n\n    env_registry = EnvRegistry()\n    env_registry.register_env(MyEnv, ['env-id-1', 'env-id-2'])\n\n    network_registry = NetworkRegistry()\n    network_registry.register_custom_net(MyNet)\n    network_registry.register_submodule(MySubModule1D)\n\n    main(\n        parse_args(),\n        agent_registry=agent_registry,\n        env_registry=env_registry,\n        net_registry=network_registry\n    )\n```\n* Call your script like this: `python my_script.py --agent MyAgent --env \nenv-id-1 --custom-network MyNet`\n* You can see all the args [here](adept/scripts/local.py) or how to implement\n the stubs in the examples section above.\n\n## Features\n### Scripts\n**Local (Single-node, Single-GPU)**\n* Best place to [start](adept/scripts/local.py) if you're trying to understand code.\n\n**Distributed (Multi-node, Multi-GPU)**\n* Uses NCCL backend to all-reduce gradients across GPUs without a parameter \nserver or host process.\n* Supports NVLINK and InfiniBand to reduce communication overhead\n* InfiniBand untested since we do not have a setup to test on.\n\n**Importance Weighted Actor Learner Architectures, IMPALA (Single Node, Multi-GPU)**\n* Our implementation uses GPU workers rather than CPU workers for forward \npasses.\n* On Atari we achieve ~4k SPS = ~16k FPS with two GPUs and an 8-core CPU.\n* \"Note that the shallow IMPALA experiment completes training over 200 \nmillion frames in less than one hour.\"\n* IMPALA official experiments use 48 cores.\n* Ours: 2000 frame / (second * # CPU core) DeepMind: 1157 frame / (second * # CPU core)\n* Does not yet support multiple nodes or direct GPU memory transfers.\n\n### Agents\n* Advantage Actor Critic, A2C ([paper](https://arxiv.org/pdf/1708.05144.pdf) | [code](adept/agents/actor_critic.py))\n* Actor Critic Vtrace, IMPALA ([paper](https://arxiv.org/pdf/1802.01561.pdf) | [code](https://arxiv.org/pdf/1802.01561.pdf))\n\n### Networks\n* Modular Network Interface: supports arbitrary input and output shapes up to\n 4D via a SubModule API.\n* Stateful networks (ie. LSTMs)\n* Batch normalization ([paper](https://arxiv.org/pdf/1502.03167.pdf))\n\n### Environments\n* OpenAI Gym\n* StarCraft 2 (unstable)\n\n## Performance\n* ~ 3,000 Steps/second = 12,000 FPS (Atari)\n  * Local Mode\n  * 64 environments\n  * GeForce 2080 Ti\n  * Ryzen 2700x 8-core\n* Used to win a \n[Doom competition](https://www.crowdai.org/challenges/visual-doom-ai-competition-2018-track-2) \n(Ben Bell / Marv2in)\n![architecture](images/benchmark.png)\n* Trained for 50M Steps / 200M Frames\n* Up to 30 no-ops at start of each episode\n* Evaluated on different seeds than trained on\n* Architecture: [Four Convs](./adept/networks/net3d/four_conv.py) (F=32)\nfollowed by an [LSTM](./adept/networks/net1d/lstm.py) (F=512)\n* Reproduce with `python -m adept.app local --logdir ~/local64_benchmark --eval \n-y --nb-step 50e6 --env <env-id>`\n\n## Acknowledgements\nWe borrow pieces of OpenAI's [gym](https://github.com/openai/gym) and \n[baselines](https://github.com/openai/baselines) code. We indicate where this\n is done.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/heronsystems/adeptRL", "keywords": "", "license": "GNU", "maintainer": "", "maintainer_email": "", "name": "adeptRL", "package_url": "https://pypi.org/project/adeptRL/", "platform": "", "project_url": "https://pypi.org/project/adeptRL/", "project_urls": {"Homepage": "https://github.com/heronsystems/adeptRL"}, "release_url": "https://pypi.org/project/adeptRL/0.2.0/", "requires_dist": ["numpy (>=1.14)", "gym[atari] (>=0.10)", "absl-py (>=0.2)", "tensorboard (>=1.14)", "cloudpickle (>=0.5)", "opencv-python-headless (>=3.4)", "pyzmq (>=17.1.2)", "docopt (>=0.6)", "torch (>=1.3.1)", "torchvision (>=0.4.2)", "mpi4py (>=3.0) ; extra == 'all'", "pysc2 (>=2.0) ; extra == 'all'", "pyinstrument (>=2.0) ; extra == 'all'", "pytest ; extra == 'all'", "mpi4py (>=3.0) ; extra == 'mpi'", "pyinstrument (>=2.0) ; extra == 'profiler'", "pysc2 (>=2.0) ; extra == 'sc2'"], "requires_python": ">=3.5.0", "summary": "Reinforcement Learning Framework", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"banner\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fd76b43b78edc69dc61289021a550a1365419466/696d616765732f62616e6e65722e706e67\"></p>\n<p>adept is a reinforcement learning framework designed to accelerate research\nby providing:</p>\n<ul>\n<li>a modular interface for using custom networks, agents, and environments</li>\n<li>baseline reinforcement learning models and algorithms for PyTorch</li>\n<li>multi-GPU support</li>\n<li>access to various environments</li>\n<li>built-in tensorboard logging, model saving, reloading, evaluation, and\nrendering</li>\n<li>proven hyperparameter defaults</li>\n</ul>\n<p>This code is early-access, expect rough edges. Interfaces subject to change.\nWe're happy to accept feedback and contributions.</p>\n<h3>Read More</h3>\n<ul>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#quickstart\" rel=\"nofollow\">Quickstart</a></li>\n<li><a href=\"#features\" rel=\"nofollow\">Features</a></li>\n<li><a href=\"#performance\" rel=\"nofollow\">Performance</a></li>\n</ul>\n<h3>Documentation</h3>\n<ul>\n<li><a href=\"docs/api_overview.md\" rel=\"nofollow\">Architecture Overview</a></li>\n<li><a href=\"docs/modular_network.md\" rel=\"nofollow\">ModularNetwork Overview</a></li>\n<li><a href=\"docs/resume_training.md\" rel=\"nofollow\">Resume training</a></li>\n<li>Evaluate a model</li>\n<li>Render environment</li>\n</ul>\n<h3>Examples</h3>\n<ul>\n<li>Custom Network (<a href=\"examples/custom_network_stub.py\" rel=\"nofollow\">stub</a> | example)</li>\n<li>Custom SubModule (<a href=\"examples/custom_submodule_stub.py\" rel=\"nofollow\">stub</a> | <a href=\"adept/networks/net1d/lstm.py\" rel=\"nofollow\">example</a>)</li>\n<li>Custom Agent (<a href=\"examples/custom_agent_stub.py\" rel=\"nofollow\">stub</a> | <a href=\"adept/agents/actor_critic.py\" rel=\"nofollow\">example</a>)</li>\n<li>Custom Environment (<a href=\"examples/custom_environment_stub.py\" rel=\"nofollow\">stub</a> | <a href=\"adept/environments/openai_gym.py\" rel=\"nofollow\">example</a>)</li>\n</ul>\n<h2>Installation</h2>\n<p><strong>Dependencies:</strong></p>\n<ul>\n<li>gym</li>\n<li>PyTorch 1.x</li>\n<li>Python 3.5+</li>\n<li>We recommend CUDA 10, pytorch 1.0, python 3.6</li>\n</ul>\n<p><strong>From source:</strong></p>\n<ul>\n<li>Follow instructions for <a href=\"https://pytorch.org/\" rel=\"nofollow\">PyTorch</a></li>\n<li>(Optional) Follow instructions for\n<a href=\"https://github.com/Blizzard/s2client-proto#downloads\" rel=\"nofollow\">StarCraft 2</a></li>\n</ul>\n<pre>git clone https://github.com/heronsystems/adeptRL\n<span class=\"nb\">cd</span> adeptRL\n<span class=\"c1\"># Remove mpi, sc2, profiler if you don't plan on using these features:</span>\npip install .<span class=\"o\">[</span>mpi,sc2,profiler<span class=\"o\">]</span>\n</pre>\n<p><strong>From docker:</strong></p>\n<ul>\n<li><a href=\"./docker/\" rel=\"nofollow\">docker instructions</a></li>\n</ul>\n<h2>Quickstart</h2>\n<p><strong>Train an Agent</strong>\nLogs go to <code>/tmp/adept_logs/</code> by default. The log directory contains the\ntensorboard file, saved models, and other metadata.</p>\n<pre><span class=\"c1\"># Local Mode (A2C)</span>\n<span class=\"c1\"># We recommend 4GB+ GPU memory, 8GB+ RAM, 4+ Cores</span>\npython -m adept.app <span class=\"nb\">local</span> --env BeamRiderNoFrameskip-v4\n\n<span class=\"c1\"># Distributed Mode (A2C, requires NCCL)</span>\n<span class=\"c1\"># We recommend 2+ GPUs, 8GB+ GPU memory, 32GB+ RAM, 4+ Cores</span>\npython -m adept.app distrib --env BeamRiderNoFrameskip-v4\n\n<span class=\"c1\"># IMPALA (requires mpi4py and is resource intensive)</span>\n<span class=\"c1\"># We recommend 2+ GPUs, 8GB+ GPU memory, 32GB+ RAM, 4+ Cores</span>\npython -m adept.app impala --agent ActorCriticVtrace --env BeamRiderNoFrameskip-v4\n\n<span class=\"c1\"># StarCraft 2 (IMPALA not supported yet)</span>\n<span class=\"c1\"># Warning: much more resource intensive than Atari</span>\npython -m adept.app <span class=\"nb\">local</span> --env CollectMineralShards\n\n<span class=\"c1\"># To see a full list of options:</span>\npython -m adept.app -h\npython -m adept.app <span class=\"nb\">help</span> &lt;command&gt;\n</pre>\n<p><strong>Use your own Agent, Environment, Network, or SubModule</strong></p>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">my_script.py</span>\n\n<span class=\"sd\">Train an agent on a single GPU.</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"kn\">from</span> <span class=\"nn\">adept.scripts.local</span> <span class=\"kn\">import</span> <span class=\"n\">parse_args</span><span class=\"p\">,</span> <span class=\"n\">main</span>\n<span class=\"kn\">from</span> <span class=\"nn\">adept.networks</span> <span class=\"kn\">import</span> <span class=\"n\">NetworkModule</span><span class=\"p\">,</span> <span class=\"n\">NetworkRegistry</span><span class=\"p\">,</span> <span class=\"n\">SubModule1D</span>\n<span class=\"kn\">from</span> <span class=\"nn\">adept.agents</span> <span class=\"kn\">import</span> <span class=\"n\">AgentModule</span><span class=\"p\">,</span> <span class=\"n\">AgentRegistry</span>\n<span class=\"kn\">from</span> <span class=\"nn\">adept.environments</span> <span class=\"kn\">import</span> <span class=\"n\">EnvModule</span><span class=\"p\">,</span> <span class=\"n\">EnvRegistry</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">MyAgent</span><span class=\"p\">(</span><span class=\"n\">AgentModule</span><span class=\"p\">):</span>\n    <span class=\"k\">pass</span>  <span class=\"c1\"># Implement</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">MyEnv</span><span class=\"p\">(</span><span class=\"n\">EnvModule</span><span class=\"p\">):</span>\n    <span class=\"k\">pass</span>  <span class=\"c1\"># Implement</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">MyNet</span><span class=\"p\">(</span><span class=\"n\">NetworkModule</span><span class=\"p\">):</span>\n    <span class=\"k\">pass</span>  <span class=\"c1\"># Implement</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">MySubModule1D</span><span class=\"p\">(</span><span class=\"n\">SubModule1D</span><span class=\"p\">):</span>\n    <span class=\"k\">pass</span>  <span class=\"c1\"># Implement</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span><span class=\"p\">:</span>\n    <span class=\"n\">agent_registry</span> <span class=\"o\">=</span> <span class=\"n\">AgentRegistry</span><span class=\"p\">()</span>\n    <span class=\"n\">agent_registry</span><span class=\"o\">.</span><span class=\"n\">register_agent</span><span class=\"p\">(</span><span class=\"n\">MyAgent</span><span class=\"p\">)</span>\n\n    <span class=\"n\">env_registry</span> <span class=\"o\">=</span> <span class=\"n\">EnvRegistry</span><span class=\"p\">()</span>\n    <span class=\"n\">env_registry</span><span class=\"o\">.</span><span class=\"n\">register_env</span><span class=\"p\">(</span><span class=\"n\">MyEnv</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s1\">'env-id-1'</span><span class=\"p\">,</span> <span class=\"s1\">'env-id-2'</span><span class=\"p\">])</span>\n\n    <span class=\"n\">network_registry</span> <span class=\"o\">=</span> <span class=\"n\">NetworkRegistry</span><span class=\"p\">()</span>\n    <span class=\"n\">network_registry</span><span class=\"o\">.</span><span class=\"n\">register_custom_net</span><span class=\"p\">(</span><span class=\"n\">MyNet</span><span class=\"p\">)</span>\n    <span class=\"n\">network_registry</span><span class=\"o\">.</span><span class=\"n\">register_submodule</span><span class=\"p\">(</span><span class=\"n\">MySubModule1D</span><span class=\"p\">)</span>\n\n    <span class=\"n\">main</span><span class=\"p\">(</span>\n        <span class=\"n\">parse_args</span><span class=\"p\">(),</span>\n        <span class=\"n\">agent_registry</span><span class=\"o\">=</span><span class=\"n\">agent_registry</span><span class=\"p\">,</span>\n        <span class=\"n\">env_registry</span><span class=\"o\">=</span><span class=\"n\">env_registry</span><span class=\"p\">,</span>\n        <span class=\"n\">net_registry</span><span class=\"o\">=</span><span class=\"n\">network_registry</span>\n    <span class=\"p\">)</span>\n</pre>\n<ul>\n<li>Call your script like this: <code>python my_script.py --agent MyAgent --env env-id-1 --custom-network MyNet</code></li>\n<li>You can see all the args <a href=\"adept/scripts/local.py\" rel=\"nofollow\">here</a> or how to implement\nthe stubs in the examples section above.</li>\n</ul>\n<h2>Features</h2>\n<h3>Scripts</h3>\n<p><strong>Local (Single-node, Single-GPU)</strong></p>\n<ul>\n<li>Best place to <a href=\"adept/scripts/local.py\" rel=\"nofollow\">start</a> if you're trying to understand code.</li>\n</ul>\n<p><strong>Distributed (Multi-node, Multi-GPU)</strong></p>\n<ul>\n<li>Uses NCCL backend to all-reduce gradients across GPUs without a parameter\nserver or host process.</li>\n<li>Supports NVLINK and InfiniBand to reduce communication overhead</li>\n<li>InfiniBand untested since we do not have a setup to test on.</li>\n</ul>\n<p><strong>Importance Weighted Actor Learner Architectures, IMPALA (Single Node, Multi-GPU)</strong></p>\n<ul>\n<li>Our implementation uses GPU workers rather than CPU workers for forward\npasses.</li>\n<li>On Atari we achieve ~4k SPS = ~16k FPS with two GPUs and an 8-core CPU.</li>\n<li>\"Note that the shallow IMPALA experiment completes training over 200\nmillion frames in less than one hour.\"</li>\n<li>IMPALA official experiments use 48 cores.</li>\n<li>Ours: 2000 frame / (second * # CPU core) DeepMind: 1157 frame / (second * # CPU core)</li>\n<li>Does not yet support multiple nodes or direct GPU memory transfers.</li>\n</ul>\n<h3>Agents</h3>\n<ul>\n<li>Advantage Actor Critic, A2C (<a href=\"https://arxiv.org/pdf/1708.05144.pdf\" rel=\"nofollow\">paper</a> | <a href=\"adept/agents/actor_critic.py\" rel=\"nofollow\">code</a>)</li>\n<li>Actor Critic Vtrace, IMPALA (<a href=\"https://arxiv.org/pdf/1802.01561.pdf\" rel=\"nofollow\">paper</a> | <a href=\"https://arxiv.org/pdf/1802.01561.pdf\" rel=\"nofollow\">code</a>)</li>\n</ul>\n<h3>Networks</h3>\n<ul>\n<li>Modular Network Interface: supports arbitrary input and output shapes up to\n4D via a SubModule API.</li>\n<li>Stateful networks (ie. LSTMs)</li>\n<li>Batch normalization (<a href=\"https://arxiv.org/pdf/1502.03167.pdf\" rel=\"nofollow\">paper</a>)</li>\n</ul>\n<h3>Environments</h3>\n<ul>\n<li>OpenAI Gym</li>\n<li>StarCraft 2 (unstable)</li>\n</ul>\n<h2>Performance</h2>\n<ul>\n<li>~ 3,000 Steps/second = 12,000 FPS (Atari)\n<ul>\n<li>Local Mode</li>\n<li>64 environments</li>\n<li>GeForce 2080 Ti</li>\n<li>Ryzen 2700x 8-core</li>\n</ul>\n</li>\n<li>Used to win a\n<a href=\"https://www.crowdai.org/challenges/visual-doom-ai-competition-2018-track-2\" rel=\"nofollow\">Doom competition</a>\n(Ben Bell / Marv2in)\n<img alt=\"architecture\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2c708eaf6633347d3bb33c48fdc5e6dc2b4e7d98/696d616765732f62656e63686d61726b2e706e67\"></li>\n<li>Trained for 50M Steps / 200M Frames</li>\n<li>Up to 30 no-ops at start of each episode</li>\n<li>Evaluated on different seeds than trained on</li>\n<li>Architecture: <a href=\"./adept/networks/net3d/four_conv.py\" rel=\"nofollow\">Four Convs</a> (F=32)\nfollowed by an <a href=\"./adept/networks/net1d/lstm.py\" rel=\"nofollow\">LSTM</a> (F=512)</li>\n<li>Reproduce with <code>python -m adept.app local --logdir ~/local64_benchmark --eval -y --nb-step 50e6 --env &lt;env-id&gt;</code></li>\n</ul>\n<h2>Acknowledgements</h2>\n<p>We borrow pieces of OpenAI's <a href=\"https://github.com/openai/gym\" rel=\"nofollow\">gym</a> and\n<a href=\"https://github.com/openai/baselines\" rel=\"nofollow\">baselines</a> code. We indicate where this\nis done.</p>\n\n          </div>"}, "last_serial": 6527785, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "9b5432e28539a7aa86a12ed0a0dd6fa7", "sha256": "8b1db429029149009097ba1bf9e51f48d9c41a36ad3e9fa150eea69855a26c8b"}, "downloads": -1, "filename": "adeptRL-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9b5432e28539a7aa86a12ed0a0dd6fa7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5.0", "size": 93078, "upload_time": "2018-08-28T15:03:02", "upload_time_iso_8601": "2018-08-28T15:03:02.168420Z", "url": "https://files.pythonhosted.org/packages/d4/be/20b489139f7471e98a6a021d25a766e0dd0c7eadd108bebc062eaf88a218/adeptRL-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b5ad655a9592aa7eb2f699d0880df5ea", "sha256": "0001c87dc1f707cd8877035e9524852509df99c02ef8607f453f5dc145f1671e"}, "downloads": -1, "filename": "adeptRL-0.1.1.tar.gz", "has_sig": false, "md5_digest": "b5ad655a9592aa7eb2f699d0880df5ea", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 48866, "upload_time": "2018-08-28T15:03:04", "upload_time_iso_8601": "2018-08-28T15:03:04.258617Z", "url": "https://files.pythonhosted.org/packages/19/99/d994495f282af042c75f382184d946aef3ec0918dd9d8dd749bd6926084a/adeptRL-0.1.1.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "4490838656cb0650ac9fc4d2e49e8421", "sha256": "f2ad6228c71a783bfb040099981530a246983627502d79410946509435bfb3bd"}, "downloads": -1, "filename": "adeptRL-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "4490838656cb0650ac9fc4d2e49e8421", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5.0", "size": 153973, "upload_time": "2020-01-27T18:13:09", "upload_time_iso_8601": "2020-01-27T18:13:09.792240Z", "url": "https://files.pythonhosted.org/packages/35/70/6659eef7a0d695ffab0511a5601f40423f38a0c27eedf4955e9025b0fa3f/adeptRL-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7c56a40231743739d2103fcf06783270", "sha256": "859502c63595e7acc2126d663638883875d22bd24a5f0dbce0600f94da677aaf"}, "downloads": -1, "filename": "adeptRL-0.2.0.tar.gz", "has_sig": false, "md5_digest": "7c56a40231743739d2103fcf06783270", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 73065, "upload_time": "2020-01-27T18:13:11", "upload_time_iso_8601": "2020-01-27T18:13:11.866778Z", "url": "https://files.pythonhosted.org/packages/ea/d0/d33e65f92fdb05be4be393722c972a38597000f6fb12a72cd75afbd92147/adeptRL-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "4490838656cb0650ac9fc4d2e49e8421", "sha256": "f2ad6228c71a783bfb040099981530a246983627502d79410946509435bfb3bd"}, "downloads": -1, "filename": "adeptRL-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "4490838656cb0650ac9fc4d2e49e8421", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5.0", "size": 153973, "upload_time": "2020-01-27T18:13:09", "upload_time_iso_8601": "2020-01-27T18:13:09.792240Z", "url": "https://files.pythonhosted.org/packages/35/70/6659eef7a0d695ffab0511a5601f40423f38a0c27eedf4955e9025b0fa3f/adeptRL-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7c56a40231743739d2103fcf06783270", "sha256": "859502c63595e7acc2126d663638883875d22bd24a5f0dbce0600f94da677aaf"}, "downloads": -1, "filename": "adeptRL-0.2.0.tar.gz", "has_sig": false, "md5_digest": "7c56a40231743739d2103fcf06783270", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 73065, "upload_time": "2020-01-27T18:13:11", "upload_time_iso_8601": "2020-01-27T18:13:11.866778Z", "url": "https://files.pythonhosted.org/packages/ea/d0/d33e65f92fdb05be4be393722c972a38597000f6fb12a72cd75afbd92147/adeptRL-0.2.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 16:23:27 2020"}