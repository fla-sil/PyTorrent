{"info": {"author": "Marc Brockschmidt", "author_email": "mabrocks@microsoft.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Graph Neural Networks in TF2\n\nImplementation and example training scripts of various flavours of graph neural network in \nTensorFlow 2.0.\nMuch of it is based on the code in the [tf-gnn-samples](https://github.com/microsoft/tf-gnn-samples) repo.\n\n## Installation\nYou can install the `tf2_gnn` module from the Python Package Index using \n`pip install tf2_gnn`.\n\nAlternatively (for example, for development), you can check out this repository,\nnavigate to it and run `pip install -e ./` to install it as a local editable package.\n\nYou will then be able to use the `tf2_gnn.layers.GNN` class and related utilities.\n\nThis code was tested in Python 3.6 and 3.7 with TensorFlow 2.0 and 2.1.\n\nThe code is maintained by the [All Data AI](https://www.microsoft.com/en-us/research/group/ada/)\ngroup at Microsoft Research, Cambridge, UK.\nWe are [hiring](https://www.microsoft.com/en-us/research/theme/ada/#!opportunities).\n\n\n## Testing the Installation\n\nTo test if all components are set up correctly, you can run a simple experiment on the\nprotein-protein interaction (PPI) task first described by \n[Zitnik & Leskovec, 2017](#zitnik-leskovec-2017).\nYou can download the data for this task from https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/ppi.zip\nand unzip it into a local directory (e.g., `data/ppi`).\nThen, you can use the convenience utility `tf2_gnn_train` (see `--help` for a description\nof options) to train a Relational Graph Convoluational Network model as follows:\n```\n$ tf2_gnn_train RGCN PPI data/ppi/\nSetting random seed 0.\nTrying to load task/model-specific default parameters from /dpuhome/files/users/mabrocks/Projects/TF2-GNN/tf2_gnn/cli_utils/default_hypers/PPI_RGCN.json ... File found.\n Dataset default parameters: {'max_nodes_per_batch': 10000, 'add_self_loop_edges': True, 'tie_fwd_bkwd_edges': False}\nLoading data from data/ppi/.\n Loading PPI train data from data/ppi/.\n Loading PPI valid data from data/ppi/.\n[...]\nDataset parameters: {\"max_nodes_per_batch\": 8000, \"add_self_loop_edges\": true, \"tie_fwd_bkwd_edges\": false}\nModel parameters: {\"gnn_aggregation_function\": \"sum\", \"gnn_message_activation_function\": \"ReLU\", \"gnn_hidden_dim\": 320, \"gnn_use_target_state_as_input\": false, \"gnn_normalize_by_num_incoming\": true, \"gnn_num_edge_MLP_hidden_layers\": 0, \"gnn_message_calculation_class\": \"RGCN\", \"gnn_initial_node_representation_activation\": \"tanh\", \"gnn_dense_intermediate_layer_activation\": \"tanh\", \"gnn_num_layers\": 4, \"gnn_dense_every_num_layers\": 10000, \"gnn_residual_every_num_layers\": 10000, \"gnn_use_inter_layer_layernorm\": false, \"gnn_layer_input_dropout_rate\": 0.1, \"gnn_global_exchange_mode\": \"gru\", \"gnn_global_exchange_every_num_layers\": 10000, \"gnn_global_exchange_weighting_fun\": \"softmax\", \"gnn_global_exchange_num_heads\": 4, \"gnn_global_exchange_dropout_rate\": 0.2, \"optimizer\": \"Adam\", \"learning_rate\": 0.001, \"learning_rate_decay\": 0.98, \"momentum\": 0.85, \"gradient_clip_value\": 1.0}\nInitial valid metric: Avg MicroF1: 0.368.\n   (Stored model metadata to trained_model/RGCN_PPI__2020-02-25_11-10-38_best.pkl and weights to trained_model/RGCN_PPI__2020-02-25_11-10-38_best.hdf5)\n== Epoch 1\n Train:  25.6870 loss | Avg MicroF1: 0.401 | 2.63 graphs/s\n Valid:  33.1668 loss | Avg MicroF1: 0.419 | 4.01 graphs/s\n  (Best epoch so far, target metric decreased to -0.41886 from -0.36762.)\n   (Stored model metadata to trained_model/RGCN_PPI__2020-02-25_11-10-38_best.pkl and weights to trained_model/RGCN_PPI__2020-02-25_11-10-38_best.hdf5)\n[...]\n```\n\nAfter training finished, `tf2_gnn_test trained_model/RGCN_PPI__2020-02-25_11-10-38_best.pkl data/ppi` can be used to test the trained model.\n\n\n# Code Structure\n\n## Layers\n\nThe core functionality of the library is implemented as TensorFlow 2 (Keras) layers,\nenabling easy integration into other code.\n\n\n### `tf2_gnn.layers.GNN`\n\nThis implements a deep Graph Neural Network, stacking several layers of message passing.\nOn construction, a dictionary of hyperparameters needs to be provided (default\nvalues can be obtained from `GNN.get_default_hyperparameters()`).\nThese hyperparameters configure the exact stack of GNN layers:\n* `\"num_layers\"` sets the number of GNN message passing layers (usually, a number\n  between 2 and 16)\n\n* `\"message_calculation_class\"` configures the message passing style.\n  This chooses the `tf2_gnn.layers.message_passing.*` layer used in each step.\n  \n  We currently support the following:\n    * `GGNN`: Gated Graph Neural Networks ([Li et al., 2015](#li-et-al-2015)).\n    * `RGCN`: Relational Graph Convolutional Networks ([Schlichtkrull et al., 2017](#schlichtkrull-et-al-2017)).\n    * `RGAT`: Relational Graph Attention Networks ([Veli\u010dkovi\u0107 et al., 2018](#veli\u010dkovi\u0107-et-al-2018)).\n    * `RGIN`: Relational Graph Isomorphism Networks ([Xu et al., 2019](#xu-et-al-2019)).\n    * `GNN-Edge-MLP`: Graph Neural Network with Edge MLPs - a variant of RGCN in which messages on edges are computed using full MLPs, not just a single layer applied to the source state.\n    * `GNN-FiLM`: Graph Neural Networks with Feature-wise Linear Modulation ([Brockschmidt, 2019](#brockschmidt-2019)) - a new extension of RGCN with FiLM layers.\n  \n  Some of these expose additional hyperparameters; refer to their implementation for\n  details.\n\n* `\"hidden_dim\"` sets the size of the output of all message passing layers.\n\n* `\"layer_input_dropout_rate\"` sets the dropout rate (during training) for the\n  input of each message passing layer.\n\n* `\"residual_every_num_layers\"` sets how often a residual connection is inserted\n  between message passing layers. Concretely, a value of `k` means that every layer\n  `l` that is a multiple of `k` (and only those!) will not receive the outputs of\n  layer `l-1` as input, but instead the mean of the outputs of layers `l-1` and `l-k`.\n\n* `\"use_inter_layer_layernorm\"` is a boolean flag indicating if `LayerNorm` should be\n  used between different message passing layers.\n\n* `\"dense_every_num_layers\"` configures how often a per-node representation dense layer\n  is inserted between the message passing layers.\n  Setting this to a large value (greather than `\"num_layers\"`) means that no dense\n  layers are inserted at all.\n  \n  `\"dense_intermediate_layer_activation\"` configures the activation function used after\n  the dense layer; the default of `\"tanh\"` can help stabilise training of very deep\n  GNNs.\n\n* `\"global_exchange_every_num_layers\"` configures how often a graph-level exchange of\n  information is performed.\n  For this, a graph level representation (see `tf2_gnn.layers.NodesToGraphRepresentation`\n  below) is computed and then used to update the representation of each node.\n  The style of this update is configured by `\"global_exchange_mode\"`, offering three\n  modes:\n    * `\"mean\"`, which just computes the arithmetic mean of the node and graph-level\n      representation.\n    * `\"mlp\"`, which computes a new representation using an MLP that gets the\n      concatenation of node and graph level representations as input.\n    * `\"gru\"`, which uses a GRU cell that gets the old node representation as state\n      and the graph representation as input.\n\nThe `GNN` layer takes a `GNNInput` named tuple as input, which encapsulates initial\nnode features, adjacency lists, and auxiliary information.\nThe easiest way to construct such a tuple is to use the provided [dataset](datasets)\nclasses in combination with the provided [model](models).\n\n\n### `tf2_gnn.layers.NodesToGraphRepresentation`\n\nThis implements the task of computing a graph-level representation given node-level\nrepresentations (e.g., obtained by the `GNN` layer).\n\nCurrently, this is only implemented by the `WeightedSumGraphRepresentation` layer,\nwhich produces a graph representation by a multi-headed weighted sum of (transformed) \nnode representations, configured by the following hyperparameters set in the\nlayer constructor:\n* `graph_representation_size` sets the size of the computed representation.\n  By setting this to `1`, this layer can be used to directly implement graph-level\n  regression tasks.\n* `num_heads` configures the number of parallel (independent) weighted sums that\n  are computed, whose results are concatenated to obtain the final result.\n  Note that this means that the `graph_representation_size` needs to be a multiple\n  of the `num_heads` value.\n* `weighting_fun` can take two values:\n  * `\"sigmoid\"` computes a weight for each node independently by first computing\n    a per-node score, which is then squashed through a sigmoid.\n    This is appropriate for tasks that are related to counting occurrences of a \n    feature in a graph, where the node weight is used to ignore certain nodes.\n  * `\"softmax\"` computes weights for all graph nodes together by first computing\n    per-node scores, and then performing a softmax over all scores.\n    This is appropriate for tasks that require identifying important parts of\n    the graph.\n* `scoring_mlp_layers`, `scoring_mlp_activation_fun`, `scoring_mlp_dropout_rate`\n  configure the MLP that computes the per-node scores.\n* `transformation_mlp_layers`, `transformation_mlp_activation_fun`, \n  `transformation_mlp_dropout_rate` configure the MLP that computes the\n  transformed node representations that are summed up.\n\n\n## Datasets\n\nWe use a sparse representation of graphs, which requires a complex batching strategy\nin which the graphs making up a minibatch are joined into a single graph of many\ndisconnected components.\nThe extensible `tf2_gnn.data.GraphDataset` class implements this procedure, and can\nbe subclassed to handle task-specific datasets and additional properties.\nIt exposes a `get_tensorflow_dataset` method that can be used to obtain a \n`tf.data.Dataset` that can be used in training/evaluation loops.\n\nWe currently provide three implementations of this:\n* `tf2_gnn.data.PPIDataset` implements reading the protein-protein interaction (PPI)\n  data first used by [Zitnik & Leskovec, 2017](#zitnik-leskovec-2017).\n* `tf2_gnn.data.QM9Dataset` implements reading the quantum chemistry data first\n  used by [Ramakrishnan et al., 2014](#ramakrishnan-et-al-2014).\n* `tf2_gnn.data.JsonLGraphPropertyDataset` implements reading a generic dataset\n  made up of graphs with a single property, stored in JSONLines format:\n  * Files \"train.jsonl.gz\", \"valid.jsonl.gz\" and \"test.jsonl.gz\" are expected to\n    store the train/valid/test datasets.\n  * Each of the files is gzipped text file in which each line is a valid\n    JSON dictionary with\n    * a `\"graph\"` key, which in turn points to a dictionary with keys\n      * `\"node_features\"` (list of numerical initial node labels),\n      * `\"adjacency_lists\"` (list of list of directed edge pairs),\n    * a `\"Property\"` key having a a single floating point value.\n\n\n## Models\n\nWe provide some built-in models in `tf2_gnn.models`, which can either be directly\nre-used or serve as inspiration for other models:\n* `tf2_gnn.models.GraphRegressionTask` implements a graph-level regression model,\n  for example to make molecule-level predictions such as in the QM9 task.\n* `tf2_gnn.models.GraphBinaryClassificationTask` implements a binary classification\n  model.\n* `tf2_gnn.models.NodeMulticlassTask` implements a node-level multiclass classification\n  model, suitable to implement the PPI task.\n\n\n## Tasks\n\nTasks are a combination of datasets, models and specific hyperparameter settings.\nThese can be registered (and then used by name) using the utilities in\n`tf2_gnn.utils.task_utils` (where a few default tasks are defined as well) and then\nused in tools such as `tf2_gnn_train`.\n\n# Authors\n\n* [Henry Jackson-Flux](mailto:Henry.JacksonFlux@microsoft.com)\n* [Marc Brockschmidt](mailto:Marc.Brockschmidt@microsoft.com)\n* [Megan Stanley](mailto:t-mestan@microsoft.com)\n* [Pashmina Cameron](mailto:Pashmina.Cameron@microsoft.com)\n\n\n# References\n\n#### Brockschmidt, 2019\nMarc Brockschmidt. GNN-FiLM: Graph Neural Networks with Feature-wise Linear\nModulation. (https://arxiv.org/abs/1906.12192)\n\n#### Li et al., 2015\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated Graph\nSequence Neural Networks. In International Conference on Learning\nRepresentations (ICLR), 2016. (https://arxiv.org/pdf/1511.05493.pdf)\n\n#### Ramakrishnan et al., 2014\nRaghunathan Ramakrishnan, Pavlo O. Dral, Matthias Rupp, and O. Anatole\nVon Lilienfeld. Quantum Chemistry Structures and Properties of 134 Kilo\nMolecules. Scientific Data, 1, 2014.\n(https://www.nature.com/articles/sdata201422/)\n\n#### Schlichtkrull et al., 2017\nMichael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg,\nIvan Titov, and Max Welling. Modeling Relational Data with Graph\nConvolutional Networks. In Extended Semantic Web Conference (ESWC), 2018.\n(https://arxiv.org/pdf/1703.06103.pdf)\n\n#### Veli\u010dkovi\u0107 et al. 2018\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLi\u00f2, and Yoshua Bengio. Graph Attention Networks. In International Conference\non Learning Representations (ICLR), 2018. (https://arxiv.org/pdf/1710.10903.pdf)\n\n#### Xu et al. 2019\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are\nGraph Neural Networks? In International Conference on Learning Representations\n(ICLR), 2019. (https://arxiv.org/pdf/1810.00826.pdf)\n\n#### Zitnik & Leskovec, 2017\nMarinka Zitnik and Jure Leskovec. Predicting Multicellular Function Through\nMulti-layer Tissue Networks. Bioinformatics, 33, 2017.\n(https://arxiv.org/abs/1707.04638)\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/microsoft/tf2-gnn/", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tf2-gnn", "package_url": "https://pypi.org/project/tf2-gnn/", "platform": "", "project_url": "https://pypi.org/project/tf2-gnn/", "project_urls": {"Homepage": "https://github.com/microsoft/tf2-gnn/"}, "release_url": "https://pypi.org/project/tf2-gnn/2.3.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "TensorFlow 2.0 implementation of Graph Neural Networks.", "version": "2.3.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Graph Neural Networks in TF2</h1>\n<p>Implementation and example training scripts of various flavours of graph neural network in\nTensorFlow 2.0.\nMuch of it is based on the code in the <a href=\"https://github.com/microsoft/tf-gnn-samples\" rel=\"nofollow\">tf-gnn-samples</a> repo.</p>\n<h2>Installation</h2>\n<p>You can install the <code>tf2_gnn</code> module from the Python Package Index using\n<code>pip install tf2_gnn</code>.</p>\n<p>Alternatively (for example, for development), you can check out this repository,\nnavigate to it and run <code>pip install -e ./</code> to install it as a local editable package.</p>\n<p>You will then be able to use the <code>tf2_gnn.layers.GNN</code> class and related utilities.</p>\n<p>This code was tested in Python 3.6 and 3.7 with TensorFlow 2.0 and 2.1.</p>\n<p>The code is maintained by the <a href=\"https://www.microsoft.com/en-us/research/group/ada/\" rel=\"nofollow\">All Data AI</a>\ngroup at Microsoft Research, Cambridge, UK.\nWe are <a href=\"https://www.microsoft.com/en-us/research/theme/ada/#!opportunities\" rel=\"nofollow\">hiring</a>.</p>\n<h2>Testing the Installation</h2>\n<p>To test if all components are set up correctly, you can run a simple experiment on the\nprotein-protein interaction (PPI) task first described by\n<a href=\"#zitnik-leskovec-2017\" rel=\"nofollow\">Zitnik &amp; Leskovec, 2017</a>.\nYou can download the data for this task from <a href=\"https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/ppi.zip\" rel=\"nofollow\">https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/ppi.zip</a>\nand unzip it into a local directory (e.g., <code>data/ppi</code>).\nThen, you can use the convenience utility <code>tf2_gnn_train</code> (see <code>--help</code> for a description\nof options) to train a Relational Graph Convoluational Network model as follows:</p>\n<pre><code>$ tf2_gnn_train RGCN PPI data/ppi/\nSetting random seed 0.\nTrying to load task/model-specific default parameters from /dpuhome/files/users/mabrocks/Projects/TF2-GNN/tf2_gnn/cli_utils/default_hypers/PPI_RGCN.json ... File found.\n Dataset default parameters: {'max_nodes_per_batch': 10000, 'add_self_loop_edges': True, 'tie_fwd_bkwd_edges': False}\nLoading data from data/ppi/.\n Loading PPI train data from data/ppi/.\n Loading PPI valid data from data/ppi/.\n[...]\nDataset parameters: {\"max_nodes_per_batch\": 8000, \"add_self_loop_edges\": true, \"tie_fwd_bkwd_edges\": false}\nModel parameters: {\"gnn_aggregation_function\": \"sum\", \"gnn_message_activation_function\": \"ReLU\", \"gnn_hidden_dim\": 320, \"gnn_use_target_state_as_input\": false, \"gnn_normalize_by_num_incoming\": true, \"gnn_num_edge_MLP_hidden_layers\": 0, \"gnn_message_calculation_class\": \"RGCN\", \"gnn_initial_node_representation_activation\": \"tanh\", \"gnn_dense_intermediate_layer_activation\": \"tanh\", \"gnn_num_layers\": 4, \"gnn_dense_every_num_layers\": 10000, \"gnn_residual_every_num_layers\": 10000, \"gnn_use_inter_layer_layernorm\": false, \"gnn_layer_input_dropout_rate\": 0.1, \"gnn_global_exchange_mode\": \"gru\", \"gnn_global_exchange_every_num_layers\": 10000, \"gnn_global_exchange_weighting_fun\": \"softmax\", \"gnn_global_exchange_num_heads\": 4, \"gnn_global_exchange_dropout_rate\": 0.2, \"optimizer\": \"Adam\", \"learning_rate\": 0.001, \"learning_rate_decay\": 0.98, \"momentum\": 0.85, \"gradient_clip_value\": 1.0}\nInitial valid metric: Avg MicroF1: 0.368.\n   (Stored model metadata to trained_model/RGCN_PPI__2020-02-25_11-10-38_best.pkl and weights to trained_model/RGCN_PPI__2020-02-25_11-10-38_best.hdf5)\n== Epoch 1\n Train:  25.6870 loss | Avg MicroF1: 0.401 | 2.63 graphs/s\n Valid:  33.1668 loss | Avg MicroF1: 0.419 | 4.01 graphs/s\n  (Best epoch so far, target metric decreased to -0.41886 from -0.36762.)\n   (Stored model metadata to trained_model/RGCN_PPI__2020-02-25_11-10-38_best.pkl and weights to trained_model/RGCN_PPI__2020-02-25_11-10-38_best.hdf5)\n[...]\n</code></pre>\n<p>After training finished, <code>tf2_gnn_test trained_model/RGCN_PPI__2020-02-25_11-10-38_best.pkl data/ppi</code> can be used to test the trained model.</p>\n<h1>Code Structure</h1>\n<h2>Layers</h2>\n<p>The core functionality of the library is implemented as TensorFlow 2 (Keras) layers,\nenabling easy integration into other code.</p>\n<h3><code>tf2_gnn.layers.GNN</code></h3>\n<p>This implements a deep Graph Neural Network, stacking several layers of message passing.\nOn construction, a dictionary of hyperparameters needs to be provided (default\nvalues can be obtained from <code>GNN.get_default_hyperparameters()</code>).\nThese hyperparameters configure the exact stack of GNN layers:</p>\n<ul>\n<li>\n<p><code>\"num_layers\"</code> sets the number of GNN message passing layers (usually, a number\nbetween 2 and 16)</p>\n</li>\n<li>\n<p><code>\"message_calculation_class\"</code> configures the message passing style.\nThis chooses the <code>tf2_gnn.layers.message_passing.*</code> layer used in each step.</p>\n<p>We currently support the following:</p>\n<ul>\n<li><code>GGNN</code>: Gated Graph Neural Networks (<a href=\"#li-et-al-2015\" rel=\"nofollow\">Li et al., 2015</a>).</li>\n<li><code>RGCN</code>: Relational Graph Convolutional Networks (<a href=\"#schlichtkrull-et-al-2017\" rel=\"nofollow\">Schlichtkrull et al., 2017</a>).</li>\n<li><code>RGAT</code>: Relational Graph Attention Networks (<a href=\"#veli%C4%8Dkovi%C4%87-et-al-2018\" rel=\"nofollow\">Veli\u010dkovi\u0107 et al., 2018</a>).</li>\n<li><code>RGIN</code>: Relational Graph Isomorphism Networks (<a href=\"#xu-et-al-2019\" rel=\"nofollow\">Xu et al., 2019</a>).</li>\n<li><code>GNN-Edge-MLP</code>: Graph Neural Network with Edge MLPs - a variant of RGCN in which messages on edges are computed using full MLPs, not just a single layer applied to the source state.</li>\n<li><code>GNN-FiLM</code>: Graph Neural Networks with Feature-wise Linear Modulation (<a href=\"#brockschmidt-2019\" rel=\"nofollow\">Brockschmidt, 2019</a>) - a new extension of RGCN with FiLM layers.</li>\n</ul>\n<p>Some of these expose additional hyperparameters; refer to their implementation for\ndetails.</p>\n</li>\n<li>\n<p><code>\"hidden_dim\"</code> sets the size of the output of all message passing layers.</p>\n</li>\n<li>\n<p><code>\"layer_input_dropout_rate\"</code> sets the dropout rate (during training) for the\ninput of each message passing layer.</p>\n</li>\n<li>\n<p><code>\"residual_every_num_layers\"</code> sets how often a residual connection is inserted\nbetween message passing layers. Concretely, a value of <code>k</code> means that every layer\n<code>l</code> that is a multiple of <code>k</code> (and only those!) will not receive the outputs of\nlayer <code>l-1</code> as input, but instead the mean of the outputs of layers <code>l-1</code> and <code>l-k</code>.</p>\n</li>\n<li>\n<p><code>\"use_inter_layer_layernorm\"</code> is a boolean flag indicating if <code>LayerNorm</code> should be\nused between different message passing layers.</p>\n</li>\n<li>\n<p><code>\"dense_every_num_layers\"</code> configures how often a per-node representation dense layer\nis inserted between the message passing layers.\nSetting this to a large value (greather than <code>\"num_layers\"</code>) means that no dense\nlayers are inserted at all.</p>\n<p><code>\"dense_intermediate_layer_activation\"</code> configures the activation function used after\nthe dense layer; the default of <code>\"tanh\"</code> can help stabilise training of very deep\nGNNs.</p>\n</li>\n<li>\n<p><code>\"global_exchange_every_num_layers\"</code> configures how often a graph-level exchange of\ninformation is performed.\nFor this, a graph level representation (see <code>tf2_gnn.layers.NodesToGraphRepresentation</code>\nbelow) is computed and then used to update the representation of each node.\nThe style of this update is configured by <code>\"global_exchange_mode\"</code>, offering three\nmodes:</p>\n<ul>\n<li><code>\"mean\"</code>, which just computes the arithmetic mean of the node and graph-level\nrepresentation.</li>\n<li><code>\"mlp\"</code>, which computes a new representation using an MLP that gets the\nconcatenation of node and graph level representations as input.</li>\n<li><code>\"gru\"</code>, which uses a GRU cell that gets the old node representation as state\nand the graph representation as input.</li>\n</ul>\n</li>\n</ul>\n<p>The <code>GNN</code> layer takes a <code>GNNInput</code> named tuple as input, which encapsulates initial\nnode features, adjacency lists, and auxiliary information.\nThe easiest way to construct such a tuple is to use the provided <a href=\"datasets\" rel=\"nofollow\">dataset</a>\nclasses in combination with the provided <a href=\"models\" rel=\"nofollow\">model</a>.</p>\n<h3><code>tf2_gnn.layers.NodesToGraphRepresentation</code></h3>\n<p>This implements the task of computing a graph-level representation given node-level\nrepresentations (e.g., obtained by the <code>GNN</code> layer).</p>\n<p>Currently, this is only implemented by the <code>WeightedSumGraphRepresentation</code> layer,\nwhich produces a graph representation by a multi-headed weighted sum of (transformed)\nnode representations, configured by the following hyperparameters set in the\nlayer constructor:</p>\n<ul>\n<li><code>graph_representation_size</code> sets the size of the computed representation.\nBy setting this to <code>1</code>, this layer can be used to directly implement graph-level\nregression tasks.</li>\n<li><code>num_heads</code> configures the number of parallel (independent) weighted sums that\nare computed, whose results are concatenated to obtain the final result.\nNote that this means that the <code>graph_representation_size</code> needs to be a multiple\nof the <code>num_heads</code> value.</li>\n<li><code>weighting_fun</code> can take two values:\n<ul>\n<li><code>\"sigmoid\"</code> computes a weight for each node independently by first computing\na per-node score, which is then squashed through a sigmoid.\nThis is appropriate for tasks that are related to counting occurrences of a\nfeature in a graph, where the node weight is used to ignore certain nodes.</li>\n<li><code>\"softmax\"</code> computes weights for all graph nodes together by first computing\nper-node scores, and then performing a softmax over all scores.\nThis is appropriate for tasks that require identifying important parts of\nthe graph.</li>\n</ul>\n</li>\n<li><code>scoring_mlp_layers</code>, <code>scoring_mlp_activation_fun</code>, <code>scoring_mlp_dropout_rate</code>\nconfigure the MLP that computes the per-node scores.</li>\n<li><code>transformation_mlp_layers</code>, <code>transformation_mlp_activation_fun</code>,\n<code>transformation_mlp_dropout_rate</code> configure the MLP that computes the\ntransformed node representations that are summed up.</li>\n</ul>\n<h2>Datasets</h2>\n<p>We use a sparse representation of graphs, which requires a complex batching strategy\nin which the graphs making up a minibatch are joined into a single graph of many\ndisconnected components.\nThe extensible <code>tf2_gnn.data.GraphDataset</code> class implements this procedure, and can\nbe subclassed to handle task-specific datasets and additional properties.\nIt exposes a <code>get_tensorflow_dataset</code> method that can be used to obtain a\n<code>tf.data.Dataset</code> that can be used in training/evaluation loops.</p>\n<p>We currently provide three implementations of this:</p>\n<ul>\n<li><code>tf2_gnn.data.PPIDataset</code> implements reading the protein-protein interaction (PPI)\ndata first used by <a href=\"#zitnik-leskovec-2017\" rel=\"nofollow\">Zitnik &amp; Leskovec, 2017</a>.</li>\n<li><code>tf2_gnn.data.QM9Dataset</code> implements reading the quantum chemistry data first\nused by <a href=\"#ramakrishnan-et-al-2014\" rel=\"nofollow\">Ramakrishnan et al., 2014</a>.</li>\n<li><code>tf2_gnn.data.JsonLGraphPropertyDataset</code> implements reading a generic dataset\nmade up of graphs with a single property, stored in JSONLines format:\n<ul>\n<li>Files \"train.jsonl.gz\", \"valid.jsonl.gz\" and \"test.jsonl.gz\" are expected to\nstore the train/valid/test datasets.</li>\n<li>Each of the files is gzipped text file in which each line is a valid\nJSON dictionary with\n<ul>\n<li>a <code>\"graph\"</code> key, which in turn points to a dictionary with keys\n<ul>\n<li><code>\"node_features\"</code> (list of numerical initial node labels),</li>\n<li><code>\"adjacency_lists\"</code> (list of list of directed edge pairs),</li>\n</ul>\n</li>\n<li>a <code>\"Property\"</code> key having a a single floating point value.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Models</h2>\n<p>We provide some built-in models in <code>tf2_gnn.models</code>, which can either be directly\nre-used or serve as inspiration for other models:</p>\n<ul>\n<li><code>tf2_gnn.models.GraphRegressionTask</code> implements a graph-level regression model,\nfor example to make molecule-level predictions such as in the QM9 task.</li>\n<li><code>tf2_gnn.models.GraphBinaryClassificationTask</code> implements a binary classification\nmodel.</li>\n<li><code>tf2_gnn.models.NodeMulticlassTask</code> implements a node-level multiclass classification\nmodel, suitable to implement the PPI task.</li>\n</ul>\n<h2>Tasks</h2>\n<p>Tasks are a combination of datasets, models and specific hyperparameter settings.\nThese can be registered (and then used by name) using the utilities in\n<code>tf2_gnn.utils.task_utils</code> (where a few default tasks are defined as well) and then\nused in tools such as <code>tf2_gnn_train</code>.</p>\n<h1>Authors</h1>\n<ul>\n<li><a href=\"mailto:Henry.JacksonFlux@microsoft.com\">Henry Jackson-Flux</a></li>\n<li><a href=\"mailto:Marc.Brockschmidt@microsoft.com\">Marc Brockschmidt</a></li>\n<li><a href=\"mailto:t-mestan@microsoft.com\">Megan Stanley</a></li>\n<li><a href=\"mailto:Pashmina.Cameron@microsoft.com\">Pashmina Cameron</a></li>\n</ul>\n<h1>References</h1>\n<h4>Brockschmidt, 2019</h4>\n<p>Marc Brockschmidt. GNN-FiLM: Graph Neural Networks with Feature-wise Linear\nModulation. (<a href=\"https://arxiv.org/abs/1906.12192\" rel=\"nofollow\">https://arxiv.org/abs/1906.12192</a>)</p>\n<h4>Li et al., 2015</h4>\n<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated Graph\nSequence Neural Networks. In International Conference on Learning\nRepresentations (ICLR), 2016. (<a href=\"https://arxiv.org/pdf/1511.05493.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1511.05493.pdf</a>)</p>\n<h4>Ramakrishnan et al., 2014</h4>\n<p>Raghunathan Ramakrishnan, Pavlo O. Dral, Matthias Rupp, and O. Anatole\nVon Lilienfeld. Quantum Chemistry Structures and Properties of 134 Kilo\nMolecules. Scientific Data, 1, 2014.\n(<a href=\"https://www.nature.com/articles/sdata201422/\" rel=\"nofollow\">https://www.nature.com/articles/sdata201422/</a>)</p>\n<h4>Schlichtkrull et al., 2017</h4>\n<p>Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg,\nIvan Titov, and Max Welling. Modeling Relational Data with Graph\nConvolutional Networks. In Extended Semantic Web Conference (ESWC), 2018.\n(<a href=\"https://arxiv.org/pdf/1703.06103.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1703.06103.pdf</a>)</p>\n<h4>Veli\u010dkovi\u0107 et al. 2018</h4>\n<p>Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLi\u00f2, and Yoshua Bengio. Graph Attention Networks. In International Conference\non Learning Representations (ICLR), 2018. (<a href=\"https://arxiv.org/pdf/1710.10903.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1710.10903.pdf</a>)</p>\n<h4>Xu et al. 2019</h4>\n<p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are\nGraph Neural Networks? In International Conference on Learning Representations\n(ICLR), 2019. (<a href=\"https://arxiv.org/pdf/1810.00826.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1810.00826.pdf</a>)</p>\n<h4>Zitnik &amp; Leskovec, 2017</h4>\n<p>Marinka Zitnik and Jure Leskovec. Predicting Multicellular Function Through\nMulti-layer Tissue Networks. Bioinformatics, 33, 2017.\n(<a href=\"https://arxiv.org/abs/1707.04638\" rel=\"nofollow\">https://arxiv.org/abs/1707.04638</a>)</p>\n<h1>Contributing</h1>\n<p>This project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <a href=\"https://cla.opensource.microsoft.com\" rel=\"nofollow\">https://cla.opensource.microsoft.com</a>.</p>\n<p>When you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.</p>\n<p>This project has adopted the <a href=\"https://opensource.microsoft.com/codeofconduct/\" rel=\"nofollow\">Microsoft Open Source Code of Conduct</a>.\nFor more information see the <a href=\"https://opensource.microsoft.com/codeofconduct/faq/\" rel=\"nofollow\">Code of Conduct FAQ</a> or\ncontact <a href=\"mailto:opencode@microsoft.com\">opencode@microsoft.com</a> with any additional questions or comments.</p>\n\n          </div>"}, "last_serial": 7076297, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "892e170c2bc2cb205d40a53f335b83d3", "sha256": "95c3f5789dadad1cef1cfd479cf48c03cdb90c73fe2f5998c21c10a203b5e8a9"}, "downloads": -1, "filename": "tf2_gnn-1.0.0.tar.gz", "has_sig": false, "md5_digest": "892e170c2bc2cb205d40a53f335b83d3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 41326, "upload_time": "2020-02-26T19:14:12", "upload_time_iso_8601": "2020-02-26T19:14:12.004963Z", "url": "https://files.pythonhosted.org/packages/21/8b/9b38a47b473e1c88a5a8143c065945ca2abf225ef931d1e6c163fe16d0f1/tf2_gnn-1.0.0.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "ecb94305ed96b7a72d65938af4dec8ff", "sha256": "e2ac94d19fca2a37fc2ffdafb5a5824ed9922d72ca6bdba8164c61ea9dff99a8"}, "downloads": -1, "filename": "tf2_gnn-1.1.0.tar.gz", "has_sig": false, "md5_digest": "ecb94305ed96b7a72d65938af4dec8ff", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 43584, "upload_time": "2020-03-03T09:01:25", "upload_time_iso_8601": "2020-03-03T09:01:25.790336Z", "url": "https://files.pythonhosted.org/packages/cd/b0/840fae816923432a838e87c82690e7e799636bbb1c0799010cc3cecd4361/tf2_gnn-1.1.0.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "7ec17c645800a21458e8aa6461e90b95", "sha256": "59c52e6179e4742adb7b9acc0a243e8788718c49387727ad240e315be52eb5f3"}, "downloads": -1, "filename": "tf2_gnn-1.2.0.tar.gz", "has_sig": false, "md5_digest": "7ec17c645800a21458e8aa6461e90b95", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 43683, "upload_time": "2020-03-04T15:28:45", "upload_time_iso_8601": "2020-03-04T15:28:45.007115Z", "url": "https://files.pythonhosted.org/packages/ac/69/fb73c581b1013f30829c5227ea5d22e54a08377f6ec96e470be0ec2b4443/tf2_gnn-1.2.0.tar.gz", "yanked": false}], "1.2.1": [{"comment_text": "", "digests": {"md5": "d48de81236c92c395b713428b9d79e40", "sha256": "b1b5aa8fa0694c32064ce41ab1fdb9e2264c4362e49fe4eed8bdd1808e571df0"}, "downloads": -1, "filename": "tf2_gnn-1.2.1.tar.gz", "has_sig": false, "md5_digest": "d48de81236c92c395b713428b9d79e40", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 43718, "upload_time": "2020-03-05T20:10:55", "upload_time_iso_8601": "2020-03-05T20:10:55.936395Z", "url": "https://files.pythonhosted.org/packages/1f/a9/5c4b2b30d203ede8322c7879862a8f5526948a6baf827bc86df8e2ee0681/tf2_gnn-1.2.1.tar.gz", "yanked": false}], "1.2.2": [{"comment_text": "", "digests": {"md5": "e18ceb7a01f036162f49a27f7aab2d5e", "sha256": "bea6e4ab1c2361e652b8bb69cafc50887c1b0d8db87b9ccda3a85dafc103b606"}, "downloads": -1, "filename": "tf2_gnn-1.2.2.tar.gz", "has_sig": false, "md5_digest": "e18ceb7a01f036162f49a27f7aab2d5e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 43739, "upload_time": "2020-03-06T14:28:02", "upload_time_iso_8601": "2020-03-06T14:28:02.284992Z", "url": "https://files.pythonhosted.org/packages/36/e2/d03c315cda48d9f1cbf3fff62ef24ed8adbc95d7d30c9b3cee59c96faf0f/tf2_gnn-1.2.2.tar.gz", "yanked": false}], "1.2.3": [{"comment_text": "", "digests": {"md5": "0048b2e325afaa4466b133664e7ea979", "sha256": "a565c4cd7b7f63c6155ea63d40c3f6c3b912e8dd020f1478bc457d029d8e8c01"}, "downloads": -1, "filename": "tf2_gnn-1.2.3.tar.gz", "has_sig": false, "md5_digest": "0048b2e325afaa4466b133664e7ea979", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 43879, "upload_time": "2020-03-18T18:45:28", "upload_time_iso_8601": "2020-03-18T18:45:28.545212Z", "url": "https://files.pythonhosted.org/packages/16/8d/6e88bda71113d7f9783ff9e14d8a4cb72424b5443d911908e9569b19b2af/tf2_gnn-1.2.3.tar.gz", "yanked": false}], "1.3.0": [{"comment_text": "", "digests": {"md5": "8bb3b6174db6abb8b9c69bbb575008fb", "sha256": "c15ff608b3e1f9677178132697e69f26d43cd0b9c99f99da0ab73acfbd2093a1"}, "downloads": -1, "filename": "tf2_gnn-1.3.0.tar.gz", "has_sig": false, "md5_digest": "8bb3b6174db6abb8b9c69bbb575008fb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 43897, "upload_time": "2020-03-20T17:51:30", "upload_time_iso_8601": "2020-03-20T17:51:30.713924Z", "url": "https://files.pythonhosted.org/packages/c9/aa/42fb2eb41c8d1f3703c985ab2352b6ec3d89db5099fab1051814276fd8bc/tf2_gnn-1.3.0.tar.gz", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "9240401728c17ef6bee56e4848d7ad59", "sha256": "cb8a141d3d24095992dd171fd6e8297b41851205014581c24c87700e45f480c0"}, "downloads": -1, "filename": "tf2_gnn-2.0.0.tar.gz", "has_sig": false, "md5_digest": "9240401728c17ef6bee56e4848d7ad59", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 44079, "upload_time": "2020-03-26T15:35:33", "upload_time_iso_8601": "2020-03-26T15:35:33.073700Z", "url": "https://files.pythonhosted.org/packages/10/23/6c38d00809a94f40c971b20099d7336a76926e9a820448ce7b04d2362425/tf2_gnn-2.0.0.tar.gz", "yanked": false}], "2.1.0": [{"comment_text": "", "digests": {"md5": "0b7740dc05077800315f2944ae1b3d1b", "sha256": "36d3c739a7f7d97a72038b244a91d71e6bd80a5105f428383cd3693f63b6316d"}, "downloads": -1, "filename": "tf2_gnn-2.1.0.tar.gz", "has_sig": false, "md5_digest": "0b7740dc05077800315f2944ae1b3d1b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 44247, "upload_time": "2020-04-03T16:21:08", "upload_time_iso_8601": "2020-04-03T16:21:08.348548Z", "url": "https://files.pythonhosted.org/packages/cd/db/758cc364a117d04a57ff44f08437980592cab16476ecf30a239786cccf6d/tf2_gnn-2.1.0.tar.gz", "yanked": false}], "2.1.1": [{"comment_text": "", "digests": {"md5": "f0e3aafae94e3fc494b0756f1a8f44e6", "sha256": "889a365ee7030fe4933e7d9312ed6587ef2f8eb2fd95979b520d22fc73968625"}, "downloads": -1, "filename": "tf2_gnn-2.1.1.tar.gz", "has_sig": false, "md5_digest": "f0e3aafae94e3fc494b0756f1a8f44e6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 42622, "upload_time": "2020-04-14T13:56:44", "upload_time_iso_8601": "2020-04-14T13:56:44.366020Z", "url": "https://files.pythonhosted.org/packages/d1/3e/fc0708bfc732f4eb691664a0ae38548f14367069209b43d3bad538e7dc14/tf2_gnn-2.1.1.tar.gz", "yanked": false}], "2.2.0": [{"comment_text": "", "digests": {"md5": "6c4bacd4014802cd02db3b9a520e7c05", "sha256": "e2c0fc03b5f26e51ada491b1c5bd88ddb7d959c9481824ce66961c94439cceed"}, "downloads": -1, "filename": "tf2_gnn-2.2.0.tar.gz", "has_sig": false, "md5_digest": "6c4bacd4014802cd02db3b9a520e7c05", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 42554, "upload_time": "2020-04-16T13:48:38", "upload_time_iso_8601": "2020-04-16T13:48:38.383552Z", "url": "https://files.pythonhosted.org/packages/3f/06/87df08143dca01916d8b9e4a42d428e473c6166704e7f0b634ad5ce1ebeb/tf2_gnn-2.2.0.tar.gz", "yanked": false}], "2.3.1": [{"comment_text": "", "digests": {"md5": "2c25db9895a30449bc9e6c43ab7c963b", "sha256": "3cbdeceeb3d7786b1ca9e277dac091047d335b8c6891116f1c56a4d6ce41b669"}, "downloads": -1, "filename": "tf2_gnn-2.3.1.tar.gz", "has_sig": false, "md5_digest": "2c25db9895a30449bc9e6c43ab7c963b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 42900, "upload_time": "2020-04-22T12:59:43", "upload_time_iso_8601": "2020-04-22T12:59:43.547074Z", "url": "https://files.pythonhosted.org/packages/43/02/be5a8f0b7f2d67cf66562d50399a093eb6418672ad2156c4b81de2640a39/tf2_gnn-2.3.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2c25db9895a30449bc9e6c43ab7c963b", "sha256": "3cbdeceeb3d7786b1ca9e277dac091047d335b8c6891116f1c56a4d6ce41b669"}, "downloads": -1, "filename": "tf2_gnn-2.3.1.tar.gz", "has_sig": false, "md5_digest": "2c25db9895a30449bc9e6c43ab7c963b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 42900, "upload_time": "2020-04-22T12:59:43", "upload_time_iso_8601": "2020-04-22T12:59:43.547074Z", "url": "https://files.pythonhosted.org/packages/43/02/be5a8f0b7f2d67cf66562d50399a093eb6418672ad2156c4b81de2640a39/tf2_gnn-2.3.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:54:54 2020"}