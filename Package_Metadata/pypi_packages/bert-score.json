{"info": {"author": "Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi", "author_email": "tzhang@asapp.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# BERTScore\n[![made-with-python](https://img.shields.io/badge/Made%20with-Python-red.svg)](#python) [![PyPI version bert-score](https://badge.fury.io/py/bert-score.svg)](https://pypi.python.org/pypi/bert-score/) [![Downloads](https://pepy.tech/badge/bert-score)](https://pepy.tech/project/bert-score) [![Downloads](https://pepy.tech/badge/bert-score/month)](https://pepy.tech/project/bert-score/month) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) \n\nAutomatic Evaluation Metric described in the paper [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675) (ICLR 2020).\n#### News:\n- Updated to version 0.3.2\n  - **Bug fixed**: fixing the bug in v0.3.1 when having multiple reference sentences.\n  - Supporting multiple reference sentences with our command line tool.\n- Updated to version 0.3.1\n  - A new `BERTScorer` object that caches the model to avoid re-loading it multiple times. Please see our [jupyter notebook example](./example/Demo.ipynb) for the usage.\n  - Supporting multiple reference sentences for each example. The `score` function now can take a list of lists of strings as the references and return the score between the candidate sentence and its closest reference sentence.\n- Updated to version 0.3.0\n  - Supporting *Baseline Rescaling*: we apply a simple linear transformation to enhance the readability of BERTscore using pre-computed \"baselines\". It has been pointed out (e.g. by #20, #23) that the numercial range of BERTScore is exceedingly small when computed with RoBERTa models. In other words, although BERTScore correctly distinguish examples through ranking, the numerical scores of good and bad examples are very similar. We detail our approach in [a separate post](./journal/rescale_baseline.md).\n- Updated to version 0.2.3\n  - Supporting DistilBERT (Sanh et al.), ALBERT (Lan et al.), and XLM-R (Conneau et al.) models.\n  - Including the version of huggingface's transformers in the hash code for reproducibility\n- BERTScore gets accepted in ICLR 2020. Please come to our poster in Addis Ababa, Ethiopia!\n- Updated to version 0.2.2\n  - **Bug fixed**: when using RoBERTaTokenizer, we now set `add_prefix_space=True` which was the default setting in huggingface's `pytorch_transformers` (when we ran the experiments in the paper) before they migrated it to `transformers`. This breaking change in `transformers` leads to a lower correlation with human evalutation. To reproduce our RoBERTa results in the paper, please use version `0.2.2`.\n  - The best number of layers for DistilRoBERTa is included\n  - Supporting loading a custom model\n- Updated to version 0.2.1\n  - [SciBERT](https://github.com/allenai/scibert) (Beltagy et al.) models are now included. Thanks to AI2 for sharing the models. By default, we use the 9th layer (the same as BERT-base), but this is not tuned. \n- Our [arXiv paper](https://arxiv.org/abs/1904.09675) has been updated to v2 with more experiments and analysis.\n- Updated to version 0.2.0\n  - Supporting BERT, XLM, XLNet, and RoBERTa models using [huggingface's Transformers library](https://github.com/huggingface/transformers)\n  - Automatically picking the best model for a given language\n  - Automatically picking the layer based a model\n  - IDF is *not* set as default as we show in the new version that the improvement brought by importance weighting is not consistent\n\n#### Authors:\n* [Tianyi Zhang](https://scholar.google.com/citations?user=OI0HSa0AAAAJ&hl=en)*\n* [Varsha Kishore](https://scholar.google.com/citations?user=B8UeYcEAAAAJ&authuser=2)*\n* [Felix Wu](https://sites.google.com/view/felixwu/home)*\n* [Kilian Q. Weinberger](http://kilian.cs.cornell.edu/index.html)\n* [Yoav Artzi](https://yoavartzi.com/)\n\n*: Equal Contribution\n\n### Overview\nBERTScore leverages the pre-trained contextual embeddings from BERT and matches\nwords in candidate and reference sentences by cosine similarity.\nIt has been shown to correlate with human judgment on sentence-level and\nsystem-level evaluation.\nMoreover, BERTScore computes precision, recall, and F1 measure, which can be\nuseful for evaluating different language generation tasks.\n\nFor an illustration, BERTScore precision can be computed as\n![](./bert_score.png \"BERTScore\")\n\nIf you find this repo useful, please cite:\n```\n@inproceedings{bert-score,\n  title={BERTScore: Evaluating Text Generation with BERT},\n  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},\n  booktitle={International Conference on Learning Representations},\n  year={2020},\n  url={https://openreview.net/forum?id=SkeHuCVFDr}\n}\n```\n\n### Installation\n* Python version >= 3.6\n* PyTorch version >= 1.0.0\n\nInstall from pypi with pip by \n\n```sh\npip install bert-score\n```\nInstall latest unstable version from the master branch on Github by:\n```\npip install git+https://github.com/Tiiiger/bert_score\n```\n\nInstall it from the source by:\n```sh\ngit clone https://github.com/Tiiiger/bert_score\ncd bert_score\npip install .\n```\nand you may test your installation by:\n```\npython -m unittest discover\n```\n\n### Usage\n\n#### Command Line Interface (CLI)\nWe provide a command line interface (CLI) of BERTScore as well as a python module. \nFor the CLI, you can use it as follows:\n1. To evaluate English text files:\n\nWe provide example inputs under `./example`.\n\n```sh\nbert-score -r example/refs.txt -c example/hyps.txt --lang en\n```\nYou will get the following output at the end:\n\nroberta-large_L17_no-idf_version=0.3.0(hug_trans=2.3.0) P: 0.957378 R: 0.961325 F1: 0.959333\n\nwhere \"roberta-large_L17_no-idf_version=0.3.0(hug_trans=2.3.0)\" is the hash code.\n\nStarting from version 0.3.0, we support rescaling the scores with baseline scores\n\n```sh\nbert-score -r example/refs.txt -c example/hyps.txt --lang en --rescale-with-baseline\n```\nYou will get:\n\nroberta-large_L17_no-idf_version=0.3.0(hug_trans=2.3.0)-rescaled P: 0.747044 R: 0.770484 F1: 0.759045 \n\nThis makes the range of the scores larger and more human-readable. Please see this [post](./journal/rescale_baseline.md) for details.\n\nWhen having multiple reference sentences, please use\n```sh\nbert-score -r example/refs.txt example/refs2.txt -c example/hyps.txt --lang en\n```\nwhere the `-r` argument supports an arbitrary number of reference files. Each reference file should have the same number of lines as your candidate/hypothesis file. The i-th line in each reference file corresponds to the i-th line in the candidate file.\n\n\n2. To evaluate text files in other languages:\n\nWe currently support the 104 languages in multilingual BERT ([full list](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages)).\n\nPlease specify the two-letter abbreviation of the language. For instance, using `--lang zh` for Chinese text. \n\nSee more options by `bert-score -h`.\n\n\n3. To load your own custom model:\nPlease specify the path to the model and the number of layers to use by `--model` and `--num_layers`.\n```sh\nbert-score -r example/refs.txt -c example/hyps.txt --model path_to_my_bert --num_layers 9\n```\n\n\n4. To visualize matching scores:\n```sh\nbert-score-show --lang en -r \"There are two bananas on the table.\" -c \"On the table are two apples.\" -f out.png\n```\nThe figure will be saved to out.png.\n\n#### Python Function\nFor the python module, we provide a [demo](./example/Demo.ipynb). \nPlease refer to [`bert_score/score.py`](./bert_score/score.py) for more details.\n\nRunning BERTScore can be computationally intensive (because it uses BERT :p).\nTherefore, a GPU is usually necessary. If you don't have access to a GPU, you\ncan try our [demo on Google Colab](https://colab.research.google.com/drive/1kpL8Y_AnUUiCxFjhxSrxCsc6-sDMNb_Q)\n\n\n#### Practical Tips\n\n* Report the hash code (e.g., `roberta-large_L17_no-idf_version=0.2.1`) in your paper so that people know what setting you use. This is inspired by [sacreBLEU](https://github.com/mjpost/sacreBLEU).\n* Unlike BERT, RoBERTa uses GPT2-style tokenizer which creates addition \" \" tokens when there are multiple spaces appearing together. It is recommended to remove addition spaces by `sent = re.sub(r' +', ' ', sent)` or `sent = re.sub(r'\\s+', ' ', sent)`.\n* Using inverse document frequency (idf) on the reference\n  sentences to weigh word importance  may correlate better with human judgment.\n  However, when the set of reference sentences become too small, the idf score \n  would become inaccurate/invalid.\n  We now make it optional. To use idf,\n  please set `--idf` when using the CLI tool or\n  `idf=True` when calling `bert_score.score` function.\n* When you are low on GPU memory, consider setting `batch_size` when calling\n  `bert_score.score` function.\n* To use a particular model please set `-m MODEL_TYPE` when using the CLI tool\n  or `model_type=MODEL_TYPE` when calling `bert_score.score` function. \n* We tune layer to use based on WMT16 metric evaluation dataset. You may use a\n  different layer by setting `-l LAYER` or `num_layers=LAYER`\n* __Limitation__: Because BERT, RoBERTa, and XLM with learned positional embeddings are pre-trained on sentences with max length 512, BERTScore is undefined between sentences longer than 510 (512 after adding \\[CLS\\] and \\[SEP\\] tokens). The sentences longer than this will be truncated. Please consider using XLNet which can support much longer inputs.\n\n### Default Behavior\n\n#### Default Model\n| Language  | Model                        |\n|:---------:|:----------------------------:|\n| en        | roberta-large                |\n| en-sci    | scibert-scivocab-uncased     |\n| zh        | bert-base-chinese            |\n| others    | bert-base-multilingual-cased |\n\n#### Default Layers\n| Model                                      | Best Layer | Max Length    |\n|:------------------------------------------:|------------| ------------- |\n| bert-base-uncased                          | 9          | 512           |\n| bert-large-uncased                         | 18         | 512           |\n| bert-base-cased-finetuned-mrpc             | 9          | 512           |\n| bert-base-multilingual-cased               | 9          | 512           |\n| bert-base-chinese                          | 8          | 512           |\n| roberta-base                               | 10         | 512           |\n| roberta-large                              | 17         | 512           |\n| roberta-large-mnli                         | 19         | 512           |\n| roberta-base-openai-detector               | 7          | 512           |\n| roberta-large-openai-detector              | 19         | 512           |\n| xlnet-base-cased                           | 5          | 1000000000000 |\n| xlnet-large-cased                          | 7          | 1000000000000 |\n| xlm-mlm-en-2048                            | 7          | 512           |\n| xlm-mlm-100-1280                           | 11         | 512           |\n| scibert-scivocab-uncased                   | 9*         | 512           |\n| scibert-scivocab-cased                     | 9*         | 512           |\n| scibert-basevocab-uncased                  | 9*         | 512           |\n| scibert-basevocab-cased                    | 9*         | 512           |\n| distilroberta-base                         | 5          | 512           |\n| distilbert-base                            | 5          | 512           |\n| distilbert-base-uncased                    | 5          | 512           |\n| distilbert-base-uncased-distilled-squad    | 4          | 512           |\n| distilbert-base-multilingual-cased         | 5          | 512           |\n| albert-base-v1                             | 10         | 512           |\n| albert-large-v1                            | 17         | 512           |\n| albert-xlarge-v1                           | 16         | 512           |\n| albert-xxlarge-v1                          | 8          | 512           |\n| albert-base-v2                             | 9          | 512           |\n| albert-large-v2                            | 14         | 512           |\n| albert-xlarge-v2                           | 13         | 512           |\n| albert-xxlarge-v2                          | 8          | 512           |\n| xlm-roberta-base                           | 9          | 512           |\n| xlm-roberta-large                          | 17         | 512           |\n\n*: Not tuned\n\n### Acknowledgement\nThis repo wouldn't be possible without the awesome\n[bert](https://github.com/google-research/bert), [fairseq](https://github.com/pytorch/fairseq), and [transformers](https://github.com/huggingface/transformers).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Tiiiger/bert_score", "keywords": "BERT NLP deep learning google metric", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "bert-score", "package_url": "https://pypi.org/project/bert-score/", "platform": "", "project_url": "https://pypi.org/project/bert-score/", "project_urls": {"Homepage": "https://github.com/Tiiiger/bert_score"}, "release_url": "https://pypi.org/project/bert-score/0.3.2/", "requires_dist": ["torch (>=1.0.0)", "numpy", "pandas (>=1.0.1)", "requests", "tqdm (>=4.31.1)", "matplotlib", "transformers (>=2.2.0)"], "requires_python": ">=3.6", "summary": "PyTorch implementation of BERT score", "version": "0.3.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>BERTScore</h1>\n<p><a href=\"#python\" rel=\"nofollow\"><img alt=\"made-with-python\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/923e2f0fb8eabb2031eb4b8bec6c938a01fa28dd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d616465253230776974682d507974686f6e2d7265642e737667\"></a> <a href=\"https://pypi.python.org/pypi/bert-score/\" rel=\"nofollow\"><img alt=\"PyPI version bert-score\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/606957e6cc51e70303c570509b10d337bb633e65/68747470733a2f2f62616467652e667572792e696f2f70792f626572742d73636f72652e737667\"></a> <a href=\"https://pepy.tech/project/bert-score\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b5584fc4c0f8615e3e1f0eba026317d594656deb/68747470733a2f2f706570792e746563682f62616467652f626572742d73636f7265\"></a> <a href=\"https://pepy.tech/project/bert-score/month\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1b761f86b3017b1f13e1e3b8704239ed495de850/68747470733a2f2f706570792e746563682f62616467652f626572742d73636f72652f6d6f6e7468\"></a> <a href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"><img alt=\"License: MIT\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8645b002dd7ec1b54275a80574942e7a318e03c6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667\"></a></p>\n<p>Automatic Evaluation Metric described in the paper <a href=\"https://arxiv.org/abs/1904.09675\" rel=\"nofollow\">BERTScore: Evaluating Text Generation with BERT</a> (ICLR 2020).</p>\n<h4>News:</h4>\n<ul>\n<li>Updated to version 0.3.2\n<ul>\n<li><strong>Bug fixed</strong>: fixing the bug in v0.3.1 when having multiple reference sentences.</li>\n<li>Supporting multiple reference sentences with our command line tool.</li>\n</ul>\n</li>\n<li>Updated to version 0.3.1\n<ul>\n<li>A new <code>BERTScorer</code> object that caches the model to avoid re-loading it multiple times. Please see our <a href=\"./example/Demo.ipynb\" rel=\"nofollow\">jupyter notebook example</a> for the usage.</li>\n<li>Supporting multiple reference sentences for each example. The <code>score</code> function now can take a list of lists of strings as the references and return the score between the candidate sentence and its closest reference sentence.</li>\n</ul>\n</li>\n<li>Updated to version 0.3.0\n<ul>\n<li>Supporting <em>Baseline Rescaling</em>: we apply a simple linear transformation to enhance the readability of BERTscore using pre-computed \"baselines\". It has been pointed out (e.g. by #20, #23) that the numercial range of BERTScore is exceedingly small when computed with RoBERTa models. In other words, although BERTScore correctly distinguish examples through ranking, the numerical scores of good and bad examples are very similar. We detail our approach in <a href=\"./journal/rescale_baseline.md\" rel=\"nofollow\">a separate post</a>.</li>\n</ul>\n</li>\n<li>Updated to version 0.2.3\n<ul>\n<li>Supporting DistilBERT (Sanh et al.), ALBERT (Lan et al.), and XLM-R (Conneau et al.) models.</li>\n<li>Including the version of huggingface's transformers in the hash code for reproducibility</li>\n</ul>\n</li>\n<li>BERTScore gets accepted in ICLR 2020. Please come to our poster in Addis Ababa, Ethiopia!</li>\n<li>Updated to version 0.2.2\n<ul>\n<li><strong>Bug fixed</strong>: when using RoBERTaTokenizer, we now set <code>add_prefix_space=True</code> which was the default setting in huggingface's <code>pytorch_transformers</code> (when we ran the experiments in the paper) before they migrated it to <code>transformers</code>. This breaking change in <code>transformers</code> leads to a lower correlation with human evalutation. To reproduce our RoBERTa results in the paper, please use version <code>0.2.2</code>.</li>\n<li>The best number of layers for DistilRoBERTa is included</li>\n<li>Supporting loading a custom model</li>\n</ul>\n</li>\n<li>Updated to version 0.2.1\n<ul>\n<li><a href=\"https://github.com/allenai/scibert\" rel=\"nofollow\">SciBERT</a> (Beltagy et al.) models are now included. Thanks to AI2 for sharing the models. By default, we use the 9th layer (the same as BERT-base), but this is not tuned.</li>\n</ul>\n</li>\n<li>Our <a href=\"https://arxiv.org/abs/1904.09675\" rel=\"nofollow\">arXiv paper</a> has been updated to v2 with more experiments and analysis.</li>\n<li>Updated to version 0.2.0\n<ul>\n<li>Supporting BERT, XLM, XLNet, and RoBERTa models using <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">huggingface's Transformers library</a></li>\n<li>Automatically picking the best model for a given language</li>\n<li>Automatically picking the layer based a model</li>\n<li>IDF is <em>not</em> set as default as we show in the new version that the improvement brought by importance weighting is not consistent</li>\n</ul>\n</li>\n</ul>\n<h4>Authors:</h4>\n<ul>\n<li><a href=\"https://scholar.google.com/citations?user=OI0HSa0AAAAJ&amp;hl=en\" rel=\"nofollow\">Tianyi Zhang</a>*</li>\n<li><a href=\"https://scholar.google.com/citations?user=B8UeYcEAAAAJ&amp;authuser=2\" rel=\"nofollow\">Varsha Kishore</a>*</li>\n<li><a href=\"https://sites.google.com/view/felixwu/home\" rel=\"nofollow\">Felix Wu</a>*</li>\n<li><a href=\"http://kilian.cs.cornell.edu/index.html\" rel=\"nofollow\">Kilian Q. Weinberger</a></li>\n<li><a href=\"https://yoavartzi.com/\" rel=\"nofollow\">Yoav Artzi</a></li>\n</ul>\n<p>*: Equal Contribution</p>\n<h3>Overview</h3>\n<p>BERTScore leverages the pre-trained contextual embeddings from BERT and matches\nwords in candidate and reference sentences by cosine similarity.\nIt has been shown to correlate with human judgment on sentence-level and\nsystem-level evaluation.\nMoreover, BERTScore computes precision, recall, and F1 measure, which can be\nuseful for evaluating different language generation tasks.</p>\n<p>For an illustration, BERTScore precision can be computed as\n<img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d360bf2b921cc7e08b249133dd73f7cf2a6bfd48/2e2f626572745f73636f72652e706e67\"></p>\n<p>If you find this repo useful, please cite:</p>\n<pre><code>@inproceedings{bert-score,\n  title={BERTScore: Evaluating Text Generation with BERT},\n  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},\n  booktitle={International Conference on Learning Representations},\n  year={2020},\n  url={https://openreview.net/forum?id=SkeHuCVFDr}\n}\n</code></pre>\n<h3>Installation</h3>\n<ul>\n<li>Python version &gt;= 3.6</li>\n<li>PyTorch version &gt;= 1.0.0</li>\n</ul>\n<p>Install from pypi with pip by</p>\n<pre>pip install bert-score\n</pre>\n<p>Install latest unstable version from the master branch on Github by:</p>\n<pre><code>pip install git+https://github.com/Tiiiger/bert_score\n</code></pre>\n<p>Install it from the source by:</p>\n<pre>git clone https://github.com/Tiiiger/bert_score\n<span class=\"nb\">cd</span> bert_score\npip install .\n</pre>\n<p>and you may test your installation by:</p>\n<pre><code>python -m unittest discover\n</code></pre>\n<h3>Usage</h3>\n<h4>Command Line Interface (CLI)</h4>\n<p>We provide a command line interface (CLI) of BERTScore as well as a python module.\nFor the CLI, you can use it as follows:</p>\n<ol>\n<li>To evaluate English text files:</li>\n</ol>\n<p>We provide example inputs under <code>./example</code>.</p>\n<pre>bert-score -r example/refs.txt -c example/hyps.txt --lang en\n</pre>\n<p>You will get the following output at the end:</p>\n<p>roberta-large_L17_no-idf_version=0.3.0(hug_trans=2.3.0) P: 0.957378 R: 0.961325 F1: 0.959333</p>\n<p>where \"roberta-large_L17_no-idf_version=0.3.0(hug_trans=2.3.0)\" is the hash code.</p>\n<p>Starting from version 0.3.0, we support rescaling the scores with baseline scores</p>\n<pre>bert-score -r example/refs.txt -c example/hyps.txt --lang en --rescale-with-baseline\n</pre>\n<p>You will get:</p>\n<p>roberta-large_L17_no-idf_version=0.3.0(hug_trans=2.3.0)-rescaled P: 0.747044 R: 0.770484 F1: 0.759045</p>\n<p>This makes the range of the scores larger and more human-readable. Please see this <a href=\"./journal/rescale_baseline.md\" rel=\"nofollow\">post</a> for details.</p>\n<p>When having multiple reference sentences, please use</p>\n<pre>bert-score -r example/refs.txt example/refs2.txt -c example/hyps.txt --lang en\n</pre>\n<p>where the <code>-r</code> argument supports an arbitrary number of reference files. Each reference file should have the same number of lines as your candidate/hypothesis file. The i-th line in each reference file corresponds to the i-th line in the candidate file.</p>\n<ol>\n<li>To evaluate text files in other languages:</li>\n</ol>\n<p>We currently support the 104 languages in multilingual BERT (<a href=\"https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages\" rel=\"nofollow\">full list</a>).</p>\n<p>Please specify the two-letter abbreviation of the language. For instance, using <code>--lang zh</code> for Chinese text.</p>\n<p>See more options by <code>bert-score -h</code>.</p>\n<ol>\n<li>To load your own custom model:\nPlease specify the path to the model and the number of layers to use by <code>--model</code> and <code>--num_layers</code>.</li>\n</ol>\n<pre>bert-score -r example/refs.txt -c example/hyps.txt --model path_to_my_bert --num_layers <span class=\"m\">9</span>\n</pre>\n<ol>\n<li>To visualize matching scores:</li>\n</ol>\n<pre>bert-score-show --lang en -r <span class=\"s2\">\"There are two bananas on the table.\"</span> -c <span class=\"s2\">\"On the table are two apples.\"</span> -f out.png\n</pre>\n<p>The figure will be saved to out.png.</p>\n<h4>Python Function</h4>\n<p>For the python module, we provide a <a href=\"./example/Demo.ipynb\" rel=\"nofollow\">demo</a>.\nPlease refer to <a href=\"./bert_score/score.py\" rel=\"nofollow\"><code>bert_score/score.py</code></a> for more details.</p>\n<p>Running BERTScore can be computationally intensive (because it uses BERT :p).\nTherefore, a GPU is usually necessary. If you don't have access to a GPU, you\ncan try our <a href=\"https://colab.research.google.com/drive/1kpL8Y_AnUUiCxFjhxSrxCsc6-sDMNb_Q\" rel=\"nofollow\">demo on Google Colab</a></p>\n<h4>Practical Tips</h4>\n<ul>\n<li>Report the hash code (e.g., <code>roberta-large_L17_no-idf_version=0.2.1</code>) in your paper so that people know what setting you use. This is inspired by <a href=\"https://github.com/mjpost/sacreBLEU\" rel=\"nofollow\">sacreBLEU</a>.</li>\n<li>Unlike BERT, RoBERTa uses GPT2-style tokenizer which creates addition \" \" tokens when there are multiple spaces appearing together. It is recommended to remove addition spaces by <code>sent = re.sub(r' +', ' ', sent)</code> or <code>sent = re.sub(r'\\s+', ' ', sent)</code>.</li>\n<li>Using inverse document frequency (idf) on the reference\nsentences to weigh word importance  may correlate better with human judgment.\nHowever, when the set of reference sentences become too small, the idf score\nwould become inaccurate/invalid.\nWe now make it optional. To use idf,\nplease set <code>--idf</code> when using the CLI tool or\n<code>idf=True</code> when calling <code>bert_score.score</code> function.</li>\n<li>When you are low on GPU memory, consider setting <code>batch_size</code> when calling\n<code>bert_score.score</code> function.</li>\n<li>To use a particular model please set <code>-m MODEL_TYPE</code> when using the CLI tool\nor <code>model_type=MODEL_TYPE</code> when calling <code>bert_score.score</code> function.</li>\n<li>We tune layer to use based on WMT16 metric evaluation dataset. You may use a\ndifferent layer by setting <code>-l LAYER</code> or <code>num_layers=LAYER</code></li>\n<li><strong>Limitation</strong>: Because BERT, RoBERTa, and XLM with learned positional embeddings are pre-trained on sentences with max length 512, BERTScore is undefined between sentences longer than 510 (512 after adding [CLS] and [SEP] tokens). The sentences longer than this will be truncated. Please consider using XLNet which can support much longer inputs.</li>\n</ul>\n<h3>Default Behavior</h3>\n<h4>Default Model</h4>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Language</th>\n<th align=\"center\">Model</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">en</td>\n<td align=\"center\">roberta-large</td>\n</tr>\n<tr>\n<td align=\"center\">en-sci</td>\n<td align=\"center\">scibert-scivocab-uncased</td>\n</tr>\n<tr>\n<td align=\"center\">zh</td>\n<td align=\"center\">bert-base-chinese</td>\n</tr>\n<tr>\n<td align=\"center\">others</td>\n<td align=\"center\">bert-base-multilingual-cased</td>\n</tr></tbody></table>\n<h4>Default Layers</h4>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Model</th>\n<th>Best Layer</th>\n<th>Max Length</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">bert-base-uncased</td>\n<td>9</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">bert-large-uncased</td>\n<td>18</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">bert-base-cased-finetuned-mrpc</td>\n<td>9</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">bert-base-multilingual-cased</td>\n<td>9</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">bert-base-chinese</td>\n<td>8</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">roberta-base</td>\n<td>10</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">roberta-large</td>\n<td>17</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">roberta-large-mnli</td>\n<td>19</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">roberta-base-openai-detector</td>\n<td>7</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">roberta-large-openai-detector</td>\n<td>19</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">xlnet-base-cased</td>\n<td>5</td>\n<td>1000000000000</td>\n</tr>\n<tr>\n<td align=\"center\">xlnet-large-cased</td>\n<td>7</td>\n<td>1000000000000</td>\n</tr>\n<tr>\n<td align=\"center\">xlm-mlm-en-2048</td>\n<td>7</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">xlm-mlm-100-1280</td>\n<td>11</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">scibert-scivocab-uncased</td>\n<td>9*</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">scibert-scivocab-cased</td>\n<td>9*</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">scibert-basevocab-uncased</td>\n<td>9*</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">scibert-basevocab-cased</td>\n<td>9*</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">distilroberta-base</td>\n<td>5</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">distilbert-base</td>\n<td>5</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">distilbert-base-uncased</td>\n<td>5</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">distilbert-base-uncased-distilled-squad</td>\n<td>4</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">distilbert-base-multilingual-cased</td>\n<td>5</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">albert-base-v1</td>\n<td>10</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">albert-large-v1</td>\n<td>17</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">albert-xlarge-v1</td>\n<td>16</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">albert-xxlarge-v1</td>\n<td>8</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">albert-base-v2</td>\n<td>9</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">albert-large-v2</td>\n<td>14</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">albert-xlarge-v2</td>\n<td>13</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">albert-xxlarge-v2</td>\n<td>8</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">xlm-roberta-base</td>\n<td>9</td>\n<td>512</td>\n</tr>\n<tr>\n<td align=\"center\">xlm-roberta-large</td>\n<td>17</td>\n<td>512</td>\n</tr></tbody></table>\n<p>*: Not tuned</p>\n<h3>Acknowledgement</h3>\n<p>This repo wouldn't be possible without the awesome\n<a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">bert</a>, <a href=\"https://github.com/pytorch/fairseq\" rel=\"nofollow\">fairseq</a>, and <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">transformers</a>.</p>\n\n          </div>"}, "last_serial": 7049044, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "beb7f00f02ca02098929e18791e58daf", "sha256": "d822176308ccf1352f15c3f2e4888b0b17d6784a6267043c369a13f9287d758e"}, "downloads": -1, "filename": "bert_score-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "beb7f00f02ca02098929e18791e58daf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7263, "upload_time": "2019-04-23T12:51:26", "upload_time_iso_8601": "2019-04-23T12:51:26.541641Z", "url": "https://files.pythonhosted.org/packages/7c/9d/40c76d648834646c0f7ff111dd41f5b16cc3db270610ed6247f4f21f636f/bert_score-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "10d2ae0b24524047773dc562da5ff879", "sha256": "a8794e7a5750c86d25a07ebc5847d15f8e0a0c4352dff580d7179169194ecec9"}, "downloads": -1, "filename": "bert_score-0.1.0.tar.gz", "has_sig": false, "md5_digest": "10d2ae0b24524047773dc562da5ff879", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5759, "upload_time": "2019-04-23T12:51:28", "upload_time_iso_8601": "2019-04-23T12:51:28.449895Z", "url": "https://files.pythonhosted.org/packages/60/76/6c460b8ef36aebf70ea891672610ab19093400c9729e57222aced7e7a715/bert_score-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "e077efb278739754b82a23e92aab20dd", "sha256": "15947a9680d2e3af9e480f7a905d8c0adf2ca0dadef4505fd2b950e7cd9d191e"}, "downloads": -1, "filename": "bert_score-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "e077efb278739754b82a23e92aab20dd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9411, "upload_time": "2019-04-27T02:50:30", "upload_time_iso_8601": "2019-04-27T02:50:30.874780Z", "url": "https://files.pythonhosted.org/packages/3c/a1/65ab39291884caf5466f443add017c072bf081b548dff073520825fbdc43/bert_score-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "521d962fcd2a09a36f18d8cb2d9d42b9", "sha256": "7ff0a70ac3d43aa94887168079d079d02623f8a5bb157efa50ed7a874de4fdfb"}, "downloads": -1, "filename": "bert_score-0.1.1.tar.gz", "has_sig": false, "md5_digest": "521d962fcd2a09a36f18d8cb2d9d42b9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6918, "upload_time": "2019-04-27T02:50:33", "upload_time_iso_8601": "2019-04-27T02:50:33.064695Z", "url": "https://files.pythonhosted.org/packages/99/73/34b87d5ae5eecaab7269d57e96adad01c4ae1a30acc3a6c26c1f89a1563e/bert_score-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "018aa69253f3a79e28d6b28fe1b596b0", "sha256": "0dc5dba7dc34849f91e0a442f8cf0067f891ef46c8c4e8a08c19b7490c177aab"}, "downloads": -1, "filename": "bert_score-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "018aa69253f3a79e28d6b28fe1b596b0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9441, "upload_time": "2019-04-27T19:05:09", "upload_time_iso_8601": "2019-04-27T19:05:09.946782Z", "url": "https://files.pythonhosted.org/packages/19/d8/a11b9f21be4de8423e9fd3448a490e6b3c8de54afe759c772dad0deded28/bert_score-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b548221a52d9f6b45faf77ad482b1d54", "sha256": "db27bb570d4adcf336ee88331139052581e2d80edf482b1bb140091cdd5c7056"}, "downloads": -1, "filename": "bert_score-0.1.2.tar.gz", "has_sig": false, "md5_digest": "b548221a52d9f6b45faf77ad482b1d54", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6921, "upload_time": "2019-04-27T19:05:12", "upload_time_iso_8601": "2019-04-27T19:05:12.254779Z", "url": "https://files.pythonhosted.org/packages/45/ad/aa216c4e58fc4c6d0d2ae07ae0d11881e9181932532ebff4bbec077d9432/bert_score-0.1.2.tar.gz", "yanked": false}], "0.2.0": [], "0.2.1": [], "0.2.2": [{"comment_text": "", "digests": {"md5": "3baa98d2fc66d0f7e98914672aa70819", "sha256": "36a11d3433999cfebc9fadc94aa43acdf3b69ea553a48f91ef692a185faf319a"}, "downloads": -1, "filename": "bert_score-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "3baa98d2fc66d0f7e98914672aa70819", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 14782, "upload_time": "2019-11-30T01:21:04", "upload_time_iso_8601": "2019-11-30T01:21:04.035943Z", "url": "https://files.pythonhosted.org/packages/47/d5/df44286c8bffc1e38730e889688b03a1a3c2575d673cb7028fd9b185f05b/bert_score-0.2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1f5179d41a08c750301c89db2a32d341", "sha256": "6591b8403c25cb3c0fc8e646b10f139990cdd0421da402e62b753d60621d094c"}, "downloads": -1, "filename": "bert_score-0.2.2.tar.gz", "has_sig": false, "md5_digest": "1f5179d41a08c750301c89db2a32d341", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 15713, "upload_time": "2019-11-30T01:21:05", "upload_time_iso_8601": "2019-11-30T01:21:05.640147Z", "url": "https://files.pythonhosted.org/packages/76/f1/424ca46eb606d5ff9b0b490ee5d1481f9ae04aac968aef06c12e2da57a08/bert_score-0.2.2.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "13cca2860cab352178255aa60c676c5c", "sha256": "ebf9d59408a7465e9ef33f14a9ed180b6b373fbb1c8ad10aeda42e053c3ab927"}, "downloads": -1, "filename": "bert_score-0.2.3-py3-none-any.whl", "has_sig": false, "md5_digest": "13cca2860cab352178255aa60c676c5c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 15608, "upload_time": "2019-12-22T19:11:54", "upload_time_iso_8601": "2019-12-22T19:11:54.392400Z", "url": "https://files.pythonhosted.org/packages/51/62/0580ae4df9720e3306127880e4d7703ec80e63c454023ddb242f4491dd14/bert_score-0.2.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0c5eae936793ebb7153f403a96eadb3d", "sha256": "833661288243fce8b638654c94c0d9edea2a8f4fd2cdc605c81efe4a99d81bb9"}, "downloads": -1, "filename": "bert_score-0.2.3.tar.gz", "has_sig": false, "md5_digest": "0c5eae936793ebb7153f403a96eadb3d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17033, "upload_time": "2019-12-22T19:11:55", "upload_time_iso_8601": "2019-12-22T19:11:55.997263Z", "url": "https://files.pythonhosted.org/packages/bf/d3/f7580cfe74c9123fa46f6fa6b279d6ed5729b7c623774793a90504beb216/bert_score-0.2.3.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "9521cf61d2f1c723b878b99616132843", "sha256": "d4d1aedb84b1c86a6e3c2f4757208b9e4f6bd2318c2589e61efadd7fd74c8572"}, "downloads": -1, "filename": "bert_score-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "9521cf61d2f1c723b878b99616132843", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 48045, "upload_time": "2020-01-14T18:32:25", "upload_time_iso_8601": "2020-01-14T18:32:25.354415Z", "url": "https://files.pythonhosted.org/packages/cd/0f/03a2b49125b420c3d6dea36122844f2308c0efd289bb447b046341439458/bert_score-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e35f1fd41afc5fde2e21fc54f9686cf1", "sha256": "c4c2a0afb7601013c5700445882952da619404df723efdf64cd568eb649dfdef"}, "downloads": -1, "filename": "bert_score-0.3.0.tar.gz", "has_sig": false, "md5_digest": "e35f1fd41afc5fde2e21fc54f9686cf1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 37294, "upload_time": "2020-01-14T18:32:27", "upload_time_iso_8601": "2020-01-14T18:32:27.250389Z", "url": "https://files.pythonhosted.org/packages/c4/ba/4031cecbea24ee723cc994ca96e11f3b1b75c406fbc5fd37a48f3d3078b8/bert_score-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "7e1a5e5576bc22e455232e0078333b3b", "sha256": "105bd58789dd58050d9c2a4d635a0a2ae6897e59a3867cc85c442a15f0eeeccb"}, "downloads": -1, "filename": "bert_score-0.3.1-py3-none-any.whl", "has_sig": false, "md5_digest": "7e1a5e5576bc22e455232e0078333b3b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 51990, "upload_time": "2020-03-05T19:08:06", "upload_time_iso_8601": "2020-03-05T19:08:06.250136Z", "url": "https://files.pythonhosted.org/packages/c8/cd/95f08447e6c773fd2580e7734b00dec0e36319b1147a1aa8028ac78d3eb4/bert_score-0.3.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a90323086a7e66b0d83878ad80d61072", "sha256": "0cd951e6d3ff1c1912707216d576d39d63adb7a0e699fdff07e8870d3f63b659"}, "downloads": -1, "filename": "bert_score-0.3.1.tar.gz", "has_sig": false, "md5_digest": "a90323086a7e66b0d83878ad80d61072", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 39286, "upload_time": "2020-03-05T19:08:07", "upload_time_iso_8601": "2020-03-05T19:08:07.686914Z", "url": "https://files.pythonhosted.org/packages/12/cc/6805830534b72d5776befd46a0fc9a5727a5b17fe9855ad5eada05bb459b/bert_score-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "d5b24857273d9c1b6de62d39b7fa8200", "sha256": "07ddc7a7995188201f107300f877474f88208c82b9bb92172288de621868e7fe"}, "downloads": -1, "filename": "bert_score-0.3.2-py3-none-any.whl", "has_sig": false, "md5_digest": "d5b24857273d9c1b6de62d39b7fa8200", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52249, "upload_time": "2020-04-18T16:57:24", "upload_time_iso_8601": "2020-04-18T16:57:24.907083Z", "url": "https://files.pythonhosted.org/packages/5b/db/391c067946b946ab6818d49bd38fe5c53c7d7108bea4f0135bde614f41bb/bert_score-0.3.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4910784becf90d1123903e4fa2493c58", "sha256": "0acfc11d93c3514eca8ad95a034293c2ee852aabd704d0babc97f4e6880a5521"}, "downloads": -1, "filename": "bert_score-0.3.2.tar.gz", "has_sig": false, "md5_digest": "4910784becf90d1123903e4fa2493c58", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 39701, "upload_time": "2020-04-18T16:57:26", "upload_time_iso_8601": "2020-04-18T16:57:26.073998Z", "url": "https://files.pythonhosted.org/packages/8a/f2/fe7f8090da39c908082f08f2f65cb5bf0e9d3144d594efbcabd5dbafe421/bert_score-0.3.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d5b24857273d9c1b6de62d39b7fa8200", "sha256": "07ddc7a7995188201f107300f877474f88208c82b9bb92172288de621868e7fe"}, "downloads": -1, "filename": "bert_score-0.3.2-py3-none-any.whl", "has_sig": false, "md5_digest": "d5b24857273d9c1b6de62d39b7fa8200", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 52249, "upload_time": "2020-04-18T16:57:24", "upload_time_iso_8601": "2020-04-18T16:57:24.907083Z", "url": "https://files.pythonhosted.org/packages/5b/db/391c067946b946ab6818d49bd38fe5c53c7d7108bea4f0135bde614f41bb/bert_score-0.3.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4910784becf90d1123903e4fa2493c58", "sha256": "0acfc11d93c3514eca8ad95a034293c2ee852aabd704d0babc97f4e6880a5521"}, "downloads": -1, "filename": "bert_score-0.3.2.tar.gz", "has_sig": false, "md5_digest": "4910784becf90d1123903e4fa2493c58", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 39701, "upload_time": "2020-04-18T16:57:26", "upload_time_iso_8601": "2020-04-18T16:57:26.073998Z", "url": "https://files.pythonhosted.org/packages/8a/f2/fe7f8090da39c908082f08f2f65cb5bf0e9d3144d594efbcabd5dbafe421/bert_score-0.3.2.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:45 2020"}