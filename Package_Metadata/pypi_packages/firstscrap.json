{"info": {"author": "Teddy Coder", "author_email": "fedor_coder@mail.ru", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.7"], "description": "# First_scrap\n\nhttps://theodor85.github.io/first_scrap/\n\n- - -\n[English](README.md), [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](README-ru.md)\n- - -\n\nFirst_scrap is a library for multithread scraping sites with random proxies and user-agents.\n\n## Installation\n\nTo get started with the first_scrap library, activate (or create if necessary) your virtual environment. For example, as follows:\n\n    python3 -m venv env\n    source ./env/bin/activate\n\nTo install First_scrap use pip package manager:\n\n    pip install firstscrap\n\nAnother installing approach is getting source code from GitHub. For this execute the commands in your console:\n\n    git clone http://github.com/theodor85/first_scrap\n    cd first_scrap\n    python setup.py develop\n\n## How to use\n\nUsing example for exctracting data from one web page:\n\n\n```python\nfrom firstscrap import pagehandler\n\n@pagehandler(parser=\"BeautifulSoup\")\ndef get_data(url, soup=None):\n    # your only beatifulsoup code, without any requests, proxies, etc\n    span = soup.find( name=\"span\", attrs={\"class\": \"p-nickname vcard-username d-block\"} )\n    text = span.get_text().strip()\n    return text\n\nif __name__ == '__main__' :\n    print( get_data('https://github.com/theodor85') )\n\n    # output:\n    # theodor85\n```\n\n## What's under hood\n\nWhen extracting data from a single page:\n\n1. Random proxy server and user-agent are selected from the lists stored in the file.\n2. These proxies and user-agents are used to access the page we need.\n3. With BeautifulSoup the data is retrieved from the page.\n\n## The most interesting thing is plenty identical pages processing\n\nHere is the example:\n\n```python\nfrom firstscrap import listhandler\n\nTEST_URLLIST_OLX = [\n    'https://www.olx.ua/obyavlenie/spetsialist-po-podklyucheniyu-interneta-IDGnCkB.html',\n    'https://www.olx.ua/obyavlenie/menedzher-po-robot-s-klentami-IDGkGK6.html',\n]\n\n@listhandler(threads_limit=5, parser='BeautifulSoup')\ndef get_date_time_from_olx(urllist, soup=None):\n    ''' Beautifulsoup code for one page '''\n    em = soup.find('em')\n    row_text = em.get_text().strip()\n    return row_text\n\nif __name__ == '__main__' :\n    data = get_date_time_from_olx(TEST_URLLIST_OLX)\n    for item in data:\n        print(item)\n# output:\n# \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043e: \u0432 16:49, 26 \u0434\u0435\u043a\u0430\u0431\u0440\u044f 2019, \u041d\u043e\u043c\u0435\u0440 \u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u0438\u044f: 626235005\n# \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043e: \u0432 16:18, 29 \u0434\u0435\u043a\u0430\u0431\u0440\u044f 2019, \u041d\u043e\u043c\u0435\u0440 \u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u0438\u044f: 625536978\n\n```\n\n## What's under hood\n\nThe program processes each page in a separate thread, and the number of threads running at the same time does not exceed `threads_limit`.\n\nEvery thread makes request using random proxy and user-agent.\n\n## Running the tests\n\nTo run the tests type in your console:\n\n    python -m unittest -v tests/tests.py\n\nBefore running the tests enjure that your internet connection is active.\n\n## Contributing\n\nMerge you code to the 'develop' branch for contributing please.\n\nForks and pull requests are welcome! If you like first_scrap, do not forget to put a star!\n\n## Bug reports\n\nTo bug report please mail to fedor_coder@mail.ru with tag \"first_scrap bug reporting\".\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.txt](LICENSE.txt) file for details.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/theodor85/first_scrap", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "firstscrap", "package_url": "https://pypi.org/project/firstscrap/", "platform": "", "project_url": "https://pypi.org/project/firstscrap/", "project_urls": {"Homepage": "https://github.com/theodor85/first_scrap"}, "release_url": "https://pypi.org/project/firstscrap/0.2.0/", "requires_dist": ["beautifulsoup4 (==4.9.0)", "requests (==2.23.0)", "selenium (==3.141.0)"], "requires_python": "", "summary": "Scraping sites with multithreading, random proxies and user-agents", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>First_scrap</h1>\n<p><a href=\"https://theodor85.github.io/first_scrap/\" rel=\"nofollow\">https://theodor85.github.io/first_scrap/</a></p>\n<hr>\n<p><a href=\"README.md\" rel=\"nofollow\">English</a>, <a href=\"README-ru.md\" rel=\"nofollow\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a></p>\n<hr>\n<p>First_scrap is a library for multithread scraping sites with random proxies and user-agents.</p>\n<h2>Installation</h2>\n<p>To get started with the first_scrap library, activate (or create if necessary) your virtual environment. For example, as follows:</p>\n<pre><code>python3 -m venv env\nsource ./env/bin/activate\n</code></pre>\n<p>To install First_scrap use pip package manager:</p>\n<pre><code>pip install firstscrap\n</code></pre>\n<p>Another installing approach is getting source code from GitHub. For this execute the commands in your console:</p>\n<pre><code>git clone http://github.com/theodor85/first_scrap\ncd first_scrap\npython setup.py develop\n</code></pre>\n<h2>How to use</h2>\n<p>Using example for exctracting data from one web page:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">firstscrap</span> <span class=\"kn\">import</span> <span class=\"n\">pagehandler</span>\n\n<span class=\"nd\">@pagehandler</span><span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"o\">=</span><span class=\"s2\">\"BeautifulSoup\"</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_data</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">soup</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n    <span class=\"c1\"># your only beatifulsoup code, without any requests, proxies, etc</span>\n    <span class=\"n\">span</span> <span class=\"o\">=</span> <span class=\"n\">soup</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"span\"</span><span class=\"p\">,</span> <span class=\"n\">attrs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"class\"</span><span class=\"p\">:</span> <span class=\"s2\">\"p-nickname vcard-username d-block\"</span><span class=\"p\">}</span> <span class=\"p\">)</span>\n    <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">span</span><span class=\"o\">.</span><span class=\"n\">get_text</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n    <span class=\"k\">return</span> <span class=\"n\">text</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span> <span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span> <span class=\"n\">get_data</span><span class=\"p\">(</span><span class=\"s1\">'https://github.com/theodor85'</span><span class=\"p\">)</span> <span class=\"p\">)</span>\n\n    <span class=\"c1\"># output:</span>\n    <span class=\"c1\"># theodor85</span>\n</pre>\n<h2>What's under hood</h2>\n<p>When extracting data from a single page:</p>\n<ol>\n<li>Random proxy server and user-agent are selected from the lists stored in the file.</li>\n<li>These proxies and user-agents are used to access the page we need.</li>\n<li>With BeautifulSoup the data is retrieved from the page.</li>\n</ol>\n<h2>The most interesting thing is plenty identical pages processing</h2>\n<p>Here is the example:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">firstscrap</span> <span class=\"kn\">import</span> <span class=\"n\">listhandler</span>\n\n<span class=\"n\">TEST_URLLIST_OLX</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s1\">'https://www.olx.ua/obyavlenie/spetsialist-po-podklyucheniyu-interneta-IDGnCkB.html'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'https://www.olx.ua/obyavlenie/menedzher-po-robot-s-klentami-IDGkGK6.html'</span><span class=\"p\">,</span>\n<span class=\"p\">]</span>\n\n<span class=\"nd\">@listhandler</span><span class=\"p\">(</span><span class=\"n\">threads_limit</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">parser</span><span class=\"o\">=</span><span class=\"s1\">'BeautifulSoup'</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_date_time_from_olx</span><span class=\"p\">(</span><span class=\"n\">urllist</span><span class=\"p\">,</span> <span class=\"n\">soup</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n    <span class=\"sd\">''' Beautifulsoup code for one page '''</span>\n    <span class=\"n\">em</span> <span class=\"o\">=</span> <span class=\"n\">soup</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">'em'</span><span class=\"p\">)</span>\n    <span class=\"n\">row_text</span> <span class=\"o\">=</span> <span class=\"n\">em</span><span class=\"o\">.</span><span class=\"n\">get_text</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n    <span class=\"k\">return</span> <span class=\"n\">row_text</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span> <span class=\"p\">:</span>\n    <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">get_date_time_from_olx</span><span class=\"p\">(</span><span class=\"n\">TEST_URLLIST_OLX</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">item</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">item</span><span class=\"p\">)</span>\n<span class=\"c1\"># output:</span>\n<span class=\"c1\"># \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043e: \u0432 16:49, 26 \u0434\u0435\u043a\u0430\u0431\u0440\u044f 2019, \u041d\u043e\u043c\u0435\u0440 \u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u0438\u044f: 626235005</span>\n<span class=\"c1\"># \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043e: \u0432 16:18, 29 \u0434\u0435\u043a\u0430\u0431\u0440\u044f 2019, \u041d\u043e\u043c\u0435\u0440 \u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u0438\u044f: 625536978</span>\n</pre>\n<h2>What's under hood</h2>\n<p>The program processes each page in a separate thread, and the number of threads running at the same time does not exceed <code>threads_limit</code>.</p>\n<p>Every thread makes request using random proxy and user-agent.</p>\n<h2>Running the tests</h2>\n<p>To run the tests type in your console:</p>\n<pre><code>python -m unittest -v tests/tests.py\n</code></pre>\n<p>Before running the tests enjure that your internet connection is active.</p>\n<h2>Contributing</h2>\n<p>Merge you code to the 'develop' branch for contributing please.</p>\n<p>Forks and pull requests are welcome! If you like first_scrap, do not forget to put a star!</p>\n<h2>Bug reports</h2>\n<p>To bug report please mail to <a href=\"mailto:fedor_coder@mail.ru\">fedor_coder@mail.ru</a> with tag \"first_scrap bug reporting\".</p>\n<h2>License</h2>\n<p>This project is licensed under the MIT License - see the <a href=\"LICENSE.txt\" rel=\"nofollow\">LICENSE.txt</a> file for details.</p>\n\n          </div>"}, "last_serial": 6998217, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "4e64cc171ef00ddb50444698a679acfc", "sha256": "ddb4f8d4895f8bb4f407d6ad6f4d1475af441c986f29985f56d7cee8c0096dbc"}, "downloads": -1, "filename": "firstscrap-0.1.0.tar.gz", "has_sig": false, "md5_digest": "4e64cc171ef00ddb50444698a679acfc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14176, "upload_time": "2019-05-13T17:33:28", "upload_time_iso_8601": "2019-05-13T17:33:28.217941Z", "url": "https://files.pythonhosted.org/packages/b8/d1/c2e8dcb60847608fdb015ea4b03cd738ef3b892e06ada84994132d5e2383/firstscrap-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "d909ba4d00c7913285cb21fc9d248c97", "sha256": "14f7306fa249fb385c82e63e53dd729b0bba242372a763a93aa338b24c2ed321"}, "downloads": -1, "filename": "firstscrap-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d909ba4d00c7913285cb21fc9d248c97", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 14144, "upload_time": "2020-04-11T06:06:47", "upload_time_iso_8601": "2020-04-11T06:06:47.275812Z", "url": "https://files.pythonhosted.org/packages/a6/60/05ca2400fa44123bb01aeff7ebeef7af1a9fa06309ecc8a1cb59499630b1/firstscrap-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c12b48eb9c17d44e23af67df870fdcfa", "sha256": "bab0ef34f1564737ee0993318a740997534d35ccd0cf063e7d56f90140ef3a25"}, "downloads": -1, "filename": "firstscrap-0.2.0.tar.gz", "has_sig": false, "md5_digest": "c12b48eb9c17d44e23af67df870fdcfa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10140, "upload_time": "2020-04-11T06:06:48", "upload_time_iso_8601": "2020-04-11T06:06:48.940730Z", "url": "https://files.pythonhosted.org/packages/74/68/5b6498936281bf92089d827f8356ccc116afb7c6a90990caba8bb13a06a7/firstscrap-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d909ba4d00c7913285cb21fc9d248c97", "sha256": "14f7306fa249fb385c82e63e53dd729b0bba242372a763a93aa338b24c2ed321"}, "downloads": -1, "filename": "firstscrap-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d909ba4d00c7913285cb21fc9d248c97", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 14144, "upload_time": "2020-04-11T06:06:47", "upload_time_iso_8601": "2020-04-11T06:06:47.275812Z", "url": "https://files.pythonhosted.org/packages/a6/60/05ca2400fa44123bb01aeff7ebeef7af1a9fa06309ecc8a1cb59499630b1/firstscrap-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c12b48eb9c17d44e23af67df870fdcfa", "sha256": "bab0ef34f1564737ee0993318a740997534d35ccd0cf063e7d56f90140ef3a25"}, "downloads": -1, "filename": "firstscrap-0.2.0.tar.gz", "has_sig": false, "md5_digest": "c12b48eb9c17d44e23af67df870fdcfa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10140, "upload_time": "2020-04-11T06:06:48", "upload_time_iso_8601": "2020-04-11T06:06:48.940730Z", "url": "https://files.pythonhosted.org/packages/74/68/5b6498936281bf92089d827f8356ccc116afb7c6a90990caba8bb13a06a7/firstscrap-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:42:09 2020"}