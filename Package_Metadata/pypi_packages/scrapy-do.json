{"info": {"author": "Lukasz Janyst", "author_email": "xyz@jany.st", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Environment :: No Input/Output (Daemon)", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.7", "Topic :: Internet :: WWW/HTTP"], "description": "\n=========\nScrapy Do\n=========\n\n.. image:: https://api.travis-ci.org/ljanyst/scrapy-do.svg?branch=master\n   :target: https://travis-ci.org/ljanyst/scrapy-do\n\n.. image:: https://coveralls.io/repos/github/ljanyst/scrapy-do/badge.svg?branch=master\n   :target: https://coveralls.io/github/ljanyst/scrapy-do?branch=master\n\n.. image:: https://img.shields.io/pypi/v/scrapy-do.svg\n   :target: https://pypi.python.org/pypi/scrapy-do\n   :alt: PyPI Version\n\n\nScrapy Do is a daemon that provides a convenient way to run `Scrapy\n<https://scrapy.org/>`_ spiders. It can either do it once - immediately; or it\ncan run them periodically, at specified time intervals. It's been inspired by\n`scrapyd <https://github.com/scrapy/scrapyd>`_ but written from scratch. It\ncomes with a REST API, a command line client, and an interactive web interface.\n\n * Homepage: `https://jany.st/scrapy-do.html <https://jany.st/scrapy-do.html>`_\n * Documentation: `https://scrapy-do.readthedocs.io/en/latest/ <https://scrapy-do.readthedocs.io/en/latest/>`_\n\n-----------\nQuick Start\n-----------\n\n* Install ``scrapy-do`` using ``pip``:\n\n  .. code-block:: console\n\n       $ pip install scrapy-do\n\n* Start the daemon in the foreground:\n\n  .. code-block:: console\n\n       $ scrapy-do -n scrapy-do\n\n* Open another terminal window, download the Scrapy's Quotesbot example, and\n  push the code to the server:\n\n  .. code-block:: console\n\n       $ git clone https://github.com/scrapy/quotesbot.git\n       $ cd quotesbot\n       $ scrapy-do-cl push-project\n       +----------------+\n       | quotesbot      |\n       |----------------|\n       | toscrape-css   |\n       | toscrape-xpath |\n       +----------------+\n\n* Schedule some jobs:\n\n  .. code-block:: console\n\n       $ scrapy-do-cl schedule-job --project quotesbot \\\n           --spider toscrape-css --when 'every 5 to 15 minutes'\n       +--------------------------------------+\n       | identifier                           |\n       |--------------------------------------|\n       | 0a3db618-d8e1-48dc-a557-4e8d705d599c |\n       +--------------------------------------+\n\n       $ scrapy-do-cl schedule-job --project quotesbot --spider toscrape-css\n       +--------------------------------------+\n       | identifier                           |\n       |--------------------------------------|\n       | b3a61347-92ef-4095-bb68-0702270a52b8 |\n       +--------------------------------------+\n\n* See what's going on:\n\n  .. figure:: https://github.com/ljanyst/scrapy-do/raw/master/docs/_static/jobs-active.png\n     :scale: 50 %\n     :alt: Active Jobs\n\n     The web interface is available at http://localhost:7654 by default.\n\n--------------------\nBuilding from source\n--------------------\n\nBoth of the steps below require `nodejs` to be installed.\n\n* Check if things work fine:\n\n  .. code-block:: console\n\n       $ pip install -rrequirements-dev.txt\n       $ tox\n\n* Build the wheel:\n\n  .. code-block:: console\n\n       $ python setup.py bdist_wheel\n\n---------\nChangeLog\n---------\n\nVersion 0.4.0\n-------------\n\n* Migration to the Bootstrap 4 UI\n* Make it possible to add a short description to jobs\n* Make it possible to specify user-defined payload in each job that is passed\n  on as a parameter to the python crawler\n* UI updates to support the above\n* New log viewers in the web UI\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://jany.st/scrapy-do.html", "keywords": "", "license": "BSD License", "maintainer": "", "maintainer_email": "", "name": "scrapy-do", "package_url": "https://pypi.org/project/scrapy-do/", "platform": "", "project_url": "https://pypi.org/project/scrapy-do/", "project_urls": {"Homepage": "https://jany.st/scrapy-do.html"}, "release_url": "https://pypi.org/project/scrapy-do/0.4.0/", "requires_dist": ["scrapy", "twisted", "pyOpenSSL", "psutil", "python-dateutil", "schedule", "pem", "tabulate", "requests", "autobahn", "tzlocal"], "requires_python": "", "summary": "A daemon for scheduling Scrapy spiders", "version": "0.4.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://travis-ci.org/ljanyst/scrapy-do\" rel=\"nofollow\"><img alt=\"https://api.travis-ci.org/ljanyst/scrapy-do.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8b91370cfc03b1882f24f26a6b102b045ff385f2/68747470733a2f2f6170692e7472617669732d63692e6f72672f6c6a616e7973742f7363726170792d646f2e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/ljanyst/scrapy-do?branch=master\" rel=\"nofollow\"><img alt=\"https://coveralls.io/repos/github/ljanyst/scrapy-do/badge.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/17aa45bc6f7c8d74a06e6ed61dbc3594a6fa5c50/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6c6a616e7973742f7363726170792d646f2f62616467652e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://pypi.python.org/pypi/scrapy-do\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e9e6298f74d988953a958f14d9d37174fc4113fb/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363726170792d646f2e737667\"></a>\n<p>Scrapy Do is a daemon that provides a convenient way to run <a href=\"https://scrapy.org/\" rel=\"nofollow\">Scrapy</a> spiders. It can either do it once - immediately; or it\ncan run them periodically, at specified time intervals. It\u2019s been inspired by\n<a href=\"https://github.com/scrapy/scrapyd\" rel=\"nofollow\">scrapyd</a> but written from scratch. It\ncomes with a REST API, a command line client, and an interactive web interface.</p>\n<blockquote>\n<ul>\n<li>Homepage: <a href=\"https://jany.st/scrapy-do.html\" rel=\"nofollow\">https://jany.st/scrapy-do.html</a></li>\n<li>Documentation: <a href=\"https://scrapy-do.readthedocs.io/en/latest/\" rel=\"nofollow\">https://scrapy-do.readthedocs.io/en/latest/</a></li>\n</ul>\n</blockquote>\n<div id=\"quick-start\">\n<h2>Quick Start</h2>\n<ul>\n<li><p>Install <tt><span class=\"pre\">scrapy-do</span></tt> using <tt>pip</tt>:</p>\n<pre><span class=\"gp\">$</span> pip install scrapy-do\n</pre>\n</li>\n<li><p>Start the daemon in the foreground:</p>\n<pre><span class=\"gp\">$</span> scrapy-do -n scrapy-do\n</pre>\n</li>\n<li><p>Open another terminal window, download the Scrapy\u2019s Quotesbot example, and\npush the code to the server:</p>\n<pre><span class=\"gp\">$</span> git clone https://github.com/scrapy/quotesbot.git\n<span class=\"gp\">$</span> <span class=\"nb\">cd</span> quotesbot\n<span class=\"gp\">$</span> scrapy-do-cl push-project\n<span class=\"go\">+----------------+\n| quotesbot      |\n|----------------|\n| toscrape-css   |\n| toscrape-xpath |\n+----------------+</span>\n</pre>\n</li>\n<li><p>Schedule some jobs:</p>\n<pre><span class=\"gp\">$</span> scrapy-do-cl schedule-job --project quotesbot <span class=\"se\">\\\n</span>    --spider toscrape-css --when <span class=\"s1\">'every 5 to 15 minutes'</span>\n<span class=\"go\">+--------------------------------------+\n| identifier                           |\n|--------------------------------------|\n| 0a3db618-d8e1-48dc-a557-4e8d705d599c |\n+--------------------------------------+\n\n</span><span class=\"gp\">$</span> scrapy-do-cl schedule-job --project quotesbot --spider toscrape-css\n<span class=\"go\">+--------------------------------------+\n| identifier                           |\n|--------------------------------------|\n| b3a61347-92ef-4095-bb68-0702270a52b8 |\n+--------------------------------------+</span>\n</pre>\n</li>\n<li><p>See what\u2019s going on:</p>\n<div>\n<img alt=\"Active Jobs\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d377d13b37d2981e6dad0bbf10b75a1627ce3cd4/68747470733a2f2f6769746875622e636f6d2f6c6a616e7973742f7363726170792d646f2f7261772f6d61737465722f646f63732f5f7374617469632f6a6f62732d6163746976652e706e67\">\n<p>The web interface is available at <a href=\"http://localhost:7654\" rel=\"nofollow\">http://localhost:7654</a> by default.</p>\n</div>\n</li>\n</ul>\n</div>\n<div id=\"building-from-source\">\n<h2>Building from source</h2>\n<p>Both of the steps below require <cite>nodejs</cite> to be installed.</p>\n<ul>\n<li><p>Check if things work fine:</p>\n<pre><span class=\"gp\">$</span> pip install -rrequirements-dev.txt\n<span class=\"gp\">$</span> tox\n</pre>\n</li>\n<li><p>Build the wheel:</p>\n<pre><span class=\"gp\">$</span> python setup.py bdist_wheel\n</pre>\n</li>\n</ul>\n</div>\n<div id=\"changelog\">\n<h2>ChangeLog</h2>\n<h2 id=\"version-0-4-0\"><span class=\"section-subtitle\">Version 0.4.0</span></h2>\n<ul>\n<li>Migration to the Bootstrap 4 UI</li>\n<li>Make it possible to add a short description to jobs</li>\n<li>Make it possible to specify user-defined payload in each job that is passed\non as a parameter to the python crawler</li>\n<li>UI updates to support the above</li>\n<li>New log viewers in the web UI</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 6390044, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "b34748d95f5503b9a1f731e8c3b2295b", "sha256": "d90388c240648c391b1dc4d911d158d735a487f7c3c2c82d6b3d08db353afecd"}, "downloads": -1, "filename": "scrapy_do-0.1.0-py3-none-any.whl", "has_sig": true, "md5_digest": "b34748d95f5503b9a1f731e8c3b2295b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 20685, "upload_time": "2017-12-11T17:40:11", "upload_time_iso_8601": "2017-12-11T17:40:11.707866Z", "url": "https://files.pythonhosted.org/packages/f3/3f/0a9fdcbbde07dacaccaa13f23c311e1d58808b2637b9b89d870093f79681/scrapy_do-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0e6b6e12f968af9161b414c5dd710649", "sha256": "b63188f4450a304f9e78fd1899ce2053f0ca9bb631226c42f18105c27039f791"}, "downloads": -1, "filename": "scrapy-do-0.1.0.tar.gz", "has_sig": true, "md5_digest": "0e6b6e12f968af9161b414c5dd710649", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15531, "upload_time": "2017-12-11T17:40:14", "upload_time_iso_8601": "2017-12-11T17:40:14.847387Z", "url": "https://files.pythonhosted.org/packages/4c/40/3337dc90bc3a35fdc54c4d590375f7589dc7b88e2c495615bc7b0f342bb3/scrapy-do-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "5e180cd2a352befea28275d61d3a84ef", "sha256": "8580bb7421a777ee3b6103e77aa808ada9c67c222b9f9e1d7bc6b0e307ac5fb8"}, "downloads": -1, "filename": "scrapy_do-0.2.0-py3-none-any.whl", "has_sig": true, "md5_digest": "5e180cd2a352befea28275d61d3a84ef", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 29200, "upload_time": "2018-01-27T10:14:23", "upload_time_iso_8601": "2018-01-27T10:14:23.739060Z", "url": "https://files.pythonhosted.org/packages/5b/78/bd7f2adcdabddc939218d1cc8fa54b081697f3826048743b698fb2481496/scrapy_do-0.2.0-py3-none-any.whl", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "a1e65f5486dedbdcfdba62ce4319f2ae", "sha256": "9d1f97626df8a6dbef7d367c368091292e04af0cbf732dfa175b0798bdf0c5ff"}, "downloads": -1, "filename": "scrapy_do-0.3.0-py3-none-any.whl", "has_sig": true, "md5_digest": "a1e65f5486dedbdcfdba62ce4319f2ae", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1138614, "upload_time": "2018-02-27T14:03:27", "upload_time_iso_8601": "2018-02-27T14:03:27.827481Z", "url": "https://files.pythonhosted.org/packages/c5/01/ec88115bf83ba73300ea881d8b5c73198fd5b285d12a021309e11d730e17/scrapy_do-0.3.0-py3-none-any.whl", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "84e0963186065fae859b2468b50eb3d9", "sha256": "7559f913422677234ad3e1b8a3ed92cd9903d6caec841b48c7e37842e4781b46"}, "downloads": -1, "filename": "scrapy_do-0.3.1-py3-none-any.whl", "has_sig": true, "md5_digest": "84e0963186065fae859b2468b50eb3d9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 796940, "upload_time": "2019-04-13T18:02:04", "upload_time_iso_8601": "2019-04-13T18:02:04.416384Z", "url": "https://files.pythonhosted.org/packages/aa/d6/7e599857bb5881837521abb1fce6c002e0901d2b29648d4338ab723cacd7/scrapy_do-0.3.1-py3-none-any.whl", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "4f030e92fd36ae1af00ff800a78aa2a1", "sha256": "04c8ca4ac75443ee26574e8f9e716ca613477a7cc9c5c375c1f9ed790ed5a79c"}, "downloads": -1, "filename": "scrapy_do-0.3.2-py3-none-any.whl", "has_sig": true, "md5_digest": "4f030e92fd36ae1af00ff800a78aa2a1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 797034, "upload_time": "2019-04-13T18:26:28", "upload_time_iso_8601": "2019-04-13T18:26:28.542219Z", "url": "https://files.pythonhosted.org/packages/63/1c/d629fea4aac794d8695d31b9b9846e63b565037501618a075cb0a0a1436e/scrapy_do-0.3.2-py3-none-any.whl", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "59280c978ffa27ddec955bc56c1416e3", "sha256": "0e91d58db821cebbdddc4157cf04ac53694087b89ad3a9f8737d655d30a356dc"}, "downloads": -1, "filename": "scrapy_do-0.4.0-py3-none-any.whl", "has_sig": true, "md5_digest": "59280c978ffa27ddec955bc56c1416e3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2051976, "upload_time": "2020-01-03T08:52:13", "upload_time_iso_8601": "2020-01-03T08:52:13.824148Z", "url": "https://files.pythonhosted.org/packages/ad/0c/bd05aa858dc78f219196d52495a93f020115913c95edaca100ea5ba2eeb7/scrapy_do-0.4.0-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "59280c978ffa27ddec955bc56c1416e3", "sha256": "0e91d58db821cebbdddc4157cf04ac53694087b89ad3a9f8737d655d30a356dc"}, "downloads": -1, "filename": "scrapy_do-0.4.0-py3-none-any.whl", "has_sig": true, "md5_digest": "59280c978ffa27ddec955bc56c1416e3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2051976, "upload_time": "2020-01-03T08:52:13", "upload_time_iso_8601": "2020-01-03T08:52:13.824148Z", "url": "https://files.pythonhosted.org/packages/ad/0c/bd05aa858dc78f219196d52495a93f020115913c95edaca100ea5ba2eeb7/scrapy_do-0.4.0-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 02:56:47 2020"}