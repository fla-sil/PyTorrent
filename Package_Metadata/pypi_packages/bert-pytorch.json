{"info": {"author": "Junseong Kim", "author_email": "codertimo@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# BERT-pytorch\n\n[![LICENSE](https://img.shields.io/github/license/codertimo/BERT-pytorch.svg)](https://github.com/codertimo/BERT-pytorch/blob/master/LICENSE)\n![GitHub issues](https://img.shields.io/github/issues/codertimo/BERT-pytorch.svg)\n[![GitHub stars](https://img.shields.io/github/stars/codertimo/BERT-pytorch.svg)](https://github.com/codertimo/BERT-pytorch/stargazers)\n[![CircleCI](https://circleci.com/gh/codertimo/BERT-pytorch.svg?style=shield)](https://circleci.com/gh/codertimo/BERT-pytorch)\n[![PyPI](https://img.shields.io/pypi/v/bert-pytorch.svg)](https://pypi.org/project/bert_pytorch/)\n[![PyPI - Status](https://img.shields.io/pypi/status/bert-pytorch.svg)](https://pypi.org/project/bert_pytorch/)\n[![Documentation Status](https://readthedocs.org/projects/bert-pytorch/badge/?version=latest)](https://bert-pytorch.readthedocs.io/en/latest/?badge=latest)\n\nPytorch implementation of Google AI's 2018 BERT, with simple annotation\n\n> BERT 2018 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n> Paper URL : https://arxiv.org/abs/1810.04805\n\n\n## Introduction\n\nGoogle AI's BERT paper shows the amazing result on various NLP task (new 17 NLP tasks SOTA), \nincluding outperform the human F1 score on SQuAD v1.1 QA task. \nThis paper proved that Transformer(self-attention) based encoder can be powerfully used as \nalternative of previous language model with proper language model training method. \nAnd more importantly, they showed us that this pre-trained language model can be transfer \ninto any NLP task without making task specific model architecture.\n\nThis amazing result would be record in NLP history, \nand I expect many further papers about BERT will be published very soon.\n\nThis repo is implementation of BERT. Code is very simple and easy to understand fastly.\nSome of these codes are based on [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n\nCurrently this project is working on progress. And the code is not verified yet.\n\n## Installation\n```\npip install bert-pytorch\n```\n\n## Quickstart\n\n**NOTICE : Your corpus should be prepared with two sentences in one line with tab(\\t) separator**\n\n### 0. Prepare your corpus\n```\nWelcome to the \\t the jungle\\n\nI can stay \\t here all night\\n\n```\n\nor tokenized corpus (tokenization is not in package)\n```\nWel_ _come _to _the \\t _the _jungle\\n\n_I _can _stay \\t _here _all _night\\n\n```\n\n\n### 1. Building vocab based on your corpus\n```shell\nbert-vocab -c data/corpus.small -o data/vocab.small\n```\n\n### 2. Train your own BERT model\n```shell\nbert -c data/corpus.small -v data/vocab.small -o output/bert.model\n```\n\n## Language Model Pre-training\n\nIn the paper, authors shows the new language model training methods, \nwhich are \"masked language model\" and \"predict next sentence\".\n\n\n### Masked Language Model \n\n> Original Paper : 3.3.1 Task #1: Masked LM \n\n```\nInput Sequence  : The man went to [MASK] store with [MASK] dog\nTarget Sequence :                  the                his\n```\n\n#### Rules:\nRandomly 15% of input token will be changed into something, based on under sub-rules\n\n1. Randomly 80% of tokens, gonna be a `[MASK]` token\n2. Randomly 10% of tokens, gonna be a `[RANDOM]` token(another word)\n3. Randomly 10% of tokens, will be remain as same. But need to be predicted.\n\n### Predict Next Sentence\n\n> Original Paper : 3.3.2 Task #2: Next Sentence Prediction\n\n```\nInput : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP]\nLabel : Is Next\n\nInput = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP]\nLabel = NotNext\n```\n\n\"Is this sentence can be continuously connected?\"\n\n understanding the relationship, between two text sentences, which is\nnot directly captured by language modeling\n\n#### Rules:\n\n1. Randomly 50% of next sentence, gonna be continuous sentence.\n2. Randomly 50% of next sentence, gonna be unrelated sentence.\n\n\n## Author\nJunseong Kim, Scatter Lab (codertimo@gmail.com / junseong.kim@scatter.co.kr)\n\n## License\n\nThis project following Apache 2.0 License as written in LICENSE file\n\nCopyright 2018 Junseong Kim, Scatter Lab, respective BERT contributors\n\nCopyright (c) 2018 Alexander Rush : [The Annotated Trasnformer](https://github.com/harvardnlp/annotated-transformer)\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/codertimo/BERT-pytorch", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "bert-pytorch", "package_url": "https://pypi.org/project/bert-pytorch/", "platform": "", "project_url": "https://pypi.org/project/bert-pytorch/", "project_urls": {"Homepage": "https://github.com/codertimo/BERT-pytorch"}, "release_url": "https://pypi.org/project/bert-pytorch/0.0.1a4/", "requires_dist": ["numpy", "torch (>=0.4.0)", "tqdm"], "requires_python": "", "summary": "Google AI 2018 BERT pytorch implementation", "version": "0.0.1a4", "yanked": false, "html_description": "<div class=\"project-description\">\n            # BERT-pytorch<br><br>[![LICENSE](https://img.shields.io/github/license/codertimo/BERT-pytorch.svg)](https://github.com/codertimo/BERT-pytorch/blob/master/LICENSE)<br>![GitHub issues](https://img.shields.io/github/issues/codertimo/BERT-pytorch.svg)<br>[![GitHub stars](https://img.shields.io/github/stars/codertimo/BERT-pytorch.svg)](https://github.com/codertimo/BERT-pytorch/stargazers)<br>[![CircleCI](https://circleci.com/gh/codertimo/BERT-pytorch.svg?style=shield)](https://circleci.com/gh/codertimo/BERT-pytorch)<br>[![PyPI](https://img.shields.io/pypi/v/bert-pytorch.svg)](https://pypi.org/project/bert_pytorch/)<br>[![PyPI - Status](https://img.shields.io/pypi/status/bert-pytorch.svg)](https://pypi.org/project/bert_pytorch/)<br>[![Documentation Status](https://readthedocs.org/projects/bert-pytorch/badge/?version=latest)](https://bert-pytorch.readthedocs.io/en/latest/?badge=latest)<br><br>Pytorch implementation of Google AI's 2018 BERT, with simple annotation<br><br>&gt; BERT 2018 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>&gt; Paper URL : https://arxiv.org/abs/1810.04805<br><br><br>## Introduction<br><br>Google AI's BERT paper shows the amazing result on various NLP task (new 17 NLP tasks SOTA), <br>including outperform the human F1 score on SQuAD v1.1 QA task. <br>This paper proved that Transformer(self-attention) based encoder can be powerfully used as <br>alternative of previous language model with proper language model training method. <br>And more importantly, they showed us that this pre-trained language model can be transfer <br>into any NLP task without making task specific model architecture.<br><br>This amazing result would be record in NLP history, <br>and I expect many further papers about BERT will be published very soon.<br><br>This repo is implementation of BERT. Code is very simple and easy to understand fastly.<br>Some of these codes are based on [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)<br><br>Currently this project is working on progress. And the code is not verified yet.<br><br>## Installation<br>```<br>pip install bert-pytorch<br>```<br><br>## Quickstart<br><br>**NOTICE : Your corpus should be prepared with two sentences in one line with tab(\\t) separator**<br><br>### 0. Prepare your corpus<br>```<br>Welcome to the \\t the jungle\\n<br>I can stay \\t here all night\\n<br>```<br><br>or tokenized corpus (tokenization is not in package)<br>```<br>Wel_ _come _to _the \\t _the _jungle\\n<br>_I _can _stay \\t _here _all _night\\n<br>```<br><br><br>### 1. Building vocab based on your corpus<br>```shell<br>bert-vocab -c data/corpus.small -o data/vocab.small<br>```<br><br>### 2. Train your own BERT model<br>```shell<br>bert -c data/corpus.small -v data/vocab.small -o output/bert.model<br>```<br><br>## Language Model Pre-training<br><br>In the paper, authors shows the new language model training methods, <br>which are \"masked language model\" and \"predict next sentence\".<br><br><br>### Masked Language Model <br><br>&gt; Original Paper : 3.3.1 Task #1: Masked LM <br><br>```<br>Input Sequence  : The man went to [MASK] store with [MASK] dog<br>Target Sequence :                  the                his<br>```<br><br>#### Rules:<br>Randomly 15% of input token will be changed into something, based on under sub-rules<br><br>1. Randomly 80% of tokens, gonna be a `[MASK]` token<br>2. Randomly 10% of tokens, gonna be a `[RANDOM]` token(another word)<br>3. Randomly 10% of tokens, will be remain as same. But need to be predicted.<br><br>### Predict Next Sentence<br><br>&gt; Original Paper : 3.3.2 Task #2: Next Sentence Prediction<br><br>```<br>Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP]<br>Label : Is Next<br><br>Input = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP]<br>Label = NotNext<br>```<br><br>\"Is this sentence can be continuously connected?\"<br><br> understanding the relationship, between two text sentences, which is<br>not directly captured by language modeling<br><br>#### Rules:<br><br>1. Randomly 50% of next sentence, gonna be continuous sentence.<br>2. Randomly 50% of next sentence, gonna be unrelated sentence.<br><br><br>## Author<br>Junseong Kim, Scatter Lab (codertimo@gmail.com / junseong.kim@scatter.co.kr)<br><br>## License<br><br>This project following Apache 2.0 License as written in LICENSE file<br><br>Copyright 2018 Junseong Kim, Scatter Lab, respective BERT contributors<br><br>Copyright (c) 2018 Alexander Rush : [The Annotated Trasnformer](https://github.com/harvardnlp/annotated-transformer)<br><br><br>\n          </div>"}, "last_serial": 4405251, "releases": {"0.0.1a0": [{"comment_text": "", "digests": {"md5": "4b8f7b00898ba150512e7a45a2b288b6", "sha256": "41abd00b58663e40bc271b2df35a66ed17dea104bfab7779e830ccc24a0a1d91"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a0-py3-none-any.whl", "has_sig": false, "md5_digest": "4b8f7b00898ba150512e7a45a2b288b6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 21330, "upload_time": "2018-10-18T16:53:27", "upload_time_iso_8601": "2018-10-18T16:53:27.627469Z", "url": "https://files.pythonhosted.org/packages/b0/12/61851974a69c2e90d205565c2bfea42902735b216c521744fa2b4f0a5052/bert_pytorch-0.0.1a0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e212e55d2d27941b3ef6f70b02bc7123", "sha256": "a973d0448e7a75082318535842e48ba4837875c447ca102bc7cec2aa7b6cfa06"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a0.tar.gz", "has_sig": false, "md5_digest": "e212e55d2d27941b3ef6f70b02bc7123", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11876, "upload_time": "2018-10-18T16:53:29", "upload_time_iso_8601": "2018-10-18T16:53:29.107853Z", "url": "https://files.pythonhosted.org/packages/73/7b/f4b88bf0c61c344e3a584f56c86e1e053dadb072a9d27d008352025391cf/bert_pytorch-0.0.1a0.tar.gz", "yanked": false}], "0.0.1a1": [{"comment_text": "", "digests": {"md5": "a4827f5aac4f9a675d27ac3ddd972e53", "sha256": "c17cba109ac00a137824b0505c0d08bbe28da677b04742422791b5294b1df143"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a1-py3-none-any.whl", "has_sig": false, "md5_digest": "a4827f5aac4f9a675d27ac3ddd972e53", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23034, "upload_time": "2018-10-18T17:21:29", "upload_time_iso_8601": "2018-10-18T17:21:29.107329Z", "url": "https://files.pythonhosted.org/packages/eb/8c/8285f7121756b7d3e67e927facac62a1fed49fafddf71045400598ab529c/bert_pytorch-0.0.1a1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "31a9ff36a8e70d636084deb63cc56db7", "sha256": "08e0dfc54aca5da3aa8f3124b4b2a33adee80a00b4efde34c48eabbc8d2b20a4"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a1.tar.gz", "has_sig": false, "md5_digest": "31a9ff36a8e70d636084deb63cc56db7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12883, "upload_time": "2018-10-18T17:21:30", "upload_time_iso_8601": "2018-10-18T17:21:30.056232Z", "url": "https://files.pythonhosted.org/packages/7c/bd/bef8bd44f93ad86b6d521a4b9e0a4f42520e65fb9cfd76d6ae2982590e3c/bert_pytorch-0.0.1a1.tar.gz", "yanked": false}], "0.0.1a2": [{"comment_text": "", "digests": {"md5": "2cf4ef49d1ea807dd1dc2cdf8b4a9fe4", "sha256": "9f69942bbded810a816c3f8357951e586d4b7d0b174a5478cbb4906bb445c87f"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a2-py3-none-any.whl", "has_sig": false, "md5_digest": "2cf4ef49d1ea807dd1dc2cdf8b4a9fe4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23129, "upload_time": "2018-10-19T09:23:53", "upload_time_iso_8601": "2018-10-19T09:23:53.724603Z", "url": "https://files.pythonhosted.org/packages/28/06/88f8e6511a2e5f00ec21d2625a415d7c78d62955eb620edcb6a4e478d463/bert_pytorch-0.0.1a2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "25c844b6b742bcb40ff15f1ad003b34c", "sha256": "53f63ec2e5267571367fc8d25598bb6f33ce710eae69c0d958766a0fb89e55eb"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a2.tar.gz", "has_sig": false, "md5_digest": "25c844b6b742bcb40ff15f1ad003b34c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12735, "upload_time": "2018-10-19T09:23:55", "upload_time_iso_8601": "2018-10-19T09:23:55.041008Z", "url": "https://files.pythonhosted.org/packages/d1/4a/594b80858fabc765117ed070e52ee12b1c602225b407bfc65cab82a75946/bert_pytorch-0.0.1a2.tar.gz", "yanked": false}], "0.0.1a3": [{"comment_text": "", "digests": {"md5": "66e7339f7e7487b12ed5be44214cfa98", "sha256": "38b25f9be641c40721c1ed6e6043e2d7bc00cc8521e97c30c553ea9fa103fde3"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a3-py3-none-any.whl", "has_sig": false, "md5_digest": "66e7339f7e7487b12ed5be44214cfa98", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 21423, "upload_time": "2018-10-20T05:31:58", "upload_time_iso_8601": "2018-10-20T05:31:58.533587Z", "url": "https://files.pythonhosted.org/packages/72/8a/cebe0dc57b414fedad0ce7727eee1a31096ca629f070d942b86622338848/bert_pytorch-0.0.1a3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ed1fba215c4ff28d57c43e44d12b06ac", "sha256": "86c915c9caf400d93e90a920fdec1570f38428d4f095cad7786ba7b7befdcae9"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a3.tar.gz", "has_sig": false, "md5_digest": "ed1fba215c4ff28d57c43e44d12b06ac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12064, "upload_time": "2018-10-20T05:31:59", "upload_time_iso_8601": "2018-10-20T05:31:59.653932Z", "url": "https://files.pythonhosted.org/packages/eb/ec/dad28936ea6e2d1f1ca3948ca0bee2fe001466e30f48d45a32852f8a4a8b/bert_pytorch-0.0.1a3.tar.gz", "yanked": false}], "0.0.1a4": [{"comment_text": "", "digests": {"md5": "0843ed91f3c435e349fc873ba38c7fe9", "sha256": "1bdb6ff4f5ab922b1e9877914f4804331f8770ed08f0ebbb406fcee57d3951fa"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a4-py3-none-any.whl", "has_sig": false, "md5_digest": "0843ed91f3c435e349fc873ba38c7fe9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22631, "upload_time": "2018-10-23T07:23:02", "upload_time_iso_8601": "2018-10-23T07:23:02.584634Z", "url": "https://files.pythonhosted.org/packages/4c/4d/328ca0670162d1a569854460cb1801e7e79f0e37238883cbd1d8c37cd6f4/bert_pytorch-0.0.1a4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9fb8aea43365b9ab9cb6c536b8ec9c28", "sha256": "76f5a6d83b059c941990f9c6f9a188a30f934b3f6134a7007f919e095d4123b5"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a4.tar.gz", "has_sig": false, "md5_digest": "9fb8aea43365b9ab9cb6c536b8ec9c28", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14890, "upload_time": "2018-10-23T07:23:04", "upload_time_iso_8601": "2018-10-23T07:23:04.167981Z", "url": "https://files.pythonhosted.org/packages/bd/7f/9c194be3b46f66ac07839cf064d966a7831b66a20e2bf5237fdbfc05ee73/bert_pytorch-0.0.1a4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0843ed91f3c435e349fc873ba38c7fe9", "sha256": "1bdb6ff4f5ab922b1e9877914f4804331f8770ed08f0ebbb406fcee57d3951fa"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a4-py3-none-any.whl", "has_sig": false, "md5_digest": "0843ed91f3c435e349fc873ba38c7fe9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22631, "upload_time": "2018-10-23T07:23:02", "upload_time_iso_8601": "2018-10-23T07:23:02.584634Z", "url": "https://files.pythonhosted.org/packages/4c/4d/328ca0670162d1a569854460cb1801e7e79f0e37238883cbd1d8c37cd6f4/bert_pytorch-0.0.1a4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9fb8aea43365b9ab9cb6c536b8ec9c28", "sha256": "76f5a6d83b059c941990f9c6f9a188a30f934b3f6134a7007f919e095d4123b5"}, "downloads": -1, "filename": "bert_pytorch-0.0.1a4.tar.gz", "has_sig": false, "md5_digest": "9fb8aea43365b9ab9cb6c536b8ec9c28", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14890, "upload_time": "2018-10-23T07:23:04", "upload_time_iso_8601": "2018-10-23T07:23:04.167981Z", "url": "https://files.pythonhosted.org/packages/bd/7f/9c194be3b46f66ac07839cf064d966a7831b66a20e2bf5237fdbfc05ee73/bert_pytorch-0.0.1a4.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:45 2020"}