{"info": {"author": "Zachary Pardos, Anirudhan Badrinath, Matthew Jade Johnson, Christian Garay", "author_email": "zp@berkeley.edu, abadrinath@berkeley.edu, mattjj@csail.mit.edu, c.garay@berkeley.edu", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "# pyBKT\n\nPython implementation of the Bayesian Knowledge Tracing algorithm, modeling student cognitive mastery from problem solving sequences.\n\n[Quick-start compile/train example in Colab notebook](https://colab.research.google.com/drive/1TKJkKYPAIub5jJSpAe04HJmP08EFYtMV?usp=sharing \"pyBKT quick start in Colab\") \n\nBased on the work of Zachary A. Pardos (zp@berkeley.edu) and Matthew J. Johnson (mattjj@csail.mit.edu) @ https://github.com/CAHLR/xBKT. Python adaptation by Cristian Garay (c.garay@berkeley.edu). Contributions by Anirudhan Badrinath (abadrinath@berkeley.edu). For formulas and technical implementation details, please refer to section 4.3 of Xu, Johnson, & Pardos (2015) ICML workshop [paper](http://ml4ed.cc/attachments/XuY.pdf). \n\n## Requirements\nPython >= 3.5 (Ubuntu >= 16.04)\n\nLibboost >= 1.58\n\nSupported OS: Linux only. Pure Python port in-development for OSX, Windows, and all other platforms. \n## Supported model variants\npyBKT can be used to define and fit many BKT variants, including these from the literature: \n\n* Individual student priors, learn rate, guess, and slip [1,2]\n* Individual item guess and slip [3,4,5]\n* Individual item or resource learn rate [4,5]\n\n1. Pardos, Z. A., Heffernan, N. T. (2010) Modeling Individualization in a Bayesian Networks Implementation of Knowledge Tracing. In P. De Bra, A. Kobsa, D. Chin (Eds.) *Proceedings of the 18th International Conference on User Modeling, Adaptation and Personalization* (UMAP). Big Island of Hawaii. Pages. Springer. Pages 255-266. [[doi]](https://doi.org/10.1007/978-3-642-13470-8_24\n)\n\n1. Pardos, Z. A., Heffernan, N. T. (2010) Using HMMs and bagged decision trees to leverage rich features of user and skill from an intelligent tutoring system dataset. In J. Stamper & A. Niculescu-Mizil (Eds.) *Proceedings of the KDD Cup Workshop at the 16th ACM Conference on Knowledge Discovery and Data Mining* (SIGKDD). Washington, D.C. ACM. Pages 24-35. [[kdd cup]](https://pslcdatashop.web.cmu.edu/KDDCup/workshop/papers/pardos_heffernan_KDD_Cup_2010_article.pdf)\n\n1. Pardos, Z. & Heffernan, N. (2011) KT-IDEM: Introducing Item Difficulty to the Knowledge Tracing Model. In Konstant et al. (eds.) *Proceedings of the 20th International Conference on User Modeling, Adaptation and Personalization* (UMAP). Girona, Spain. Springer. Pages 243-254. [[doi]](https://doi.org/10.1007/978-3-642-22362-4_21)\n\n1. Pardos, Z. A., Bergner, Y., Seaton, D., Pritchard, D.E. (2013) Adapting Bayesian Knowledge Tracing to a Massive Open Online College Course in edX. In S.K. D\u2019Mello, R.A. Calvo, & A. Olney (Eds.) *Proceedings of the 6th International Conference on Educational Data Mining* (EDM). Memphis, TN. Pages 137-144. [[edm]](http://educationaldatamining.org/EDM2013/proceedings/paper_20.pdf)\n\n1. Xu, Y., Johnson, M. J., Pardos, Z. A. (2015) Scaling cognitive modeling to massive open environments. In *Proceedings of the Workshop on Machine Learning for Education at the 32nd International Conference on Machine Learning* (ICML). Lille, France. [[icml ml4ed]](http://ml4ed.cc/attachments/XuY.pdf)\n\n\n\n\n\n# Installation and setup\nThis is intended as a quick overview of steps to install and setup and to run pyBKT locally.\n\n## Installing Boost-Python ##\n\nOn Ubuntu machines, install all the Boost Libraries using `sudo apt install libboost-all-dev`. Use whichever package manager is appropriately suited to the Linux distribution to install `libboost`.\n\n## Installing ##\n\nOnce `libboost` is installed (check by doing a quick `ldconfig -p | grep libboost_python`, which should yield an output), you can simply run:\n\n```\n    pip install git+https://github.com/CAHLR/pyBKT.git\n``` \nAlternatively, if `pip` poses some problems, you can clone the repository as such and then run the `setup.py` script manually.\n\n```\n    git clone https://github.com/CAHLR/pyBKT.git\n    cd pyBKT\n    python3 setup.py install\n```\n\n# Preparing Data and Running Model #\n## Input and Output Data ##\n_pyBKT_ models student mastery of a skills as they progress through series of learning resources and checks for understanding. Mastery is modelled as a latent variable has two states - \"knowing\" and \"not knowing\". At each checkpoint, students may be given a learning resource (i.e. watch a video) and/or question(s) to check for understanding. The model finds the probability of learning, forgetting, slipping and guessing that maximizes the likelihood of observed student responses to questions. \n\nTo run the pyBKT model, define the following variables:\n* `num_subparts`: The number of unique questions used to check understanding. Each subpart has a unique set of emission probabilities.\n* `num_resources`: The number of unique learning resources available to students.\n* `num_fit_initialization`: The number of iterations in the EM step.\n\n\nNext, create an input object `Data`, containing the following attributes: \n* `data`: a matrix containing sequential checkpoints for all students, with their responses. Each row represents a different subpart, and each column a checkpoint for a student. There are three potential values: {0 = no response or no question asked, 1 = wrong response, 2 = correct response}. If at a checkpoint, a resource was given but no question asked, the associated column would have `0` values in all rows. For example, to set up data containing 5 subparts given to two students over 2-3 checkpoints, the matrix would look as follows:\n\n        | 0  0  0  0  2 |\n        | 0  1  0  0  0 |\n        | 0  0  0  0  0 |\n        | 0  0  0  0  0 |\n        | 0  0  2  0  0 |   \n\n  In the above example, the first student starts out with just a learning resource, and no checks for understanding. In subsequent checkpoints, this student also responds to subpart 2 and 5, and gets the first wrong and the second correct.   \n\n* `starts`: defines each student's starting column on the `data` matrix. For the above matrix, `starts` would be defined as: \n\n        | 1  4 |\n\n* `lengths`: defines the number of check point for each student. For the above matrix, `lengths` would be defined as: \n\n        | 3  2 |\n\n* `resources`: defines the sequential id of the resources at each checkpoint. Each position in the vector corresponds to the column in the `data` matrix. For the above matrix, the learning `resources` at each checkpoint would be structured as: \n\n        | 1  2  1  1  3 |\n\n* `stateseqs`: this attribute is the true knowledge state for above data and should be left undefined before running the `pyBKT` model. \n\n\nThe output of the model can will be stored in a `fitmodel` object, containing the following probabilities as attributes: \n* `As`: the transition probability between the \"knowing\" and \"not knowing\" state. Includes both the `learns` and `forgets` probabilities, and their inverse. `As` creates a separate transition probability for each resource.\n* `learns`: the probability of transitioning to the \"knowing\" state given \"not known\".\n* `forgets`: the probability of transitioning to the \"not knowing\" state given \"known\".\n* `prior`: the prior probability of \"knowing\".\n\nThe `fitmodel` also includes the following emission probabilities:\n* `guesses`: the probability of guessing correctly, given \"not knowing\" state.\n* `slips`: the probability of picking incorrect answer, given \"knowing\" state.\n\n\n## Running pyBKT ##\nYou can add the folder path to the PYTHONPATH env variable in order to run the model from anywhere in your system. In Unix-based systems edit your _.bash_rc_ or _.bash_profile_ file and add:\n\n```\nexport PYTHONPATH=\"${PYTHONPATH}:/path_to_folder_containing_pyBKT_folder\"\n```\n\nTo start the EM algorithm, initiate a randomly generated `fitmodel`, with two potential options:\n\n1. `generate.random_model_uni`: generates a model from uniform distribution and sets the `forgets` probability to 0.\n\n2. `generate.random_model`: generates a model from dirichlet distribution and allows the `forgets` probability to vary. \n\nFor data observed during a short period of learning activity with a low probability of forgetting, the uniform model is recommended. The following example will initiate fitmodel using the uniform distribution: \n\n         fitmodel = random_model.random_model_uni(num_resources, num_subparts)\n\nOnce the `fitmodel` is generated, the following function can be used to generate an updated `fitmodel` and `log_likelihoods`:\n\n        (fitmodel, log_likelihoods) = EM_fit.EM_fit(fitmodel, data)\n\n## Example ##\n[TODO: Update Example Model]\n\nSee the file `test/hand_specified_model.py` for a fairly complete example,\nwhich you can run with `python test/hand_specified_model.py`.\n\nHere's a simplified version:\n\n```python\nimport sys\nsys.path.append('../') #path containing pyBKT\nimport numpy as np\nfrom pyBKT.generate import synthetic_data, random_model_uni\nfrom pyBKT.fit import EM_fit\nfrom copy import deepcopy\n\n#parameters classes\nnum_gs = 1 #number of guess/slip classes\nnum_learns = 1 #number of learning rates\n\nnum_fit_initializations = 20\n\n#true params used for synthetic data generation\np_T = 0.30\np_F = 0.00\np_G = 0.10\np_S = 0.03\np_L0 = 0.10\n\n#generate synthetic model and data.\ntruemodel = {}\n\ntruemodel[\"As\"] =  np.zeros((num_learns,2,2), dtype=np.float_)\nfor i in range(num_learns):\n    truemodel[\"As\"][i] = np.transpose([[1-p_T, p_T], [p_F, 1-p_F]])\n\ntruemodel[\"learns\"] = truemodel[\"As\"][:,1, 0,]\ntruemodel[\"forgets\"] = truemodel[\"As\"][:,0, 1]\n\ntruemodel[\"pi_0\"] = np.array([[1-p_L0], [p_L0]])\ntruemodel[\"prior\"] = truemodel[\"pi_0\"][1][0]\n\ntruemodel[\"guesses\"] = np.full(num_gs, p_G, dtype=np.float_)\ntruemodel[\"slips\"] = np.full(num_gs, p_S, dtype=np.float_)\n#can optionally set learn class sequence - set randomly by synthetic_data if not included\n#truemodel[\"resources\"] = np.random.randint(1, high = num_resources, size = sum(observation_sequence_lengths))\n\n#data!\nprint(\"generating data...\")\nobservation_sequence_lengths = np.full(500, 100, dtype=np.int) #specifies 500 students with 100 observations for synthetic data\ndata = synthetic_data.synthetic_data(truemodel, observation_sequence_lengths)\n\n#fit models, starting with random initializations\nprint('fitting! each dot is a new EM initialization')\n\nnum_fit_initializations = 5\nbest_likelihood = float(\"-inf\")\n\nfor i in range(num_fit_initializations):\n\tfitmodel = random_model_uni.random_model_uni(num_learns, num_gs) # include this line to randomly set initial param values\n\t(fitmodel, log_likelihoods) = EM_fit.EM_fit(fitmodel, data)\n\tif(log_likelihoods[-1] > best_likelihood):\n\t\tbest_likelihood = log_likelihoods[-1]\n\t\tbest_model = fitmodel\n\n# compare the fit model to the true model\n\nprint('')\nprint('\\ttruth\\tlearned')\nprint('prior\\t%.4f\\t%.4f' % (truemodel['prior'], best_model[\"pi_0\"][1][0]))\nfor r in range(num_learns):\n    print('learn%d\\t%.4f\\t%.4f' % (r+1, truemodel['As'][r, 1, 0].squeeze(), best_model['As'][r, 1, 0].squeeze()))\nfor r in range(num_learns):\n    print('forget%d\\t%.4f\\t%.4f' % (r+1, truemodel['As'][r, 0, 1].squeeze(), best_model['As'][r, 0, 1].squeeze()))\n\nfor s in range(num_gs):\n    print('guess%d\\t%.4f\\t%.4f' % (s+1, truemodel['guesses'][s], best_model['guesses'][s]))\nfor s in range(num_gs):\n    print('slip%d\\t%.4f\\t%.4f' % (s+1, truemodel['slips'][s], best_model['slips'][s]))\n\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/CAHLR/pyBKT/archive/1.0.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/CAHLR/pyBKT", "keywords": "BKT,Bayesian Knowledge Tracing,Bayesian Network,Hidden Markov Model,Intelligent Tutoring Systems,Adaptive Learning", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "pyBKT", "package_url": "https://pypi.org/project/pyBKT/", "platform": "", "project_url": "https://pypi.org/project/pyBKT/", "project_urls": {"Download": "https://github.com/CAHLR/pyBKT/archive/1.0.tar.gz", "Homepage": "https://github.com/CAHLR/pyBKT"}, "release_url": "https://pypi.org/project/pyBKT/1.0/", "requires_dist": null, "requires_python": "", "summary": "PyBKT - Python Implentation of Bayesian Knowledge Tracing", "version": "1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>pyBKT</h1>\n<p>Python implementation of the Bayesian Knowledge Tracing algorithm, modeling student cognitive mastery from problem solving sequences.</p>\n<p><a href=\"https://colab.research.google.com/drive/1TKJkKYPAIub5jJSpAe04HJmP08EFYtMV?usp=sharing\" rel=\"nofollow\" title=\"pyBKT quick start in Colab\">Quick-start compile/train example in Colab notebook</a></p>\n<p>Based on the work of Zachary A. Pardos (<a href=\"mailto:zp@berkeley.edu\">zp@berkeley.edu</a>) and Matthew J. Johnson (<a href=\"mailto:mattjj@csail.mit.edu\">mattjj@csail.mit.edu</a>) @ <a href=\"https://github.com/CAHLR/xBKT\" rel=\"nofollow\">https://github.com/CAHLR/xBKT</a>. Python adaptation by Cristian Garay (<a href=\"mailto:c.garay@berkeley.edu\">c.garay@berkeley.edu</a>). Contributions by Anirudhan Badrinath (<a href=\"mailto:abadrinath@berkeley.edu\">abadrinath@berkeley.edu</a>). For formulas and technical implementation details, please refer to section 4.3 of Xu, Johnson, &amp; Pardos (2015) ICML workshop <a href=\"http://ml4ed.cc/attachments/XuY.pdf\" rel=\"nofollow\">paper</a>.</p>\n<h2>Requirements</h2>\n<p>Python &gt;= 3.5 (Ubuntu &gt;= 16.04)</p>\n<p>Libboost &gt;= 1.58</p>\n<p>Supported OS: Linux only. Pure Python port in-development for OSX, Windows, and all other platforms.</p>\n<h2>Supported model variants</h2>\n<p>pyBKT can be used to define and fit many BKT variants, including these from the literature:</p>\n<ul>\n<li>Individual student priors, learn rate, guess, and slip [1,2]</li>\n<li>Individual item guess and slip [3,4,5]</li>\n<li>Individual item or resource learn rate [4,5]</li>\n</ul>\n<ol>\n<li>\n<p>Pardos, Z. A., Heffernan, N. T. (2010) Modeling Individualization in a Bayesian Networks Implementation of Knowledge Tracing. In P. De Bra, A. Kobsa, D. Chin (Eds.) <em>Proceedings of the 18th International Conference on User Modeling, Adaptation and Personalization</em> (UMAP). Big Island of Hawaii. Pages. Springer. Pages 255-266. <a href=\"https://doi.org/10.1007/978-3-642-13470-8_24\" rel=\"nofollow\">[doi]</a></p>\n</li>\n<li>\n<p>Pardos, Z. A., Heffernan, N. T. (2010) Using HMMs and bagged decision trees to leverage rich features of user and skill from an intelligent tutoring system dataset. In J. Stamper &amp; A. Niculescu-Mizil (Eds.) <em>Proceedings of the KDD Cup Workshop at the 16th ACM Conference on Knowledge Discovery and Data Mining</em> (SIGKDD). Washington, D.C. ACM. Pages 24-35. <a href=\"https://pslcdatashop.web.cmu.edu/KDDCup/workshop/papers/pardos_heffernan_KDD_Cup_2010_article.pdf\" rel=\"nofollow\">[kdd cup]</a></p>\n</li>\n<li>\n<p>Pardos, Z. &amp; Heffernan, N. (2011) KT-IDEM: Introducing Item Difficulty to the Knowledge Tracing Model. In Konstant et al. (eds.) <em>Proceedings of the 20th International Conference on User Modeling, Adaptation and Personalization</em> (UMAP). Girona, Spain. Springer. Pages 243-254. <a href=\"https://doi.org/10.1007/978-3-642-22362-4_21\" rel=\"nofollow\">[doi]</a></p>\n</li>\n<li>\n<p>Pardos, Z. A., Bergner, Y., Seaton, D., Pritchard, D.E. (2013) Adapting Bayesian Knowledge Tracing to a Massive Open Online College Course in edX. In S.K. D\u2019Mello, R.A. Calvo, &amp; A. Olney (Eds.) <em>Proceedings of the 6th International Conference on Educational Data Mining</em> (EDM). Memphis, TN. Pages 137-144. <a href=\"http://educationaldatamining.org/EDM2013/proceedings/paper_20.pdf\" rel=\"nofollow\">[edm]</a></p>\n</li>\n<li>\n<p>Xu, Y., Johnson, M. J., Pardos, Z. A. (2015) Scaling cognitive modeling to massive open environments. In <em>Proceedings of the Workshop on Machine Learning for Education at the 32nd International Conference on Machine Learning</em> (ICML). Lille, France. <a href=\"http://ml4ed.cc/attachments/XuY.pdf\" rel=\"nofollow\">[icml ml4ed]</a></p>\n</li>\n</ol>\n<h1>Installation and setup</h1>\n<p>This is intended as a quick overview of steps to install and setup and to run pyBKT locally.</p>\n<h2>Installing Boost-Python</h2>\n<p>On Ubuntu machines, install all the Boost Libraries using <code>sudo apt install libboost-all-dev</code>. Use whichever package manager is appropriately suited to the Linux distribution to install <code>libboost</code>.</p>\n<h2>Installing</h2>\n<p>Once <code>libboost</code> is installed (check by doing a quick <code>ldconfig -p | grep libboost_python</code>, which should yield an output), you can simply run:</p>\n<pre><code>    pip install git+https://github.com/CAHLR/pyBKT.git\n</code></pre>\n<p>Alternatively, if <code>pip</code> poses some problems, you can clone the repository as such and then run the <code>setup.py</code> script manually.</p>\n<pre><code>    git clone https://github.com/CAHLR/pyBKT.git\n    cd pyBKT\n    python3 setup.py install\n</code></pre>\n<h1>Preparing Data and Running Model</h1>\n<h2>Input and Output Data</h2>\n<p><em>pyBKT</em> models student mastery of a skills as they progress through series of learning resources and checks for understanding. Mastery is modelled as a latent variable has two states - \"knowing\" and \"not knowing\". At each checkpoint, students may be given a learning resource (i.e. watch a video) and/or question(s) to check for understanding. The model finds the probability of learning, forgetting, slipping and guessing that maximizes the likelihood of observed student responses to questions.</p>\n<p>To run the pyBKT model, define the following variables:</p>\n<ul>\n<li><code>num_subparts</code>: The number of unique questions used to check understanding. Each subpart has a unique set of emission probabilities.</li>\n<li><code>num_resources</code>: The number of unique learning resources available to students.</li>\n<li><code>num_fit_initialization</code>: The number of iterations in the EM step.</li>\n</ul>\n<p>Next, create an input object <code>Data</code>, containing the following attributes:</p>\n<ul>\n<li>\n<p><code>data</code>: a matrix containing sequential checkpoints for all students, with their responses. Each row represents a different subpart, and each column a checkpoint for a student. There are three potential values: {0 = no response or no question asked, 1 = wrong response, 2 = correct response}. If at a checkpoint, a resource was given but no question asked, the associated column would have <code>0</code> values in all rows. For example, to set up data containing 5 subparts given to two students over 2-3 checkpoints, the matrix would look as follows:</p>\n<pre><code>  | 0  0  0  0  2 |\n  | 0  1  0  0  0 |\n  | 0  0  0  0  0 |\n  | 0  0  0  0  0 |\n  | 0  0  2  0  0 |   \n</code></pre>\n<p>In the above example, the first student starts out with just a learning resource, and no checks for understanding. In subsequent checkpoints, this student also responds to subpart 2 and 5, and gets the first wrong and the second correct.</p>\n</li>\n<li>\n<p><code>starts</code>: defines each student's starting column on the <code>data</code> matrix. For the above matrix, <code>starts</code> would be defined as:</p>\n<pre><code>  | 1  4 |\n</code></pre>\n</li>\n<li>\n<p><code>lengths</code>: defines the number of check point for each student. For the above matrix, <code>lengths</code> would be defined as:</p>\n<pre><code>  | 3  2 |\n</code></pre>\n</li>\n<li>\n<p><code>resources</code>: defines the sequential id of the resources at each checkpoint. Each position in the vector corresponds to the column in the <code>data</code> matrix. For the above matrix, the learning <code>resources</code> at each checkpoint would be structured as:</p>\n<pre><code>  | 1  2  1  1  3 |\n</code></pre>\n</li>\n<li>\n<p><code>stateseqs</code>: this attribute is the true knowledge state for above data and should be left undefined before running the <code>pyBKT</code> model.</p>\n</li>\n</ul>\n<p>The output of the model can will be stored in a <code>fitmodel</code> object, containing the following probabilities as attributes:</p>\n<ul>\n<li><code>As</code>: the transition probability between the \"knowing\" and \"not knowing\" state. Includes both the <code>learns</code> and <code>forgets</code> probabilities, and their inverse. <code>As</code> creates a separate transition probability for each resource.</li>\n<li><code>learns</code>: the probability of transitioning to the \"knowing\" state given \"not known\".</li>\n<li><code>forgets</code>: the probability of transitioning to the \"not knowing\" state given \"known\".</li>\n<li><code>prior</code>: the prior probability of \"knowing\".</li>\n</ul>\n<p>The <code>fitmodel</code> also includes the following emission probabilities:</p>\n<ul>\n<li><code>guesses</code>: the probability of guessing correctly, given \"not knowing\" state.</li>\n<li><code>slips</code>: the probability of picking incorrect answer, given \"knowing\" state.</li>\n</ul>\n<h2>Running pyBKT</h2>\n<p>You can add the folder path to the PYTHONPATH env variable in order to run the model from anywhere in your system. In Unix-based systems edit your <em>.bash_rc</em> or <em>.bash_profile</em> file and add:</p>\n<pre><code>export PYTHONPATH=\"${PYTHONPATH}:/path_to_folder_containing_pyBKT_folder\"\n</code></pre>\n<p>To start the EM algorithm, initiate a randomly generated <code>fitmodel</code>, with two potential options:</p>\n<ol>\n<li>\n<p><code>generate.random_model_uni</code>: generates a model from uniform distribution and sets the <code>forgets</code> probability to 0.</p>\n</li>\n<li>\n<p><code>generate.random_model</code>: generates a model from dirichlet distribution and allows the <code>forgets</code> probability to vary.</p>\n</li>\n</ol>\n<p>For data observed during a short period of learning activity with a low probability of forgetting, the uniform model is recommended. The following example will initiate fitmodel using the uniform distribution:</p>\n<pre><code>     fitmodel = random_model.random_model_uni(num_resources, num_subparts)\n</code></pre>\n<p>Once the <code>fitmodel</code> is generated, the following function can be used to generate an updated <code>fitmodel</code> and <code>log_likelihoods</code>:</p>\n<pre><code>    (fitmodel, log_likelihoods) = EM_fit.EM_fit(fitmodel, data)\n</code></pre>\n<h2>Example</h2>\n<p>[TODO: Update Example Model]</p>\n<p>See the file <code>test/hand_specified_model.py</code> for a fairly complete example,\nwhich you can run with <code>python test/hand_specified_model.py</code>.</p>\n<p>Here's a simplified version:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n<span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"s1\">'../'</span><span class=\"p\">)</span> <span class=\"c1\">#path containing pyBKT</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pyBKT.generate</span> <span class=\"kn\">import</span> <span class=\"n\">synthetic_data</span><span class=\"p\">,</span> <span class=\"n\">random_model_uni</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pyBKT.fit</span> <span class=\"kn\">import</span> <span class=\"n\">EM_fit</span>\n<span class=\"kn\">from</span> <span class=\"nn\">copy</span> <span class=\"kn\">import</span> <span class=\"n\">deepcopy</span>\n\n<span class=\"c1\">#parameters classes</span>\n<span class=\"n\">num_gs</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"c1\">#number of guess/slip classes</span>\n<span class=\"n\">num_learns</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"c1\">#number of learning rates</span>\n\n<span class=\"n\">num_fit_initializations</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n\n<span class=\"c1\">#true params used for synthetic data generation</span>\n<span class=\"n\">p_T</span> <span class=\"o\">=</span> <span class=\"mf\">0.30</span>\n<span class=\"n\">p_F</span> <span class=\"o\">=</span> <span class=\"mf\">0.00</span>\n<span class=\"n\">p_G</span> <span class=\"o\">=</span> <span class=\"mf\">0.10</span>\n<span class=\"n\">p_S</span> <span class=\"o\">=</span> <span class=\"mf\">0.03</span>\n<span class=\"n\">p_L0</span> <span class=\"o\">=</span> <span class=\"mf\">0.10</span>\n\n<span class=\"c1\">#generate synthetic model and data.</span>\n<span class=\"n\">truemodel</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n\n<span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"As\"</span><span class=\"p\">]</span> <span class=\"o\">=</span>  <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_learns</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float_</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_learns</span><span class=\"p\">):</span>\n    <span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"As\"</span><span class=\"p\">][</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">([[</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">p_T</span><span class=\"p\">,</span> <span class=\"n\">p_T</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"n\">p_F</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">p_F</span><span class=\"p\">]])</span>\n\n<span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"learns\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"As\"</span><span class=\"p\">][:,</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,]</span>\n<span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"forgets\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"As\"</span><span class=\"p\">][:,</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n\n<span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"pi_0\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">p_L0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"n\">p_L0</span><span class=\"p\">]])</span>\n<span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"prior\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"pi_0\"</span><span class=\"p\">][</span><span class=\"mi\">1</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n\n<span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"guesses\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">(</span><span class=\"n\">num_gs</span><span class=\"p\">,</span> <span class=\"n\">p_G</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float_</span><span class=\"p\">)</span>\n<span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s2\">\"slips\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">(</span><span class=\"n\">num_gs</span><span class=\"p\">,</span> <span class=\"n\">p_S</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float_</span><span class=\"p\">)</span>\n<span class=\"c1\">#can optionally set learn class sequence - set randomly by synthetic_data if not included</span>\n<span class=\"c1\">#truemodel[\"resources\"] = np.random.randint(1, high = num_resources, size = sum(observation_sequence_lengths))</span>\n\n<span class=\"c1\">#data!</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"generating data...\"</span><span class=\"p\">)</span>\n<span class=\"n\">observation_sequence_lengths</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int</span><span class=\"p\">)</span> <span class=\"c1\">#specifies 500 students with 100 observations for synthetic data</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">synthetic_data</span><span class=\"o\">.</span><span class=\"n\">synthetic_data</span><span class=\"p\">(</span><span class=\"n\">truemodel</span><span class=\"p\">,</span> <span class=\"n\">observation_sequence_lengths</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#fit models, starting with random initializations</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'fitting! each dot is a new EM initialization'</span><span class=\"p\">)</span>\n\n<span class=\"n\">num_fit_initializations</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n<span class=\"n\">best_likelihood</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">\"-inf\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_fit_initializations</span><span class=\"p\">):</span>\n\t<span class=\"n\">fitmodel</span> <span class=\"o\">=</span> <span class=\"n\">random_model_uni</span><span class=\"o\">.</span><span class=\"n\">random_model_uni</span><span class=\"p\">(</span><span class=\"n\">num_learns</span><span class=\"p\">,</span> <span class=\"n\">num_gs</span><span class=\"p\">)</span> <span class=\"c1\"># include this line to randomly set initial param values</span>\n\t<span class=\"p\">(</span><span class=\"n\">fitmodel</span><span class=\"p\">,</span> <span class=\"n\">log_likelihoods</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">EM_fit</span><span class=\"o\">.</span><span class=\"n\">EM_fit</span><span class=\"p\">(</span><span class=\"n\">fitmodel</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">)</span>\n\t<span class=\"k\">if</span><span class=\"p\">(</span><span class=\"n\">log_likelihoods</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"n\">best_likelihood</span><span class=\"p\">):</span>\n\t\t<span class=\"n\">best_likelihood</span> <span class=\"o\">=</span> <span class=\"n\">log_likelihoods</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n\t\t<span class=\"n\">best_model</span> <span class=\"o\">=</span> <span class=\"n\">fitmodel</span>\n\n<span class=\"c1\"># compare the fit model to the true model</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">''</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'</span><span class=\"se\">\\t</span><span class=\"s1\">truth</span><span class=\"se\">\\t</span><span class=\"s1\">learned'</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'prior</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s1\">'prior'</span><span class=\"p\">],</span> <span class=\"n\">best_model</span><span class=\"p\">[</span><span class=\"s2\">\"pi_0\"</span><span class=\"p\">][</span><span class=\"mi\">1</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n<span class=\"k\">for</span> <span class=\"n\">r</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_learns</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'learn</span><span class=\"si\">%d</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s1\">'As'</span><span class=\"p\">][</span><span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(),</span> <span class=\"n\">best_model</span><span class=\"p\">[</span><span class=\"s1\">'As'</span><span class=\"p\">][</span><span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()))</span>\n<span class=\"k\">for</span> <span class=\"n\">r</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_learns</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'forget</span><span class=\"si\">%d</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s1\">'As'</span><span class=\"p\">][</span><span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(),</span> <span class=\"n\">best_model</span><span class=\"p\">[</span><span class=\"s1\">'As'</span><span class=\"p\">][</span><span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">()))</span>\n\n<span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_gs</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'guess</span><span class=\"si\">%d</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">s</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s1\">'guesses'</span><span class=\"p\">][</span><span class=\"n\">s</span><span class=\"p\">],</span> <span class=\"n\">best_model</span><span class=\"p\">[</span><span class=\"s1\">'guesses'</span><span class=\"p\">][</span><span class=\"n\">s</span><span class=\"p\">]))</span>\n<span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_gs</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'slip</span><span class=\"si\">%d</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"se\">\\t</span><span class=\"si\">%.4f</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">s</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">truemodel</span><span class=\"p\">[</span><span class=\"s1\">'slips'</span><span class=\"p\">][</span><span class=\"n\">s</span><span class=\"p\">],</span> <span class=\"n\">best_model</span><span class=\"p\">[</span><span class=\"s1\">'slips'</span><span class=\"p\">][</span><span class=\"n\">s</span><span class=\"p\">]))</span>\n</pre>\n\n          </div>"}, "last_serial": 6983053, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "ea6a1b8929e717a8b1c5f7888b16fd1e", "sha256": "6bcf47e49f4fe2fe191c1db4afa257a938d71a15fbff3c75ffdd121e16d7f3e1"}, "downloads": -1, "filename": "pyBKT-1.0.tar.gz", "has_sig": false, "md5_digest": "ea6a1b8929e717a8b1c5f7888b16fd1e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13050037, "upload_time": "2020-04-09T03:26:53", "upload_time_iso_8601": "2020-04-09T03:26:53.491639Z", "url": "https://files.pythonhosted.org/packages/07/06/3180ff73cf8e6bed7bfdef36075a5a9db80055e7d3d0cf2ad2aa1aba3025/pyBKT-1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ea6a1b8929e717a8b1c5f7888b16fd1e", "sha256": "6bcf47e49f4fe2fe191c1db4afa257a938d71a15fbff3c75ffdd121e16d7f3e1"}, "downloads": -1, "filename": "pyBKT-1.0.tar.gz", "has_sig": false, "md5_digest": "ea6a1b8929e717a8b1c5f7888b16fd1e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13050037, "upload_time": "2020-04-09T03:26:53", "upload_time_iso_8601": "2020-04-09T03:26:53.491639Z", "url": "https://files.pythonhosted.org/packages/07/06/3180ff73cf8e6bed7bfdef36075a5a9db80055e7d3d0cf2ad2aa1aba3025/pyBKT-1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:09:11 2020"}