{"info": {"author": "Marius-Constantin Dinu", "author_email": "dinu.marius-constantin@hotmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: BSD License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Lighter\n\nLightweight extension for torch to speed up project prototyping and enable dependency injection of object instances.\nThis framework includes a setup script to get started with a pre-defined project structure and the documentation offers examples and an overview of the initial usage.\nFor more examples follow the [link to the GitHub repository](https://github.com/Xpitfire/lighter).\n\n## Install\n\n```shell script\n$> pip install torch-lighter\n```\n\n## Quickstart\n\nCreate a JSON file `config.json`:\n\n```json\n{\n  \"num_of_epochs\": 50\n}\n```\n\nCreate a python file and use the decorators to inject the config instance:\n\n```python\n# creates the application context\nContext.create()\n\nclass Demo:\n    @config(path='config.json', property='config')\n    def __init__(self):\n        pass\n\n    def run(self):\n        print(self.config.num_of_epochs)\n\n# creates the object instance and injects the configs\nDemo().run()\n```\n\nThis will print the following output:\n\n```shell script\n[]: 50\n```\n\nLighter can be simply added to any existing project or one can create a new lighter template project as shown in the next section.\n\n\n## Create a project\n\n### Initialize project\n\nType the following in your command line / bash:\n```shell script\n$> lighter-init <project-name>\n``` \n\n### Structure overview\nThe above command will create the following project structure:\n```text\nmy_project:\n  * configs\n    - modules.config.json\n  * criterions\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * data_builders\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * datasets\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * experiments\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * models\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * optimizers\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * tests\n    - __init__.py\n    - test_experiment.py\n  * transforms\n    - __init__.py\n    - defaults.py\n  * __init__.py\n  * README.md\n  * LICENSE\n  * setup.py\n```\n\nOf course this is only a preset example and the projects can be structured at will.\n\n### Modules description\n- collectible: are used to gather metrics over multiple steps / epochs\n- criterion: torch criterion classes\n- data_builder: data builder classes creating the data loaders\n- dataset: dataset used by the data builder\n- experiment: experiment logic files containing the main `run()`, `train()` and `eval()` methods\n- metric: metrics used to measure performance\n- module: references all available modules\n- optimizer: optimizers used for performing the step\n- test: contains the main testing sources for the project and mark the main entrance point\n- transform: data transforms are used by the datasets to reshape the data structures\n- writer: tensorboard writers\n\n### TODOs\n\nEach directory containing a python file may also contain a config file outsourcing properties for testing.\nModules that require further code insertion are marked with `TODO` flags.\n\n### Run an experiment\n\nAfter creating a demo project and inserting code into the `TODO` placeholders simply run an experiment by calling:\n```python\n$> python tests/test_experiment.py\n```\n\nIt might be necessary to set your `PYTHONPATH` before running the experiment file:\n\n```python\n$> PYTHONPATH=. python tests/test_experiment.py\n```\n\nOne can also simply change the global device setting or initialize an experiment from command line by specifying theses arguments:\n```python\n$> python tests/test_experiment.py --device cuda:1 --config <path-to-config>\n```\n\n## Advanced\n\n### Inheritance from base class\n\nOne can simply extend from any base class within the lighter framework:\n```python\nclass SimpleExperiment(DefaultExperiment):\n    @config(path='experiments/defaults.config.json', property='experiments')\n    @strategy(config='configs/modules.config.json')\n    @references\n    def __init__(self):\n        super(SimpleExperiment, self).__init__(epochs=self.config.experiments.num_epochs)\n\n    def eval(self):\n        ...\n\n    def train(self):\n        ...\n        preds = self.model(inputs)\n        loss = self.metric(preds, targets)\n        ...\n```\nIn this example we subclass the `DefaultExperiment` class and create a new experiment with some custom `eval()` and `train()` behaviour. \nThe `run()` work-flow is still used as defined in the base class `DefaultExperiment`.\nThe `@references` decorator injects by default some properties into the current `SimpleExperiment` object instance. \nIf not other specified the `@reference` tries to inject the properties `['dataset', 'data_builder', 'criterion', 'model', 'writer', 'optimizer', 'transfom', 'metric', 'collectible']` which in this case are declared in the `configs/modules.defaults.config.json` config.\nSince `model` and `metric` are injected via `@references` they can be directly accessed via the `self` instance.\n\n\nThe `@config` decorator injects additional configs for the current experiment instance by referencing `defaults.config.json` with an `experiments` alias. \nAll configs are by default created as a sub-property of `self.config` and are now referencable through the `experiments` alias.\nOne can then access the properties as follows `self.config.experiments.<property>`.\n\n### Decorators\n\nIn general it is also possible to simply use the required decorators without base class sub-classing.\n\n\nThere are currently the following decorators available:\n* `@config` - injects the default config instance and allows to load new configs\n* `@context` - injects the application context which holds config and registry instances\n* `@transform` - injects the default data transform instance\n* `@dataset` - injects the default dataset instance\n* `@model` - injects the default model instance\n* `@metric` - injects the default metric instance\n* `@reference` - injects a singe defined instance\n* `@references` - injects a set of pre-set or defined instance(s)\n* `@strategy` - defines and loads a training strategy from a specified config path and injects the default object instances\n* `@register` - allows to quickly register a new type to the context registry and inject the instance into the current instance\n* `@hook` - allows to hook and overwrite an existing method to change the execution logic\n* `@inject` - allows to inject single instances into different context options (registry, types, instances, configs)\n* `@search` - allows to register hyper-parameters to parse a config schedule for parallelled executions\n\nBy default the templates for a new project are creating a config file in the `configs/modules.config.json` path, which uses the following default names for the dependency injection `['dataset', 'data_builder', 'criterion', 'model', 'writer', 'optimizer', 'transfom', 'metric', 'collectible']`.\nAn experiment gets these instances automatically instantiated and injected at runtime.\n```json\n{\n  \"strategy\": {\n      \"transform\": \"type::transforms.defaults.Transform\",\n      \"dataset\": \"type::datasets.defaults.Dataset\",\n      \"data_builder\": \"type::data_builders.defaults.DataBuilder\",\n      \"criterion\": \"type::criterions.defaults.Criterion\",\n      \"model\": \"type::models.defaults.Model\",\n      \"optimizer\": \"type::optimizers.defaults.Optimizer\",\n      \"metric\": \"type::lighter.metric.BaseMetric\",\n      \"collectible\": \"type::lighter.collectible.BaseCollectible\",\n      \"writer\": \"type::lighter.writer.BaseWriter\"\n  }\n}\n```\n\nThe above example groups the types in the `strategy` alias name to avoid config name collisions if other classes import configs using the same general names (model, metric, etc.) across the application.\n\nIt is also simple to deviate from this naming convention or even inject custom types by exchanging names within the referencing configs:\n```json\n{\n  ...\n  \"other_model\": \"type::models.other.Network\",\n  \"other_metric\": \"type::metrics.other.Metric\",\n  ...\n}\n```\n\nThe defined name `other_model_name` is now uniquely assigned in the instance context and can be accessed via `@references(names=['other_model', 'other_metric'])`\n\nIt is also possible to register types on the fly within the code at the class or method level:\n\n```python\nclass Demo:\n    @register(type='envs.defaults.Environment', property='env')\n    @reference(name='env')\n    def __init__(self):\n        ...\n        obs = self.env.reset()\n        ...\n```\n\nHere the `@reference` decorator requires a name argument to match the instance.\n\n### Config templates\n\nAll configs are based on `json` files. Yet, when importing the configs `lighter` parses some escape sequences allowing for more modularization of your project structure:\n\n* `config::<config-file-path>` - imports another config into the current json config instance\n* `import::<config-file-path>` - imports all properties of the referenced config file into the current config instance (attention may override existing properties)\n* `type::<type-file-path>` - imports a python type which is registered to the application context `registry.types` and can be used to instantiate new objects\n\n### Parameter search parser\n\nOne can simply use lighter for searching hyper-parameters or different setting using the `@search` decorator, which registers parameters referenced by configs to the global context.\nThis generates a permutation of different settings using the parameter parser, which then allows for parallel execution either with the default built in scheduler or any other specialized framework for distributed optimization, such as [ray](https://github.com/ray-project/ray).\n\n```python\nclass ParameterSearchRegistration:\n  @search(group='sgd',\n          params=[('lr', GridParameter(ref='optimizer.lr', min=0.001, max=0.005, step=0.001)),\n                  ('weight_decay', ListParameter(ref='optimizer.weight_decay', options=[0.0, 0.9])),\n                  ('freeze_pretrained', BinaryParameter(ref='model.freeze_pretrained')),\n                  ('hidden_units', GridParameter(ref='model.hidden_units', min=100, max=200, step=100)),\n                  ('optimizer', SetParameter(ref='strategy.optimizer', option=\"type::optimizers.defaults.Optimizer\")),\n                  ('model_output', SetParameter(ref='model.output', option=1)),\n                  ('model_pretrained', SetParameter(ref='model.pretrained', option=True)),\n                  ('strategy', StrategyParameter(ref='strategy', options=['searches/coco_looking.config.json'])),\n                  ('model', ListParameter(ref='strategy.model',\n                                          options=['type::models.alexnet.AlexNetFeatureExtractionModel',\n                                                   'type::models.resnet.ResNetFeatureExtractionModel']))])\n  def __init__(self):\n    pass\n```\n\nHere `group` marks the grouping alias under which the upcoming parameters are registered in the context.\nThe `params` argument takes in a list of two-tuples `(<name: str>, <param: Parameter>)`, where `name` marks the parameter alias within the group and `param` the type of search parameter used.\n\nCurrently lighter offers the following parameter types:\n\n* `GridParameter`: Search a range of values (min, max, steps) with fixed step size\n* `ListParameter`: Commit a list of parameters to lighter which is sequentially executed\n* `CallableGridParameter`: Search a range of values (min, max, callable) but with a callable lambda function which allows for non-linear step behaviour\n* `AnnealParameter`: Same as `CallableGridParameter` but for annealing a parameter\n* `BinaryParameter`: Return `True` and `False` values for a parameter\n* `StrategyParameter`: Allows to define different training strategies and switch between them\n* `SetParameter`: Represents a simple setter of properties that is used to register settings, which don't change, but need to be imported\n\nTo trigger the parameter config parsing run:\n\n```python\nfrom searches.defaults import ParameterSearchRegistration\nfrom lighter.parser import ConfigParser\nfrom lighter.context import Context\n\ncontext = Context.create(auto_instantiate_types=False)\npsr = ParameterSearchRegistration()\ncp = ConfigParser(experiment=psr)\ncp.parse()\n```\n\nBy default this saves all permuted configs to the `runs/search/<gen-name>` directory.\nThe `auto_instantiate_types` parameter prevents that the context instantiates the registered types when loading in configs.\n\n### Parameter search scheduler\n\nLighter comes with a built in config scheduler that can execute experiments in parallel for single machine experiments.\nFor many prototyping cases this is sufficient and helps during development, but naturally flexibility is highest priority and it is possible and recommended to use 3rd party frameworks to schedule decentralized parallel executions on clusters of machines.\n\nFor the simple case, \n\n```python\nimport torch\nfrom lighter.scheduler import Scheduler\n\n# required to execute torch in parallel processes\ntorch.multiprocessing.set_start_method('spawn', force=True)\nnum_devices = torch.cuda.device_count()\n# sets the path to the list of configs\nscheduler = Scheduler(path='runs/search/witty-poodle',\n                      experiment='experiments.defaults.Experiment',\n                      device_name='cuda',\n                      num_workers=num_devices)\nscheduler.run()\n```\n\nThe scheduler in the code snipped above starts as many processes as there are GPUs available and schedules the list of configs across these devices.\nSince we defined a simple writer, one can simply monitor the progress using tensorboard.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Xpitfire/lighter", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "torch-lighter", "package_url": "https://pypi.org/project/torch-lighter/", "platform": "", "project_url": "https://pypi.org/project/torch-lighter/", "project_urls": {"Homepage": "https://github.com/Xpitfire/lighter"}, "release_url": "https://pypi.org/project/torch-lighter/0.2.23/", "requires_dist": ["ffmpeg", "natsort", "numpy", "tqdm", "torch", "torchvision", "tensorboard", "coloredlogs", "petname", "pandas", "setproctitle", "multiprocess", "python-box"], "requires_python": "", "summary": "Lightweight extension for torch to speed-up prototyping.", "version": "0.2.23", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Lighter</h1>\n<p>Lightweight extension for torch to speed up project prototyping and enable dependency injection of object instances.\nThis framework includes a setup script to get started with a pre-defined project structure and the documentation offers examples and an overview of the initial usage.\nFor more examples follow the <a href=\"https://github.com/Xpitfire/lighter\" rel=\"nofollow\">link to the GitHub repository</a>.</p>\n<h2>Install</h2>\n<pre>$&gt; pip install torch-lighter\n</pre>\n<h2>Quickstart</h2>\n<p>Create a JSON file <code>config.json</code>:</p>\n<pre><span class=\"p\">{</span>\n  <span class=\"nt\">\"num_of_epochs\"</span><span class=\"p\">:</span> <span class=\"mi\">50</span>\n<span class=\"p\">}</span>\n</pre>\n<p>Create a python file and use the decorators to inject the config instance:</p>\n<pre><span class=\"c1\"># creates the application context</span>\n<span class=\"n\">Context</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">()</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">Demo</span><span class=\"p\">:</span>\n    <span class=\"nd\">@config</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'config.json'</span><span class=\"p\">,</span> <span class=\"nb\">property</span><span class=\"o\">=</span><span class=\"s1\">'config'</span><span class=\"p\">)</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">run</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">num_of_epochs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># creates the object instance and injects the configs</span>\n<span class=\"n\">Demo</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n<p>This will print the following output:</p>\n<pre><span class=\"o\">[]</span>: <span class=\"m\">50</span>\n</pre>\n<p>Lighter can be simply added to any existing project or one can create a new lighter template project as shown in the next section.</p>\n<h2>Create a project</h2>\n<h3>Initialize project</h3>\n<p>Type the following in your command line / bash:</p>\n<pre>$&gt; lighter-init &lt;project-name&gt;\n</pre>\n<h3>Structure overview</h3>\n<p>The above command will create the following project structure:</p>\n<pre>my_project:\n  * configs\n    - modules.config.json\n  * criterions\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * data_builders\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * datasets\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * experiments\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * models\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * optimizers\n    - __init__.py\n    - defaults.config.json\n    - defaults.py\n  * tests\n    - __init__.py\n    - test_experiment.py\n  * transforms\n    - __init__.py\n    - defaults.py\n  * __init__.py\n  * README.md\n  * LICENSE\n  * setup.py\n</pre>\n<p>Of course this is only a preset example and the projects can be structured at will.</p>\n<h3>Modules description</h3>\n<ul>\n<li>collectible: are used to gather metrics over multiple steps / epochs</li>\n<li>criterion: torch criterion classes</li>\n<li>data_builder: data builder classes creating the data loaders</li>\n<li>dataset: dataset used by the data builder</li>\n<li>experiment: experiment logic files containing the main <code>run()</code>, <code>train()</code> and <code>eval()</code> methods</li>\n<li>metric: metrics used to measure performance</li>\n<li>module: references all available modules</li>\n<li>optimizer: optimizers used for performing the step</li>\n<li>test: contains the main testing sources for the project and mark the main entrance point</li>\n<li>transform: data transforms are used by the datasets to reshape the data structures</li>\n<li>writer: tensorboard writers</li>\n</ul>\n<h3>TODOs</h3>\n<p>Each directory containing a python file may also contain a config file outsourcing properties for testing.\nModules that require further code insertion are marked with <code>TODO</code> flags.</p>\n<h3>Run an experiment</h3>\n<p>After creating a demo project and inserting code into the <code>TODO</code> placeholders simply run an experiment by calling:</p>\n<pre><span class=\"err\">$</span><span class=\"o\">&gt;</span> <span class=\"n\">python</span> <span class=\"n\">tests</span><span class=\"o\">/</span><span class=\"n\">test_experiment</span><span class=\"o\">.</span><span class=\"n\">py</span>\n</pre>\n<p>It might be necessary to set your <code>PYTHONPATH</code> before running the experiment file:</p>\n<pre><span class=\"err\">$</span><span class=\"o\">&gt;</span> <span class=\"n\">PYTHONPATH</span><span class=\"o\">=.</span> <span class=\"n\">python</span> <span class=\"n\">tests</span><span class=\"o\">/</span><span class=\"n\">test_experiment</span><span class=\"o\">.</span><span class=\"n\">py</span>\n</pre>\n<p>One can also simply change the global device setting or initialize an experiment from command line by specifying theses arguments:</p>\n<pre><span class=\"err\">$</span><span class=\"o\">&gt;</span> <span class=\"n\">python</span> <span class=\"n\">tests</span><span class=\"o\">/</span><span class=\"n\">test_experiment</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"o\">--</span><span class=\"n\">device</span> <span class=\"n\">cuda</span><span class=\"p\">:</span><span class=\"mi\">1</span> <span class=\"o\">--</span><span class=\"n\">config</span> <span class=\"o\">&lt;</span><span class=\"n\">path</span><span class=\"o\">-</span><span class=\"n\">to</span><span class=\"o\">-</span><span class=\"n\">config</span><span class=\"o\">&gt;</span>\n</pre>\n<h2>Advanced</h2>\n<h3>Inheritance from base class</h3>\n<p>One can simply extend from any base class within the lighter framework:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">SimpleExperiment</span><span class=\"p\">(</span><span class=\"n\">DefaultExperiment</span><span class=\"p\">):</span>\n    <span class=\"nd\">@config</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'experiments/defaults.config.json'</span><span class=\"p\">,</span> <span class=\"nb\">property</span><span class=\"o\">=</span><span class=\"s1\">'experiments'</span><span class=\"p\">)</span>\n    <span class=\"nd\">@strategy</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"o\">=</span><span class=\"s1\">'configs/modules.config.json'</span><span class=\"p\">)</span>\n    <span class=\"nd\">@references</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">SimpleExperiment</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">experiments</span><span class=\"o\">.</span><span class=\"n\">num_epochs</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">eval</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"o\">...</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"o\">...</span>\n        <span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">metric</span><span class=\"p\">(</span><span class=\"n\">preds</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n        <span class=\"o\">...</span>\n</pre>\n<p>In this example we subclass the <code>DefaultExperiment</code> class and create a new experiment with some custom <code>eval()</code> and <code>train()</code> behaviour.\nThe <code>run()</code> work-flow is still used as defined in the base class <code>DefaultExperiment</code>.\nThe <code>@references</code> decorator injects by default some properties into the current <code>SimpleExperiment</code> object instance.\nIf not other specified the <code>@reference</code> tries to inject the properties <code>['dataset', 'data_builder', 'criterion', 'model', 'writer', 'optimizer', 'transfom', 'metric', 'collectible']</code> which in this case are declared in the <code>configs/modules.defaults.config.json</code> config.\nSince <code>model</code> and <code>metric</code> are injected via <code>@references</code> they can be directly accessed via the <code>self</code> instance.</p>\n<p>The <code>@config</code> decorator injects additional configs for the current experiment instance by referencing <code>defaults.config.json</code> with an <code>experiments</code> alias.\nAll configs are by default created as a sub-property of <code>self.config</code> and are now referencable through the <code>experiments</code> alias.\nOne can then access the properties as follows <code>self.config.experiments.&lt;property&gt;</code>.</p>\n<h3>Decorators</h3>\n<p>In general it is also possible to simply use the required decorators without base class sub-classing.</p>\n<p>There are currently the following decorators available:</p>\n<ul>\n<li><code>@config</code> - injects the default config instance and allows to load new configs</li>\n<li><code>@context</code> - injects the application context which holds config and registry instances</li>\n<li><code>@transform</code> - injects the default data transform instance</li>\n<li><code>@dataset</code> - injects the default dataset instance</li>\n<li><code>@model</code> - injects the default model instance</li>\n<li><code>@metric</code> - injects the default metric instance</li>\n<li><code>@reference</code> - injects a singe defined instance</li>\n<li><code>@references</code> - injects a set of pre-set or defined instance(s)</li>\n<li><code>@strategy</code> - defines and loads a training strategy from a specified config path and injects the default object instances</li>\n<li><code>@register</code> - allows to quickly register a new type to the context registry and inject the instance into the current instance</li>\n<li><code>@hook</code> - allows to hook and overwrite an existing method to change the execution logic</li>\n<li><code>@inject</code> - allows to inject single instances into different context options (registry, types, instances, configs)</li>\n<li><code>@search</code> - allows to register hyper-parameters to parse a config schedule for parallelled executions</li>\n</ul>\n<p>By default the templates for a new project are creating a config file in the <code>configs/modules.config.json</code> path, which uses the following default names for the dependency injection <code>['dataset', 'data_builder', 'criterion', 'model', 'writer', 'optimizer', 'transfom', 'metric', 'collectible']</code>.\nAn experiment gets these instances automatically instantiated and injected at runtime.</p>\n<pre><span class=\"p\">{</span>\n  <span class=\"nt\">\"strategy\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n      <span class=\"nt\">\"transform\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::transforms.defaults.Transform\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"dataset\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::datasets.defaults.Dataset\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"data_builder\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::data_builders.defaults.DataBuilder\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"criterion\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::criterions.defaults.Criterion\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"model\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::models.defaults.Model\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"optimizer\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::optimizers.defaults.Optimizer\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"metric\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::lighter.metric.BaseMetric\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"collectible\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::lighter.collectible.BaseCollectible\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"writer\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::lighter.writer.BaseWriter\"</span>\n  <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</pre>\n<p>The above example groups the types in the <code>strategy</code> alias name to avoid config name collisions if other classes import configs using the same general names (model, metric, etc.) across the application.</p>\n<p>It is also simple to deviate from this naming convention or even inject custom types by exchanging names within the referencing configs:</p>\n<pre><span class=\"p\">{</span>\n  <span class=\"err\">...</span>\n  <span class=\"nt\">\"other_model\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::models.other.Network\"</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"other_metric\"</span><span class=\"p\">:</span> <span class=\"s2\">\"type::metrics.other.Metric\"</span><span class=\"p\">,</span>\n  <span class=\"err\">...</span>\n<span class=\"p\">}</span>\n</pre>\n<p>The defined name <code>other_model_name</code> is now uniquely assigned in the instance context and can be accessed via <code>@references(names=['other_model', 'other_metric'])</code></p>\n<p>It is also possible to register types on the fly within the code at the class or method level:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">Demo</span><span class=\"p\">:</span>\n    <span class=\"nd\">@register</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"s1\">'envs.defaults.Environment'</span><span class=\"p\">,</span> <span class=\"nb\">property</span><span class=\"o\">=</span><span class=\"s1\">'env'</span><span class=\"p\">)</span>\n    <span class=\"nd\">@reference</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'env'</span><span class=\"p\">)</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"o\">...</span>\n        <span class=\"n\">obs</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n        <span class=\"o\">...</span>\n</pre>\n<p>Here the <code>@reference</code> decorator requires a name argument to match the instance.</p>\n<h3>Config templates</h3>\n<p>All configs are based on <code>json</code> files. Yet, when importing the configs <code>lighter</code> parses some escape sequences allowing for more modularization of your project structure:</p>\n<ul>\n<li><code>config::&lt;config-file-path&gt;</code> - imports another config into the current json config instance</li>\n<li><code>import::&lt;config-file-path&gt;</code> - imports all properties of the referenced config file into the current config instance (attention may override existing properties)</li>\n<li><code>type::&lt;type-file-path&gt;</code> - imports a python type which is registered to the application context <code>registry.types</code> and can be used to instantiate new objects</li>\n</ul>\n<h3>Parameter search parser</h3>\n<p>One can simply use lighter for searching hyper-parameters or different setting using the <code>@search</code> decorator, which registers parameters referenced by configs to the global context.\nThis generates a permutation of different settings using the parameter parser, which then allows for parallel execution either with the default built in scheduler or any other specialized framework for distributed optimization, such as <a href=\"https://github.com/ray-project/ray\" rel=\"nofollow\">ray</a>.</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">ParameterSearchRegistration</span><span class=\"p\">:</span>\n  <span class=\"nd\">@search</span><span class=\"p\">(</span><span class=\"n\">group</span><span class=\"o\">=</span><span class=\"s1\">'sgd'</span><span class=\"p\">,</span>\n          <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">'lr'</span><span class=\"p\">,</span> <span class=\"n\">GridParameter</span><span class=\"p\">(</span><span class=\"n\">ref</span><span class=\"o\">=</span><span class=\"s1\">'optimizer.lr'</span><span class=\"p\">,</span> <span class=\"nb\">min</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">,</span> <span class=\"nb\">max</span><span class=\"o\">=</span><span class=\"mf\">0.005</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">)),</span>\n                  <span class=\"p\">(</span><span class=\"s1\">'weight_decay'</span><span class=\"p\">,</span> <span class=\"n\">ListParameter</span><span class=\"p\">(</span><span class=\"n\">ref</span><span class=\"o\">=</span><span class=\"s1\">'optimizer.weight_decay'</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.9</span><span class=\"p\">])),</span>\n                  <span class=\"p\">(</span><span class=\"s1\">'freeze_pretrained'</span><span class=\"p\">,</span> <span class=\"n\">BinaryParameter</span><span class=\"p\">(</span><span class=\"n\">ref</span><span class=\"o\">=</span><span class=\"s1\">'model.freeze_pretrained'</span><span class=\"p\">)),</span>\n                  <span class=\"p\">(</span><span class=\"s1\">'hidden_units'</span><span class=\"p\">,</span> <span class=\"n\">GridParameter</span><span class=\"p\">(</span><span class=\"n\">ref</span><span class=\"o\">=</span><span class=\"s1\">'model.hidden_units'</span><span class=\"p\">,</span> <span class=\"nb\">min</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"nb\">max</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)),</span>\n                  <span class=\"p\">(</span><span class=\"s1\">'optimizer'</span><span class=\"p\">,</span> <span class=\"n\">SetParameter</span><span class=\"p\">(</span><span class=\"n\">ref</span><span class=\"o\">=</span><span class=\"s1\">'strategy.optimizer'</span><span class=\"p\">,</span> <span class=\"n\">option</span><span class=\"o\">=</span><span class=\"s2\">\"type::optimizers.defaults.Optimizer\"</span><span class=\"p\">)),</span>\n                  <span class=\"p\">(</span><span class=\"s1\">'model_output'</span><span class=\"p\">,</span> <span class=\"n\">SetParameter</span><span class=\"p\">(</span><span class=\"n\">ref</span><span class=\"o\">=</span><span class=\"s1\">'model.output'</span><span class=\"p\">,</span> <span class=\"n\">option</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)),</span>\n                  <span class=\"p\">(</span><span class=\"s1\">'model_pretrained'</span><span class=\"p\">,</span> <span class=\"n\">SetParameter</span><span class=\"p\">(</span><span class=\"n\">ref</span><span class=\"o\">=</span><span class=\"s1\">'model.pretrained'</span><span class=\"p\">,</span> <span class=\"n\">option</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)),</span>\n                  <span class=\"p\">(</span><span class=\"s1\">'strategy'</span><span class=\"p\">,</span> <span class=\"n\">StrategyParameter</span><span class=\"p\">(</span><span class=\"n\">ref</span><span class=\"o\">=</span><span class=\"s1\">'strategy'</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'searches/coco_looking.config.json'</span><span class=\"p\">])),</span>\n                  <span class=\"p\">(</span><span class=\"s1\">'model'</span><span class=\"p\">,</span> <span class=\"n\">ListParameter</span><span class=\"p\">(</span><span class=\"n\">ref</span><span class=\"o\">=</span><span class=\"s1\">'strategy.model'</span><span class=\"p\">,</span>\n                                          <span class=\"n\">options</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'type::models.alexnet.AlexNetFeatureExtractionModel'</span><span class=\"p\">,</span>\n                                                   <span class=\"s1\">'type::models.resnet.ResNetFeatureExtractionModel'</span><span class=\"p\">]))])</span>\n  <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n    <span class=\"k\">pass</span>\n</pre>\n<p>Here <code>group</code> marks the grouping alias under which the upcoming parameters are registered in the context.\nThe <code>params</code> argument takes in a list of two-tuples <code>(&lt;name: str&gt;, &lt;param: Parameter&gt;)</code>, where <code>name</code> marks the parameter alias within the group and <code>param</code> the type of search parameter used.</p>\n<p>Currently lighter offers the following parameter types:</p>\n<ul>\n<li><code>GridParameter</code>: Search a range of values (min, max, steps) with fixed step size</li>\n<li><code>ListParameter</code>: Commit a list of parameters to lighter which is sequentially executed</li>\n<li><code>CallableGridParameter</code>: Search a range of values (min, max, callable) but with a callable lambda function which allows for non-linear step behaviour</li>\n<li><code>AnnealParameter</code>: Same as <code>CallableGridParameter</code> but for annealing a parameter</li>\n<li><code>BinaryParameter</code>: Return <code>True</code> and <code>False</code> values for a parameter</li>\n<li><code>StrategyParameter</code>: Allows to define different training strategies and switch between them</li>\n<li><code>SetParameter</code>: Represents a simple setter of properties that is used to register settings, which don't change, but need to be imported</li>\n</ul>\n<p>To trigger the parameter config parsing run:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">searches.defaults</span> <span class=\"kn\">import</span> <span class=\"n\">ParameterSearchRegistration</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lighter.parser</span> <span class=\"kn\">import</span> <span class=\"n\">ConfigParser</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lighter.context</span> <span class=\"kn\">import</span> <span class=\"n\">Context</span>\n\n<span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"n\">Context</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">auto_instantiate_types</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">psr</span> <span class=\"o\">=</span> <span class=\"n\">ParameterSearchRegistration</span><span class=\"p\">()</span>\n<span class=\"n\">cp</span> <span class=\"o\">=</span> <span class=\"n\">ConfigParser</span><span class=\"p\">(</span><span class=\"n\">experiment</span><span class=\"o\">=</span><span class=\"n\">psr</span><span class=\"p\">)</span>\n<span class=\"n\">cp</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"p\">()</span>\n</pre>\n<p>By default this saves all permuted configs to the <code>runs/search/&lt;gen-name&gt;</code> directory.\nThe <code>auto_instantiate_types</code> parameter prevents that the context instantiates the registered types when loading in configs.</p>\n<h3>Parameter search scheduler</h3>\n<p>Lighter comes with a built in config scheduler that can execute experiments in parallel for single machine experiments.\nFor many prototyping cases this is sufficient and helps during development, but naturally flexibility is highest priority and it is possible and recommended to use 3rd party frameworks to schedule decentralized parallel executions on clusters of machines.</p>\n<p>For the simple case,</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lighter.scheduler</span> <span class=\"kn\">import</span> <span class=\"n\">Scheduler</span>\n\n<span class=\"c1\"># required to execute torch in parallel processes</span>\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">multiprocessing</span><span class=\"o\">.</span><span class=\"n\">set_start_method</span><span class=\"p\">(</span><span class=\"s1\">'spawn'</span><span class=\"p\">,</span> <span class=\"n\">force</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">num_devices</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()</span>\n<span class=\"c1\"># sets the path to the list of configs</span>\n<span class=\"n\">scheduler</span> <span class=\"o\">=</span> <span class=\"n\">Scheduler</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'runs/search/witty-poodle'</span><span class=\"p\">,</span>\n                      <span class=\"n\">experiment</span><span class=\"o\">=</span><span class=\"s1\">'experiments.defaults.Experiment'</span><span class=\"p\">,</span>\n                      <span class=\"n\">device_name</span><span class=\"o\">=</span><span class=\"s1\">'cuda'</span><span class=\"p\">,</span>\n                      <span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"n\">num_devices</span><span class=\"p\">)</span>\n<span class=\"n\">scheduler</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n<p>The scheduler in the code snipped above starts as many processes as there are GPUs available and schedules the list of configs across these devices.\nSince we defined a simple writer, one can simply monitor the progress using tensorboard.</p>\n\n          </div>"}, "last_serial": 6979350, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "346178ea3496abdc14a7d5b8c78b9886", "sha256": "945fa044a5e16efb2704d31f97fc723f3e0055855f73829fedabffd48d1b1b01"}, "downloads": -1, "filename": "torch_lighter-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "346178ea3496abdc14a7d5b8c78b9886", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 45280, "upload_time": "2019-11-15T17:27:53", "upload_time_iso_8601": "2019-11-15T17:27:53.386578Z", "url": "https://files.pythonhosted.org/packages/ce/40/8805a8710ae1129ed0e6d58f9f60de95ce392d476b7bc6ba02b6a0d5723d/torch_lighter-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d1f930a60dae7e9654ca41f053c555c4", "sha256": "c68f36a7d0289416bdf7d2a5d2506518513dd59e86f2cf9b1a4e062c75c70a2e"}, "downloads": -1, "filename": "torch-lighter-0.1.0.tar.gz", "has_sig": false, "md5_digest": "d1f930a60dae7e9654ca41f053c555c4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21716, "upload_time": "2019-11-15T17:27:55", "upload_time_iso_8601": "2019-11-15T17:27:55.867136Z", "url": "https://files.pythonhosted.org/packages/5c/f6/728d7a773c9801e5228dd64fbcd1330b02915fc076a0f3158e865b6725d7/torch-lighter-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "2bfa37ee3c5b0bd5a96e8c5ea15aa23e", "sha256": "4cd23d6502e0d4186068ef2aa5a049662eb394355b536c4579f6997486dc4902"}, "downloads": -1, "filename": "torch_lighter-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "2bfa37ee3c5b0bd5a96e8c5ea15aa23e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 47999, "upload_time": "2019-11-19T21:44:41", "upload_time_iso_8601": "2019-11-19T21:44:41.821718Z", "url": "https://files.pythonhosted.org/packages/a9/85/80c779a124e71f503d8203071d0282cbaeb4f97a855f9ebf1a0be1ea1e35/torch_lighter-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2c4d49b11c0b9c291aa0b71f6000c8ed", "sha256": "d183e2f8cc29a147a5c81dee2e3d5e8f302595708007e2ac8afc927cfdc00def"}, "downloads": -1, "filename": "torch-lighter-0.1.1.tar.gz", "has_sig": false, "md5_digest": "2c4d49b11c0b9c291aa0b71f6000c8ed", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23366, "upload_time": "2019-11-19T21:44:43", "upload_time_iso_8601": "2019-11-19T21:44:43.738852Z", "url": "https://files.pythonhosted.org/packages/e9/50/f5c7c7a8b265b57e1ff4ea164e85dc693a1180b4069658f3d34d07cc9719/torch-lighter-0.1.1.tar.gz", "yanked": false}], "0.2.10": [{"comment_text": "", "digests": {"md5": "1c5b14ac10aee60258d64247a415903f", "sha256": "4296e8de7b864d4a44e9adee090246b754a6ef9d6e4c86ac2f2b299894ef1e9d"}, "downloads": -1, "filename": "torch_lighter-0.2.10-py3-none-any.whl", "has_sig": false, "md5_digest": "1c5b14ac10aee60258d64247a415903f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 58317, "upload_time": "2019-12-18T15:01:29", "upload_time_iso_8601": "2019-12-18T15:01:29.380040Z", "url": "https://files.pythonhosted.org/packages/1b/e2/2c0c3c9a32120e38a0dae513cd0aea61466861529e11ccb3f1d0c2777550/torch_lighter-0.2.10-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9e4d18120b12efb9ff133df03464b504", "sha256": "6dbe49cfd2787b573416b6d82f13c6574a1bf4183a86a0f2f96876e3322b3dac"}, "downloads": -1, "filename": "torch-lighter-0.2.10.tar.gz", "has_sig": false, "md5_digest": "9e4d18120b12efb9ff133df03464b504", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31958, "upload_time": "2019-12-18T15:01:31", "upload_time_iso_8601": "2019-12-18T15:01:31.030891Z", "url": "https://files.pythonhosted.org/packages/00/2a/eb2c9a3ed823cb245efd4f0fc0e4851b3f1d96e1c015d55ef36582474556/torch-lighter-0.2.10.tar.gz", "yanked": false}], "0.2.11": [{"comment_text": "", "digests": {"md5": "d82723b437fbba862ff6ae63821fe272", "sha256": "057bd73b27aecb97af72abb9c3ddc9e2c0b601dd64f39831802cd3dbbd17be88"}, "downloads": -1, "filename": "torch_lighter-0.2.11-py3-none-any.whl", "has_sig": false, "md5_digest": "d82723b437fbba862ff6ae63821fe272", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 58316, "upload_time": "2019-12-18T15:05:44", "upload_time_iso_8601": "2019-12-18T15:05:44.927131Z", "url": "https://files.pythonhosted.org/packages/bc/d4/63e583d41250570607500cafd7ad7fa07a68684cb0d4c5994a09d5de717c/torch_lighter-0.2.11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2b83fdbc4be73f7d0d4c2782c622282f", "sha256": "1b95dec81ce8e437499427021b1d649869e4e285c595ed16615017687036a72c"}, "downloads": -1, "filename": "torch-lighter-0.2.11.tar.gz", "has_sig": false, "md5_digest": "2b83fdbc4be73f7d0d4c2782c622282f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31957, "upload_time": "2019-12-18T15:05:46", "upload_time_iso_8601": "2019-12-18T15:05:46.495525Z", "url": "https://files.pythonhosted.org/packages/a7/2c/9ade801c974085bd806955c0edeb70e59c9a5b94f625b86244ac8f45c391/torch-lighter-0.2.11.tar.gz", "yanked": false}], "0.2.12": [{"comment_text": "", "digests": {"md5": "0d2f4d864cde351edcbb21ba90ba67ea", "sha256": "f55d5a3cd285fd51a6f8a02b41786850c4cb0e4114940be139c053b34a3f4813"}, "downloads": -1, "filename": "torch_lighter-0.2.12-py3-none-any.whl", "has_sig": false, "md5_digest": "0d2f4d864cde351edcbb21ba90ba67ea", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 58445, "upload_time": "2020-01-09T09:58:34", "upload_time_iso_8601": "2020-01-09T09:58:34.684810Z", "url": "https://files.pythonhosted.org/packages/46/ee/7d2f8aceb4f46834a0e492ab08e1cd94f1a295b9ab2c42fe354be9564d8b/torch_lighter-0.2.12-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f70474d0028f0e80acc59f707881b4f0", "sha256": "94ee7aae3f9283faf8e512316dd01d114496b660058c84ec5ffe8795d00ce9a3"}, "downloads": -1, "filename": "torch-lighter-0.2.12.tar.gz", "has_sig": false, "md5_digest": "f70474d0028f0e80acc59f707881b4f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32153, "upload_time": "2020-01-09T09:58:37", "upload_time_iso_8601": "2020-01-09T09:58:37.374778Z", "url": "https://files.pythonhosted.org/packages/82/7e/1aa39a3bb9cf4f6d79936bbf9756f0759f5108975d37cce1613cc721e815/torch-lighter-0.2.12.tar.gz", "yanked": false}], "0.2.14": [{"comment_text": "", "digests": {"md5": "c3a00261e2069068afbe74bba0fd5fd9", "sha256": "6c7f8965daa7a8b34edb47354966019a04d46344b94d94f159405917b03824ee"}, "downloads": -1, "filename": "torch_lighter-0.2.14-py3-none-any.whl", "has_sig": false, "md5_digest": "c3a00261e2069068afbe74bba0fd5fd9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 58566, "upload_time": "2020-01-14T15:32:20", "upload_time_iso_8601": "2020-01-14T15:32:20.397166Z", "url": "https://files.pythonhosted.org/packages/e5/30/399cc7bd265acea459f94c2e8692a0410510e146f62f2dc3649c4db0a92a/torch_lighter-0.2.14-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9a7ea2afc76f464d99a6e2f5fa6d92b3", "sha256": "ef78c001d5b3c5209794fc2b0268c88ea404c28686e43d36590fe26caa3fb381"}, "downloads": -1, "filename": "torch-lighter-0.2.14.tar.gz", "has_sig": false, "md5_digest": "9a7ea2afc76f464d99a6e2f5fa6d92b3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32273, "upload_time": "2020-01-14T15:32:22", "upload_time_iso_8601": "2020-01-14T15:32:22.293875Z", "url": "https://files.pythonhosted.org/packages/a4/6d/a0c575699451d76a372f91b61f5a091ed11c635e674de013c358270c0249/torch-lighter-0.2.14.tar.gz", "yanked": false}], "0.2.16": [{"comment_text": "", "digests": {"md5": "7ff3a0fbf866fe070a7e1fd5ff947111", "sha256": "3249933a8f435e3c2b04f749199ed34973433509ac64b6756b08a54137868ab4"}, "downloads": -1, "filename": "torch_lighter-0.2.16-py3-none-any.whl", "has_sig": false, "md5_digest": "7ff3a0fbf866fe070a7e1fd5ff947111", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 58618, "upload_time": "2020-01-15T12:37:53", "upload_time_iso_8601": "2020-01-15T12:37:53.171986Z", "url": "https://files.pythonhosted.org/packages/cc/53/64f3b6843e16866833f132a6dacc82f3d6eb63ecdbcce7f8bcdc2a456376/torch_lighter-0.2.16-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "90f6db6218bfe4261d38e83410b12b86", "sha256": "815561af6642da21a5d5071771d2ed403458af90ae4123e81eefe1ff2e783bc5"}, "downloads": -1, "filename": "torch-lighter-0.2.16.tar.gz", "has_sig": false, "md5_digest": "90f6db6218bfe4261d38e83410b12b86", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32317, "upload_time": "2020-01-15T12:37:55", "upload_time_iso_8601": "2020-01-15T12:37:55.016644Z", "url": "https://files.pythonhosted.org/packages/4d/d4/acfbc3f36f2cfc73f25ebd3955821f779b82f712d506103b96d815e3c2bb/torch-lighter-0.2.16.tar.gz", "yanked": false}], "0.2.17": [{"comment_text": "", "digests": {"md5": "c09d200fec602fa94fa6ff1990b16ce1", "sha256": "43715a6a13fbcc7d743ee40e560d18530fe014555d2cf0faf1406b29724c3607"}, "downloads": -1, "filename": "torch_lighter-0.2.17-py3-none-any.whl", "has_sig": false, "md5_digest": "c09d200fec602fa94fa6ff1990b16ce1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 58621, "upload_time": "2020-01-16T15:51:14", "upload_time_iso_8601": "2020-01-16T15:51:14.923404Z", "url": "https://files.pythonhosted.org/packages/e2/f9/db6a975b3a7007594c8b3f7fd1fed9189fa8d1700790f5747f053c31962e/torch_lighter-0.2.17-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "01655d9bf5259f9dbadd8e62469e39cd", "sha256": "c3e64c82b2f4433d165a8fef63fc194da68488b5f6a5e8dddb8ba2dcab043925"}, "downloads": -1, "filename": "torch-lighter-0.2.17.tar.gz", "has_sig": false, "md5_digest": "01655d9bf5259f9dbadd8e62469e39cd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32327, "upload_time": "2020-01-16T15:51:16", "upload_time_iso_8601": "2020-01-16T15:51:16.682992Z", "url": "https://files.pythonhosted.org/packages/84/94/29f311f7de868c3f8fd0ab468fa6696342fe7f78b7de780f2293cf5ba684/torch-lighter-0.2.17.tar.gz", "yanked": false}], "0.2.18": [{"comment_text": "", "digests": {"md5": "522c0a0a6be7ee318d22d2450cb597c0", "sha256": "8cf082b8d3066fa900ee205fe16ab4c4bcd6abc94aac349606b141cd49a799d4"}, "downloads": -1, "filename": "torch_lighter-0.2.18-py3-none-any.whl", "has_sig": false, "md5_digest": "522c0a0a6be7ee318d22d2450cb597c0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 58818, "upload_time": "2020-01-17T11:17:37", "upload_time_iso_8601": "2020-01-17T11:17:37.082395Z", "url": "https://files.pythonhosted.org/packages/b9/4e/5ce1a87e10b9d913744a0f55db5f657377f4d3602bb87696bf4319b897cb/torch_lighter-0.2.18-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d92afab09691a1d812fada70456ed6e2", "sha256": "8a495481e01b21bd57eee05d744ff4d985c22abb8e890700d910de29e629def4"}, "downloads": -1, "filename": "torch-lighter-0.2.18.tar.gz", "has_sig": false, "md5_digest": "d92afab09691a1d812fada70456ed6e2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32524, "upload_time": "2020-01-17T11:17:39", "upload_time_iso_8601": "2020-01-17T11:17:39.342028Z", "url": "https://files.pythonhosted.org/packages/95/7e/56b8f3a05eb34282e743c419a49833b2cc654d6814bc5738a177dd214545/torch-lighter-0.2.18.tar.gz", "yanked": false}], "0.2.19": [{"comment_text": "", "digests": {"md5": "e635510ace1b77cea39f1695cc85392a", "sha256": "2f817ba8838c14a01d541e478b08e4db271e61c58d750034336c26303dd62e49"}, "downloads": -1, "filename": "torch_lighter-0.2.19-py3-none-any.whl", "has_sig": false, "md5_digest": "e635510ace1b77cea39f1695cc85392a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 59183, "upload_time": "2020-01-23T11:59:01", "upload_time_iso_8601": "2020-01-23T11:59:01.158436Z", "url": "https://files.pythonhosted.org/packages/b7/25/b5f7f43cf645c3532bd6a75cb260a6950736b15ae41acc1bf73a72f8dce4/torch_lighter-0.2.19-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0ce9b8d9b63802e823cdedabaf65793a", "sha256": "7357f90951783686e7da0864b99c1f94de1e3c902954961d978609bfdeee07cd"}, "downloads": -1, "filename": "torch-lighter-0.2.19.tar.gz", "has_sig": false, "md5_digest": "0ce9b8d9b63802e823cdedabaf65793a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32864, "upload_time": "2020-01-23T11:59:03", "upload_time_iso_8601": "2020-01-23T11:59:03.519110Z", "url": "https://files.pythonhosted.org/packages/c8/94/3eccf421db4254719900053bda79640a4bcace9249e18c10ade84b89726e/torch-lighter-0.2.19.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "26714bb6fffe751f757e6d549a6ca4f4", "sha256": "b384a4a071f41ee9971a7f1c629ce0e6198bce669f4efd21be42405d4ec552df"}, "downloads": -1, "filename": "torch_lighter-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "26714bb6fffe751f757e6d549a6ca4f4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 52815, "upload_time": "2019-11-25T15:06:04", "upload_time_iso_8601": "2019-11-25T15:06:04.609830Z", "url": "https://files.pythonhosted.org/packages/11/5a/ec7df61a9b2ed5f25fad2e261cc792609cc9263bb7f045a40445fea049c4/torch_lighter-0.2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5bd106f3d7d27f8e91484440eb05a74c", "sha256": "545dc6647b5ae57bb03a2ef2a15558021ce6674be635b1b7c0952409d902bc21"}, "downloads": -1, "filename": "torch-lighter-0.2.2.tar.gz", "has_sig": false, "md5_digest": "5bd106f3d7d27f8e91484440eb05a74c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29560, "upload_time": "2019-11-25T15:06:06", "upload_time_iso_8601": "2019-11-25T15:06:06.142959Z", "url": "https://files.pythonhosted.org/packages/19/e7/489eecb9934834b64c0ce778df0cda15b7b38fd2274aaa97414964629f80/torch-lighter-0.2.2.tar.gz", "yanked": false}], "0.2.20": [{"comment_text": "", "digests": {"md5": "86f4e14da0c0cedc4ed7a95e0ff73799", "sha256": "946868cb89914586af08e189097a9df687ec68be74c43566eaf6a2892d79b3e1"}, "downloads": -1, "filename": "torch_lighter-0.2.20-py3-none-any.whl", "has_sig": false, "md5_digest": "86f4e14da0c0cedc4ed7a95e0ff73799", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 59812, "upload_time": "2020-03-10T12:21:51", "upload_time_iso_8601": "2020-03-10T12:21:51.708082Z", "url": "https://files.pythonhosted.org/packages/46/ce/dc177e5c31f74f1bc246c414e17d75df82c654034234cd5f0b0a4a469673/torch_lighter-0.2.20-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e588e69ecda5e086cdae4484897bcc1a", "sha256": "44730b2bf2254cd82f390e1272ff77b5c99d0ea01992e831b3b5d81404b6616c"}, "downloads": -1, "filename": "torch-lighter-0.2.20.tar.gz", "has_sig": false, "md5_digest": "e588e69ecda5e086cdae4484897bcc1a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 33039, "upload_time": "2020-03-10T12:21:53", "upload_time_iso_8601": "2020-03-10T12:21:53.490782Z", "url": "https://files.pythonhosted.org/packages/bc/00/c4c041e81f78df5ce22cb5bc93855c2e21cf8b519acb33010b1831375842/torch-lighter-0.2.20.tar.gz", "yanked": false}], "0.2.22": [{"comment_text": "", "digests": {"md5": "ab7aa624875c6f939af94f0b94f2824e", "sha256": "b79cac464e28ce5c7dd080a6f54d47514ef03a4989eefc5e2b8e4abb29f457a5"}, "downloads": -1, "filename": "torch_lighter-0.2.22-py3-none-any.whl", "has_sig": false, "md5_digest": "ab7aa624875c6f939af94f0b94f2824e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 61550, "upload_time": "2020-03-28T00:19:43", "upload_time_iso_8601": "2020-03-28T00:19:43.352860Z", "url": "https://files.pythonhosted.org/packages/53/cd/fdbd055c6b3d53e00d89ba398843fdde115b3eae736dfc06b27c7f02caf5/torch_lighter-0.2.22-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7465b25b3e0959e114b88ecd6e2e54fd", "sha256": "691bb7c06f6e4faa847ad53edcbfadafb0dda701010a7059c1eef5371cae9ea1"}, "downloads": -1, "filename": "torch-lighter-0.2.22.tar.gz", "has_sig": false, "md5_digest": "7465b25b3e0959e114b88ecd6e2e54fd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 34443, "upload_time": "2020-03-28T00:19:44", "upload_time_iso_8601": "2020-03-28T00:19:44.937154Z", "url": "https://files.pythonhosted.org/packages/db/fb/521aedd73467d164e8c5cf794a370fefde2b83b66832b375306c5601f280/torch-lighter-0.2.22.tar.gz", "yanked": false}], "0.2.23": [{"comment_text": "", "digests": {"md5": "8051c1b6c7357f8cbbdcdeedc62c77a3", "sha256": "124e7c7c82012a927786b2dfd76c0aeceb019257968b29df7a8fcda28bd74956"}, "downloads": -1, "filename": "torch_lighter-0.2.23-py3-none-any.whl", "has_sig": false, "md5_digest": "8051c1b6c7357f8cbbdcdeedc62c77a3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 60698, "upload_time": "2020-04-08T17:21:40", "upload_time_iso_8601": "2020-04-08T17:21:40.655981Z", "url": "https://files.pythonhosted.org/packages/8a/39/a6f62c225a60ec2bd591a97a5a696f58f9b35a079226a40aa15b59682350/torch_lighter-0.2.23-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d86735acf1a0d1ca56b966be57e1d9b4", "sha256": "2448e291e83a4569e3b8f4991c354ed2a43cf0e0d05821f77bb44cba43120301"}, "downloads": -1, "filename": "torch-lighter-0.2.23.tar.gz", "has_sig": false, "md5_digest": "d86735acf1a0d1ca56b966be57e1d9b4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 33981, "upload_time": "2020-04-08T17:21:41", "upload_time_iso_8601": "2020-04-08T17:21:41.893957Z", "url": "https://files.pythonhosted.org/packages/0d/9b/6810078587273e0328211ca030ee2ccb8f3beb41bec8c60f3bed0c9e712f/torch-lighter-0.2.23.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "e6340e0c4eabef9339f7ae7504a410d3", "sha256": "4bcc6eefda87daeb2d4d034d5e7e22ceea15a1336c1ca492d9c9491bba6b9553"}, "downloads": -1, "filename": "torch_lighter-0.2.3-py3-none-any.whl", "has_sig": false, "md5_digest": "e6340e0c4eabef9339f7ae7504a410d3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 52932, "upload_time": "2019-11-26T13:14:11", "upload_time_iso_8601": "2019-11-26T13:14:11.445042Z", "url": "https://files.pythonhosted.org/packages/c8/c4/4357483e5301583756a827c82d9d8bf5989bfaeed7f3569b5a2ebe3e1348/torch_lighter-0.2.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7c08331f6f97d495868fc515fe44bd57", "sha256": "2e306caa9db9159785ea3363f4e3966e10ed68f089004392e6b76a568565b5de"}, "downloads": -1, "filename": "torch-lighter-0.2.3.tar.gz", "has_sig": false, "md5_digest": "7c08331f6f97d495868fc515fe44bd57", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29648, "upload_time": "2019-11-26T13:14:12", "upload_time_iso_8601": "2019-11-26T13:14:12.855233Z", "url": "https://files.pythonhosted.org/packages/b2/f8/d2e2af05faac6d890b5faef1d7790976ff82a135bf60fdb7cd788b2ce8ff/torch-lighter-0.2.3.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "161cefb8b5834ebd0254fadff7e4b583", "sha256": "28e73f2db1fd1e4d8dbe16d7328877c3ed77967e320c25d91f6577f0f8f5be01"}, "downloads": -1, "filename": "torch_lighter-0.2.4-py3-none-any.whl", "has_sig": false, "md5_digest": "161cefb8b5834ebd0254fadff7e4b583", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 52972, "upload_time": "2019-11-26T16:56:42", "upload_time_iso_8601": "2019-11-26T16:56:42.059779Z", "url": "https://files.pythonhosted.org/packages/48/22/5a1bd0ac07a0f26ff1987168288e3c2fa5ebe8a33373283f1c048846d9c4/torch_lighter-0.2.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0c2a1cd2b7877ddedf56194812b9e96c", "sha256": "8939ba2a7050225d7f955f1c647a83c0917a63e79d11e59bab737a597eb93a75"}, "downloads": -1, "filename": "torch-lighter-0.2.4.tar.gz", "has_sig": false, "md5_digest": "0c2a1cd2b7877ddedf56194812b9e96c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29696, "upload_time": "2019-11-26T16:56:43", "upload_time_iso_8601": "2019-11-26T16:56:43.864461Z", "url": "https://files.pythonhosted.org/packages/fc/45/23249df4661094574af8bd0a9aad0183c605218113f266cc616670336008/torch-lighter-0.2.4.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "b9bbfa5205e88cb078112a9aea18a1d6", "sha256": "2123baf1958cd47923d527148982380b8c96acf96508c5d15107276e24ecaca7"}, "downloads": -1, "filename": "torch_lighter-0.2.5-py3-none-any.whl", "has_sig": false, "md5_digest": "b9bbfa5205e88cb078112a9aea18a1d6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 53407, "upload_time": "2019-11-27T12:45:00", "upload_time_iso_8601": "2019-11-27T12:45:00.909691Z", "url": "https://files.pythonhosted.org/packages/8b/74/276a9050b98f34e4ba789786c847068042e745d185a0aa55d7ece9d8c092/torch_lighter-0.2.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6cb3783c5c676082986413d8d036b027", "sha256": "0de657e4c7b5165c15eb58ae9b8dc01e6dd4a90a3e5f0c4b326c9c6eb4edaac8"}, "downloads": -1, "filename": "torch-lighter-0.2.5.tar.gz", "has_sig": false, "md5_digest": "6cb3783c5c676082986413d8d036b027", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29938, "upload_time": "2019-11-27T12:45:02", "upload_time_iso_8601": "2019-11-27T12:45:02.412747Z", "url": "https://files.pythonhosted.org/packages/ab/45/936492112b7154176a18585f21a8713532a212fb14a565e979a9c66be688/torch-lighter-0.2.5.tar.gz", "yanked": false}], "0.2.6": [{"comment_text": "", "digests": {"md5": "f8847037a1a98f37d5dae8b368258cef", "sha256": "dadee229eca72b56f0e712c789b3795e7815071ffd629b12fa77592f6ee1546f"}, "downloads": -1, "filename": "torch_lighter-0.2.6-py3-none-any.whl", "has_sig": false, "md5_digest": "f8847037a1a98f37d5dae8b368258cef", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 53345, "upload_time": "2019-11-29T09:09:45", "upload_time_iso_8601": "2019-11-29T09:09:45.138370Z", "url": "https://files.pythonhosted.org/packages/2a/6d/18db7881fbfaab0ae14cc0772e2dad93c8ba87a1d0d2a8ad02839b7446df/torch_lighter-0.2.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2ce47a7bb4b422ad5677fe7888e8aeaf", "sha256": "fbfe093d5c0cd887cad0d3d64abfed0967b5ca22bca205a2256fc98230036e1f"}, "downloads": -1, "filename": "torch-lighter-0.2.6.tar.gz", "has_sig": false, "md5_digest": "2ce47a7bb4b422ad5677fe7888e8aeaf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29901, "upload_time": "2019-11-29T09:09:46", "upload_time_iso_8601": "2019-11-29T09:09:46.490781Z", "url": "https://files.pythonhosted.org/packages/d7/84/f5d67c6ab5d6453121c35098928940aa350f0ab2634134e7f325077bf029/torch-lighter-0.2.6.tar.gz", "yanked": false}], "0.2.7": [{"comment_text": "", "digests": {"md5": "4d7e7a045a99501d7c6e3977d9019482", "sha256": "cf14f54af082725e385dccad34d26a6ff7e268861d26aaa201b2134546cffb30"}, "downloads": -1, "filename": "torch_lighter-0.2.7-py3-none-any.whl", "has_sig": false, "md5_digest": "4d7e7a045a99501d7c6e3977d9019482", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 53368, "upload_time": "2019-12-01T14:43:41", "upload_time_iso_8601": "2019-12-01T14:43:41.295698Z", "url": "https://files.pythonhosted.org/packages/19/2a/f7771aeca9272ed1947f12b38a68a7eeae92fbfd97e882bbdd4c43d1b83a/torch_lighter-0.2.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b408585185f636acede25b03eb0e7daf", "sha256": "eba4d1ead8fd13c60fecbd38a18ca26c7f2109b1f4b2b2f9dbe19d9777cd1a1d"}, "downloads": -1, "filename": "torch-lighter-0.2.7.tar.gz", "has_sig": false, "md5_digest": "b408585185f636acede25b03eb0e7daf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29917, "upload_time": "2019-12-01T14:43:43", "upload_time_iso_8601": "2019-12-01T14:43:43.788402Z", "url": "https://files.pythonhosted.org/packages/b8/d8/4e20d005d028b5502e277193f27b488d1c73c32a8292e79c316d5243d63f/torch-lighter-0.2.7.tar.gz", "yanked": false}], "0.2.8": [{"comment_text": "", "digests": {"md5": "cf6ae7c0489d35423c98493c65fe92e4", "sha256": "c3fc5f1a0117fbcb847598b1995ea9df67425b4db397da1723d90736a6eebbf7"}, "downloads": -1, "filename": "torch_lighter-0.2.8-py3-none-any.whl", "has_sig": false, "md5_digest": "cf6ae7c0489d35423c98493c65fe92e4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 55286, "upload_time": "2019-12-02T12:55:44", "upload_time_iso_8601": "2019-12-02T12:55:44.156453Z", "url": "https://files.pythonhosted.org/packages/ee/37/c4897c65a9bd02cbbda724afbe4d0a57828c9f342c5d49ab83a0ab5c77db/torch_lighter-0.2.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "30ae15d875eab61ccedb2116c060c34c", "sha256": "135505965012faca213823d26606d39d5ecae41ded7c307621083ed22a831b49"}, "downloads": -1, "filename": "torch-lighter-0.2.8.tar.gz", "has_sig": false, "md5_digest": "30ae15d875eab61ccedb2116c060c34c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31472, "upload_time": "2019-12-02T12:55:46", "upload_time_iso_8601": "2019-12-02T12:55:46.098731Z", "url": "https://files.pythonhosted.org/packages/d2/a6/d5dd897d5d44cfe72ad940bb252ded6db773fe2382e399c30573b566c300/torch-lighter-0.2.8.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8051c1b6c7357f8cbbdcdeedc62c77a3", "sha256": "124e7c7c82012a927786b2dfd76c0aeceb019257968b29df7a8fcda28bd74956"}, "downloads": -1, "filename": "torch_lighter-0.2.23-py3-none-any.whl", "has_sig": false, "md5_digest": "8051c1b6c7357f8cbbdcdeedc62c77a3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 60698, "upload_time": "2020-04-08T17:21:40", "upload_time_iso_8601": "2020-04-08T17:21:40.655981Z", "url": "https://files.pythonhosted.org/packages/8a/39/a6f62c225a60ec2bd591a97a5a696f58f9b35a079226a40aa15b59682350/torch_lighter-0.2.23-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d86735acf1a0d1ca56b966be57e1d9b4", "sha256": "2448e291e83a4569e3b8f4991c354ed2a43cf0e0d05821f77bb44cba43120301"}, "downloads": -1, "filename": "torch-lighter-0.2.23.tar.gz", "has_sig": false, "md5_digest": "d86735acf1a0d1ca56b966be57e1d9b4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 33981, "upload_time": "2020-04-08T17:21:41", "upload_time_iso_8601": "2020-04-08T17:21:41.893957Z", "url": "https://files.pythonhosted.org/packages/0d/9b/6810078587273e0328211ca030ee2ccb8f3beb41bec8c60f3bed0c9e712f/torch-lighter-0.2.23.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:20 2020"}