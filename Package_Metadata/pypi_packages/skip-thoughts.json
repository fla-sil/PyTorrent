{"info": {"author": "Julien Nyambal", "author_email": "jnyambal@retrorabbit.co.za", "bugtrack_url": null, "classifiers": [], "description": "# Skip-Thought Vectors\n\nThis is a TensorFlow implementation of the model described in:\n\nJamie Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel,\nAntonio Torralba, Raquel Urtasun, Sanja Fidler.\n[Skip-Thought Vectors](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf).\n*In NIPS, 2015.*\n\n\n## Contact\n***Code author:*** Chris Shallue\n\n***Pull requests and issues:*** @cshallue\n\n## Contents\n* [Model Overview](#model-overview)\n* [Getting Started](#getting-started)\n    * [Install Required Packages](#install-required-packages)\n    * [Download Pretrained Models (Optional)](#download-pretrained-models-optional)\n* [Training a Model](#training-a-model)\n    * [Prepare the Training Data](#prepare-the-training-data)\n    * [Run the Training Script](#run-the-training-script)\n    * [Track Training Progress](#track-training-progress)\n* [Expanding the Vocabulary](#expanding-the-vocabulary)\n    * [Overview](#overview)\n    * [Preparation](#preparation)\n    * [Run the Vocabulary Expansion Script](#run-the-vocabulary-expansion-script)\n* [Evaluating a Model](#evaluating-a-model)\n    * [Overview](#overview-1)\n    * [Preparation](#preparation-1)\n    * [Run the Evaluation Tasks](#run-the-evaluation-tasks)\n* [Encoding Sentences](#encoding-sentences)\n\n## Model overview\n\nThe *Skip-Thoughts* model is a sentence encoder. It learns to encode input\nsentences into a fixed-dimensional vector representation that is useful for many\ntasks, for example to detect paraphrases or to classify whether a product review\nis positive or negative. See the\n[Skip-Thought Vectors](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf)\npaper for details of the model architecture and more example applications.\n\nA trained *Skip-Thoughts* model will encode similar sentences nearby each other\nin the embedding vector space. The following examples show the nearest neighbor by\ncosine similarity of some sentences from the\n[movie review dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/).\n\n\n| Input sentence | Nearest Neighbor |\n|----------------|------------------|\n| Simplistic, silly and tedious. | Trite, banal, cliched, mostly inoffensive. |\n| Not so much farcical as sour. | Not only unfunny, but downright repellent. |\n| A sensitive and astute first feature by Anne-Sophie Birot. | Absorbing character study by Andr\u00e9 Turpin . |\n| An enthralling, entertaining feature. |  A slick, engrossing melodrama. |\n\n## Getting Started\n\n### Install Required Packages\nFirst ensure that you have installed the following required packages:\n\n* **Bazel** ([instructions](http://bazel.build/docs/install.html))\n* **TensorFlow** ([instructions](https://www.tensorflow.org/install/))\n* **NumPy** ([instructions](http://www.scipy.org/install.html))\n* **scikit-learn** ([instructions](http://scikit-learn.org/stable/install.html))\n* **Natural Language Toolkit (NLTK)**\n    * First install NLTK ([instructions](http://www.nltk.org/install.html))\n    * Then install the NLTK data ([instructions](http://www.nltk.org/data.html))\n* **gensim** ([instructions](https://radimrehurek.com/gensim/install.html))\n    * Only required if you will be expanding your vocabulary with the [word2vec](https://code.google.com/archive/p/word2vec/) model.\n\n\n### Download Pretrained Models (Optional)\n\nYou can download model checkpoints pretrained on the\n[BookCorpus](http://yknzhu.wixsite.com/mbweb) dataset in the following\nconfigurations:\n\n* Unidirectional RNN encoder (\"uni-skip\" in the paper)\n* Bidirectional RNN encoder (\"bi-skip\" in the paper)\n\n```shell\n# Directory to download the pretrained models to.\nPRETRAINED_MODELS_DIR=\"${HOME}/skip_thoughts/pretrained/\"\n\nmkdir -p ${PRETRAINED_MODELS_DIR}\ncd ${PRETRAINED_MODELS_DIR}\n\n# Download and extract the unidirectional model.\nwget \"http://download.tensorflow.org/models/skip_thoughts_uni_2017_02_02.tar.gz\"\ntar -xvf skip_thoughts_uni_2017_02_02.tar.gz\nrm skip_thoughts_uni_2017_02_02.tar.gz\n\n# Download and extract the bidirectional model.\nwget \"http://download.tensorflow.org/models/skip_thoughts_bi_2017_02_16.tar.gz\"\ntar -xvf skip_thoughts_bi_2017_02_16.tar.gz\nrm skip_thoughts_bi_2017_02_16.tar.gz\n```\n\nYou can now skip to the sections [Evaluating a Model](#evaluating-a-model) and\n[Encoding Sentences](#encoding-sentences).\n\n\n## Training a Model\n\n### Prepare the Training Data\n\nTo train a model you will need to provide training data in TFRecord format. The\nTFRecord format consists of a set of sharded files containing serialized\n`tf.Example` protocol buffers. Each `tf.Example` proto contains three\nsentences:\n\n  * `encode`: The sentence to encode.\n  * `decode_pre`: The sentence preceding `encode` in the original text.\n  * `decode_post`: The sentence following `encode` in the original text.\n\nEach sentence is a list of words. During preprocessing, a dictionary is created\nthat assigns each word in the vocabulary to an integer-valued id. Each sentence\nis encoded as a list of integer word ids in the `tf.Example` protos.\n\nWe have provided a script to preprocess any set of text-files into this format.\nYou may wish to use the [BookCorpus](http://yknzhu.wixsite.com/mbweb) dataset.\nNote that the preprocessing script may take **12 hours** or more to complete\non this large dataset.\n\n```shell\n# Comma-separated list of globs matching the input input files. The format of\n# the input files is assumed to be a list of newline-separated sentences, where\n# each sentence is already tokenized.\nINPUT_FILES=\"${HOME}/skip_thoughts/bookcorpus/*.txt\"\n\n# Location to save the preprocessed training and validation data.\nDATA_DIR=\"${HOME}/skip_thoughts/data\"\n\n# Build the preprocessing script.\ncd tensorflow-models/skip_thoughts\nbazel build -c opt //skip_thoughts/data:preprocess_dataset\n\n# Run the preprocessing script.\nbazel-bin/skip_thoughts/data/preprocess_dataset \\\n  --input_files=${INPUT_FILES} \\\n  --output_dir=${DATA_DIR}\n```\n\nWhen the script finishes you will find 100 training files and 1 validation file\nin `DATA_DIR`. The files will match the patterns `train-?????-of-00100` and\n`validation-00000-of-00001` respectively.\n\nThe script will also produce a file named `vocab.txt`. The format of this file\nis a list of newline-separated words where the word id is the corresponding 0-\nbased line index. Words are sorted by descending order of frequency in the input\ndata. Only the top 20,000 words are assigned unique ids; all other words are\nassigned the \"unknown id\" of 1 in the processed data.\n\n### Run the Training Script\n\nExecute the following commands to start the training script. By default it will\nrun for 500k steps (around 9 days on a GeForce GTX 1080 GPU).\n\n```shell\n# Directory containing the preprocessed data.\nDATA_DIR=\"${HOME}/skip_thoughts/data\"\n\n# Directory to save the model.\nMODEL_DIR=\"${HOME}/skip_thoughts/model\"\n\n# Build the model.\ncd tensorflow-models/skip_thoughts\nbazel build -c opt //skip_thoughts/...\n\n# Run the training script.\nbazel-bin/skip_thoughts/train \\\n  --input_file_pattern=\"${DATA_DIR}/train-?????-of-00100\" \\\n  --train_dir=\"${MODEL_DIR}/train\"\n```\n\n### Track Training Progress\n\nOptionally, you can run the `track_perplexity` script in a separate process.\nThis will log per-word perplexity on the validation set which allows training\nprogress to be monitored on\n[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n\nNote that you may run out of memory if you run the this script on the same GPU\nas the training script. You can set the environment variable\n`CUDA_VISIBLE_DEVICES=\"\"` to force the script to run on CPU. If it runs too\nslowly on CPU, you can decrease the value of `--num_eval_examples`.\n\n```shell\nDATA_DIR=\"${HOME}/skip_thoughts/data\"\nMODEL_DIR=\"${HOME}/skip_thoughts/model\"\n\n# Ignore GPU devices (only necessary if your GPU is currently memory\n# constrained, for example, by running the training script).\nexport CUDA_VISIBLE_DEVICES=\"\"\n\n# Run the evaluation script. This will run in a loop, periodically loading the\n# latest model checkpoint file and computing evaluation metrics.\nbazel-bin/skip_thoughts/track_perplexity \\\n  --input_file_pattern=\"${DATA_DIR}/validation-?????-of-00001\" \\\n  --checkpoint_dir=\"${MODEL_DIR}/train\" \\\n  --eval_dir=\"${MODEL_DIR}/val\" \\\n  --num_eval_examples=50000\n```\n\nIf you started the `track_perplexity` script, run a\n[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\nserver in a separate process for real-time monitoring of training summaries and\nvalidation perplexity.\n\n```shell\nMODEL_DIR=\"${HOME}/skip_thoughts/model\"\n\n# Run a TensorBoard server.\ntensorboard --logdir=\"${MODEL_DIR}\"\n```\n\n## Expanding the Vocabulary\n\n### Overview\n\nThe vocabulary generated by the preprocessing script contains only 20,000 words\nwhich is insufficient for many tasks. For example, a sentence from Wikipedia\nmight contain nouns that do not appear in this vocabulary.\n\nA solution to this problem described in the\n[Skip-Thought Vectors](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf)\npaper is to learn a mapping that transfers word representations from one model to\nanother. This idea is based on the \"Translation Matrix\" method from the paper\n[Exploiting Similarities Among Languages for Machine Translation](https://arxiv.org/abs/1309.4168).\n\n\nSpecifically, we will load the word embeddings from a trained *Skip-Thoughts*\nmodel and from a trained [word2vec model](https://arxiv.org/pdf/1301.3781.pdf)\n(which has a much larger vocabulary). We will train a linear regression model\nwithout regularization to learn a linear mapping from the word2vec embedding\nspace to the *Skip-Thoughts* embedding space. We will then apply the linear\nmodel to all words in the word2vec vocabulary, yielding vectors in the *Skip-\nThoughts* word embedding space for the union of the two vocabularies.\n\nThe linear regression task is to learn a parameter matrix *W* to minimize\n*|| X - Y \\* W ||<sup>2</sup>*, where *X* is a matrix of *Skip-Thoughts*\nembeddings of shape `[num_words, dim1]`, *Y* is a matrix of word2vec embeddings\nof shape `[num_words, dim2]`, and *W* is a matrix of shape `[dim2, dim1]`.\n\n### Preparation\n\nFirst you will need to download and unpack a pretrained\n[word2vec model](https://arxiv.org/pdf/1301.3781.pdf) from\n[this website](https://code.google.com/archive/p/word2vec/)\n([direct download link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)).\nThis model was trained on the Google News dataset (about 100 billion words).\n\n\nAlso ensure that you have already [installed gensim](https://radimrehurek.com/gensim/install.html).\n\n### Run the Vocabulary Expansion Script\n\n```shell\n# Path to checkpoint file or a directory containing checkpoint files (the script\n# will select the most recent).\nCHECKPOINT_PATH=\"${HOME}/skip_thoughts/model/train\"\n\n# Vocabulary file generated by the preprocessing script.\nSKIP_THOUGHTS_VOCAB=\"${HOME}/skip_thoughts/data/vocab.txt\"\n\n# Path to downloaded word2vec model.\nWORD2VEC_MODEL=\"${HOME}/skip_thoughts/googlenews/GoogleNews-vectors-negative300.bin\"\n\n# Output directory.\nEXP_VOCAB_DIR=\"${HOME}/skip_thoughts/exp_vocab\"\n\n# Build the vocabulary expansion script.\ncd tensorflow-models/skip_thoughts\nbazel build -c opt //skip_thoughts:vocabulary_expansion\n\n# Run the vocabulary expansion script.\nbazel-bin/skip_thoughts/vocabulary_expansion \\\n  --skip_thoughts_model=${CHECKPOINT_PATH} \\\n  --skip_thoughts_vocab=${SKIP_THOUGHTS_VOCAB} \\\n  --word2vec_model=${WORD2VEC_MODEL} \\\n  --output_dir=${EXP_VOCAB_DIR}\n```\n\n## Evaluating a Model\n\n### Overview\n\nThe model can be evaluated using the benchmark tasks described in the\n[Skip-Thought Vectors](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf)\npaper. The following tasks are supported (refer to the paper for full details):\n\n * **SICK** semantic relatedness task.\n * **MSRP** (Microsoft Research Paraphrase Corpus) paraphrase detection task.\n * Binary classification tasks:\n   * **MR** movie review sentiment task.\n   * **CR** customer product review task.\n   * **SUBJ** subjectivity/objectivity task.\n   * **MPQA** opinion polarity task.\n   * **TREC** question-type classification task.\n\n### Preparation\n\nYou will need to clone or download the\n[skip-thoughts GitHub repository](https://github.com/ryankiros/skip-thoughts) by\n[ryankiros](https://github.com/ryankiros) (the first author of the Skip-Thoughts\npaper):\n\n```shell\n# Folder to clone the repository to.\nST_KIROS_DIR=\"${HOME}/skip_thoughts/skipthoughts_kiros\"\n\n# Clone the repository.\ngit clone git@github.com:ryankiros/skip-thoughts.git \"${ST_KIROS_DIR}/skipthoughts\"\n\n# Make the package importable.\nexport PYTHONPATH=\"${ST_KIROS_DIR}/:${PYTHONPATH}\"\n```\n\nYou will also need to download the data needed for each evaluation task. See the\ninstructions [here](https://github.com/ryankiros/skip-thoughts).\n\nFor example, the CR (customer review) dataset is found [here](http://nlp.stanford.edu/~sidaw/home/projects:nbsvm). For this task we want the\nfiles `custrev.pos` and `custrev.neg`.\n\n### Run the Evaluation Tasks\n\nIn the following example we will evaluate a unidirectional model (\"uni-skip\" in\nthe paper) on the CR task. To use a bidirectional model (\"bi-skip\" in the\npaper),  simply pass the flags `--bi_vocab_file`, `--bi_embeddings_file` and\n`--bi_checkpoint_path` instead. To use the \"combine-skip\" model described in the\npaper you will need to pass both the unidirectional and bidirectional flags.\n\n```shell\n# Path to checkpoint file or a directory containing checkpoint files (the script\n# will select the most recent).\nCHECKPOINT_PATH=\"${HOME}/skip_thoughts/model/train\"\n\n# Vocabulary file generated by the vocabulary expansion script.\nVOCAB_FILE=\"${HOME}/skip_thoughts/exp_vocab/vocab.txt\"\n\n# Embeddings file generated by the vocabulary expansion script.\nEMBEDDINGS_FILE=\"${HOME}/skip_thoughts/exp_vocab/embeddings.npy\"\n\n# Directory containing files custrev.pos and custrev.neg.\nEVAL_DATA_DIR=\"${HOME}/skip_thoughts/eval_data\"\n\n# Build the evaluation script.\ncd tensorflow-models/skip_thoughts\nbazel build -c opt //skip_thoughts:evaluate\n\n# Run the evaluation script.\nbazel-bin/skip_thoughts/evaluate \\\n  --eval_task=CR \\\n  --data_dir=${EVAL_DATA_DIR} \\\n  --uni_vocab_file=${VOCAB_FILE} \\\n  --uni_embeddings_file=${EMBEDDINGS_FILE} \\\n  --uni_checkpoint_path=${CHECKPOINT_PATH}\n```\n\nOutput:\n\n```python\n[0.82539682539682535, 0.84084880636604775, 0.83023872679045096,\n 0.86206896551724133, 0.83554376657824936, 0.85676392572944293,\n 0.84084880636604775, 0.83023872679045096, 0.85145888594164454,\n 0.82758620689655171]\n```\n\nThe output is a list of accuracies of 10 cross-validation classification models.\nTo get a single number, simply take the average:\n\n```python\nipython  # Launch iPython.\n\nIn [0]:\nimport numpy as np\nnp.mean([0.82539682539682535, 0.84084880636604775, 0.83023872679045096,\n         0.86206896551724133, 0.83554376657824936, 0.85676392572944293,\n         0.84084880636604775, 0.83023872679045096, 0.85145888594164454,\n         0.82758620689655171])\n\nOut [0]: 0.84009936423729525\n```\n\n## Encoding Sentences\n\nIn this example we will encode data from the\n[movie review dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/)\n(specifically the [sentence polarity dataset v1.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz)).\n\n```python\nipython  # Launch iPython.\n\nIn [0]:\n\n# Imports.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport os.path\nimport scipy.spatial.distance as sd\nfrom skip_thoughts import configuration\nfrom skip_thoughts import encoder_manager\n\nIn [1]:\n# Set paths to the model.\nVOCAB_FILE = \"/path/to/vocab.txt\"\nEMBEDDING_MATRIX_FILE = \"/path/to/embeddings.npy\"\nCHECKPOINT_PATH = \"/path/to/model.ckpt-9999\"\n# The following directory should contain files rt-polarity.neg and\n# rt-polarity.pos.\nMR_DATA_DIR = \"/dir/containing/mr/data\"\n\nIn [2]:\n# Set up the encoder. Here we are using a single unidirectional model.\n# To use a bidirectional model as well, call load_model() again with\n# configuration.model_config(bidirectional_encoder=True) and paths to the\n# bidirectional model's files. The encoder will use the concatenation of\n# all loaded models.\nencoder = encoder_manager.EncoderManager()\nencoder.load_model(configuration.model_config(),\n                   vocabulary_file=VOCAB_FILE,\n                   embedding_matrix_file=EMBEDDING_MATRIX_FILE,\n                   checkpoint_path=CHECKPOINT_PATH)\n\nIn [3]:\n# Load the movie review dataset.\ndata = []\nwith open(os.path.join(MR_DATA_DIR, 'rt-polarity.neg'), 'rb') as f:\n  data.extend([line.decode('latin-1').strip() for line in f])\nwith open(os.path.join(MR_DATA_DIR, 'rt-polarity.pos'), 'rb') as f:\n  data.extend([line.decode('latin-1').strip() for line in f])\n\nIn [4]:\n# Generate Skip-Thought Vectors for each sentence in the dataset.\nencodings = encoder.encode(data)\n\nIn [5]:\n# Define a helper function to generate nearest neighbors.\ndef get_nn(ind, num=10):\n  encoding = encodings[ind]\n  scores = sd.cdist([encoding], encodings, \"cosine\")[0]\n  sorted_ids = np.argsort(scores)\n  print(\"Sentence:\")\n  print(\"\", data[ind])\n  print(\"\\nNearest neighbors:\")\n  for i in range(1, num + 1):\n    print(\" %d. %s (%.3f)\" %\n          (i, data[sorted_ids[i]], scores[sorted_ids[i]]))\n\nIn [6]:\n# Compute nearest neighbors of the first sentence in the dataset.\nget_nn(0)\n```\n\nOutput:\n\n```\nSentence:\n simplistic , silly and tedious .\n\nNearest neighbors:\n 1. trite , banal , cliched , mostly inoffensive . (0.247)\n 2. banal and predictable . (0.253)\n 3. witless , pointless , tasteless and idiotic . (0.272)\n 4. loud , silly , stupid and pointless . (0.295)\n 5. grating and tedious . (0.299)\n 6. idiotic and ugly . (0.330)\n 7. black-and-white and unrealistic . (0.335)\n 8. hopelessly inane , humorless and under-inspired . (0.335)\n 9. shallow , noisy and pretentious . (0.340)\n 10. . . . unlikable , uninteresting , unfunny , and completely , utterly inept . (0.346)\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/RetroRabbit/skip_thoughts", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "skip-thoughts", "package_url": "https://pypi.org/project/skip-thoughts/", "platform": "", "project_url": "https://pypi.org/project/skip-thoughts/", "project_urls": {"Homepage": "https://github.com/RetroRabbit/skip_thoughts"}, "release_url": "https://pypi.org/project/skip-thoughts/0.1.1/", "requires_dist": null, "requires_python": ">=2.7", "summary": "A pip installable version of skip_thoughts branched from tensorflow/research", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Skip-Thought Vectors</h1>\n<p>This is a TensorFlow implementation of the model described in:</p>\n<p>Jamie Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel,\nAntonio Torralba, Raquel Urtasun, Sanja Fidler.\n<a href=\"https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf\" rel=\"nofollow\">Skip-Thought Vectors</a>.\n<em>In NIPS, 2015.</em></p>\n<h2>Contact</h2>\n<p><em><strong>Code author:</strong></em> Chris Shallue</p>\n<p><em><strong>Pull requests and issues:</strong></em> @cshallue</p>\n<h2>Contents</h2>\n<ul>\n<li><a href=\"#model-overview\" rel=\"nofollow\">Model Overview</a></li>\n<li><a href=\"#getting-started\" rel=\"nofollow\">Getting Started</a>\n<ul>\n<li><a href=\"#install-required-packages\" rel=\"nofollow\">Install Required Packages</a></li>\n<li><a href=\"#download-pretrained-models-optional\" rel=\"nofollow\">Download Pretrained Models (Optional)</a></li>\n</ul>\n</li>\n<li><a href=\"#training-a-model\" rel=\"nofollow\">Training a Model</a>\n<ul>\n<li><a href=\"#prepare-the-training-data\" rel=\"nofollow\">Prepare the Training Data</a></li>\n<li><a href=\"#run-the-training-script\" rel=\"nofollow\">Run the Training Script</a></li>\n<li><a href=\"#track-training-progress\" rel=\"nofollow\">Track Training Progress</a></li>\n</ul>\n</li>\n<li><a href=\"#expanding-the-vocabulary\" rel=\"nofollow\">Expanding the Vocabulary</a>\n<ul>\n<li><a href=\"#overview\" rel=\"nofollow\">Overview</a></li>\n<li><a href=\"#preparation\" rel=\"nofollow\">Preparation</a></li>\n<li><a href=\"#run-the-vocabulary-expansion-script\" rel=\"nofollow\">Run the Vocabulary Expansion Script</a></li>\n</ul>\n</li>\n<li><a href=\"#evaluating-a-model\" rel=\"nofollow\">Evaluating a Model</a>\n<ul>\n<li><a href=\"#overview-1\" rel=\"nofollow\">Overview</a></li>\n<li><a href=\"#preparation-1\" rel=\"nofollow\">Preparation</a></li>\n<li><a href=\"#run-the-evaluation-tasks\" rel=\"nofollow\">Run the Evaluation Tasks</a></li>\n</ul>\n</li>\n<li><a href=\"#encoding-sentences\" rel=\"nofollow\">Encoding Sentences</a></li>\n</ul>\n<h2>Model overview</h2>\n<p>The <em>Skip-Thoughts</em> model is a sentence encoder. It learns to encode input\nsentences into a fixed-dimensional vector representation that is useful for many\ntasks, for example to detect paraphrases or to classify whether a product review\nis positive or negative. See the\n<a href=\"https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf\" rel=\"nofollow\">Skip-Thought Vectors</a>\npaper for details of the model architecture and more example applications.</p>\n<p>A trained <em>Skip-Thoughts</em> model will encode similar sentences nearby each other\nin the embedding vector space. The following examples show the nearest neighbor by\ncosine similarity of some sentences from the\n<a href=\"https://www.cs.cornell.edu/people/pabo/movie-review-data/\" rel=\"nofollow\">movie review dataset</a>.</p>\n<table>\n<thead>\n<tr>\n<th>Input sentence</th>\n<th>Nearest Neighbor</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Simplistic, silly and tedious.</td>\n<td>Trite, banal, cliched, mostly inoffensive.</td>\n</tr>\n<tr>\n<td>Not so much farcical as sour.</td>\n<td>Not only unfunny, but downright repellent.</td>\n</tr>\n<tr>\n<td>A sensitive and astute first feature by Anne-Sophie Birot.</td>\n<td>Absorbing character study by Andr\u00e9 Turpin .</td>\n</tr>\n<tr>\n<td>An enthralling, entertaining feature.</td>\n<td>A slick, engrossing melodrama.</td>\n</tr></tbody></table>\n<h2>Getting Started</h2>\n<h3>Install Required Packages</h3>\n<p>First ensure that you have installed the following required packages:</p>\n<ul>\n<li><strong>Bazel</strong> (<a href=\"http://bazel.build/docs/install.html\" rel=\"nofollow\">instructions</a>)</li>\n<li><strong>TensorFlow</strong> (<a href=\"https://www.tensorflow.org/install/\" rel=\"nofollow\">instructions</a>)</li>\n<li><strong>NumPy</strong> (<a href=\"http://www.scipy.org/install.html\" rel=\"nofollow\">instructions</a>)</li>\n<li><strong>scikit-learn</strong> (<a href=\"http://scikit-learn.org/stable/install.html\" rel=\"nofollow\">instructions</a>)</li>\n<li><strong>Natural Language Toolkit (NLTK)</strong>\n<ul>\n<li>First install NLTK (<a href=\"http://www.nltk.org/install.html\" rel=\"nofollow\">instructions</a>)</li>\n<li>Then install the NLTK data (<a href=\"http://www.nltk.org/data.html\" rel=\"nofollow\">instructions</a>)</li>\n</ul>\n</li>\n<li><strong>gensim</strong> (<a href=\"https://radimrehurek.com/gensim/install.html\" rel=\"nofollow\">instructions</a>)\n<ul>\n<li>Only required if you will be expanding your vocabulary with the <a href=\"https://code.google.com/archive/p/word2vec/\" rel=\"nofollow\">word2vec</a> model.</li>\n</ul>\n</li>\n</ul>\n<h3>Download Pretrained Models (Optional)</h3>\n<p>You can download model checkpoints pretrained on the\n<a href=\"http://yknzhu.wixsite.com/mbweb\" rel=\"nofollow\">BookCorpus</a> dataset in the following\nconfigurations:</p>\n<ul>\n<li>Unidirectional RNN encoder (\"uni-skip\" in the paper)</li>\n<li>Bidirectional RNN encoder (\"bi-skip\" in the paper)</li>\n</ul>\n<pre><span class=\"c1\"># Directory to download the pretrained models to.</span>\n<span class=\"nv\">PRETRAINED_MODELS_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/pretrained/\"</span>\n\nmkdir -p <span class=\"si\">${</span><span class=\"nv\">PRETRAINED_MODELS_DIR</span><span class=\"si\">}</span>\n<span class=\"nb\">cd</span> <span class=\"si\">${</span><span class=\"nv\">PRETRAINED_MODELS_DIR</span><span class=\"si\">}</span>\n\n<span class=\"c1\"># Download and extract the unidirectional model.</span>\nwget <span class=\"s2\">\"http://download.tensorflow.org/models/skip_thoughts_uni_2017_02_02.tar.gz\"</span>\ntar -xvf skip_thoughts_uni_2017_02_02.tar.gz\nrm skip_thoughts_uni_2017_02_02.tar.gz\n\n<span class=\"c1\"># Download and extract the bidirectional model.</span>\nwget <span class=\"s2\">\"http://download.tensorflow.org/models/skip_thoughts_bi_2017_02_16.tar.gz\"</span>\ntar -xvf skip_thoughts_bi_2017_02_16.tar.gz\nrm skip_thoughts_bi_2017_02_16.tar.gz\n</pre>\n<p>You can now skip to the sections <a href=\"#evaluating-a-model\" rel=\"nofollow\">Evaluating a Model</a> and\n<a href=\"#encoding-sentences\" rel=\"nofollow\">Encoding Sentences</a>.</p>\n<h2>Training a Model</h2>\n<h3>Prepare the Training Data</h3>\n<p>To train a model you will need to provide training data in TFRecord format. The\nTFRecord format consists of a set of sharded files containing serialized\n<code>tf.Example</code> protocol buffers. Each <code>tf.Example</code> proto contains three\nsentences:</p>\n<ul>\n<li><code>encode</code>: The sentence to encode.</li>\n<li><code>decode_pre</code>: The sentence preceding <code>encode</code> in the original text.</li>\n<li><code>decode_post</code>: The sentence following <code>encode</code> in the original text.</li>\n</ul>\n<p>Each sentence is a list of words. During preprocessing, a dictionary is created\nthat assigns each word in the vocabulary to an integer-valued id. Each sentence\nis encoded as a list of integer word ids in the <code>tf.Example</code> protos.</p>\n<p>We have provided a script to preprocess any set of text-files into this format.\nYou may wish to use the <a href=\"http://yknzhu.wixsite.com/mbweb\" rel=\"nofollow\">BookCorpus</a> dataset.\nNote that the preprocessing script may take <strong>12 hours</strong> or more to complete\non this large dataset.</p>\n<pre><span class=\"c1\"># Comma-separated list of globs matching the input input files. The format of</span>\n<span class=\"c1\"># the input files is assumed to be a list of newline-separated sentences, where</span>\n<span class=\"c1\"># each sentence is already tokenized.</span>\n<span class=\"nv\">INPUT_FILES</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/bookcorpus/*.txt\"</span>\n\n<span class=\"c1\"># Location to save the preprocessed training and validation data.</span>\n<span class=\"nv\">DATA_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/data\"</span>\n\n<span class=\"c1\"># Build the preprocessing script.</span>\n<span class=\"nb\">cd</span> tensorflow-models/skip_thoughts\nbazel build -c opt //skip_thoughts/data:preprocess_dataset\n\n<span class=\"c1\"># Run the preprocessing script.</span>\nbazel-bin/skip_thoughts/data/preprocess_dataset <span class=\"se\">\\</span>\n  --input_files<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">INPUT_FILES</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n  --output_dir<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">DATA_DIR</span><span class=\"si\">}</span>\n</pre>\n<p>When the script finishes you will find 100 training files and 1 validation file\nin <code>DATA_DIR</code>. The files will match the patterns <code>train-?????-of-00100</code> and\n<code>validation-00000-of-00001</code> respectively.</p>\n<p>The script will also produce a file named <code>vocab.txt</code>. The format of this file\nis a list of newline-separated words where the word id is the corresponding 0-\nbased line index. Words are sorted by descending order of frequency in the input\ndata. Only the top 20,000 words are assigned unique ids; all other words are\nassigned the \"unknown id\" of 1 in the processed data.</p>\n<h3>Run the Training Script</h3>\n<p>Execute the following commands to start the training script. By default it will\nrun for 500k steps (around 9 days on a GeForce GTX 1080 GPU).</p>\n<pre><span class=\"c1\"># Directory containing the preprocessed data.</span>\n<span class=\"nv\">DATA_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/data\"</span>\n\n<span class=\"c1\"># Directory to save the model.</span>\n<span class=\"nv\">MODEL_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/model\"</span>\n\n<span class=\"c1\"># Build the model.</span>\n<span class=\"nb\">cd</span> tensorflow-models/skip_thoughts\nbazel build -c opt //skip_thoughts/...\n\n<span class=\"c1\"># Run the training script.</span>\nbazel-bin/skip_thoughts/train <span class=\"se\">\\</span>\n  --input_file_pattern<span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">DATA_DIR</span><span class=\"si\">}</span><span class=\"s2\">/train-?????-of-00100\"</span> <span class=\"se\">\\</span>\n  --train_dir<span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">MODEL_DIR</span><span class=\"si\">}</span><span class=\"s2\">/train\"</span>\n</pre>\n<h3>Track Training Progress</h3>\n<p>Optionally, you can run the <code>track_perplexity</code> script in a separate process.\nThis will log per-word perplexity on the validation set which allows training\nprogress to be monitored on\n<a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" rel=\"nofollow\">TensorBoard</a>.</p>\n<p>Note that you may run out of memory if you run the this script on the same GPU\nas the training script. You can set the environment variable\n<code>CUDA_VISIBLE_DEVICES=\"\"</code> to force the script to run on CPU. If it runs too\nslowly on CPU, you can decrease the value of <code>--num_eval_examples</code>.</p>\n<pre><span class=\"nv\">DATA_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/data\"</span>\n<span class=\"nv\">MODEL_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/model\"</span>\n\n<span class=\"c1\"># Ignore GPU devices (only necessary if your GPU is currently memory</span>\n<span class=\"c1\"># constrained, for example, by running the training script).</span>\n<span class=\"nb\">export</span> <span class=\"nv\">CUDA_VISIBLE_DEVICES</span><span class=\"o\">=</span><span class=\"s2\">\"\"</span>\n\n<span class=\"c1\"># Run the evaluation script. This will run in a loop, periodically loading the</span>\n<span class=\"c1\"># latest model checkpoint file and computing evaluation metrics.</span>\nbazel-bin/skip_thoughts/track_perplexity <span class=\"se\">\\</span>\n  --input_file_pattern<span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">DATA_DIR</span><span class=\"si\">}</span><span class=\"s2\">/validation-?????-of-00001\"</span> <span class=\"se\">\\</span>\n  --checkpoint_dir<span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">MODEL_DIR</span><span class=\"si\">}</span><span class=\"s2\">/train\"</span> <span class=\"se\">\\</span>\n  --eval_dir<span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">MODEL_DIR</span><span class=\"si\">}</span><span class=\"s2\">/val\"</span> <span class=\"se\">\\</span>\n  --num_eval_examples<span class=\"o\">=</span><span class=\"m\">50000</span>\n</pre>\n<p>If you started the <code>track_perplexity</code> script, run a\n<a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" rel=\"nofollow\">TensorBoard</a>\nserver in a separate process for real-time monitoring of training summaries and\nvalidation perplexity.</p>\n<pre><span class=\"nv\">MODEL_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/model\"</span>\n\n<span class=\"c1\"># Run a TensorBoard server.</span>\ntensorboard --logdir<span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">MODEL_DIR</span><span class=\"si\">}</span><span class=\"s2\">\"</span>\n</pre>\n<h2>Expanding the Vocabulary</h2>\n<h3>Overview</h3>\n<p>The vocabulary generated by the preprocessing script contains only 20,000 words\nwhich is insufficient for many tasks. For example, a sentence from Wikipedia\nmight contain nouns that do not appear in this vocabulary.</p>\n<p>A solution to this problem described in the\n<a href=\"https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf\" rel=\"nofollow\">Skip-Thought Vectors</a>\npaper is to learn a mapping that transfers word representations from one model to\nanother. This idea is based on the \"Translation Matrix\" method from the paper\n<a href=\"https://arxiv.org/abs/1309.4168\" rel=\"nofollow\">Exploiting Similarities Among Languages for Machine Translation</a>.</p>\n<p>Specifically, we will load the word embeddings from a trained <em>Skip-Thoughts</em>\nmodel and from a trained <a href=\"https://arxiv.org/pdf/1301.3781.pdf\" rel=\"nofollow\">word2vec model</a>\n(which has a much larger vocabulary). We will train a linear regression model\nwithout regularization to learn a linear mapping from the word2vec embedding\nspace to the <em>Skip-Thoughts</em> embedding space. We will then apply the linear\nmodel to all words in the word2vec vocabulary, yielding vectors in the <em>Skip-\nThoughts</em> word embedding space for the union of the two vocabularies.</p>\n<p>The linear regression task is to learn a parameter matrix <em>W</em> to minimize\n<em>|| X - Y * W ||<sup>2</sup></em>, where <em>X</em> is a matrix of <em>Skip-Thoughts</em>\nembeddings of shape <code>[num_words, dim1]</code>, <em>Y</em> is a matrix of word2vec embeddings\nof shape <code>[num_words, dim2]</code>, and <em>W</em> is a matrix of shape <code>[dim2, dim1]</code>.</p>\n<h3>Preparation</h3>\n<p>First you will need to download and unpack a pretrained\n<a href=\"https://arxiv.org/pdf/1301.3781.pdf\" rel=\"nofollow\">word2vec model</a> from\n<a href=\"https://code.google.com/archive/p/word2vec/\" rel=\"nofollow\">this website</a>\n(<a href=\"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\" rel=\"nofollow\">direct download link</a>).\nThis model was trained on the Google News dataset (about 100 billion words).</p>\n<p>Also ensure that you have already <a href=\"https://radimrehurek.com/gensim/install.html\" rel=\"nofollow\">installed gensim</a>.</p>\n<h3>Run the Vocabulary Expansion Script</h3>\n<pre><span class=\"c1\"># Path to checkpoint file or a directory containing checkpoint files (the script</span>\n<span class=\"c1\"># will select the most recent).</span>\n<span class=\"nv\">CHECKPOINT_PATH</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/model/train\"</span>\n\n<span class=\"c1\"># Vocabulary file generated by the preprocessing script.</span>\n<span class=\"nv\">SKIP_THOUGHTS_VOCAB</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/data/vocab.txt\"</span>\n\n<span class=\"c1\"># Path to downloaded word2vec model.</span>\n<span class=\"nv\">WORD2VEC_MODEL</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/googlenews/GoogleNews-vectors-negative300.bin\"</span>\n\n<span class=\"c1\"># Output directory.</span>\n<span class=\"nv\">EXP_VOCAB_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/exp_vocab\"</span>\n\n<span class=\"c1\"># Build the vocabulary expansion script.</span>\n<span class=\"nb\">cd</span> tensorflow-models/skip_thoughts\nbazel build -c opt //skip_thoughts:vocabulary_expansion\n\n<span class=\"c1\"># Run the vocabulary expansion script.</span>\nbazel-bin/skip_thoughts/vocabulary_expansion <span class=\"se\">\\</span>\n  --skip_thoughts_model<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">CHECKPOINT_PATH</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n  --skip_thoughts_vocab<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">SKIP_THOUGHTS_VOCAB</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n  --word2vec_model<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">WORD2VEC_MODEL</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n  --output_dir<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">EXP_VOCAB_DIR</span><span class=\"si\">}</span>\n</pre>\n<h2>Evaluating a Model</h2>\n<h3>Overview</h3>\n<p>The model can be evaluated using the benchmark tasks described in the\n<a href=\"https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf\" rel=\"nofollow\">Skip-Thought Vectors</a>\npaper. The following tasks are supported (refer to the paper for full details):</p>\n<ul>\n<li><strong>SICK</strong> semantic relatedness task.</li>\n<li><strong>MSRP</strong> (Microsoft Research Paraphrase Corpus) paraphrase detection task.</li>\n<li>Binary classification tasks:\n<ul>\n<li><strong>MR</strong> movie review sentiment task.</li>\n<li><strong>CR</strong> customer product review task.</li>\n<li><strong>SUBJ</strong> subjectivity/objectivity task.</li>\n<li><strong>MPQA</strong> opinion polarity task.</li>\n<li><strong>TREC</strong> question-type classification task.</li>\n</ul>\n</li>\n</ul>\n<h3>Preparation</h3>\n<p>You will need to clone or download the\n<a href=\"https://github.com/ryankiros/skip-thoughts\" rel=\"nofollow\">skip-thoughts GitHub repository</a> by\n<a href=\"https://github.com/ryankiros\" rel=\"nofollow\">ryankiros</a> (the first author of the Skip-Thoughts\npaper):</p>\n<pre><span class=\"c1\"># Folder to clone the repository to.</span>\n<span class=\"nv\">ST_KIROS_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/skipthoughts_kiros\"</span>\n\n<span class=\"c1\"># Clone the repository.</span>\ngit clone git@github.com:ryankiros/skip-thoughts.git <span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">ST_KIROS_DIR</span><span class=\"si\">}</span><span class=\"s2\">/skipthoughts\"</span>\n\n<span class=\"c1\"># Make the package importable.</span>\n<span class=\"nb\">export</span> <span class=\"nv\">PYTHONPATH</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">ST_KIROS_DIR</span><span class=\"si\">}</span><span class=\"s2\">/:</span><span class=\"si\">${</span><span class=\"nv\">PYTHONPATH</span><span class=\"si\">}</span><span class=\"s2\">\"</span>\n</pre>\n<p>You will also need to download the data needed for each evaluation task. See the\ninstructions <a href=\"https://github.com/ryankiros/skip-thoughts\" rel=\"nofollow\">here</a>.</p>\n<p>For example, the CR (customer review) dataset is found <a href=\"http://nlp.stanford.edu/%7Esidaw/home/projects:nbsvm\" rel=\"nofollow\">here</a>. For this task we want the\nfiles <code>custrev.pos</code> and <code>custrev.neg</code>.</p>\n<h3>Run the Evaluation Tasks</h3>\n<p>In the following example we will evaluate a unidirectional model (\"uni-skip\" in\nthe paper) on the CR task. To use a bidirectional model (\"bi-skip\" in the\npaper),  simply pass the flags <code>--bi_vocab_file</code>, <code>--bi_embeddings_file</code> and\n<code>--bi_checkpoint_path</code> instead. To use the \"combine-skip\" model described in the\npaper you will need to pass both the unidirectional and bidirectional flags.</p>\n<pre><span class=\"c1\"># Path to checkpoint file or a directory containing checkpoint files (the script</span>\n<span class=\"c1\"># will select the most recent).</span>\n<span class=\"nv\">CHECKPOINT_PATH</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/model/train\"</span>\n\n<span class=\"c1\"># Vocabulary file generated by the vocabulary expansion script.</span>\n<span class=\"nv\">VOCAB_FILE</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/exp_vocab/vocab.txt\"</span>\n\n<span class=\"c1\"># Embeddings file generated by the vocabulary expansion script.</span>\n<span class=\"nv\">EMBEDDINGS_FILE</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/exp_vocab/embeddings.npy\"</span>\n\n<span class=\"c1\"># Directory containing files custrev.pos and custrev.neg.</span>\n<span class=\"nv\">EVAL_DATA_DIR</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"si\">${</span><span class=\"nv\">HOME</span><span class=\"si\">}</span><span class=\"s2\">/skip_thoughts/eval_data\"</span>\n\n<span class=\"c1\"># Build the evaluation script.</span>\n<span class=\"nb\">cd</span> tensorflow-models/skip_thoughts\nbazel build -c opt //skip_thoughts:evaluate\n\n<span class=\"c1\"># Run the evaluation script.</span>\nbazel-bin/skip_thoughts/evaluate <span class=\"se\">\\</span>\n  --eval_task<span class=\"o\">=</span>CR <span class=\"se\">\\</span>\n  --data_dir<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">EVAL_DATA_DIR</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n  --uni_vocab_file<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">VOCAB_FILE</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n  --uni_embeddings_file<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">EMBEDDINGS_FILE</span><span class=\"si\">}</span> <span class=\"se\">\\</span>\n  --uni_checkpoint_path<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">CHECKPOINT_PATH</span><span class=\"si\">}</span>\n</pre>\n<p>Output:</p>\n<pre><span class=\"p\">[</span><span class=\"mf\">0.82539682539682535</span><span class=\"p\">,</span> <span class=\"mf\">0.84084880636604775</span><span class=\"p\">,</span> <span class=\"mf\">0.83023872679045096</span><span class=\"p\">,</span>\n <span class=\"mf\">0.86206896551724133</span><span class=\"p\">,</span> <span class=\"mf\">0.83554376657824936</span><span class=\"p\">,</span> <span class=\"mf\">0.85676392572944293</span><span class=\"p\">,</span>\n <span class=\"mf\">0.84084880636604775</span><span class=\"p\">,</span> <span class=\"mf\">0.83023872679045096</span><span class=\"p\">,</span> <span class=\"mf\">0.85145888594164454</span><span class=\"p\">,</span>\n <span class=\"mf\">0.82758620689655171</span><span class=\"p\">]</span>\n</pre>\n<p>The output is a list of accuracies of 10 cross-validation classification models.\nTo get a single number, simply take the average:</p>\n<pre><span class=\"n\">ipython</span>  <span class=\"c1\"># Launch iPython.</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]:</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">([</span><span class=\"mf\">0.82539682539682535</span><span class=\"p\">,</span> <span class=\"mf\">0.84084880636604775</span><span class=\"p\">,</span> <span class=\"mf\">0.83023872679045096</span><span class=\"p\">,</span>\n         <span class=\"mf\">0.86206896551724133</span><span class=\"p\">,</span> <span class=\"mf\">0.83554376657824936</span><span class=\"p\">,</span> <span class=\"mf\">0.85676392572944293</span><span class=\"p\">,</span>\n         <span class=\"mf\">0.84084880636604775</span><span class=\"p\">,</span> <span class=\"mf\">0.83023872679045096</span><span class=\"p\">,</span> <span class=\"mf\">0.85145888594164454</span><span class=\"p\">,</span>\n         <span class=\"mf\">0.82758620689655171</span><span class=\"p\">])</span>\n\n<span class=\"n\">Out</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]:</span> <span class=\"mf\">0.84009936423729525</span>\n</pre>\n<h2>Encoding Sentences</h2>\n<p>In this example we will encode data from the\n<a href=\"https://www.cs.cornell.edu/people/pabo/movie-review-data/\" rel=\"nofollow\">movie review dataset</a>\n(specifically the <a href=\"https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\" rel=\"nofollow\">sentence polarity dataset v1.0</a>).</p>\n<pre><span class=\"n\">ipython</span>  <span class=\"c1\"># Launch iPython.</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]:</span>\n\n<span class=\"c1\"># Imports.</span>\n<span class=\"kn\">from</span> <span class=\"nn\">__future__</span> <span class=\"kn\">import</span> <span class=\"n\">absolute_import</span>\n<span class=\"kn\">from</span> <span class=\"nn\">__future__</span> <span class=\"kn\">import</span> <span class=\"n\">division</span>\n<span class=\"kn\">from</span> <span class=\"nn\">__future__</span> <span class=\"kn\">import</span> <span class=\"n\">print_function</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">os.path</span>\n<span class=\"kn\">import</span> <span class=\"nn\">scipy.spatial.distance</span> <span class=\"k\">as</span> <span class=\"nn\">sd</span>\n<span class=\"kn\">from</span> <span class=\"nn\">skip_thoughts</span> <span class=\"kn\">import</span> <span class=\"n\">configuration</span>\n<span class=\"kn\">from</span> <span class=\"nn\">skip_thoughts</span> <span class=\"kn\">import</span> <span class=\"n\">encoder_manager</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span>\n<span class=\"c1\"># Set paths to the model.</span>\n<span class=\"n\">VOCAB_FILE</span> <span class=\"o\">=</span> <span class=\"s2\">\"/path/to/vocab.txt\"</span>\n<span class=\"n\">EMBEDDING_MATRIX_FILE</span> <span class=\"o\">=</span> <span class=\"s2\">\"/path/to/embeddings.npy\"</span>\n<span class=\"n\">CHECKPOINT_PATH</span> <span class=\"o\">=</span> <span class=\"s2\">\"/path/to/model.ckpt-9999\"</span>\n<span class=\"c1\"># The following directory should contain files rt-polarity.neg and</span>\n<span class=\"c1\"># rt-polarity.pos.</span>\n<span class=\"n\">MR_DATA_DIR</span> <span class=\"o\">=</span> <span class=\"s2\">\"/dir/containing/mr/data\"</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span>\n<span class=\"c1\"># Set up the encoder. Here we are using a single unidirectional model.</span>\n<span class=\"c1\"># To use a bidirectional model as well, call load_model() again with</span>\n<span class=\"c1\"># configuration.model_config(bidirectional_encoder=True) and paths to the</span>\n<span class=\"c1\"># bidirectional model's files. The encoder will use the concatenation of</span>\n<span class=\"c1\"># all loaded models.</span>\n<span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">encoder_manager</span><span class=\"o\">.</span><span class=\"n\">EncoderManager</span><span class=\"p\">()</span>\n<span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"n\">configuration</span><span class=\"o\">.</span><span class=\"n\">model_config</span><span class=\"p\">(),</span>\n                   <span class=\"n\">vocabulary_file</span><span class=\"o\">=</span><span class=\"n\">VOCAB_FILE</span><span class=\"p\">,</span>\n                   <span class=\"n\">embedding_matrix_file</span><span class=\"o\">=</span><span class=\"n\">EMBEDDING_MATRIX_FILE</span><span class=\"p\">,</span>\n                   <span class=\"n\">checkpoint_path</span><span class=\"o\">=</span><span class=\"n\">CHECKPOINT_PATH</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span>\n<span class=\"c1\"># Load the movie review dataset.</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">MR_DATA_DIR</span><span class=\"p\">,</span> <span class=\"s1\">'rt-polarity.neg'</span><span class=\"p\">),</span> <span class=\"s1\">'rb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n  <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">extend</span><span class=\"p\">([</span><span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s1\">'latin-1'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">])</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">MR_DATA_DIR</span><span class=\"p\">,</span> <span class=\"s1\">'rt-polarity.pos'</span><span class=\"p\">),</span> <span class=\"s1\">'rb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n  <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">extend</span><span class=\"p\">([</span><span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s1\">'latin-1'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">])</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span>\n<span class=\"c1\"># Generate Skip-Thought Vectors for each sentence in the dataset.</span>\n<span class=\"n\">encodings</span> <span class=\"o\">=</span> <span class=\"n\">encoder</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">]:</span>\n<span class=\"c1\"># Define a helper function to generate nearest neighbors.</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_nn</span><span class=\"p\">(</span><span class=\"n\">ind</span><span class=\"p\">,</span> <span class=\"n\">num</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">):</span>\n  <span class=\"n\">encoding</span> <span class=\"o\">=</span> <span class=\"n\">encodings</span><span class=\"p\">[</span><span class=\"n\">ind</span><span class=\"p\">]</span>\n  <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">sd</span><span class=\"o\">.</span><span class=\"n\">cdist</span><span class=\"p\">([</span><span class=\"n\">encoding</span><span class=\"p\">],</span> <span class=\"n\">encodings</span><span class=\"p\">,</span> <span class=\"s2\">\"cosine\"</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n  <span class=\"n\">sorted_ids</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">(</span><span class=\"n\">scores</span><span class=\"p\">)</span>\n  <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Sentence:\"</span><span class=\"p\">)</span>\n  <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"\"</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">ind</span><span class=\"p\">])</span>\n  <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"</span><span class=\"se\">\\n</span><span class=\"s2\">Nearest neighbors:\"</span><span class=\"p\">)</span>\n  <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">num</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\" </span><span class=\"si\">%d</span><span class=\"s2\">. </span><span class=\"si\">%s</span><span class=\"s2\"> (</span><span class=\"si\">%.3f</span><span class=\"s2\">)\"</span> <span class=\"o\">%</span>\n          <span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">sorted_ids</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]],</span> <span class=\"n\">scores</span><span class=\"p\">[</span><span class=\"n\">sorted_ids</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]]))</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">]:</span>\n<span class=\"c1\"># Compute nearest neighbors of the first sentence in the dataset.</span>\n<span class=\"n\">get_nn</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n</pre>\n<p>Output:</p>\n<pre><code>Sentence:\n simplistic , silly and tedious .\n\nNearest neighbors:\n 1. trite , banal , cliched , mostly inoffensive . (0.247)\n 2. banal and predictable . (0.253)\n 3. witless , pointless , tasteless and idiotic . (0.272)\n 4. loud , silly , stupid and pointless . (0.295)\n 5. grating and tedious . (0.299)\n 6. idiotic and ugly . (0.330)\n 7. black-and-white and unrealistic . (0.335)\n 8. hopelessly inane , humorless and under-inspired . (0.335)\n 9. shallow , noisy and pretentious . (0.340)\n 10. . . . unlikable , uninteresting , unfunny , and completely , utterly inept . (0.346)\n</code></pre>\n\n          </div>"}, "last_serial": 4426668, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "39cc11cbbdfeb9f1ce99ecf3b3578cb1", "sha256": "3c7f2d11f48c88b67e316c9c4fcdc6f6e1171e160b29ea6f8dbcfbe9958a139f"}, "downloads": -1, "filename": "skip_thoughts-0.1.0-py2-none-any.whl", "has_sig": false, "md5_digest": "39cc11cbbdfeb9f1ce99ecf3b3578cb1", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 41243, "upload_time": "2018-10-25T09:24:42", "upload_time_iso_8601": "2018-10-25T09:24:42.554592Z", "url": "https://files.pythonhosted.org/packages/b6/60/348dc285b3b3390a94f63e05c3583883a4416e7d6a0c7a82cd6ad88524dd/skip_thoughts-0.1.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "79f4ffe188735e28ac86d89dc2b189d9", "sha256": "492bb993158e7416d4e92a6139980fc7cc9045e158d962dd0de2db2a11ac21c3"}, "downloads": -1, "filename": "skip_thoughts-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "79f4ffe188735e28ac86d89dc2b189d9", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 41260, "upload_time": "2018-10-25T09:24:44", "upload_time_iso_8601": "2018-10-25T09:24:44.612719Z", "url": "https://files.pythonhosted.org/packages/a9/ab/12cd0846459599d0c449be59de45a5523628e78cfb82e657dab041aae968/skip_thoughts-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "89c06cffc90185081fc96275d22b5708", "sha256": "f18b1a5cbe84ffa6212e3310301cf0b2d7412b31dc3c927d9746517c4f2de84e"}, "downloads": -1, "filename": "skip_thoughts-0.1.0.tar.gz", "has_sig": false, "md5_digest": "89c06cffc90185081fc96275d22b5708", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31773, "upload_time": "2018-10-25T09:24:46", "upload_time_iso_8601": "2018-10-25T09:24:46.854443Z", "url": "https://files.pythonhosted.org/packages/c9/39/d9d75493b5f110156a01fea86307291b48df9a6d7d1b5edd5087d285495d/skip_thoughts-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "a914f0f5afe2706dd719ab58fdd065af", "sha256": "8a5d66f8ca95eba880e83bd21614d3a8e77428ae588e34af9bc759406f95e4f2"}, "downloads": -1, "filename": "skip_thoughts-0.1.1.tar.gz", "has_sig": false, "md5_digest": "a914f0f5afe2706dd719ab58fdd065af", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 31779, "upload_time": "2018-10-29T08:34:32", "upload_time_iso_8601": "2018-10-29T08:34:32.305364Z", "url": "https://files.pythonhosted.org/packages/22/99/83e63afe2ac59c11c018912e61bdfb73a5d24037f5ce0d61190bdf51d5b3/skip_thoughts-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a914f0f5afe2706dd719ab58fdd065af", "sha256": "8a5d66f8ca95eba880e83bd21614d3a8e77428ae588e34af9bc759406f95e4f2"}, "downloads": -1, "filename": "skip_thoughts-0.1.1.tar.gz", "has_sig": false, "md5_digest": "a914f0f5afe2706dd719ab58fdd065af", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 31779, "upload_time": "2018-10-29T08:34:32", "upload_time_iso_8601": "2018-10-29T08:34:32.305364Z", "url": "https://files.pythonhosted.org/packages/22/99/83e63afe2ac59c11c018912e61bdfb73a5d24037f5ce0d61190bdf51d5b3/skip_thoughts-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:08:58 2020"}