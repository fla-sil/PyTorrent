{"info": {"author": "Machine Learning Team @ Zuru Tech", "author_email": "ml@zuru.tech", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3.7"], "description": "<div align=\"center\">\n    <img src=\"https://blog.zuru.tech/images/ashpy/logo_lq.png\" />\n</div>\n\n# AshPy\n\n![Contributions](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)\n![Python - Version](https://img.shields.io/pypi/pyversions/ashpy.svg)\n![PyPy - Version](https://badge.fury.io/py/ashpy.svg)\n![PyPI - License](https://img.shields.io/pypi/l/ashpy.svg)\n![Ashpy - Badge](https://img.shields.io/badge/package-ashpy-brightgreen.svg)\n[![codecov](https://codecov.io/gh/zurutech/ashpy/branch/master/graph/badge.svg)](https://codecov.io/gh/zurutech/ashpy)\n[![Build Status](https://travis-ci.org/zurutech/ashpy.svg?branch=master)](https://travis-ci.org/zurutech/ashpy)\n[![Documentation Status](https://readthedocs.org/projects/ashpy/badge/?version=latest)](https://ashpy.readthedocs.io/en/latest/?badge=latest)\n[![Black - Badge](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n[![CodeFactor](https://www.codefactor.io/repository/github/zurutech/ashpy/badge)](https://www.codefactor.io/repository/github/zurutech/ashpy)\n\nAshPy is a TensorFlow 2.1 library for (**distributed**) training, evaluation, model selection, and fast prototyping.\nIt is designed to ease the burden of setting up all the nuances of the architectures built to train complex custom deep learning models.\n\n[Quick Example](#quick-example) | [Features](#features) | [Set Up](#set-up) | [Usage](#usage) | [Dataset Output Format](#dataset-output-format) | [Test](#test)\n\n## Quick Example\n\n```python\n# define a distribution strategy\nstrategy = tf.distribute.MirroredStrategy()\n\n# work inside the scope of the created strategy\nwith strategy.scope():\n\n    # get the MNIST dataset\n    train, validation = tf.keras.datasets.mnist.load_data()\n\n    # process data if needed\n    def process(images, labels):\n        data_images = tf.data.Dataset.from_tensor_slices((images)).map(\n            lambda x: tf.reshape(x, (28 * 28,))\n        )\n        data_images = data_images.map(\n            lambda x: tf.image.convert_image_dtype(x, tf.float32)\n        )\n        data_labels = tf.data.Dataset.from_tensor_slices((labels))\n        dataset = tf.data.Dataset.zip((data_images, data_labels))\n        dataset = dataset.batch(1024 * 1)\n        return dataset\n\n    # apply the process function to the data\n    train, validation = (\n        process(train[0], train[1]),\n        process(validation[0], validation[1]),\n    )\n\n    # create the model\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(10, activation=tf.nn.sigmoid),\n            tf.keras.layers.Dense(10),\n        ]\n    )\n\n    # define the optimizer\n    optimizer = tf.optimizers.Adam(1e-3)\n\n    # the loss is provided by the AshPy library\n    loss = ClassifierLoss(tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n    logdir = \"testlog\"\n    epochs = 10\n\n    # the metrics are provided by the AshPy library\n    # and every metric with model_selection_operator != None performs\n    # model selection, saving the best model in a different folder per metric.\n    metrics = [\n        ClassifierMetric(\n            tf.metrics.Accuracy(), model_selection_operator=operator.gt\n        ),\n        ClassifierMetric(\n            tf.metrics.BinaryAccuracy(), model_selection_operator=operator.gt\n        ),\n    ]\n\n    # define the AshPy trainer\n    trainer = ClassifierTrainer(\n        model, optimizer, loss, epochs, metrics, logdir=logdir\n    )\n\n    # run the training process\n    trainer(train, validation)\n```\n\n## Features\n\nAshPy is a library designed to ease the burden of setting up all the nuances of the architectures built to train complex custom deep learning models. It provides both fully convolutional and fully connected models such as:\n\n- autoencoder\n- decoder\n- encoder\n\nand a fully convolutional:\n\n- unet\n\nMoreover, it provides already prepared trainers for a classifier model and GAN networks. In particular, in regards of the latter, it offers a basic GAN architecture with a Generator-Discriminator structure and an enhanced GAN architecture version made up of a Encoder-Generator-Discriminator structure.\n\n---\n\nAshPy it is developed around the concepts of _Executor_, _Context_, _Metric_, and _Strategies_ that represents its foundations.\n\n**Executor** An Executor is a class that helps to better generalize a training loop. With an Executor you can construct, for example, a custom loss function and put whatever computation you need inside it. You should define a `call` function inside your class and decorate it with `@Executor.reduce` header. Inside the `call` function you can take advantage of a context.\n\n**Context** A Context is a useful class in which all the models, metrics, dataset and mode of your network are set. Passing the context around means that you can any time access to all what you need in order to performs any type of computation.\n\n**Metric** A Metric is a class from which you can inherit to create your custom metric that can automatically keep track of the best performance of the model during training and, automatically save the best one doing what is called the *model selection*.\n\n**Strategies** If you want to distribute your training across multiple GPUs, there is the `tf.distribute.Strategy` TensorFlow API with which you can distribute your models and training code with minimal code changes. AshPy implements this type of strategies internally and will check everything for you to apply the distribution strategy correctly. All you need to do is as simple as doing the following:\n\n```python\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n\n    generator = ConvGenerator(\n        layer_spec_input_res=(7, 7),\n        layer_spec_target_res=(28, 28),\n        kernel_size=(5, 5),\n        initial_filters=256,\n        filters_cap=16,\n        channels=1,\n    )\n    # rest of the code\n    # with trainer definition and so on\n```\n\ni.e., create the strategy and put the rest of the code inside its scope.\n\nIn general AshPy aims to:\n\n- Rapid model prototyping\n- Enforcement of best practices & API consistency\n- Remove duplicated and boilerplate code\n- General usability by new project\n\n**NOTE:** We invite you to read the full documentation on [the official website](https://ashpy.zurutech.io/).\n\nThe following README aims to help you understand what you need to do to setup AshPy on your system and, with some examples, what you need to do to setup a complete training of your network. Moreover, it will explain some fundamental modules you need to understand to fully exploit the potential of the library.\n\n## Set up\n\n### Pip install\n```bash\npip install ashpy\n```\n\n### Source install\n\nClone this repo, go inside the downloaded folder and install with:\n```bash\npip install -e .\n```\n\n## Usage\n\nLet's quickly start with some examples.\n\n### Classifier\n\nLet's say we want to train a classifier.\n\n```python\nimport operator\nimport tensorflow as tf\nfrom ashpy.metrics import ClassifierMetric\nfrom ashpy.trainers.classifier import ClassifierTrainer\nfrom ashpy.losses.classifier import ClassifierLoss\n\ndef toy_dataset():\n    inputs = tf.expand_dims(tf.range(1, 1000.0), -1)\n    labels = tf.expand_dims([1 if tf.equal(tf.math.mod(tf.squeeze(i), 2), 0) else 0 for i in inputs], -1)\n    return tf.data.Dataset.from_tensor_slices((inputs,labels)).shuffle(10).batch(2)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation=tf.nn.sigmoid),\n    tf.keras.layers.Dense(2)\n])\n\noptimizer = tf.optimizers.Adam(1e-3)\nloss = ClassifierLoss(tf.losses.SparseCategoricalCrossentropy(from_logits=True))\nlogdir = \"testlog\"\nepochs = 2\n\nmetrics = [\n    ClassifierMetric(tf.metrics.Accuracy(), model_selection_operator=operator.gt),\n    ClassifierMetric(tf.metrics.BinaryAccuracy(), model_selection_operator=operator.gt),\n]\n\ntrainer = ClassifierTrainer(model, optimizer, loss, epochs, metrics, logdir=logdir)\n\ntrain, validation = toy_dataset(), toy_dataset()\ntrainer(train, validation)\n```\n\nSkipping the `toy_dataset()` function that creates a toy dataset, we'll give a look to the code step by step.\n\nSo, first of all we define a model and its optimizer. Here, the model is a very simple sequential Keras model defined as:\n\n```python\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation=tf.nn.sigmoid),\n    tf.keras.layers.Dense(2)\n])\n\noptimizer = tf.optimizers.Adam(1e-3)\n```\n\nThen we define the loss:\n\n```python\nloss = ClassifierLoss(tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n```\n\nThe `ClassifierLoss` loss defined above it is defined using an internal class called \"`Executor`\". The Executor is a class that let you define, alongside with a desired loss, the function that you want to use to \"evaluate\" that loss with all the needed parameters.\n\nThis works in conjunction with the following line (we will speak about the \"_metrics_\" and the other few definition lines in a minute):\n\n```python\ntrainer = ClassifierTrainer(model, optimizer, loss, epochs, metrics, logdir=logdir)\n```\n\nwhere a `ClassifierTrainer` is an object designed to run a specific training procedure adjusted, in this case, for a classifier.\n\nThe arguments of this function are the model, the optimizer, the loss, the number of epochs, the metrics and the logdir. We have already seen the definition of the model, the optimizer and of the loss. The definition of epochs, metrics and logdir happens here:\n\n```python\nlogdir = \"testlog\"\nepochs = 2\n\nmetrics = [\n    ClassifierMetric(tf.metrics.Accuracy(), model_selection_operator=operator.gt),\n    ClassifierMetric(\n    tf.metrics.BinaryAccuracy(),model_selection_operator=operator.gt),\n]\n```\n\nWhat we need to underline here is the definition of the metrics because as you can see they are defined through the use of specific classes: `ClassifierMetric`. As for the `ClassifierTrainer`, the `ClassifierMetric` it is a specified designed class for the Classifier. If you want to create a different metric you should inheriting from the Metric class provided by the Ash library. This kind of Metrics are useful because you can indicate a processing function to apply on predictions (e.g., tf.argmax) and an operator (e.g., operator.gt is the \"greater than\" operator) if you desire to activate the model selection during the training process based on that particular metric.\n\nFinally, once the datasets has been set, you can start the training procedure calling the trainer object:\n\n```python\ntrain, validation = toy_dataset(), toy_dataset()\ntrainer(train, validation)\n```\n\n## GAN - Generative Adversarial Network\n\nAshPy is equipped with two types of GAN network architectures:\n\n- A plain GAN network with the classic structure Generator - Discriminator.\n- A more elaborated GAN network architecture with the classic Generator - Discriminator structure plus an Encoder model (BiGAN like).\n\nAs for the previous classifier training example, let's see for first a simple example of an entire \"toy\" code, regarding a simple plain GAN. At the end we will briefly touch upon the differences with the GAN network with the Encoder.\n\n```python\nimport operator\nimport tensorflow as tf\nfrom ashpy.models.gans import ConvGenerator, ConvDiscriminator\nfrom ashpy.metrics import InceptionScore\nfrom ashpy.losses.gan import DiscriminatorMinMax, GeneratorBCE\n\ngenerator = ConvGenerator(\n    layer_spec_input_res=(7, 7),\n    layer_spec_target_res=(28, 28),\n    kernel_size=(5, 5),\n    initial_filters=32,\n    filters_cap=16,\n    channels=1,\n)\n\ndiscriminator = ConvDiscriminator(\n    layer_spec_input_res=(28, 28),\n    layer_spec_target_res=(7, 7),\n    kernel_size=(5, 5),\n    initial_filters=16,\n    filters_cap=32,\n    output_shape=1,\n)\n\n# Losses\ngenerator_bce = GeneratorBCE()\nminmax = DiscriminatorMinMax()\n\n# Real data\nbatch_size = 2\nmnist_x, mnist_y = tf.zeros((100,28,28)), tf.zeros((100,))\n\n# Trainer\nepochs = 2\nlogdir = \"testlog/adversarial\"\n\nmetrics = [\n    InceptionScore(\n        # Fake inception model\n        ConvDiscriminator(\n            layer_spec_input_res=(299, 299),\n            layer_spec_target_res=(7, 7),\n            kernel_size=(5, 5),\n            initial_filters=16,\n            filters_cap=32,\n            output_shape=10,\n        ),\n        model_selection_operator=operator.gt,\n        logdir=logdir,\n    )\n]\n\ntrainer = AdversarialTrainer(\n    generator,\n    discriminator,\n    tf.optimizers.Adam(1e-4),\n    tf.optimizers.Adam(1e-4),\n    generator_bce,\n    minmax,\n    epochs,\n    metrics,\n    logdir,\n)\n\n# Dataset\nnoise_dataset = tf.data.Dataset.from_tensors(0).repeat().map(\n    lambda _: tf.random.normal(shape=(100,), dtype=tf.float32, mean=0.0, stddev=1)\n).batch(batch_size).prefetch(1)\n\n# take only 2 samples to speed up tests\nreal_data = tf.data.Dataset.from_tensor_slices(\n        (tf.expand_dims(mnist_x, -1), tf.expand_dims(mnist_y, -1))\n    ).take(batch_size).batch(batch_size).prefetch(1)\n\n# Add noise in the same dataset, just by mapping.\n# The return type of the dataset must be: tuple(tuple(a,b), noise)\ndataset = real_data.map(lambda x, y: ((x, y), tf.random.normal(shape=(batch_size, 100))))\n\ntrainer(dataset)\n```\n\nFirst we define the generator and discriminator of the GAN architecture:\n\n```python\ngenerator = ConvGenerator(\n    layer_spec_input_res=(7, 7),\n    layer_spec_target_res=(28, 28),\n    kernel_size=(5, 5),\n    initial_filters=32,\n    filters_cap=16,\n    channels=1,\n)\n\ndiscriminator = ConvDiscriminator(\n    layer_spec_input_res=(28, 28),\n    layer_spec_target_res=(7, 7),\n    kernel_size=(5, 5),\n    initial_filters=16,\n    filters_cap=32,\n    output_shape=1,\n)\n```\n\nand then we define the losses:\n\n```python\n# Losses\ngenerator_bce = GeneratorBCE()\nminmax = DiscriminatorMinMax()\n```\n\nwhere `GeneratorBCE()` and `DiscriminatorMinMax()` are the losses defined inheriting `Executor`. Again, as we have seen in the previous classifier example, you can customize this type (the ones inheriting from the `Executor`) of losses.\n\nThe metrics are defined as follow:\n\n```python\nmetrics = [\n    InceptionScore(\n    # Fake inception model\n        ConvDiscriminator(\n        layer_spec_input_res=(299, 299),\n        layer_spec_target_res=(7, 7),\n        kernel_size=(5, 5),\n        initial_filters=16,\n        filters_cap=32,\n        output_shape=10,\n        ),\n        model_selection_operator=operator.gt,\n        logdir=logdir,\n    )\n]\n```\n\nand in particular here we have the InceptionScore metric constructed on the fly with the ConvDiscriminator class provided by AshPy.\n\nFinally, the actual trainer is constructed and then called:\n\n```python\ntrainer = AdversarialTrainer(\n    generator,\n    discriminator,\n    tf.optimizers.Adam(1e-4),\n    tf.optimizers.Adam(1e-4),\n    generator_bce,\n    minmax,\n    epochs,\n    metrics,\n    logdir,\n)\n```\n\n```python\ntrainer(dataset)\n```\n\nThe main difference with a GAN architecture with an Encoder is that we would have the encoder loss:\n\n```python\nencoder_bce = EncoderBCE()\n```\n\nan encoder accuracy metric:\n\n```python\nmetrics = [EncodingAccuracy(classifier, model_selection_operator=operator.gt, logdir=logdir)]\n```\n\nand an EncoderTrainer:\n\n```python\ntrainer = EncoderTrainer(\n    generator,\n    discriminator,\n    encoder,\n    tf.optimizers.Adam(1e-4),\n    tf.optimizers.Adam(1e-5),\n    tf.optimizers.Adam(1e-6),\n    generator_bce,\n    minmax,\n    encoder_bce,\n    epochs,\n    metrics=metrics,\n    logdir=logdir,\n)\n```\n\nNote that the `EncoderTrainer` indicates a trainer of a GAN network with an Encoder and not a trainer of an Encoder itself.\n\n## Dataset Output Format\n\nIn order to standardize the GAN training, AshPy requires the input dataset to be in a common format. In particular, the dataset return type must always be in the format showed below, where the fist element of the tuple is the discriminator input, and the second is the generator input.\n\n```\ntuple(tuple(a,b), noise)\n```\n\nWhere `a` is the input sample, `b` is the label/condition (if any, otherwise fill it with `0`), and `noise` is the latent vector of input.\n\nTo train Pix2Pix-like architecture, that have no `noise` as ConvGenerator input, just return the values in thee format `(tuple(a,b), b)` since the condition is the generator input.\n\n## Test\nIn order to run the tests (with the doctests), linting and docs generation simply use `tox`.\n\n```bash\ntox\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/zurutech/ashpy", "keywords": "ashpy,ai,tensorflow,tensorflow-2.0,deeplearning", "license": "Apache License, Version 2.0", "maintainer": "", "maintainer_email": "", "name": "ashpy", "package_url": "https://pypi.org/project/ashpy/", "platform": "", "project_url": "https://pypi.org/project/ashpy/", "project_urls": {"Homepage": "https://github.com/zurutech/ashpy"}, "release_url": "https://pypi.org/project/ashpy/0.4.0/", "requires_dist": null, "requires_python": "", "summary": "TensorFlow 2.0 library for distributed training, evaluation, model selection, and fast prototyping.", "version": "0.4.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div>\n    <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a1332807eb18c9965c7c600eb9d30ae637ff6e99/68747470733a2f2f626c6f672e7a7572752e746563682f696d616765732f61736870792f6c6f676f5f6c712e706e67\">\n</div>\n<h1>AshPy</h1>\n<p><img alt=\"Contributions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/793ef3fadef48113e66f3c652ebc9b591bcb1745/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c6174\">\n<img alt=\"Python - Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8091695f6fb60392c0e06076f934f7fc6826116f/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f61736870792e737667\">\n<img alt=\"PyPy - Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9fc7bb84c6a6d3d08d50137a3c6065838221a6d1/68747470733a2f2f62616467652e667572792e696f2f70792f61736870792e737667\">\n<img alt=\"PyPI - License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/851c9f05493e2d9a0c7dd08443e9a7c82e378b62/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f61736870792e737667\">\n<img alt=\"Ashpy - Badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ca419099446868fac3610e06a82bdba14f114c75/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7061636b6167652d61736870792d627269676874677265656e2e737667\">\n<a href=\"https://codecov.io/gh/zurutech/ashpy\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f29ccc0f919483188ec077abf0f2416314df5d87/68747470733a2f2f636f6465636f762e696f2f67682f7a757275746563682f61736870792f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<a href=\"https://travis-ci.org/zurutech/ashpy\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e69810c487fa6a52d1347090ed25d2d3b1fcddcb/68747470733a2f2f7472617669732d63692e6f72672f7a757275746563682f61736870792e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://ashpy.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e5e2522c520d8013a865a0de72243411b1b97980/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f61736870792f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<a href=\"https://github.com/python/black\" rel=\"nofollow\"><img alt=\"Black - Badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fbfdc7754183ecf079bc71ddeabaf88f6cbc5c00/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\"></a>\n<a href=\"https://www.codefactor.io/repository/github/zurutech/ashpy\" rel=\"nofollow\"><img alt=\"CodeFactor\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1c443d5d77ff576760516db6e82ef11c8aae8d15/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f7a757275746563682f61736870792f6261646765\"></a></p>\n<p>AshPy is a TensorFlow 2.1 library for (<strong>distributed</strong>) training, evaluation, model selection, and fast prototyping.\nIt is designed to ease the burden of setting up all the nuances of the architectures built to train complex custom deep learning models.</p>\n<p><a href=\"#quick-example\" rel=\"nofollow\">Quick Example</a> | <a href=\"#features\" rel=\"nofollow\">Features</a> | <a href=\"#set-up\" rel=\"nofollow\">Set Up</a> | <a href=\"#usage\" rel=\"nofollow\">Usage</a> | <a href=\"#dataset-output-format\" rel=\"nofollow\">Dataset Output Format</a> | <a href=\"#test\" rel=\"nofollow\">Test</a></p>\n<h2>Quick Example</h2>\n<pre><span class=\"c1\"># define a distribution strategy</span>\n<span class=\"n\">strategy</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">distribute</span><span class=\"o\">.</span><span class=\"n\">MirroredStrategy</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># work inside the scope of the created strategy</span>\n<span class=\"k\">with</span> <span class=\"n\">strategy</span><span class=\"o\">.</span><span class=\"n\">scope</span><span class=\"p\">():</span>\n\n    <span class=\"c1\"># get the MNIST dataset</span>\n    <span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">validation</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">mnist</span><span class=\"o\">.</span><span class=\"n\">load_data</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># process data if needed</span>\n    <span class=\"k\">def</span> <span class=\"nf\">process</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">):</span>\n        <span class=\"n\">data_images</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">from_tensor_slices</span><span class=\"p\">((</span><span class=\"n\">images</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span>\n            <span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">28</span> <span class=\"o\">*</span> <span class=\"mi\">28</span><span class=\"p\">,))</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">data_images</span> <span class=\"o\">=</span> <span class=\"n\">data_images</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span>\n            <span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">convert_image_dtype</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">data_labels</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">from_tensor_slices</span><span class=\"p\">((</span><span class=\"n\">labels</span><span class=\"p\">))</span>\n        <span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">zip</span><span class=\"p\">((</span><span class=\"n\">data_images</span><span class=\"p\">,</span> <span class=\"n\">data_labels</span><span class=\"p\">))</span>\n        <span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"mi\">1024</span> <span class=\"o\">*</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">dataset</span>\n\n    <span class=\"c1\"># apply the process function to the data</span>\n    <span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">validation</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n        <span class=\"n\">process</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">train</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]),</span>\n        <span class=\"n\">process</span><span class=\"p\">(</span><span class=\"n\">validation</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">validation</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]),</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"c1\"># create the model</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n        <span class=\"p\">[</span>\n            <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">sigmoid</span><span class=\"p\">),</span>\n            <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span>\n        <span class=\"p\">]</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"c1\"># define the optimizer</span>\n    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># the loss is provided by the AshPy library</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">ClassifierLoss</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">SparseCategoricalCrossentropy</span><span class=\"p\">(</span><span class=\"n\">from_logits</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>\n    <span class=\"n\">logdir</span> <span class=\"o\">=</span> <span class=\"s2\">\"testlog\"</span>\n    <span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n\n    <span class=\"c1\"># the metrics are provided by the AshPy library</span>\n    <span class=\"c1\"># and every metric with model_selection_operator != None performs</span>\n    <span class=\"c1\"># model selection, saving the best model in a different folder per metric.</span>\n    <span class=\"n\">metrics</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n        <span class=\"n\">ClassifierMetric</span><span class=\"p\">(</span>\n            <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">Accuracy</span><span class=\"p\">(),</span> <span class=\"n\">model_selection_operator</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">gt</span>\n        <span class=\"p\">),</span>\n        <span class=\"n\">ClassifierMetric</span><span class=\"p\">(</span>\n            <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">BinaryAccuracy</span><span class=\"p\">(),</span> <span class=\"n\">model_selection_operator</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">gt</span>\n        <span class=\"p\">),</span>\n    <span class=\"p\">]</span>\n\n    <span class=\"c1\"># define the AshPy trainer</span>\n    <span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">ClassifierTrainer</span><span class=\"p\">(</span>\n        <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"p\">,</span> <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"n\">logdir</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"c1\"># run the training process</span>\n    <span class=\"n\">trainer</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">validation</span><span class=\"p\">)</span>\n</pre>\n<h2>Features</h2>\n<p>AshPy is a library designed to ease the burden of setting up all the nuances of the architectures built to train complex custom deep learning models. It provides both fully convolutional and fully connected models such as:</p>\n<ul>\n<li>autoencoder</li>\n<li>decoder</li>\n<li>encoder</li>\n</ul>\n<p>and a fully convolutional:</p>\n<ul>\n<li>unet</li>\n</ul>\n<p>Moreover, it provides already prepared trainers for a classifier model and GAN networks. In particular, in regards of the latter, it offers a basic GAN architecture with a Generator-Discriminator structure and an enhanced GAN architecture version made up of a Encoder-Generator-Discriminator structure.</p>\n<hr>\n<p>AshPy it is developed around the concepts of <em>Executor</em>, <em>Context</em>, <em>Metric</em>, and <em>Strategies</em> that represents its foundations.</p>\n<p><strong>Executor</strong> An Executor is a class that helps to better generalize a training loop. With an Executor you can construct, for example, a custom loss function and put whatever computation you need inside it. You should define a <code>call</code> function inside your class and decorate it with <code>@Executor.reduce</code> header. Inside the <code>call</code> function you can take advantage of a context.</p>\n<p><strong>Context</strong> A Context is a useful class in which all the models, metrics, dataset and mode of your network are set. Passing the context around means that you can any time access to all what you need in order to performs any type of computation.</p>\n<p><strong>Metric</strong> A Metric is a class from which you can inherit to create your custom metric that can automatically keep track of the best performance of the model during training and, automatically save the best one doing what is called the <em>model selection</em>.</p>\n<p><strong>Strategies</strong> If you want to distribute your training across multiple GPUs, there is the <code>tf.distribute.Strategy</code> TensorFlow API with which you can distribute your models and training code with minimal code changes. AshPy implements this type of strategies internally and will check everything for you to apply the distribution strategy correctly. All you need to do is as simple as doing the following:</p>\n<pre><span class=\"n\">strategy</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">distribute</span><span class=\"o\">.</span><span class=\"n\">MirroredStrategy</span><span class=\"p\">()</span>\n<span class=\"k\">with</span> <span class=\"n\">strategy</span><span class=\"o\">.</span><span class=\"n\">scope</span><span class=\"p\">():</span>\n\n    <span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">ConvGenerator</span><span class=\"p\">(</span>\n        <span class=\"n\">layer_spec_input_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">),</span>\n        <span class=\"n\">layer_spec_target_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">),</span>\n        <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>\n        <span class=\"n\">initial_filters</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span>\n        <span class=\"n\">filters_cap</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n        <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n    <span class=\"c1\"># rest of the code</span>\n    <span class=\"c1\"># with trainer definition and so on</span>\n</pre>\n<p>i.e., create the strategy and put the rest of the code inside its scope.</p>\n<p>In general AshPy aims to:</p>\n<ul>\n<li>Rapid model prototyping</li>\n<li>Enforcement of best practices &amp; API consistency</li>\n<li>Remove duplicated and boilerplate code</li>\n<li>General usability by new project</li>\n</ul>\n<p><strong>NOTE:</strong> We invite you to read the full documentation on <a href=\"https://ashpy.zurutech.io/\" rel=\"nofollow\">the official website</a>.</p>\n<p>The following README aims to help you understand what you need to do to setup AshPy on your system and, with some examples, what you need to do to setup a complete training of your network. Moreover, it will explain some fundamental modules you need to understand to fully exploit the potential of the library.</p>\n<h2>Set up</h2>\n<h3>Pip install</h3>\n<pre>pip install ashpy\n</pre>\n<h3>Source install</h3>\n<p>Clone this repo, go inside the downloaded folder and install with:</p>\n<pre>pip install -e .\n</pre>\n<h2>Usage</h2>\n<p>Let's quickly start with some examples.</p>\n<h3>Classifier</h3>\n<p>Let's say we want to train a classifier.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">operator</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ashpy.metrics</span> <span class=\"kn\">import</span> <span class=\"n\">ClassifierMetric</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ashpy.trainers.classifier</span> <span class=\"kn\">import</span> <span class=\"n\">ClassifierTrainer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ashpy.losses.classifier</span> <span class=\"kn\">import</span> <span class=\"n\">ClassifierLoss</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">toy_dataset</span><span class=\"p\">():</span>\n    <span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">expand_dims</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">1000.0</span><span class=\"p\">),</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">expand_dims</span><span class=\"p\">([</span><span class=\"mi\">1</span> <span class=\"k\">if</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">equal</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">mod</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">),</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"k\">else</span> <span class=\"mi\">0</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">inputs</span><span class=\"p\">],</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">from_tensor_slices</span><span class=\"p\">((</span><span class=\"n\">inputs</span><span class=\"p\">,</span><span class=\"n\">labels</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">sigmoid</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">ClassifierLoss</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">SparseCategoricalCrossentropy</span><span class=\"p\">(</span><span class=\"n\">from_logits</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>\n<span class=\"n\">logdir</span> <span class=\"o\">=</span> <span class=\"s2\">\"testlog\"</span>\n<span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n\n<span class=\"n\">metrics</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">ClassifierMetric</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">Accuracy</span><span class=\"p\">(),</span> <span class=\"n\">model_selection_operator</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">gt</span><span class=\"p\">),</span>\n    <span class=\"n\">ClassifierMetric</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">BinaryAccuracy</span><span class=\"p\">(),</span> <span class=\"n\">model_selection_operator</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">gt</span><span class=\"p\">),</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">ClassifierTrainer</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"p\">,</span> <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"n\">logdir</span><span class=\"p\">)</span>\n\n<span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">validation</span> <span class=\"o\">=</span> <span class=\"n\">toy_dataset</span><span class=\"p\">(),</span> <span class=\"n\">toy_dataset</span><span class=\"p\">()</span>\n<span class=\"n\">trainer</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">validation</span><span class=\"p\">)</span>\n</pre>\n<p>Skipping the <code>toy_dataset()</code> function that creates a toy dataset, we'll give a look to the code step by step.</p>\n<p>So, first of all we define a model and its optimizer. Here, the model is a very simple sequential Keras model defined as:</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">sigmoid</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n</pre>\n<p>Then we define the loss:</p>\n<pre><span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">ClassifierLoss</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">SparseCategoricalCrossentropy</span><span class=\"p\">(</span><span class=\"n\">from_logits</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>\n</pre>\n<p>The <code>ClassifierLoss</code> loss defined above it is defined using an internal class called \"<code>Executor</code>\". The Executor is a class that let you define, alongside with a desired loss, the function that you want to use to \"evaluate\" that loss with all the needed parameters.</p>\n<p>This works in conjunction with the following line (we will speak about the \"<em>metrics</em>\" and the other few definition lines in a minute):</p>\n<pre><span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">ClassifierTrainer</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"p\">,</span> <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"n\">logdir</span><span class=\"p\">)</span>\n</pre>\n<p>where a <code>ClassifierTrainer</code> is an object designed to run a specific training procedure adjusted, in this case, for a classifier.</p>\n<p>The arguments of this function are the model, the optimizer, the loss, the number of epochs, the metrics and the logdir. We have already seen the definition of the model, the optimizer and of the loss. The definition of epochs, metrics and logdir happens here:</p>\n<pre><span class=\"n\">logdir</span> <span class=\"o\">=</span> <span class=\"s2\">\"testlog\"</span>\n<span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n\n<span class=\"n\">metrics</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">ClassifierMetric</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">Accuracy</span><span class=\"p\">(),</span> <span class=\"n\">model_selection_operator</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">gt</span><span class=\"p\">),</span>\n    <span class=\"n\">ClassifierMetric</span><span class=\"p\">(</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">BinaryAccuracy</span><span class=\"p\">(),</span><span class=\"n\">model_selection_operator</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">gt</span><span class=\"p\">),</span>\n<span class=\"p\">]</span>\n</pre>\n<p>What we need to underline here is the definition of the metrics because as you can see they are defined through the use of specific classes: <code>ClassifierMetric</code>. As for the <code>ClassifierTrainer</code>, the <code>ClassifierMetric</code> it is a specified designed class for the Classifier. If you want to create a different metric you should inheriting from the Metric class provided by the Ash library. This kind of Metrics are useful because you can indicate a processing function to apply on predictions (e.g., tf.argmax) and an operator (e.g., operator.gt is the \"greater than\" operator) if you desire to activate the model selection during the training process based on that particular metric.</p>\n<p>Finally, once the datasets has been set, you can start the training procedure calling the trainer object:</p>\n<pre><span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">validation</span> <span class=\"o\">=</span> <span class=\"n\">toy_dataset</span><span class=\"p\">(),</span> <span class=\"n\">toy_dataset</span><span class=\"p\">()</span>\n<span class=\"n\">trainer</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">validation</span><span class=\"p\">)</span>\n</pre>\n<h2>GAN - Generative Adversarial Network</h2>\n<p>AshPy is equipped with two types of GAN network architectures:</p>\n<ul>\n<li>A plain GAN network with the classic structure Generator - Discriminator.</li>\n<li>A more elaborated GAN network architecture with the classic Generator - Discriminator structure plus an Encoder model (BiGAN like).</li>\n</ul>\n<p>As for the previous classifier training example, let's see for first a simple example of an entire \"toy\" code, regarding a simple plain GAN. At the end we will briefly touch upon the differences with the GAN network with the Encoder.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">operator</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ashpy.models.gans</span> <span class=\"kn\">import</span> <span class=\"n\">ConvGenerator</span><span class=\"p\">,</span> <span class=\"n\">ConvDiscriminator</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ashpy.metrics</span> <span class=\"kn\">import</span> <span class=\"n\">InceptionScore</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ashpy.losses.gan</span> <span class=\"kn\">import</span> <span class=\"n\">DiscriminatorMinMax</span><span class=\"p\">,</span> <span class=\"n\">GeneratorBCE</span>\n\n<span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">ConvGenerator</span><span class=\"p\">(</span>\n    <span class=\"n\">layer_spec_input_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">),</span>\n    <span class=\"n\">layer_spec_target_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">),</span>\n    <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>\n    <span class=\"n\">initial_filters</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n    <span class=\"n\">filters_cap</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n    <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">discriminator</span> <span class=\"o\">=</span> <span class=\"n\">ConvDiscriminator</span><span class=\"p\">(</span>\n    <span class=\"n\">layer_spec_input_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">),</span>\n    <span class=\"n\">layer_spec_target_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">),</span>\n    <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>\n    <span class=\"n\">initial_filters</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n    <span class=\"n\">filters_cap</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n    <span class=\"n\">output_shape</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Losses</span>\n<span class=\"n\">generator_bce</span> <span class=\"o\">=</span> <span class=\"n\">GeneratorBCE</span><span class=\"p\">()</span>\n<span class=\"n\">minmax</span> <span class=\"o\">=</span> <span class=\"n\">DiscriminatorMinMax</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Real data</span>\n<span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n<span class=\"n\">mnist_x</span><span class=\"p\">,</span> <span class=\"n\">mnist_y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">100</span><span class=\"p\">,</span><span class=\"mi\">28</span><span class=\"p\">,</span><span class=\"mi\">28</span><span class=\"p\">)),</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">100</span><span class=\"p\">,))</span>\n\n<span class=\"c1\"># Trainer</span>\n<span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n<span class=\"n\">logdir</span> <span class=\"o\">=</span> <span class=\"s2\">\"testlog/adversarial\"</span>\n\n<span class=\"n\">metrics</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">InceptionScore</span><span class=\"p\">(</span>\n        <span class=\"c1\"># Fake inception model</span>\n        <span class=\"n\">ConvDiscriminator</span><span class=\"p\">(</span>\n            <span class=\"n\">layer_spec_input_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">299</span><span class=\"p\">,</span> <span class=\"mi\">299</span><span class=\"p\">),</span>\n            <span class=\"n\">layer_spec_target_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">),</span>\n            <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>\n            <span class=\"n\">initial_filters</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n            <span class=\"n\">filters_cap</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n            <span class=\"n\">output_shape</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"n\">model_selection_operator</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">gt</span><span class=\"p\">,</span>\n        <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"n\">logdir</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">AdversarialTrainer</span><span class=\"p\">(</span>\n    <span class=\"n\">generator</span><span class=\"p\">,</span>\n    <span class=\"n\">discriminator</span><span class=\"p\">,</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-4</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-4</span><span class=\"p\">),</span>\n    <span class=\"n\">generator_bce</span><span class=\"p\">,</span>\n    <span class=\"n\">minmax</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"p\">,</span>\n    <span class=\"n\">metrics</span><span class=\"p\">,</span>\n    <span class=\"n\">logdir</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Dataset</span>\n<span class=\"n\">noise_dataset</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">from_tensors</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">repeat</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span>\n    <span class=\"k\">lambda</span> <span class=\"n\">_</span><span class=\"p\">:</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"n\">stddev</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">prefetch</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># take only 2 samples to speed up tests</span>\n<span class=\"n\">real_data</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">from_tensor_slices</span><span class=\"p\">(</span>\n        <span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">expand_dims</span><span class=\"p\">(</span><span class=\"n\">mnist_x</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">expand_dims</span><span class=\"p\">(</span><span class=\"n\">mnist_y</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">take</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">prefetch</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Add noise in the same dataset, just by mapping.</span>\n<span class=\"c1\"># The return type of the dataset must be: tuple(tuple(a,b), noise)</span>\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">real_data</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"p\">((</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">),</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">))))</span>\n\n<span class=\"n\">trainer</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n</pre>\n<p>First we define the generator and discriminator of the GAN architecture:</p>\n<pre><span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">ConvGenerator</span><span class=\"p\">(</span>\n    <span class=\"n\">layer_spec_input_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">),</span>\n    <span class=\"n\">layer_spec_target_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">),</span>\n    <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>\n    <span class=\"n\">initial_filters</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n    <span class=\"n\">filters_cap</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n    <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">discriminator</span> <span class=\"o\">=</span> <span class=\"n\">ConvDiscriminator</span><span class=\"p\">(</span>\n    <span class=\"n\">layer_spec_input_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">),</span>\n    <span class=\"n\">layer_spec_target_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">),</span>\n    <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>\n    <span class=\"n\">initial_filters</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n    <span class=\"n\">filters_cap</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n    <span class=\"n\">output_shape</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre>\n<p>and then we define the losses:</p>\n<pre><span class=\"c1\"># Losses</span>\n<span class=\"n\">generator_bce</span> <span class=\"o\">=</span> <span class=\"n\">GeneratorBCE</span><span class=\"p\">()</span>\n<span class=\"n\">minmax</span> <span class=\"o\">=</span> <span class=\"n\">DiscriminatorMinMax</span><span class=\"p\">()</span>\n</pre>\n<p>where <code>GeneratorBCE()</code> and <code>DiscriminatorMinMax()</code> are the losses defined inheriting <code>Executor</code>. Again, as we have seen in the previous classifier example, you can customize this type (the ones inheriting from the <code>Executor</code>) of losses.</p>\n<p>The metrics are defined as follow:</p>\n<pre><span class=\"n\">metrics</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">InceptionScore</span><span class=\"p\">(</span>\n    <span class=\"c1\"># Fake inception model</span>\n        <span class=\"n\">ConvDiscriminator</span><span class=\"p\">(</span>\n        <span class=\"n\">layer_spec_input_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">299</span><span class=\"p\">,</span> <span class=\"mi\">299</span><span class=\"p\">),</span>\n        <span class=\"n\">layer_spec_target_res</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">),</span>\n        <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">),</span>\n        <span class=\"n\">initial_filters</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n        <span class=\"n\">filters_cap</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n        <span class=\"n\">output_shape</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"n\">model_selection_operator</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">gt</span><span class=\"p\">,</span>\n        <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"n\">logdir</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">]</span>\n</pre>\n<p>and in particular here we have the InceptionScore metric constructed on the fly with the ConvDiscriminator class provided by AshPy.</p>\n<p>Finally, the actual trainer is constructed and then called:</p>\n<pre><span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">AdversarialTrainer</span><span class=\"p\">(</span>\n    <span class=\"n\">generator</span><span class=\"p\">,</span>\n    <span class=\"n\">discriminator</span><span class=\"p\">,</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-4</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-4</span><span class=\"p\">),</span>\n    <span class=\"n\">generator_bce</span><span class=\"p\">,</span>\n    <span class=\"n\">minmax</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"p\">,</span>\n    <span class=\"n\">metrics</span><span class=\"p\">,</span>\n    <span class=\"n\">logdir</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre>\n<pre><span class=\"n\">trainer</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n</pre>\n<p>The main difference with a GAN architecture with an Encoder is that we would have the encoder loss:</p>\n<pre><span class=\"n\">encoder_bce</span> <span class=\"o\">=</span> <span class=\"n\">EncoderBCE</span><span class=\"p\">()</span>\n</pre>\n<p>an encoder accuracy metric:</p>\n<pre><span class=\"n\">metrics</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">EncodingAccuracy</span><span class=\"p\">(</span><span class=\"n\">classifier</span><span class=\"p\">,</span> <span class=\"n\">model_selection_operator</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">gt</span><span class=\"p\">,</span> <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"n\">logdir</span><span class=\"p\">)]</span>\n</pre>\n<p>and an EncoderTrainer:</p>\n<pre><span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">EncoderTrainer</span><span class=\"p\">(</span>\n    <span class=\"n\">generator</span><span class=\"p\">,</span>\n    <span class=\"n\">discriminator</span><span class=\"p\">,</span>\n    <span class=\"n\">encoder</span><span class=\"p\">,</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-4</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-5</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"mf\">1e-6</span><span class=\"p\">),</span>\n    <span class=\"n\">generator_bce</span><span class=\"p\">,</span>\n    <span class=\"n\">minmax</span><span class=\"p\">,</span>\n    <span class=\"n\">encoder_bce</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"p\">,</span>\n    <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"n\">metrics</span><span class=\"p\">,</span>\n    <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"n\">logdir</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre>\n<p>Note that the <code>EncoderTrainer</code> indicates a trainer of a GAN network with an Encoder and not a trainer of an Encoder itself.</p>\n<h2>Dataset Output Format</h2>\n<p>In order to standardize the GAN training, AshPy requires the input dataset to be in a common format. In particular, the dataset return type must always be in the format showed below, where the fist element of the tuple is the discriminator input, and the second is the generator input.</p>\n<pre><code>tuple(tuple(a,b), noise)\n</code></pre>\n<p>Where <code>a</code> is the input sample, <code>b</code> is the label/condition (if any, otherwise fill it with <code>0</code>), and <code>noise</code> is the latent vector of input.</p>\n<p>To train Pix2Pix-like architecture, that have no <code>noise</code> as ConvGenerator input, just return the values in thee format <code>(tuple(a,b), b)</code> since the condition is the generator input.</p>\n<h2>Test</h2>\n<p>In order to run the tests (with the doctests), linting and docs generation simply use <code>tox</code>.</p>\n<pre>tox\n</pre>\n\n          </div>"}, "last_serial": 6588926, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "7679ebda85789a4ebffaf2f309a8e082", "sha256": "116d698d7f103d482a36f5a00703be98529ae03f18c63117096b9adb0d77f74e"}, "downloads": -1, "filename": "ashpy-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "7679ebda85789a4ebffaf2f309a8e082", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 90146, "upload_time": "2019-09-05T08:51:10", "upload_time_iso_8601": "2019-09-05T08:51:10.764928Z", "url": "https://files.pythonhosted.org/packages/49/fe/1304822a4947e95ba6b324b1db7a1971e5bbf7d2ad1042357183f30d2e09/ashpy-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "36029e1ebe00bd2cda8071975e421e3e", "sha256": "7d79e6e8a7d38ab05c7e3f496cfb2de6c0b2ac7955bd62d8013f0f6a29b76098"}, "downloads": -1, "filename": "ashpy-0.1.2.tar.gz", "has_sig": false, "md5_digest": "36029e1ebe00bd2cda8071975e421e3e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 55178, "upload_time": "2019-09-10T09:16:15", "upload_time_iso_8601": "2019-09-10T09:16:15.495318Z", "url": "https://files.pythonhosted.org/packages/84/e0/8eeb619e0a5ee022a9daf197a34718e954ac32e39a35908acd9d4a809da4/ashpy-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "05c8b67a7de433849bb97289f1e1c43a", "sha256": "ce888d19d7706ada8699e8dba324f03cdc3df253b2f5420075b457fdc3b56308"}, "downloads": -1, "filename": "ashpy-0.1.3.tar.gz", "has_sig": false, "md5_digest": "05c8b67a7de433849bb97289f1e1c43a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 55175, "upload_time": "2019-09-10T10:32:44", "upload_time_iso_8601": "2019-09-10T10:32:44.127821Z", "url": "https://files.pythonhosted.org/packages/a6/cc/91739b9c3c5a5ceff6ac0b97ed9267d2762b85eb67a808a95788c141682c/ashpy-0.1.3.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "12d2b12b80875bcb8c3e2042aad63181", "sha256": "d48a2948bd56564560faa3e3f6f61100a55d7ea251d3e0f63db93702320617f2"}, "downloads": -1, "filename": "ashpy-0.2.0.tar.gz", "has_sig": false, "md5_digest": "12d2b12b80875bcb8c3e2042aad63181", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 65967, "upload_time": "2019-10-07T10:13:27", "upload_time_iso_8601": "2019-10-07T10:13:27.504976Z", "url": "https://files.pythonhosted.org/packages/36/9a/17ade8327d1acc46473dc1ecfba7ea9bd81036e4e5fb468110249edde414/ashpy-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "3d34d1417ecbb255740d80a5fbcd124d", "sha256": "27c182e0c8b409de91fc69d6c79d1170643cffcc9f7a78999710059b4b7c07da"}, "downloads": -1, "filename": "ashpy-0.3.0.tar.gz", "has_sig": false, "md5_digest": "3d34d1417ecbb255740d80a5fbcd124d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66029, "upload_time": "2020-01-28T16:08:29", "upload_time_iso_8601": "2020-01-28T16:08:29.465445Z", "url": "https://files.pythonhosted.org/packages/2c/17/81aedda56ef73d9c85a88086c579aa609e6bd837bd2a6895adf6cec4ea45/ashpy-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "83761391eb7c48ee2674b155369a27ce", "sha256": "eb7744f1b6e8e8be82024c2796add24b25b2143722505ab002cd64e37423ded5"}, "downloads": -1, "filename": "ashpy-0.3.1.tar.gz", "has_sig": false, "md5_digest": "83761391eb7c48ee2674b155369a27ce", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66495, "upload_time": "2020-01-29T16:10:07", "upload_time_iso_8601": "2020-01-29T16:10:07.699831Z", "url": "https://files.pythonhosted.org/packages/be/c4/de6f9c562de658c27abc2cd4016c8727fcc50008f1d8b66e4f20616c1156/ashpy-0.3.1.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "daab96a5b7c93412e52ddb96fede149a", "sha256": "084ac8e54c057bc07030ea0efb227770cd301437ebb5779e7d7ef9748f6d2196"}, "downloads": -1, "filename": "ashpy-0.4.0.tar.gz", "has_sig": false, "md5_digest": "daab96a5b7c93412e52ddb96fede149a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 67480, "upload_time": "2020-02-07T15:18:33", "upload_time_iso_8601": "2020-02-07T15:18:33.171354Z", "url": "https://files.pythonhosted.org/packages/63/10/9cd351a6fe09413f928bad2239dbdb2a120da6bf203143a4169ae5cef713/ashpy-0.4.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "daab96a5b7c93412e52ddb96fede149a", "sha256": "084ac8e54c057bc07030ea0efb227770cd301437ebb5779e7d7ef9748f6d2196"}, "downloads": -1, "filename": "ashpy-0.4.0.tar.gz", "has_sig": false, "md5_digest": "daab96a5b7c93412e52ddb96fede149a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 67480, "upload_time": "2020-02-07T15:18:33", "upload_time_iso_8601": "2020-02-07T15:18:33.171354Z", "url": "https://files.pythonhosted.org/packages/63/10/9cd351a6fe09413f928bad2239dbdb2a120da6bf203143a4169ae5cef713/ashpy-0.4.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:17:00 2020"}