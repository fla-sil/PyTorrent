{"info": {"author": "OpenStack", "author_email": "openstack-dev@lists.openstack.org", "bugtrack_url": null, "classifiers": ["Environment :: OpenStack", "Intended Audience :: Information Technology", "Intended Audience :: System Administrators", "License :: OSI Approved :: Apache Software License", "Operating System :: POSIX :: Linux", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4"], "description": "===============================\ngearhorn\n===============================\n\n\nA gearman worker which enables efficient broadcast communication\n\n* Free software: Apache license\n* Documentation: http://docs.openstack.org/developer/gearhorn\n* Source: http://git.openstack.org/cgit/openstack-infra/gearhorn\n* Bugs: http://bugs.launchpad.net/gearhorn\n\n=============\nFanout Worker\n=============\n\nGearman has no in-built way to copy jobs to multiple workers. This worker\nwill do that to support the common messaging pattern known as \"Fanout\"\n\nOnce running, the fanout worker will subscribe to the queues\n\"register_fanout_subscriber\" and \"fanout\".\n\nsubscribe_fanout unsubscribe_fanout\n-----------------------------------\n\nThese queues will have a JSON payload consisting of a mapping with two\nkeys: topic, client_id. The client_id will be kept as a set under the\ntopic for use in sending messages to the appropriate subscriber queue. It\nshould be unique for every intended recipient queue for every fanout\nrequest. Usually this will be unique per host.\n\nfanout\n------\n\nThis queue will have a JSON payload consisting of a mapping with two keys:\ntopic, payload. The topic will be used to search for a list of subscriber\nids. For each subscriber_id found, a copy of the payload will be sent\nto the queue named  topic_subscriber_id. So if we had two subscribers to\n\"officememos\" with ids \"bob\" and \"alice\", then a message to fanout with\nthis payload::\n\n    {\"topic\": \"officememos\",\n     \"payload\": \"please go home early today.\"}\n\nOptionally one can add a 'unique' key to make use of Gearman's\nunique/coalescing features. Also the 'background' key can be used to\nset a truthy value, which will tell gearhorn not to wait for receivers\nbefore moving on to more messages.\n\nWould result in the worker sending a copy of the payload to the queues\n\"officememos_bob\" and \"officememos_alice\".\n\nMatchmaking\n~~~~~~~~~~~\n\nIn order to match up topics with subscribers workers must share a list\nof subscribers for each topic. The mapping is maintained in a backend\ndata store. Any time new registration events are seen the list is updated\nin that store and a message is sent to the __matchmaking topic. Workers\nautomatically add and remove themselves to/from the __matchmaking list\nin order to ensure they're informed and able to clear cache whenever\nthe canonical list is updated. Workers that fail violently must be\nmanually removed.\n\n===========\nFailed Idea\n===========\n\nThe following was idea #1, and fails to be any more efficient than just\nrunnint Redis as a pub/sub and direct comm backend. It remains here as\na reminder not to reimplement Redis.\n\nThe expected way to use it is to have a gearman client that wants to\nreceive broadcasts request the given broadcast function with a unique ID\nthat is the last seen sequence ID. When this daemon receives that work,\nit looks for any items with sequence number greater than this ID, and\nif it finds them, reply with a json payload of::\n\n    {\"sequence\": 2,\n     \"payload\": \"foobarbaz\"}\n\nClients would consume this, and then submit a new job with the\nnew_seq. This allows a stream of sync jobs to become a stream of\nbroadcasted payloads with a good chance of a client receiving all\nsequences.\n\nTODO\n----\n\n* True functional tests\n\n* Make all the things configurable\n\n  * Work queue name(s)\n\n  * Tuning backlog and flush frequency\n\n* Consider a work queue that workers can use to share the current sequence\n  they should be using.\n\n  * Would this be too racy or given the \"best effort\" is it enough to\n    just try hard?\n\n  * Do we need a UUID of some kind just to allow clients to detect if\n    all the workers went away and sequences reset at 0?\n\n* Add a client helper module which implements the sequence fetching to\n  make it easier to write a consumer in python", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://www.openstack.org/", "keywords": null, "license": "UNKNOWN", "maintainer": null, "maintainer_email": null, "name": "gearhorn", "package_url": "https://pypi.org/project/gearhorn/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/gearhorn/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://www.openstack.org/"}, "release_url": "https://pypi.org/project/gearhorn/0.0.5/", "requires_dist": null, "requires_python": null, "summary": "A gearman worker which enables efficient broadcast communication", "version": "0.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"gearhorn\">\n<h2>gearhorn</h2>\n<p>A gearman worker which enables efficient broadcast communication</p>\n<ul>\n<li>Free software: Apache license</li>\n<li>Documentation: <a href=\"http://docs.openstack.org/developer/gearhorn\" rel=\"nofollow\">http://docs.openstack.org/developer/gearhorn</a></li>\n<li>Source: <a href=\"http://git.openstack.org/cgit/openstack-infra/gearhorn\" rel=\"nofollow\">http://git.openstack.org/cgit/openstack-infra/gearhorn</a></li>\n<li>Bugs: <a href=\"http://bugs.launchpad.net/gearhorn\" rel=\"nofollow\">http://bugs.launchpad.net/gearhorn</a></li>\n</ul>\n</div>\n<div id=\"fanout-worker\">\n<h2>Fanout Worker</h2>\n<p>Gearman has no in-built way to copy jobs to multiple workers. This worker\nwill do that to support the common messaging pattern known as \u201cFanout\u201d</p>\n<p>Once running, the fanout worker will subscribe to the queues\n\u201cregister_fanout_subscriber\u201d and \u201cfanout\u201d.</p>\n<div id=\"subscribe-fanout-unsubscribe-fanout\">\n<h3>subscribe_fanout unsubscribe_fanout</h3>\n<p>These queues will have a JSON payload consisting of a mapping with two\nkeys: topic, client_id. The client_id will be kept as a set under the\ntopic for use in sending messages to the appropriate subscriber queue. It\nshould be unique for every intended recipient queue for every fanout\nrequest. Usually this will be unique per host.</p>\n</div>\n<div id=\"fanout\">\n<h3>fanout</h3>\n<p>This queue will have a JSON payload consisting of a mapping with two keys:\ntopic, payload. The topic will be used to search for a list of subscriber\nids. For each subscriber_id found, a copy of the payload will be sent\nto the queue named  topic_subscriber_id. So if we had two subscribers to\n\u201cofficememos\u201d with ids \u201cbob\u201d and \u201calice\u201d, then a message to fanout with\nthis payload:</p>\n<pre>{\"topic\": \"officememos\",\n \"payload\": \"please go home early today.\"}\n</pre>\n<p>Optionally one can add a \u2018unique\u2019 key to make use of Gearman\u2019s\nunique/coalescing features. Also the \u2018background\u2019 key can be used to\nset a truthy value, which will tell gearhorn not to wait for receivers\nbefore moving on to more messages.</p>\n<p>Would result in the worker sending a copy of the payload to the queues\n\u201cofficememos_bob\u201d and \u201cofficememos_alice\u201d.</p>\n<div id=\"matchmaking\">\n<h4>Matchmaking</h4>\n<p>In order to match up topics with subscribers workers must share a list\nof subscribers for each topic. The mapping is maintained in a backend\ndata store. Any time new registration events are seen the list is updated\nin that store and a message is sent to the __matchmaking topic. Workers\nautomatically add and remove themselves to/from the __matchmaking list\nin order to ensure they\u2019re informed and able to clear cache whenever\nthe canonical list is updated. Workers that fail violently must be\nmanually removed.</p>\n</div>\n</div>\n</div>\n<div id=\"failed-idea\">\n<h2>Failed Idea</h2>\n<p>The following was idea #1, and fails to be any more efficient than just\nrunnint Redis as a pub/sub and direct comm backend. It remains here as\na reminder not to reimplement Redis.</p>\n<p>The expected way to use it is to have a gearman client that wants to\nreceive broadcasts request the given broadcast function with a unique ID\nthat is the last seen sequence ID. When this daemon receives that work,\nit looks for any items with sequence number greater than this ID, and\nif it finds them, reply with a json payload of:</p>\n<pre>{\"sequence\": 2,\n \"payload\": \"foobarbaz\"}\n</pre>\n<p>Clients would consume this, and then submit a new job with the\nnew_seq. This allows a stream of sync jobs to become a stream of\nbroadcasted payloads with a good chance of a client receiving all\nsequences.</p>\n<div id=\"todo\">\n<h3>TODO</h3>\n<ul>\n<li>True functional tests</li>\n<li>Make all the things configurable<ul>\n<li>Work queue name(s)</li>\n<li>Tuning backlog and flush frequency</li>\n</ul>\n</li>\n<li>Consider a work queue that workers can use to share the current sequence\nthey should be using.<ul>\n<li>Would this be too racy or given the \u201cbest effort\u201d is it enough to\njust try hard?</li>\n<li>Do we need a UUID of some kind just to allow clients to detect if\nall the workers went away and sequences reset at 0?</li>\n</ul>\n</li>\n<li>Add a client helper module which implements the sequence fetching to\nmake it easier to write a consumer in python</li>\n</ul>\n</div>\n</div>\n\n          </div>"}, "last_serial": 1470365, "releases": {"0.0.0.post25": [], "0.0.1": [{"comment_text": "", "digests": {"md5": "3f49d0e29a4a23d3bbb92afc0a594496", "sha256": "1e09c451f6727a54f3f20d537e79d319b036a521916ba5ead842699717cd8132"}, "downloads": -1, "filename": "gearhorn-0.0.1.tar.gz", "has_sig": false, "md5_digest": "3f49d0e29a4a23d3bbb92afc0a594496", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15098, "upload_time": "2015-03-19T23:19:12", "upload_time_iso_8601": "2015-03-19T23:19:12.146478Z", "url": "https://files.pythonhosted.org/packages/2e/35/d80714c14321df9be9198d80d1c3b5847c57b08e2c77a7a967416f6a6d0f/gearhorn-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "13a29f0a059d76225e6e4a5121cf1e37", "sha256": "7a47d03bf07e370a212f2f10975ac72b0ba473be49e49636a11134ed543767f3"}, "downloads": -1, "filename": "gearhorn-0.0.2.tar.gz", "has_sig": false, "md5_digest": "13a29f0a059d76225e6e4a5121cf1e37", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15142, "upload_time": "2015-03-20T00:03:34", "upload_time_iso_8601": "2015-03-20T00:03:34.849458Z", "url": "https://files.pythonhosted.org/packages/47/b1/a9284e5b95d3bcb48c192e8fa5b0104541dd0f7308ffd4216f86d8ed9c81/gearhorn-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "82f17a60dc79bb0518f2f0cd913eb0bf", "sha256": "bda40b8c64b9aadfac0cc40a5e3f87fa04a6779f6ac293e0bee1f92175e4f7cc"}, "downloads": -1, "filename": "gearhorn-0.0.3.tar.gz", "has_sig": false, "md5_digest": "82f17a60dc79bb0518f2f0cd913eb0bf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15196, "upload_time": "2015-03-20T12:48:21", "upload_time_iso_8601": "2015-03-20T12:48:21.677608Z", "url": "https://files.pythonhosted.org/packages/07/54/f984d3de11b3ea474e0a2b625ea452b70293bfd80511eb8fba36f9118fbc/gearhorn-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "3b20047a81c24aea6852131f120747b0", "sha256": "dd59a00a7e93734f37662f5a80c77b434b41a08be217d225e6a8c742354ad112"}, "downloads": -1, "filename": "gearhorn-0.0.4.tar.gz", "has_sig": false, "md5_digest": "3b20047a81c24aea6852131f120747b0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15259, "upload_time": "2015-03-20T13:00:36", "upload_time_iso_8601": "2015-03-20T13:00:36.694469Z", "url": "https://files.pythonhosted.org/packages/77/4e/948d6df62778816befa11485d97d048f30026984f11a61ffc7719d26f8e1/gearhorn-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "8b79334a06215df069aaf8d577f545a4", "sha256": "227a13d19210bd9079ab819cf7db4de472c749d9e8188d10169e2d8a90f4bada"}, "downloads": -1, "filename": "gearhorn-0.0.5.tar.gz", "has_sig": false, "md5_digest": "8b79334a06215df069aaf8d577f545a4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15285, "upload_time": "2015-03-20T17:27:50", "upload_time_iso_8601": "2015-03-20T17:27:50.127707Z", "url": "https://files.pythonhosted.org/packages/8c/99/6c3bb4b7045d51b41bfe423e4e72783b037d674ccd9ba472a48d21eb9581/gearhorn-0.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8b79334a06215df069aaf8d577f545a4", "sha256": "227a13d19210bd9079ab819cf7db4de472c749d9e8188d10169e2d8a90f4bada"}, "downloads": -1, "filename": "gearhorn-0.0.5.tar.gz", "has_sig": false, "md5_digest": "8b79334a06215df069aaf8d577f545a4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15285, "upload_time": "2015-03-20T17:27:50", "upload_time_iso_8601": "2015-03-20T17:27:50.127707Z", "url": "https://files.pythonhosted.org/packages/8c/99/6c3bb4b7045d51b41bfe423e4e72783b037d674ccd9ba472a48d21eb9581/gearhorn-0.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:58:39 2020"}