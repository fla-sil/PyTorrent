{"info": {"author": "Nishant Sinha", "author_email": "nishantsinha@acm.org", "bugtrack_url": null, "classifiers": ["Environment :: Console", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development"], "description": "# Tensor Shape Annotations Library (tsalib)\n[![PyPI version](https://badge.fury.io/py/tsalib.svg)](https://badge.fury.io/py/tsalib)\n[![Chat](https://img.shields.io/gitter/room/offfnote/tsalib.svg?colorB=yellow&style=plastic)](https://gitter.im/offfnote/tsalib)\n\nConventional tensor manipulation libraries \u2014 `numpy`, `pytorch`, `keras`, `tensorflow`, lack support for *naming* the dimensions of tensor variables. `tsalib` enables using *named dimensions* with existing libraries, using Python's support for *type annotations* and a new *shorthand notation* for naming tensor shapes (**TSN**). \n\nWhy *named dimensions* ?  See [References](#References).\n\n---\nUsing `tsalib`:\n\n* track shapes: label your tensor variables with their **named shapes** (`x: 'btd'` or `x: (B,T,D)`)\n\n* better debugging: write *named* shape **assertions** (`assert x.shape == (B,T,D)`).\n\n* write seamless *named* shape **transformations**:\n\n  `warp(x, '(btd)* -> btdl -> bdtl -> b,d//2,t*2,l', 'jpv')`\n\n  instead of a sequence of calls over a laundry list of APIs (`reshape`,`permute`,`stack`, `concat`)\n\n* work with arbitrary backends without changes:  `numpy`, `pytorch`, `keras`, `tensorflow`, `mxnet`, etc.\n\nExposing the *invisible* named dimensions enhances code clarity, accelerates debugging and leads to improved productivity across the board. Even complex deep learning architectures need only a small number of named dimensions.\n\nThe complete **API** in a **notebook** [here](notebooks/tsalib.ipynb), an introductory **article** [here](https://medium.com/@ekshakhs/introducing-tensor-shape-annotation-library-tsalib-963b5b13c35b).\n\n---\n## Contents\n\n- [Quick Start](#Quick-Start)\n- [Installation](#Installation) \n- [Design Principles, Model Examples](#Documentation-Design-Principles-Model-Examples) (includes [BERT](models/bert)!)\n- [API Overview](#API)\n- [Best Practices -- How to use `tsalib`](#Best-Practices)\n- [Change Log](#change-log)\n\n\n\n### Quick Start\n\n```python\nfrom tsalib import dim_vars as dvs, size_assert\nimport tensorflow as tf\nimport torch\n\n#declare dimension variables (from config arguments)\nB, C, H, W = dvs('Batch(b):32 Channels(c):3 Height(h):256 Width(w):256') \n...\n# create tensors (pytorch) using dimension variables (interpret dim vars as integers)\nx: 'bchw' = torch.randn(B, C, H, W)\nx: 'bchw' = tf.get_variable(\"x\", shape=(B, C, H, W), initializer=tf.random_normal_initializer())\n\n# perform tensor transformations, keep track of named shapes\nx: 'b,c,h//2,w//2' = maxpool(x) \n\n# check assertions: compare dynamic shapes with declared shapes\n# assertions are 'symbolic': don't change even if declared shapes change\nassert x.size() == (B, C, H // 2, W // 2)\n#or, check selected dimensions\nsize_assert (x.size(), (B,C,H//2,W//2), dims=[1,2,3])\n```\n\nWrite intuitive and crisp shape transformations:\n\n```python\nfrom tsalib import permute_transform as pt\n\n# permute: irrelevant dimensions are anonymous (underscores).\nx: 'bchw'\nx1 = x.permute(pt('_c__ -> ___c'))\nassert x1.size() == (B, H, W, C)\n\n# A powerful one-stop `warp` operator to compose multiple transforms inline\n# here: a sequence of a permute ('p') and view ('v') transformations\ny = warp(x1, 'bhwc -> bchw -> b*c,h,w', 'pv')\nassert y.size() == (B*C,H,W)\n\n#or, the same transformation sequence with anonymous dims\ny = warp (x1, ['_hwc -> _chw', 'bc,, -> b*c,,'], 'pv')\n\n# Combinations of `alignto`, `dot` and broadcast\n# Enables writing really compact code for similar patterns\nht: 'bd'; Wh: 'dd'; Y: 'bld'; WY: 'dd'\n\na: 'bd' = dot('_d.d_', ht, Wh) \nb: 'b,1,d' = alignto((a,'bd'), 'bld')\nMt: 'bld' = torch.tanh(dot('__d.d_', Y, WY) + b)\n\n```\n\n\n\n**Old vs New code**\n\n```python\ndef merge_heads_old(x):\n  x = x.permute(0, 2, 1, 3).contiguous()\n  new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n  res = x.view(*new_x_shape)\n```\n\n```python\ndef merge_heads_tsalib(x: 'bhtd'):\n    res: 'b,t,h*d' = warp(x, 'bhtd -> bthd -> b,t,h*d', 'pcv')\n```\n\n\n\nNamed shapes may be represented as tuples or *shorthand* strings:\n\n- a tuple `(B,H,D)` [long form] or a string `'b,h,d'` (or simply `'bhd'`) [shorthand]\n- a string with anonymous dimensions (`',h,'` or `_h_` is a 3-d tensor).\n\nMore details on shorthand notation [here](notebooks/shorthand.md) . \n\n## Installation\n\n`pip install [--upgrade] tsalib`\n\n## Documentation, Design Principles, Model Examples\n\nThis [notebook](notebooks/tsalib.ipynb) serves as a working documentation for the `tsalib` library and illustrates the complete `tsalib` API. The **shorthand** notation is documented [here](notebooks/shorthand.md).\n\nThe [models](models) directory contains tsalib annotations of a few well-known, complex neural architectures: \n- [BERT](models/bert). \n- [OpenAI Transformer](models/openai_transformer.py),\n- [Resnet](models/resnet.py),\n- Contrast models with and without tsalib ([pytorch](models/snippets_pytorch.py), [tensorflow](models/snippets_tf.py)).\n\nWith TSAs, we can gain deeper and immediate insight into how the module works by scanning through the `forward` (or equivalent) function.\n- `tsalib` is designed to stay light and easy to incorporate into existing workflow with minimal code changes. Choose to use `tsalib` for tensor labels and shape asserts only, or, integrate deeply by using `warp` everywhere in your code.\n- The API includes both library-independent and dependent parts, giving developers flexibility in how they choose to incorporate `tsalib` in their workflow.\n- We've carefully avoided deeper integration into popular tensor libraries to keep `tsalib` light-weight and avoid backend-inflicted bugs.\n\n\n\n## API\n\n```python\nfrom tsalib import dim_vars as dvs, get_dim_vars\nimport numpy as np\n```\n### Declarations\n\n#### Declare Dimension Variables\n```python\n#or declare dim vars with default integer values (optional)\nB, C, D, H, W = dvs('Batch:48 Channels:3 EmbedDim:300 Height Width')\n#or provide *shorthand* names and default values for dim vars [best practice]\nB, C, D, H, W = dvs('Batch(b):48 Channels(c):3 EmbedDim(d):300 Height(h) Width(w)')\n\n# switch from using config arguments to named dimensions\nB, C, D = dvs('Batch(b):{0} Channels(c):{1} EmbedDim(d):{2}'.format(config.batch_size, config.num_channels, config.embed_dim))\n\n```\n\n#### Declare Tensors and Annotate Shapes\n\nInstead of scalar variables `batch_size`, `embed_dim`, use dimension variables `B`, `D` or shorthands `b,d` throughout your code.\n\n```python\nB, D = dvs('Batch(b):{batch_size} EmbedDim(d):{embed_dim}}')\n#declare a 2-D tensor of shape(48, 300)\nx = torch.randn(B, D)\n#assertions over dimension variables (code unchanged even if dim sizes change)\nassert x.size() == (B, D)\n```\n\n\n\n```python\nB, D = get_dim_vars('b d') #lookup pre-declared dim vars\na: (B, D) = np.array([[1., 2., 3.], [10., 9., 8.]]) #(Batch, EmbedDim): (2, 3)\nb: (2, B, D) = np.stack([a, a]) #(2, Batch, EmbedDim): (2, 2, 3)\n\n#or simply, use shorthand shapes\na: 'b,d'\nb: '2bd'\n```\nAnnotations are optional and do not affect program performance. Arithmetic over dimension variables is supported. This enables easy tracking of shape changes across neural network layers.\n\n```python\nB, C, H, W = get_dim_vars('b c h w') #lookup pre-declared dim vars\nv: 'bchw' = torch.randn(B, C, h, w)\nx : 'b,c*2,h//2,w//2' = torch.nn.conv2D(C, C*2, ...)(v) \n```\n### Shape and Tensor Transformations\n\n#### One-stop shape transforms: `warp` operator\n\nThe `warp` operator enables squeezing in a **sequence** of shape transformations in a single line using [TSN](notebooks/shorthand.md). The operator takes in an input tensor, a sequence of shape transformations, and the corresponding transform types (view transform -> 'v', permute transform -> 'p'). See docs for transform types [here](notebooks/shorthand.md#warp-transformation).\n\n```python\n    x: 'btd' = torch.randn(B, T, D)\n    y = warp(x, 'btd -> b,t,4,d//4 ->  b,4,t,d//4 ', 'vp') #(v)iew, then (p)ermute, transform\n    assert(y.shape == (B,4,T,D//4))\n```\nBecause it returns transformed tensors, the `warp` operator is backend library-dependent. Currently supported backends are `numpy`, `tensorflow` and `pytorch`. New backends can be added easily (see [backend.py](tsalib/backend.py)).\n\nOr, use individual named shape transformations:\n\n```\n\t#use dimension variables directly\n\tx = torch.ones(B, T, D)\n\tx = x.view(B, T, 4, D//4)\n\tfrom tsalib import view_transform as vt, permute_transform as pt\n\ty = x.reshape(vt('btd -> b,t,4,d//4', x.shape)) #(20, 10, 300) -> (20, 10, 4, 75)\n\tassert y.shape == (B, T, 4, D//4)\n\ty = x.transpose(pt('b,,d, -> d,,b,'))\n```\n\nSee [notebook](notebooks/tsalib.ipynb) for more examples.\n\n#### More useful operators: `join`, `alignto`, `reduce_dims` ...\n<details>\n    <summary>more ..</summary>\n\nUnified `stack/concat` using `join`. Join together sequence of tensors into a single tensor in different ways using the same `join` operator. `join` is backend-dependent.\n\n```python\n    # xi : (B, T, D)\n    # \"concatenate\" along the 'T' dimension: \"(b,t,d)* -> (b,3*t,d)\"\n    x = tsalib.join([x1, x2, x3], ',*,') \n    assert x.shape == (B, 3*T, D)\n\n    # \"stack\": join by adding a new dimension to the front: \"(b,t,d)* -> (^,b,t,d)\"\n    x = join([x1, x2, x3], '^') \n    assert x.shape == (3, B, T, D)\n\n```\n\nAlign one tensor to the rank of another tensor using `alignto`.\n\n```python\n    x1 = np.random.randn(D,D)\n    x2 = np.random.randn(B,D,T,D)\n\n    x1_aligned = alignto((x1, 'dd'), 'bdtd')\n    assert x1_aligned.shape == (1,D,1,D)\n    x1_aligned = alignto((x1, 'dd'), 'bdtd', tile=True)\n    assert x1_aligned.shape == (B,D,T,D)\n```\n\n\nUse dimension names instead of cryptic indices in *reduction* (`mean`, `max`, ...) operations.\n```python\n    from tsalib import reduce_dims as rd\n    b: (2, B, D)\n    c: (D,) = np.mean(b, axis=rd('2bd -> d')) #axis = (0,1)\n```\n\n</details>\n\n#### Simplified `dot` operator\n\nEasy `matmult` specification when \n- exactly a single dimension is common between the operands and \n- the order of dimensions preserved in the output.\n\n```python\n    x = torch.randn(B, C, T)\n    y = torch.randn(C, D)\n    z = dot('_c_.c_', x, y)\n    assert z.size() == (B, T, D)\n```\n\n\n## Dependencies\n\n`sympy`. A library for building symbolic expressions in Python is the only dependency.\n\nTested with Python 3.6. Core API should work with Python 2. Contributions welcome.\n\nFor writing type annotations inline, Python >= 3.5 is required which allows optional type annotations for variables. These annotations do not affect the program performance in any way. \n\n\n## Best Practices\n\n* `tsalib` is designed for **progressive adoption** with your current deep learning models and pipelines. You can start off only with declaring and labeling variables with named shapes and writing shape assertions. This already brings tremendous improvement in productivity and code readability. Once comfortable, use other transformations: `warp`, `join`, etc.\n* Convert all *relevant* config parameters into dimension variables. Use only latter in your code.\n* Define all dimension variables upfront -- this requires some discipline. Use `get_dim_vars` to lookup pre-defined dimension variables by their shorthand names in any function context.\n* Avoid using `reshape` : use `view` and `transpose` together. An inadvertent `reshape` may not preserve your dimensions (axes). Using `view` to change shape protects against this: it throws an error if the dimensions being manipulated are not contiguous. \n* Shape *Annotations* vs *Assertions*. Shape labels (`x: (B,T,D)` or `x: 'btd'`) ease shape recall during coding. Shape assertions (`assert x.shape === (B,T,D)`) enable catching inadvertent shape bugs at runtime. Pick either or both to work with.\n\n\n## References\n\n* Blog [article](https://medium.com/@ekshakhs/introducing-tensor-shape-annotation-library-tsalib-963b5b13c35b) introducing TSA.\n* A [proposal](https://docs.google.com/document/d/1vpMse4c6DrWH5rq2tQSx3qwP_m_0lyn-Ij4WHqQqRHY/edit#heading=h.rkj7d39awayl) for designing a tensor library with named dimensions from ground-up. The TSA library takes care of some use cases, without requiring any change in the tensor libraries.\n* Pytorch Issue on Names Axes [here](https://github.com/pytorch/pytorch/issues/4164).\n* Using [einsum](http://ajcr.net/Basic-guide-to-einsum/) for tensor operations improves productivity and code readability. [blog](https://rockt.github.io/2018/04/30/einsum)\n* The [Tile](https://vertexai-plaidml.readthedocs-hosted.com/en/latest/writing_tile_code.html) DSL uses indices ranging over dimension variables to write compact, library-independent tensor operations.\n* The [datashape](https://datashape.readthedocs.io/en/latest/) library introduces a generic type system and grammar for structure data. `tsalib` focuses on shapes of homogeneous tensor data types only, with arithmetic support.\n* The [xarray](https://github.com/pydata/xarray) library.\n* The [einops](https://github.com/arogozhnikov/einops) library.\n* The [NamedTensor](http://nlp.seas.harvard.edu/NamedTensor) library.\n* The [TensorNetwork](https://github.com/google/TensorNetwork) library. Generalizes the idea of named axes and composition/decomposition/reordering of axes very nicely.\n\n\n\nWriting deep learning programs which manipulate multi-dim tensors (`numpy`, `pytorch`, `tensorflow`, ...) requires you to carefully keep track of shapes of tensors. In absence of a principled way to *name* tensor dimensions and track shapes, most developers resort to writing adhoc shape comments embedded in code (see code from [google-research/bert](https://github.com/google-research/bert/blob/a21d4848ec33eca7d53dd68710f04c4a4cc4be50/modeling.py#L664)) or spaghetti code with numeric indices: `x.view(* (x.size()[:-2] + (x.size(-2) * x.size(-1),))`. This makes both reading \u2014 figuring out `RNN` output shapes, examining/modifying deep pre-trained architectures (`resnet`, `densenet`, `elmo`) \u2014 and writing \u2014  designing new kinds of `attention` mechanisms (`multi-head attention`)\u2014 deep learning programs harder.\n\n<details>\n    <summary>\n    Developers benefit from shape annotations/assertions in many ways: \n    </summary>\n    Benefits:\n    * Quickly verify the variable shapes when writing new transformations or modifying existing modules. \n    * Assertions and annotations remain the same even if the actual dimension sizes change.\n    * Faster *debugging*: if you annotate-as-you-go, the tensor variable shapes are explicit in code, readily available for a quick inspection. No more adhoc shape `print`ing when investigating obscure shape errors.\n    * Do shape transformations using *shorthand* notation and avoid unwanted shape surgeries.\n    * Use TSAs to improve code clarity everywhere, even in your machine learning data pipelines.\n    * They serve as useful documentation to help others understand or extend your module.\n</details>\n\n## \n\n## Author\n\n * Nishant Sinha, [OffNote Labs](http://offnote.co) (nishant@offnote.co, @[medium](https://medium.com/@ekshakhs), @[twitter](https://twitter.com/ekshakhs))\n\n## Change Log\nThe library is in its early phases. Contributions/feedback welcome!\n\n* [5 Feb 2019] Added `dot` operator.\n* [4 Feb 2019] Added fully annotated and adapted BERT [model](models/bert). More illustrative pytorch and tensorflow snippets.\n* [31 Jan 2019] Added `alignto` operator.\n* [18 Dec 2018] Added the `join` operator. `warp` takes a list of (shorthand) transformations.\n* [28- Nov 2018] Added `get_dim_vars` to lookup dim vars declared earlier. Shorthand notation docs.\n* [21 Nov 2018] Added documentation [notebook](notebooks/tsalib.ipynb). \n* [18 Nov 2018] Support for `warp`, `reduce_dims`. Backend modules for `numpy`, `tensorflow` and `torch` added.\n* [9 Nov 2018] Support for shorthand notation in view/permute/expand transforms.\n* [9 Nov 2018] Support for using TSA in assertions and tensor constructors (cast to integers).\n* [25 Oct 2018] Initial Release\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ofnote/tsalib", "keywords": "", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "tsalib", "package_url": "https://pypi.org/project/tsalib/", "platform": "POSIX", "project_url": "https://pypi.org/project/tsalib/", "project_urls": {"Homepage": "https://github.com/ofnote/tsalib"}, "release_url": "https://pypi.org/project/tsalib/0.2.1/", "requires_dist": ["sympy"], "requires_python": "", "summary": "TSAlib: Support for Named Tensor Shapes", "version": "0.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Tensor Shape Annotations Library (tsalib)</h1>\n<p><a href=\"https://badge.fury.io/py/tsalib\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e69753d3c715328a909935cf2de83f498878766f/68747470733a2f2f62616467652e667572792e696f2f70792f7473616c69622e737667\"></a>\n<a href=\"https://gitter.im/offfnote/tsalib\" rel=\"nofollow\"><img alt=\"Chat\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/60fc106e1cba67cfb24ad24b4cf6559305b3663a/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6f6666666e6f74652f7473616c69622e7376673f636f6c6f72423d79656c6c6f77267374796c653d706c6173746963\"></a></p>\n<p>Conventional tensor manipulation libraries \u2014 <code>numpy</code>, <code>pytorch</code>, <code>keras</code>, <code>tensorflow</code>, lack support for <em>naming</em> the dimensions of tensor variables. <code>tsalib</code> enables using <em>named dimensions</em> with existing libraries, using Python's support for <em>type annotations</em> and a new <em>shorthand notation</em> for naming tensor shapes (<strong>TSN</strong>).</p>\n<p>Why <em>named dimensions</em> ?  See <a href=\"#References\" rel=\"nofollow\">References</a>.</p>\n<hr>\n<p>Using <code>tsalib</code>:</p>\n<ul>\n<li>\n<p>track shapes: label your tensor variables with their <strong>named shapes</strong> (<code>x: 'btd'</code> or <code>x: (B,T,D)</code>)</p>\n</li>\n<li>\n<p>better debugging: write <em>named</em> shape <strong>assertions</strong> (<code>assert x.shape == (B,T,D)</code>).</p>\n</li>\n<li>\n<p>write seamless <em>named</em> shape <strong>transformations</strong>:</p>\n<p><code>warp(x, '(btd)* -&gt; btdl -&gt; bdtl -&gt; b,d//2,t*2,l', 'jpv')</code></p>\n<p>instead of a sequence of calls over a laundry list of APIs (<code>reshape</code>,<code>permute</code>,<code>stack</code>, <code>concat</code>)</p>\n</li>\n<li>\n<p>work with arbitrary backends without changes:  <code>numpy</code>, <code>pytorch</code>, <code>keras</code>, <code>tensorflow</code>, <code>mxnet</code>, etc.</p>\n</li>\n</ul>\n<p>Exposing the <em>invisible</em> named dimensions enhances code clarity, accelerates debugging and leads to improved productivity across the board. Even complex deep learning architectures need only a small number of named dimensions.</p>\n<p>The complete <strong>API</strong> in a <strong>notebook</strong> <a href=\"notebooks/tsalib.ipynb\" rel=\"nofollow\">here</a>, an introductory <strong>article</strong> <a href=\"https://medium.com/@ekshakhs/introducing-tensor-shape-annotation-library-tsalib-963b5b13c35b\" rel=\"nofollow\">here</a>.</p>\n<hr>\n<h2>Contents</h2>\n<ul>\n<li><a href=\"#Quick-Start\" rel=\"nofollow\">Quick Start</a></li>\n<li><a href=\"#Installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#Documentation-Design-Principles-Model-Examples\" rel=\"nofollow\">Design Principles, Model Examples</a> (includes <a href=\"models/bert\" rel=\"nofollow\">BERT</a>!)</li>\n<li><a href=\"#API\" rel=\"nofollow\">API Overview</a></li>\n<li><a href=\"#Best-Practices\" rel=\"nofollow\">Best Practices -- How to use <code>tsalib</code></a></li>\n<li><a href=\"#change-log\" rel=\"nofollow\">Change Log</a></li>\n</ul>\n<h3>Quick Start</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tsalib</span> <span class=\"kn\">import</span> <span class=\"n\">dim_vars</span> <span class=\"k\">as</span> <span class=\"n\">dvs</span><span class=\"p\">,</span> <span class=\"n\">size_assert</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"c1\">#declare dimension variables (from config arguments)</span>\n<span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">dvs</span><span class=\"p\">(</span><span class=\"s1\">'Batch(b):32 Channels(c):3 Height(h):256 Width(w):256'</span><span class=\"p\">)</span> \n<span class=\"o\">...</span>\n<span class=\"c1\"># create tensors (pytorch) using dimension variables (interpret dim vars as integers)</span>\n<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"s1\">'bchw'</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"s1\">'bchw'</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">\"x\"</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">),</span> <span class=\"n\">initializer</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">random_normal_initializer</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># perform tensor transformations, keep track of named shapes</span>\n<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"s1\">'b,c,h//2,w//2'</span> <span class=\"o\">=</span> <span class=\"n\">maxpool</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> \n\n<span class=\"c1\"># check assertions: compare dynamic shapes with declared shapes</span>\n<span class=\"c1\"># assertions are 'symbolic': don't change even if declared shapes change</span>\n<span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">H</span> <span class=\"o\">//</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">W</span> <span class=\"o\">//</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"c1\">#or, check selected dimensions</span>\n<span class=\"n\">size_assert</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">(),</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span><span class=\"n\">C</span><span class=\"p\">,</span><span class=\"n\">H</span><span class=\"o\">//</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">W</span><span class=\"o\">//</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">dims</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">])</span>\n</pre>\n<p>Write intuitive and crisp shape transformations:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tsalib</span> <span class=\"kn\">import</span> <span class=\"n\">permute_transform</span> <span class=\"k\">as</span> <span class=\"n\">pt</span>\n\n<span class=\"c1\"># permute: irrelevant dimensions are anonymous (underscores).</span>\n<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"s1\">'bchw'</span>\n<span class=\"n\">x1</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">permute</span><span class=\"p\">(</span><span class=\"n\">pt</span><span class=\"p\">(</span><span class=\"s1\">'_c__ -&gt; ___c'</span><span class=\"p\">))</span>\n<span class=\"k\">assert</span> <span class=\"n\">x1</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># A powerful one-stop `warp` operator to compose multiple transforms inline</span>\n<span class=\"c1\"># here: a sequence of a permute ('p') and view ('v') transformations</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">warp</span><span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"s1\">'bhwc -&gt; bchw -&gt; b*c,h,w'</span><span class=\"p\">,</span> <span class=\"s1\">'pv'</span><span class=\"p\">)</span>\n<span class=\"k\">assert</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"o\">*</span><span class=\"n\">C</span><span class=\"p\">,</span><span class=\"n\">H</span><span class=\"p\">,</span><span class=\"n\">W</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#or, the same transformation sequence with anonymous dims</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">warp</span> <span class=\"p\">(</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s1\">'_hwc -&gt; _chw'</span><span class=\"p\">,</span> <span class=\"s1\">'bc,, -&gt; b*c,,'</span><span class=\"p\">],</span> <span class=\"s1\">'pv'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Combinations of `alignto`, `dot` and broadcast</span>\n<span class=\"c1\"># Enables writing really compact code for similar patterns</span>\n<span class=\"n\">ht</span><span class=\"p\">:</span> <span class=\"s1\">'bd'</span><span class=\"p\">;</span> <span class=\"n\">Wh</span><span class=\"p\">:</span> <span class=\"s1\">'dd'</span><span class=\"p\">;</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"s1\">'bld'</span><span class=\"p\">;</span> <span class=\"n\">WY</span><span class=\"p\">:</span> <span class=\"s1\">'dd'</span>\n\n<span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"s1\">'bd'</span> <span class=\"o\">=</span> <span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"s1\">'_d.d_'</span><span class=\"p\">,</span> <span class=\"n\">ht</span><span class=\"p\">,</span> <span class=\"n\">Wh</span><span class=\"p\">)</span> \n<span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"s1\">'b,1,d'</span> <span class=\"o\">=</span> <span class=\"n\">alignto</span><span class=\"p\">((</span><span class=\"n\">a</span><span class=\"p\">,</span><span class=\"s1\">'bd'</span><span class=\"p\">),</span> <span class=\"s1\">'bld'</span><span class=\"p\">)</span>\n<span class=\"n\">Mt</span><span class=\"p\">:</span> <span class=\"s1\">'bld'</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"s1\">'__d.d_'</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">WY</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n</pre>\n<p><strong>Old vs New code</strong></p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">merge_heads_old</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n  <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">permute</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">contiguous</span><span class=\"p\">()</span>\n  <span class=\"n\">new_x_shape</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()[:</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">),)</span>\n  <span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">new_x_shape</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"k\">def</span> <span class=\"nf\">merge_heads_tsalib</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"s1\">'bhtd'</span><span class=\"p\">):</span>\n    <span class=\"n\">res</span><span class=\"p\">:</span> <span class=\"s1\">'b,t,h*d'</span> <span class=\"o\">=</span> <span class=\"n\">warp</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'bhtd -&gt; bthd -&gt; b,t,h*d'</span><span class=\"p\">,</span> <span class=\"s1\">'pcv'</span><span class=\"p\">)</span>\n</pre>\n<p>Named shapes may be represented as tuples or <em>shorthand</em> strings:</p>\n<ul>\n<li>a tuple <code>(B,H,D)</code> [long form] or a string <code>'b,h,d'</code> (or simply <code>'bhd'</code>) [shorthand]</li>\n<li>a string with anonymous dimensions (<code>',h,'</code> or <code>_h_</code> is a 3-d tensor).</li>\n</ul>\n<p>More details on shorthand notation <a href=\"notebooks/shorthand.md\" rel=\"nofollow\">here</a> .</p>\n<h2>Installation</h2>\n<p><code>pip install [--upgrade] tsalib</code></p>\n<h2>Documentation, Design Principles, Model Examples</h2>\n<p>This <a href=\"notebooks/tsalib.ipynb\" rel=\"nofollow\">notebook</a> serves as a working documentation for the <code>tsalib</code> library and illustrates the complete <code>tsalib</code> API. The <strong>shorthand</strong> notation is documented <a href=\"notebooks/shorthand.md\" rel=\"nofollow\">here</a>.</p>\n<p>The <a href=\"models\" rel=\"nofollow\">models</a> directory contains tsalib annotations of a few well-known, complex neural architectures:</p>\n<ul>\n<li><a href=\"models/bert\" rel=\"nofollow\">BERT</a>.</li>\n<li><a href=\"models/openai_transformer.py\" rel=\"nofollow\">OpenAI Transformer</a>,</li>\n<li><a href=\"models/resnet.py\" rel=\"nofollow\">Resnet</a>,</li>\n<li>Contrast models with and without tsalib (<a href=\"models/snippets_pytorch.py\" rel=\"nofollow\">pytorch</a>, <a href=\"models/snippets_tf.py\" rel=\"nofollow\">tensorflow</a>).</li>\n</ul>\n<p>With TSAs, we can gain deeper and immediate insight into how the module works by scanning through the <code>forward</code> (or equivalent) function.</p>\n<ul>\n<li><code>tsalib</code> is designed to stay light and easy to incorporate into existing workflow with minimal code changes. Choose to use <code>tsalib</code> for tensor labels and shape asserts only, or, integrate deeply by using <code>warp</code> everywhere in your code.</li>\n<li>The API includes both library-independent and dependent parts, giving developers flexibility in how they choose to incorporate <code>tsalib</code> in their workflow.</li>\n<li>We've carefully avoided deeper integration into popular tensor libraries to keep <code>tsalib</code> light-weight and avoid backend-inflicted bugs.</li>\n</ul>\n<h2>API</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tsalib</span> <span class=\"kn\">import</span> <span class=\"n\">dim_vars</span> <span class=\"k\">as</span> <span class=\"n\">dvs</span><span class=\"p\">,</span> <span class=\"n\">get_dim_vars</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n</pre>\n<h3>Declarations</h3>\n<h4>Declare Dimension Variables</h4>\n<pre><span class=\"c1\">#or declare dim vars with default integer values (optional)</span>\n<span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">dvs</span><span class=\"p\">(</span><span class=\"s1\">'Batch:48 Channels:3 EmbedDim:300 Height Width'</span><span class=\"p\">)</span>\n<span class=\"c1\">#or provide *shorthand* names and default values for dim vars [best practice]</span>\n<span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">dvs</span><span class=\"p\">(</span><span class=\"s1\">'Batch(b):48 Channels(c):3 EmbedDim(d):300 Height(h) Width(w)'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># switch from using config arguments to named dimensions</span>\n<span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"n\">dvs</span><span class=\"p\">(</span><span class=\"s1\">'Batch(b):</span><span class=\"si\">{0}</span><span class=\"s1\"> Channels(c):</span><span class=\"si\">{1}</span><span class=\"s1\"> EmbedDim(d):</span><span class=\"si\">{2}</span><span class=\"s1\">'</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">num_channels</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">embed_dim</span><span class=\"p\">))</span>\n</pre>\n<h4>Declare Tensors and Annotate Shapes</h4>\n<p>Instead of scalar variables <code>batch_size</code>, <code>embed_dim</code>, use dimension variables <code>B</code>, <code>D</code> or shorthands <code>b,d</code> throughout your code.</p>\n<pre><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"n\">dvs</span><span class=\"p\">(</span><span class=\"s1\">'Batch(b):</span><span class=\"si\">{batch_size}</span><span class=\"s1\"> EmbedDim(d):</span><span class=\"si\">{embed_dim}</span><span class=\"s1\">}'</span><span class=\"p\">)</span>\n<span class=\"c1\">#declare a 2-D tensor of shape(48, 300)</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span>\n<span class=\"c1\">#assertions over dimension variables (code unchanged even if dim sizes change)</span>\n<span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"n\">get_dim_vars</span><span class=\"p\">(</span><span class=\"s1\">'b d'</span><span class=\"p\">)</span> <span class=\"c1\">#lookup pre-declared dim vars</span>\n<span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">2.</span><span class=\"p\">,</span> <span class=\"mf\">3.</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">10.</span><span class=\"p\">,</span> <span class=\"mf\">9.</span><span class=\"p\">,</span> <span class=\"mf\">8.</span><span class=\"p\">]])</span> <span class=\"c1\">#(Batch, EmbedDim): (2, 3)</span>\n<span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">a</span><span class=\"p\">])</span> <span class=\"c1\">#(2, Batch, EmbedDim): (2, 2, 3)</span>\n\n<span class=\"c1\">#or simply, use shorthand shapes</span>\n<span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"s1\">'b,d'</span>\n<span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"s1\">'2bd'</span>\n</pre>\n<p>Annotations are optional and do not affect program performance. Arithmetic over dimension variables is supported. This enables easy tracking of shape changes across neural network layers.</p>\n<pre><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">get_dim_vars</span><span class=\"p\">(</span><span class=\"s1\">'b c h w'</span><span class=\"p\">)</span> <span class=\"c1\">#lookup pre-declared dim vars</span>\n<span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"s1\">'bchw'</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">)</span>\n<span class=\"n\">x</span> <span class=\"p\">:</span> <span class=\"s1\">'b,c*2,h//2,w//2'</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">conv2D</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">*</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">)(</span><span class=\"n\">v</span><span class=\"p\">)</span> \n</pre>\n<h3>Shape and Tensor Transformations</h3>\n<h4>One-stop shape transforms: <code>warp</code> operator</h4>\n<p>The <code>warp</code> operator enables squeezing in a <strong>sequence</strong> of shape transformations in a single line using <a href=\"notebooks/shorthand.md\" rel=\"nofollow\">TSN</a>. The operator takes in an input tensor, a sequence of shape transformations, and the corresponding transform types (view transform -&gt; 'v', permute transform -&gt; 'p'). See docs for transform types <a href=\"notebooks/shorthand.md#warp-transformation\" rel=\"nofollow\">here</a>.</p>\n<pre>    <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"s1\">'btd'</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">warp</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'btd -&gt; b,t,4,d//4 -&gt;  b,4,t,d//4 '</span><span class=\"p\">,</span> <span class=\"s1\">'vp'</span><span class=\"p\">)</span> <span class=\"c1\">#(v)iew, then (p)ermute, transform</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"n\">T</span><span class=\"p\">,</span><span class=\"n\">D</span><span class=\"o\">//</span><span class=\"mi\">4</span><span class=\"p\">))</span>\n</pre>\n<p>Because it returns transformed tensors, the <code>warp</code> operator is backend library-dependent. Currently supported backends are <code>numpy</code>, <code>tensorflow</code> and <code>pytorch</code>. New backends can be added easily (see <a href=\"tsalib/backend.py\" rel=\"nofollow\">backend.py</a>).</p>\n<p>Or, use individual named shape transformations:</p>\n<pre><code>\t#use dimension variables directly\n\tx = torch.ones(B, T, D)\n\tx = x.view(B, T, 4, D//4)\n\tfrom tsalib import view_transform as vt, permute_transform as pt\n\ty = x.reshape(vt('btd -&gt; b,t,4,d//4', x.shape)) #(20, 10, 300) -&gt; (20, 10, 4, 75)\n\tassert y.shape == (B, T, 4, D//4)\n\ty = x.transpose(pt('b,,d, -&gt; d,,b,'))\n</code></pre>\n<p>See <a href=\"notebooks/tsalib.ipynb\" rel=\"nofollow\">notebook</a> for more examples.</p>\n<h4>More useful operators: <code>join</code>, <code>alignto</code>, <code>reduce_dims</code> ...</h4>\n<details>\n    <summary>more ..</summary>\n<p>Unified <code>stack/concat</code> using <code>join</code>. Join together sequence of tensors into a single tensor in different ways using the same <code>join</code> operator. <code>join</code> is backend-dependent.</p>\n<pre>    <span class=\"c1\"># xi : (B, T, D)</span>\n    <span class=\"c1\"># \"concatenate\" along the 'T' dimension: \"(b,t,d)* -&gt; (b,3*t,d)\"</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tsalib</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">([</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span><span class=\"p\">,</span> <span class=\"n\">x3</span><span class=\"p\">],</span> <span class=\"s1\">',*,'</span><span class=\"p\">)</span> \n    <span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"o\">*</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># \"stack\": join by adding a new dimension to the front: \"(b,t,d)* -&gt; (^,b,t,d)\"</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">join</span><span class=\"p\">([</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span><span class=\"p\">,</span> <span class=\"n\">x3</span><span class=\"p\">],</span> <span class=\"s1\">'^'</span><span class=\"p\">)</span> \n    <span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span>\n</pre>\n<p>Align one tensor to the rank of another tensor using <code>alignto</code>.</p>\n<pre>    <span class=\"n\">x1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">,</span><span class=\"n\">D</span><span class=\"p\">)</span>\n    <span class=\"n\">x2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span><span class=\"n\">D</span><span class=\"p\">,</span><span class=\"n\">T</span><span class=\"p\">,</span><span class=\"n\">D</span><span class=\"p\">)</span>\n\n    <span class=\"n\">x1_aligned</span> <span class=\"o\">=</span> <span class=\"n\">alignto</span><span class=\"p\">((</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"s1\">'dd'</span><span class=\"p\">),</span> <span class=\"s1\">'bdtd'</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"n\">x1_aligned</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">D</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">D</span><span class=\"p\">)</span>\n    <span class=\"n\">x1_aligned</span> <span class=\"o\">=</span> <span class=\"n\">alignto</span><span class=\"p\">((</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"s1\">'dd'</span><span class=\"p\">),</span> <span class=\"s1\">'bdtd'</span><span class=\"p\">,</span> <span class=\"n\">tile</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"n\">x1_aligned</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span><span class=\"n\">D</span><span class=\"p\">,</span><span class=\"n\">T</span><span class=\"p\">,</span><span class=\"n\">D</span><span class=\"p\">)</span>\n</pre>\n<p>Use dimension names instead of cryptic indices in <em>reduction</em> (<code>mean</code>, <code>max</code>, ...) operations.</p>\n<pre>    <span class=\"kn\">from</span> <span class=\"nn\">tsalib</span> <span class=\"kn\">import</span> <span class=\"n\">reduce_dims</span> <span class=\"k\">as</span> <span class=\"n\">rd</span>\n    <span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span>\n    <span class=\"n\">c</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">,)</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"n\">rd</span><span class=\"p\">(</span><span class=\"s1\">'2bd -&gt; d'</span><span class=\"p\">))</span> <span class=\"c1\">#axis = (0,1)</span>\n</pre>\n</details>\n<h4>Simplified <code>dot</code> operator</h4>\n<p>Easy <code>matmult</code> specification when</p>\n<ul>\n<li>exactly a single dimension is common between the operands and</li>\n<li>the order of dimensions preserved in the output.</li>\n</ul>\n<pre>    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span>\n    <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"s1\">'_c_.c_'</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"n\">z</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span>\n</pre>\n<h2>Dependencies</h2>\n<p><code>sympy</code>. A library for building symbolic expressions in Python is the only dependency.</p>\n<p>Tested with Python 3.6. Core API should work with Python 2. Contributions welcome.</p>\n<p>For writing type annotations inline, Python &gt;= 3.5 is required which allows optional type annotations for variables. These annotations do not affect the program performance in any way.</p>\n<h2>Best Practices</h2>\n<ul>\n<li><code>tsalib</code> is designed for <strong>progressive adoption</strong> with your current deep learning models and pipelines. You can start off only with declaring and labeling variables with named shapes and writing shape assertions. This already brings tremendous improvement in productivity and code readability. Once comfortable, use other transformations: <code>warp</code>, <code>join</code>, etc.</li>\n<li>Convert all <em>relevant</em> config parameters into dimension variables. Use only latter in your code.</li>\n<li>Define all dimension variables upfront -- this requires some discipline. Use <code>get_dim_vars</code> to lookup pre-defined dimension variables by their shorthand names in any function context.</li>\n<li>Avoid using <code>reshape</code> : use <code>view</code> and <code>transpose</code> together. An inadvertent <code>reshape</code> may not preserve your dimensions (axes). Using <code>view</code> to change shape protects against this: it throws an error if the dimensions being manipulated are not contiguous.</li>\n<li>Shape <em>Annotations</em> vs <em>Assertions</em>. Shape labels (<code>x: (B,T,D)</code> or <code>x: 'btd'</code>) ease shape recall during coding. Shape assertions (<code>assert x.shape === (B,T,D)</code>) enable catching inadvertent shape bugs at runtime. Pick either or both to work with.</li>\n</ul>\n<h2>References</h2>\n<ul>\n<li>Blog <a href=\"https://medium.com/@ekshakhs/introducing-tensor-shape-annotation-library-tsalib-963b5b13c35b\" rel=\"nofollow\">article</a> introducing TSA.</li>\n<li>A <a href=\"https://docs.google.com/document/d/1vpMse4c6DrWH5rq2tQSx3qwP_m_0lyn-Ij4WHqQqRHY/edit#heading=h.rkj7d39awayl\" rel=\"nofollow\">proposal</a> for designing a tensor library with named dimensions from ground-up. The TSA library takes care of some use cases, without requiring any change in the tensor libraries.</li>\n<li>Pytorch Issue on Names Axes <a href=\"https://github.com/pytorch/pytorch/issues/4164\" rel=\"nofollow\">here</a>.</li>\n<li>Using <a href=\"http://ajcr.net/Basic-guide-to-einsum/\" rel=\"nofollow\">einsum</a> for tensor operations improves productivity and code readability. <a href=\"https://rockt.github.io/2018/04/30/einsum\" rel=\"nofollow\">blog</a></li>\n<li>The <a href=\"https://vertexai-plaidml.readthedocs-hosted.com/en/latest/writing_tile_code.html\" rel=\"nofollow\">Tile</a> DSL uses indices ranging over dimension variables to write compact, library-independent tensor operations.</li>\n<li>The <a href=\"https://datashape.readthedocs.io/en/latest/\" rel=\"nofollow\">datashape</a> library introduces a generic type system and grammar for structure data. <code>tsalib</code> focuses on shapes of homogeneous tensor data types only, with arithmetic support.</li>\n<li>The <a href=\"https://github.com/pydata/xarray\" rel=\"nofollow\">xarray</a> library.</li>\n<li>The <a href=\"https://github.com/arogozhnikov/einops\" rel=\"nofollow\">einops</a> library.</li>\n<li>The <a href=\"http://nlp.seas.harvard.edu/NamedTensor\" rel=\"nofollow\">NamedTensor</a> library.</li>\n<li>The <a href=\"https://github.com/google/TensorNetwork\" rel=\"nofollow\">TensorNetwork</a> library. Generalizes the idea of named axes and composition/decomposition/reordering of axes very nicely.</li>\n</ul>\n<p>Writing deep learning programs which manipulate multi-dim tensors (<code>numpy</code>, <code>pytorch</code>, <code>tensorflow</code>, ...) requires you to carefully keep track of shapes of tensors. In absence of a principled way to <em>name</em> tensor dimensions and track shapes, most developers resort to writing adhoc shape comments embedded in code (see code from <a href=\"https://github.com/google-research/bert/blob/a21d4848ec33eca7d53dd68710f04c4a4cc4be50/modeling.py#L664\" rel=\"nofollow\">google-research/bert</a>) or spaghetti code with numeric indices: <code>x.view(* (x.size()[:-2] + (x.size(-2) * x.size(-1),))</code>. This makes both reading \u2014 figuring out <code>RNN</code> output shapes, examining/modifying deep pre-trained architectures (<code>resnet</code>, <code>densenet</code>, <code>elmo</code>) \u2014 and writing \u2014  designing new kinds of <code>attention</code> mechanisms (<code>multi-head attention</code>)\u2014 deep learning programs harder.</p>\n<details>\n    <summary>\n    Developers benefit from shape annotations/assertions in many ways: \n    </summary>\n    Benefits:\n    * Quickly verify the variable shapes when writing new transformations or modifying existing modules. \n    * Assertions and annotations remain the same even if the actual dimension sizes change.\n    * Faster *debugging*: if you annotate-as-you-go, the tensor variable shapes are explicit in code, readily available for a quick inspection. No more adhoc shape `print`ing when investigating obscure shape errors.\n    * Do shape transformations using *shorthand* notation and avoid unwanted shape surgeries.\n    * Use TSAs to improve code clarity everywhere, even in your machine learning data pipelines.\n    * They serve as useful documentation to help others understand or extend your module.\n</details>\n<h2></h2>\n<h2>Author</h2>\n<ul>\n<li>Nishant Sinha, <a href=\"http://offnote.co\" rel=\"nofollow\">OffNote Labs</a> (<a href=\"mailto:nishant@offnote.co\">nishant@offnote.co</a>, @<a href=\"https://medium.com/@ekshakhs\" rel=\"nofollow\">medium</a>, @<a href=\"https://twitter.com/ekshakhs\" rel=\"nofollow\">twitter</a>)</li>\n</ul>\n<h2>Change Log</h2>\n<p>The library is in its early phases. Contributions/feedback welcome!</p>\n<ul>\n<li>[5 Feb 2019] Added <code>dot</code> operator.</li>\n<li>[4 Feb 2019] Added fully annotated and adapted BERT <a href=\"models/bert\" rel=\"nofollow\">model</a>. More illustrative pytorch and tensorflow snippets.</li>\n<li>[31 Jan 2019] Added <code>alignto</code> operator.</li>\n<li>[18 Dec 2018] Added the <code>join</code> operator. <code>warp</code> takes a list of (shorthand) transformations.</li>\n<li>[28- Nov 2018] Added <code>get_dim_vars</code> to lookup dim vars declared earlier. Shorthand notation docs.</li>\n<li>[21 Nov 2018] Added documentation <a href=\"notebooks/tsalib.ipynb\" rel=\"nofollow\">notebook</a>.</li>\n<li>[18 Nov 2018] Support for <code>warp</code>, <code>reduce_dims</code>. Backend modules for <code>numpy</code>, <code>tensorflow</code> and <code>torch</code> added.</li>\n<li>[9 Nov 2018] Support for shorthand notation in view/permute/expand transforms.</li>\n<li>[9 Nov 2018] Support for using TSA in assertions and tensor constructors (cast to integers).</li>\n<li>[25 Oct 2018] Initial Release</li>\n</ul>\n\n          </div>"}, "last_serial": 5489700, "releases": {"0.1.1.0": [{"comment_text": "", "digests": {"md5": "7e74996a417b0fc2f18e699a5ea7c511", "sha256": "76015c3fca82774276e99d39200c64e6474a90ad940259075282b50bb8c06d58"}, "downloads": -1, "filename": "tsalib-0.1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "7e74996a417b0fc2f18e699a5ea7c511", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 13441, "upload_time": "2018-11-09T13:32:33", "upload_time_iso_8601": "2018-11-09T13:32:33.841172Z", "url": "https://files.pythonhosted.org/packages/12/32/59d6c30ed8392e40c642973b5c5950dc7f2388a2d53e44fbad0888dfcb22/tsalib-0.1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "19c24a0077803a054815dee66a0659ea", "sha256": "041fe062cee753ba91762cb2e68296310fb7e74537e7f363c4b37ee8be0f04c7"}, "downloads": -1, "filename": "tsalib-0.1.1.0.tar.gz", "has_sig": false, "md5_digest": "19c24a0077803a054815dee66a0659ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9069, "upload_time": "2018-11-09T13:32:36", "upload_time_iso_8601": "2018-11-09T13:32:36.006226Z", "url": "https://files.pythonhosted.org/packages/d7/bb/892bf5169f229e9dfb6d464e50b3167372e8f6bfa435ca68a19a3d69aa59/tsalib-0.1.1.0.tar.gz", "yanked": false}], "0.1.1.1": [{"comment_text": "", "digests": {"md5": "f0aeb3750a617538a5d2e2887978f10f", "sha256": "ff961455540c6562ca61cf1d4b8bfa5e66a95e6e6e29e8af4ae7ef2bdf194925"}, "downloads": -1, "filename": "tsalib-0.1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f0aeb3750a617538a5d2e2887978f10f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 14030, "upload_time": "2018-11-13T09:53:13", "upload_time_iso_8601": "2018-11-13T09:53:13.703790Z", "url": "https://files.pythonhosted.org/packages/05/a2/09d4e11ee0531f5990832bdc9d388d021ac357dadb2c4b5e75a237544dcd/tsalib-0.1.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f12d190ae59233e1ed844ceb9e6c5d03", "sha256": "77dddbaa65801d99ac2f1efc8662025e7c82372f4b0a8685200905a90eb7f024"}, "downloads": -1, "filename": "tsalib-0.1.1.1.tar.gz", "has_sig": false, "md5_digest": "f12d190ae59233e1ed844ceb9e6c5d03", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9644, "upload_time": "2018-11-13T09:53:15", "upload_time_iso_8601": "2018-11-13T09:53:15.906313Z", "url": "https://files.pythonhosted.org/packages/3b/c3/26b937f86e9a7c238ba194c354cf4aa6954c5711995f6461be1ca4c22cad/tsalib-0.1.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "788555ccb404b3c46a9dad7fedf234a8", "sha256": "2c25c18f8e3689912027422150425f088ac0820254bfd4d48bc390f435cd012d"}, "downloads": -1, "filename": "tsalib-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "788555ccb404b3c46a9dad7fedf234a8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 16680, "upload_time": "2018-11-18T16:35:58", "upload_time_iso_8601": "2018-11-18T16:35:58.348229Z", "url": "https://files.pythonhosted.org/packages/4f/8a/87045276f42884abb81c6e06d483f2547626c446379c5e2577d921436b0e/tsalib-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "48d062ecd223f5e32fd030c03c0a92bc", "sha256": "e63a9fab80ccf4ff160dceb9142fdfc73c89865523478a52bc4c12f2560a7503"}, "downloads": -1, "filename": "tsalib-0.1.2.tar.gz", "has_sig": false, "md5_digest": "48d062ecd223f5e32fd030c03c0a92bc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11759, "upload_time": "2018-11-18T16:36:00", "upload_time_iso_8601": "2018-11-18T16:36:00.691574Z", "url": "https://files.pythonhosted.org/packages/75/db/2683647ceabf2fd4aa8afd5363920a407a8295d1c0851b2b50cbbab9e382/tsalib-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "6d28703691b07d3a39e48ad89cbecbf8", "sha256": "55a19fb8a0b95f309fe405aec126fd72b2cfeceac48042df4be362903fc9c7c5"}, "downloads": -1, "filename": "tsalib-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "6d28703691b07d3a39e48ad89cbecbf8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 16732, "upload_time": "2018-11-18T17:38:08", "upload_time_iso_8601": "2018-11-18T17:38:08.719615Z", "url": "https://files.pythonhosted.org/packages/ec/c9/56326fc8b9e0052c510b646d5f5ea32006e4220cee52cb656da67ee4f3ec/tsalib-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6d453dbc74d36624944cd367e504cd12", "sha256": "bdd807b1bd36b59cbf8ad250cc3a9f4f7ed54c01cd6a6b4af99647387dc7c043"}, "downloads": -1, "filename": "tsalib-0.1.3.tar.gz", "has_sig": false, "md5_digest": "6d453dbc74d36624944cd367e504cd12", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11797, "upload_time": "2018-11-18T17:38:10", "upload_time_iso_8601": "2018-11-18T17:38:10.756987Z", "url": "https://files.pythonhosted.org/packages/a3/95/2577afcc1074b81bf14a8ad392bfaf102cbb1995b330095b66db87a782c6/tsalib-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "f402317c656b8bec143fa54983e46af5", "sha256": "57434b9451fe75f31aa0dbfa1651a523cf8a4d3bab6cb22cff87822e092db440"}, "downloads": -1, "filename": "tsalib-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "f402317c656b8bec143fa54983e46af5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17116, "upload_time": "2018-11-21T14:37:35", "upload_time_iso_8601": "2018-11-21T14:37:35.205242Z", "url": "https://files.pythonhosted.org/packages/e2/60/74f4a2434db43c74a9e642f1b92c4d7dab4b76574b316b5963cd420ad9ec/tsalib-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "19d7d50b7450f75a9fb5408e2ba9adbd", "sha256": "5f34ea6a6e685cda2b9e5386eeb36b8ebd07ff5163ef8d6654879f0f90d8b8e8"}, "downloads": -1, "filename": "tsalib-0.1.4.tar.gz", "has_sig": false, "md5_digest": "19d7d50b7450f75a9fb5408e2ba9adbd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12204, "upload_time": "2018-11-21T14:37:37", "upload_time_iso_8601": "2018-11-21T14:37:37.530343Z", "url": "https://files.pythonhosted.org/packages/d2/82/0cfa1552f6bb9269188601c2ca29559c748bae98581b6881f3b071f0be48/tsalib-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "dbf53c4612af409d787b9d7f0d2b5634", "sha256": "4a14c01c6ece4f144f6724ecea9947c187f3cf7f8d3f8277906fbc42ba2d04f5"}, "downloads": -1, "filename": "tsalib-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "dbf53c4612af409d787b9d7f0d2b5634", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17361, "upload_time": "2018-11-28T15:45:52", "upload_time_iso_8601": "2018-11-28T15:45:52.369149Z", "url": "https://files.pythonhosted.org/packages/3b/f9/81de9d048877af7e5517be5005710113009cacd2ffffc6594714629b198b/tsalib-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a7d8f48fda6bbe9fb8310d85997e2b08", "sha256": "cd9939c5b84d51080da1b544d144d95285ec83a7e60c6dcaca31df8f9aba31f2"}, "downloads": -1, "filename": "tsalib-0.1.5.tar.gz", "has_sig": false, "md5_digest": "a7d8f48fda6bbe9fb8310d85997e2b08", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12467, "upload_time": "2018-11-28T15:45:54", "upload_time_iso_8601": "2018-11-28T15:45:54.404365Z", "url": "https://files.pythonhosted.org/packages/d9/f0/1f40f2ea57fd707a57160b31273c344e3f1708cc1661392ca2f3dca03f81/tsalib-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "5184da7f3c8ca9176f642dd67c4832bb", "sha256": "c3448068916c0a8e7ce17baea9fdd0aeb573954e69cb0c61ba470ba1e2d15a05"}, "downloads": -1, "filename": "tsalib-0.1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "5184da7f3c8ca9176f642dd67c4832bb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18057, "upload_time": "2018-12-18T14:34:40", "upload_time_iso_8601": "2018-12-18T14:34:40.888633Z", "url": "https://files.pythonhosted.org/packages/1d/6d/479fdfc58cfea659f90171de3ebef1ab71654ea9cf487363b537f7b852b6/tsalib-0.1.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cb2df834b5191f83002efbdc695f9769", "sha256": "2c1c0540fea2442c9d009757c1906b4f34741a5977ddb24fd4975e539e0e3dee"}, "downloads": -1, "filename": "tsalib-0.1.6.tar.gz", "has_sig": false, "md5_digest": "cb2df834b5191f83002efbdc695f9769", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13180, "upload_time": "2018-12-18T14:34:42", "upload_time_iso_8601": "2018-12-18T14:34:42.985049Z", "url": "https://files.pythonhosted.org/packages/c5/62/f871f4e03fdf11e01b369e7f3551882c31ecd390b20347b5e8990b2c5e84/tsalib-0.1.6.tar.gz", "yanked": false}], "0.1.6.1": [{"comment_text": "", "digests": {"md5": "890181102d51ee4eec8782376bd8fdda", "sha256": "bc6132507fd183486dad11150a0dfd81fe0503ef8700e985aa0f24cb231af8cb"}, "downloads": -1, "filename": "tsalib-0.1.6.1-py3-none-any.whl", "has_sig": false, "md5_digest": "890181102d51ee4eec8782376bd8fdda", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 19122, "upload_time": "2019-01-04T06:15:12", "upload_time_iso_8601": "2019-01-04T06:15:12.520217Z", "url": "https://files.pythonhosted.org/packages/c9/c9/c4dd0dcaa5df306a3bef73d8981a57571023894a7e5fd0f2734e2df16e08/tsalib-0.1.6.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "109a107aecd9e7de21cb1c02ef8038d2", "sha256": "c5de53c42e44bae946afa60b8ce8f0c3ee2f4d88b876ce27ab379505922bd18d"}, "downloads": -1, "filename": "tsalib-0.1.6.1.tar.gz", "has_sig": false, "md5_digest": "109a107aecd9e7de21cb1c02ef8038d2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14177, "upload_time": "2019-01-04T06:15:14", "upload_time_iso_8601": "2019-01-04T06:15:14.210452Z", "url": "https://files.pythonhosted.org/packages/7e/45/1c5e16f8c50e815f1fb23f5cd49417b8ae38dcbef03dfe32315c70c5f8c5/tsalib-0.1.6.1.tar.gz", "yanked": false}], "0.1.7": [{"comment_text": "", "digests": {"md5": "dae339f6a025e006451484d9cefdd346", "sha256": "d061b72bb4367ac089b963664ae271e92f3f4170cdee07fecd3ff202c0738d6c"}, "downloads": -1, "filename": "tsalib-0.1.7-py3-none-any.whl", "has_sig": false, "md5_digest": "dae339f6a025e006451484d9cefdd346", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22021, "upload_time": "2019-02-04T18:01:09", "upload_time_iso_8601": "2019-02-04T18:01:09.827596Z", "url": "https://files.pythonhosted.org/packages/60/c3/d4c69bcc4c0133a591cb9ffd9ca8188b0fe84ec480d4e87b253360854f2d/tsalib-0.1.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7170e1e52bd47ff2cb842deaddcc87c4", "sha256": "6ead5063daff5b98a6d414552130c1f72c36ff02ccd6ca23ddc4afa43fe3f68e"}, "downloads": -1, "filename": "tsalib-0.1.7.tar.gz", "has_sig": false, "md5_digest": "7170e1e52bd47ff2cb842deaddcc87c4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16729, "upload_time": "2019-02-04T18:01:12", "upload_time_iso_8601": "2019-02-04T18:01:12.369061Z", "url": "https://files.pythonhosted.org/packages/80/9b/1cd44da99a634772fdfa7e7906829d63014b33dae082333d0c169d647ac7/tsalib-0.1.7.tar.gz", "yanked": false}], "0.1.8": [{"comment_text": "", "digests": {"md5": "e740910537ee92a30c20c45b1bd19091", "sha256": "77b68b61b6fd5c183a2b55f127fe1e357526d6feb99f0be7118076a4e06bf21e"}, "downloads": -1, "filename": "tsalib-0.1.8-py3-none-any.whl", "has_sig": false, "md5_digest": "e740910537ee92a30c20c45b1bd19091", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23304, "upload_time": "2019-02-05T15:44:49", "upload_time_iso_8601": "2019-02-05T15:44:49.638239Z", "url": "https://files.pythonhosted.org/packages/92/e1/6292351a9282761027df197dd86cf8f53220cb7bb0728a0e72f6e2b90886/tsalib-0.1.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7e5386419e486da5eeaa4b27b44649fe", "sha256": "b02e9cc1d164f63ae0214d59b1d560eeda61a477f6b0cac56526ea0836b820d3"}, "downloads": -1, "filename": "tsalib-0.1.8.tar.gz", "has_sig": false, "md5_digest": "7e5386419e486da5eeaa4b27b44649fe", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18089, "upload_time": "2019-02-05T15:44:51", "upload_time_iso_8601": "2019-02-05T15:44:51.269257Z", "url": "https://files.pythonhosted.org/packages/1f/0b/a9ccdf5afd0158e406b6b18a7c0c00ace30f491096eeacf618db722a7140/tsalib-0.1.8.tar.gz", "yanked": false}], "0.1.8.5": [{"comment_text": "", "digests": {"md5": "6d6c7d98ecc03de0db3a6901eb0cbea6", "sha256": "3ae83fb06407263140a0529d0c5e8550827563fdda54e0d931109164aa208707"}, "downloads": -1, "filename": "tsalib-0.1.8.5-py3-none-any.whl", "has_sig": false, "md5_digest": "6d6c7d98ecc03de0db3a6901eb0cbea6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23838, "upload_time": "2019-02-06T07:46:15", "upload_time_iso_8601": "2019-02-06T07:46:15.432762Z", "url": "https://files.pythonhosted.org/packages/5c/ef/08492f0efbb11c9a5f6c6d7aa418407690e66f81c220f00a7fdef24df4de/tsalib-0.1.8.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "05ab3e37bd8d82d854d51871cefeeeb7", "sha256": "47a10a500dcad11fb51518fec7e17337a9b98fb71d1cefeaa4ccc38b4c555784"}, "downloads": -1, "filename": "tsalib-0.1.8.5.tar.gz", "has_sig": false, "md5_digest": "05ab3e37bd8d82d854d51871cefeeeb7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18615, "upload_time": "2019-02-06T07:46:17", "upload_time_iso_8601": "2019-02-06T07:46:17.291738Z", "url": "https://files.pythonhosted.org/packages/0e/05/e65aa0f5fcc9d777c47b87fc375a4aa478b19b29a3d0d7657a1f7eddd1cd/tsalib-0.1.8.5.tar.gz", "yanked": false}], "0.1.8.6": [{"comment_text": "", "digests": {"md5": "bd801155a42ae3a63b81b1f48eec1f24", "sha256": "82b98feb693d855b7400fe9c9158b2f61466e8f8e098ebdb01f046f0bf3b5b7e"}, "downloads": -1, "filename": "tsalib-0.1.8.6-py3-none-any.whl", "has_sig": false, "md5_digest": "bd801155a42ae3a63b81b1f48eec1f24", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23993, "upload_time": "2019-02-13T08:19:14", "upload_time_iso_8601": "2019-02-13T08:19:14.055307Z", "url": "https://files.pythonhosted.org/packages/9c/81/689d0705994c48d7ff3f0a357ee551ad4800999838afffddbe69d2ff1003/tsalib-0.1.8.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8a8d141ce7156a0dd1ea382cde0dd196", "sha256": "d6d414f9ca538e244604d04509411145cf0e54ad8bb35b9c7ddc423672a82034"}, "downloads": -1, "filename": "tsalib-0.1.8.6.tar.gz", "has_sig": false, "md5_digest": "8a8d141ce7156a0dd1ea382cde0dd196", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18801, "upload_time": "2019-02-13T08:19:16", "upload_time_iso_8601": "2019-02-13T08:19:16.394417Z", "url": "https://files.pythonhosted.org/packages/d4/fa/4a767326b49ca8d57fbc531252f787512226379e9fa00557bcdce64bc0d5/tsalib-0.1.8.6.tar.gz", "yanked": false}], "0.1.8.7": [{"comment_text": "", "digests": {"md5": "e2c6f5566360545f47425d4a0da230fa", "sha256": "36978c4a3f34cda99540f3b9651c342b9ecd2610ce4f46e1aaafed51981478cc"}, "downloads": -1, "filename": "tsalib-0.1.8.7-py3-none-any.whl", "has_sig": false, "md5_digest": "e2c6f5566360545f47425d4a0da230fa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 24822, "upload_time": "2019-02-15T13:41:24", "upload_time_iso_8601": "2019-02-15T13:41:24.360345Z", "url": "https://files.pythonhosted.org/packages/ca/f4/247abb6449bd4f48e0508dffe50743e9973c8d4b43dde4f6003426a498f9/tsalib-0.1.8.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cfa06fca4f8c4440ef4c6cb8072eef23", "sha256": "7113a29c145f79abcc9f7da3d48b5861f474fe931fd55a8b0849951994b70520"}, "downloads": -1, "filename": "tsalib-0.1.8.7.tar.gz", "has_sig": false, "md5_digest": "cfa06fca4f8c4440ef4c6cb8072eef23", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19589, "upload_time": "2019-02-15T13:41:26", "upload_time_iso_8601": "2019-02-15T13:41:26.386807Z", "url": "https://files.pythonhosted.org/packages/c1/35/f0ae5d8a400c27211ba5ccb937dd93988f756c7ff85adec751d836001d85/tsalib-0.1.8.7.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "f7a583ca7e82350c2043ca01318bfe15", "sha256": "8ccd0985837c1028ddc5dd89b4745d8c20e69d6b3d1747a9af3fcea5d85710e5"}, "downloads": -1, "filename": "tsalib-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f7a583ca7e82350c2043ca01318bfe15", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 24935, "upload_time": "2019-05-18T11:40:25", "upload_time_iso_8601": "2019-05-18T11:40:25.337129Z", "url": "https://files.pythonhosted.org/packages/25/dd/85c53f58908bddc428123f94d41ad2c9206df4bcda985dca50fe77ba9641/tsalib-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ffa3c699e2ee2e3a02609566851cb9a2", "sha256": "4addf52b0f104df2647b6ec088de46c7d02e0865635878159bf83a2231a62c16"}, "downloads": -1, "filename": "tsalib-0.2.0.tar.gz", "has_sig": false, "md5_digest": "ffa3c699e2ee2e3a02609566851cb9a2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19734, "upload_time": "2019-05-18T11:40:27", "upload_time_iso_8601": "2019-05-18T11:40:27.792529Z", "url": "https://files.pythonhosted.org/packages/52/3f/8621701f512d69dcb2064542ec147166cc180db40329afb07c45659a809e/tsalib-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "37c1ea24f488953863b717676e553f45", "sha256": "e47a4a53a17612cdcaaaf8f7fb5b0ffbc99c99766c9d3ca646b9f8ae1fd7312a"}, "downloads": -1, "filename": "tsalib-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "37c1ea24f488953863b717676e553f45", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 24434, "upload_time": "2019-07-05T05:55:26", "upload_time_iso_8601": "2019-07-05T05:55:26.125411Z", "url": "https://files.pythonhosted.org/packages/7d/fd/d1f92d5f7be882ff57a2bf329df1df08ed1c26be9f9cc11cab0c2d7c5118/tsalib-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d2b8e87c698880d3406e6b07076aafe6", "sha256": "a8175c1deca0538a08e00cef3c31763489a2a6cc4d2af058a6a5a16898b9d1c2"}, "downloads": -1, "filename": "tsalib-0.2.1.tar.gz", "has_sig": false, "md5_digest": "d2b8e87c698880d3406e6b07076aafe6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19119, "upload_time": "2019-07-05T05:55:28", "upload_time_iso_8601": "2019-07-05T05:55:28.628152Z", "url": "https://files.pythonhosted.org/packages/5c/4c/f91a7460b9819aafd89f69b9af49428ea09e47d33325af40c255e7a4d306/tsalib-0.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "37c1ea24f488953863b717676e553f45", "sha256": "e47a4a53a17612cdcaaaf8f7fb5b0ffbc99c99766c9d3ca646b9f8ae1fd7312a"}, "downloads": -1, "filename": "tsalib-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "37c1ea24f488953863b717676e553f45", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 24434, "upload_time": "2019-07-05T05:55:26", "upload_time_iso_8601": "2019-07-05T05:55:26.125411Z", "url": "https://files.pythonhosted.org/packages/7d/fd/d1f92d5f7be882ff57a2bf329df1df08ed1c26be9f9cc11cab0c2d7c5118/tsalib-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d2b8e87c698880d3406e6b07076aafe6", "sha256": "a8175c1deca0538a08e00cef3c31763489a2a6cc4d2af058a6a5a16898b9d1c2"}, "downloads": -1, "filename": "tsalib-0.2.1.tar.gz", "has_sig": false, "md5_digest": "d2b8e87c698880d3406e6b07076aafe6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19119, "upload_time": "2019-07-05T05:55:28", "upload_time_iso_8601": "2019-07-05T05:55:28.628152Z", "url": "https://files.pythonhosted.org/packages/5c/4c/f91a7460b9819aafd89f69b9af49428ea09e47d33325af40c255e7a4d306/tsalib-0.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:45:25 2020"}