{"info": {"author": "Paul Gowder", "author_email": "paul.gowder@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3 :: Only"], "description": "Micro-library to quickly clean up text for textmining purposes.  Mostly devoted to getting rid of unicode punctuation, random extra spaces, etc. etc.\n\nThis is meant as a very quick and dirty cleanup process (hence the name). It throws out numbers, currency symbols, math, punctuation, superscripts and subscripts etc. The objective is primarily to eliminate an annoying step in bag-of-words style textmining.\n\nBasic Usage\n===========\n\nUsage is really simple::\n\n    from dirtyclean import clean\n    clean(\"some string full of punctuation and stuff\")\n\nRight now, the only options given are to convert combined letters (letters with accent marks, umlats, etc.) into their unicode primitives or not.  Default behavior is to not do so, and hence retain all characters that might actually distinguish different words.  \n\nIf you want to get rid of umlats and such, you can try ``clean(\"somestring\", simplify_letters = True)`` However, I don't guarantee that letter simplification will work.  If it breaks, please file an issue, or, better yet, a PR.\n\nMore Details\n============\n\nThis micro-library just tries to take a string with arbitrary unicode in it and emit a string with all the words, separated by single spaces, where \"word\" is defined as \"continuous sequence of letters\" and \"letter\" is defined as \"stuff in the `unicode categories <http://www.fileformat.info/info/unicode/category/index.htm>`_ Lu, Ll, Lt, and Lo.\" Does not lowercase or anything like that; that's a single method call you can do it yourself (also, I don't want to throw away case information for users who need it). \n\nOne known problem is that hyphenated words and contractions will be split up.  In view of the fact that in natural language English as actually used (at least, not sure about other languages) uses \"-\", \"'\" and various other characters with similar shapes to split words and to serve as ordinary punctuation, there's no simple and quick way to detect the difference between these uses, at least not without some kind of dictionary. So I just let stuff like \"don't\" be two words.  If this is a problem for your use case, maybe try a fancy `NLTK tokenizer <http://www.nltk.org/api/nltk.tokenize.html>`_?\n\nOne possible heuristic solution would be to replace the punctuation and such with a placeholder on the translation pass, and then on the regular expression pass, replace the with a space iff there's a space (but not newline, because of line-break hyphenation) on either side. This could work, but would involve making the regex a bit more convoluted + would smush together rather than separate compound constructions that are normally hyphenated. For example, \"injury-prone\" would become \"injuryprone\" rather than the more sensible \"injury prone.\"  Nonetheless, it's worth a try as an alternative implementation, perhaps under a flag like `heuristic-hyphens = True`.  I might implement this down the line, but I'd welcome a PR to do it before I get there.\n\nContributing, license, tests, etc.\n==================================\n\nIn general, this is a work in progress, pull requests very much desired.  Please add a test for any functionality you add, and please keep the default behavior the same (except for bugfixed) without any new arguments (i.e., for new functionality, just add another optional argument).\n\nTests are bare unittest; I use nose2 for testing, but you can use whatever. Run tests from the test directory so it can find the text file in there.\n\nMIT license.", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/paultopia/dirtyclean", "keywords": "textmining,nlp,datacleaning", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "dirtyclean", "package_url": "https://pypi.org/project/dirtyclean/", "platform": "", "project_url": "https://pypi.org/project/dirtyclean/", "project_urls": {"Homepage": "https://github.com/paultopia/dirtyclean"}, "release_url": "https://pypi.org/project/dirtyclean/0.1/", "requires_dist": null, "requires_python": ">=3", "summary": "get rid of unicode punctuation and other garbage from strings", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Micro-library to quickly clean up text for textmining purposes.  Mostly devoted to getting rid of unicode punctuation, random extra spaces, etc. etc.</p>\n<p>This is meant as a very quick and dirty cleanup process (hence the name). It throws out numbers, currency symbols, math, punctuation, superscripts and subscripts etc. The objective is primarily to eliminate an annoying step in bag-of-words style textmining.</p>\n<div id=\"basic-usage\">\n<h2>Basic Usage</h2>\n<p>Usage is really simple:</p>\n<pre>from dirtyclean import clean\nclean(\"some string full of punctuation and stuff\")\n</pre>\n<p>Right now, the only options given are to convert combined letters (letters with accent marks, umlats, etc.) into their unicode primitives or not.  Default behavior is to not do so, and hence retain all characters that might actually distinguish different words.</p>\n<p>If you want to get rid of umlats and such, you can try <tt><span class=\"pre\">clean(\"somestring\",</span> simplify_letters = True)</tt> However, I don\u2019t guarantee that letter simplification will work.  If it breaks, please file an issue, or, better yet, a PR.</p>\n</div>\n<div id=\"more-details\">\n<h2>More Details</h2>\n<p>This micro-library just tries to take a string with arbitrary unicode in it and emit a string with all the words, separated by single spaces, where \u201cword\u201d is defined as \u201ccontinuous sequence of letters\u201d and \u201cletter\u201d is defined as \u201cstuff in the <a href=\"http://www.fileformat.info/info/unicode/category/index.htm\" rel=\"nofollow\">unicode categories</a> Lu, Ll, Lt, and Lo.\u201d Does not lowercase or anything like that; that\u2019s a single method call you can do it yourself (also, I don\u2019t want to throw away case information for users who need it).</p>\n<p>One known problem is that hyphenated words and contractions will be split up.  In view of the fact that in natural language English as actually used (at least, not sure about other languages) uses \u201c-\u201c, \u201c\u2019\u201d and various other characters with similar shapes to split words and to serve as ordinary punctuation, there\u2019s no simple and quick way to detect the difference between these uses, at least not without some kind of dictionary. So I just let stuff like \u201cdon\u2019t\u201d be two words.  If this is a problem for your use case, maybe try a fancy <a href=\"http://www.nltk.org/api/nltk.tokenize.html\" rel=\"nofollow\">NLTK tokenizer</a>?</p>\n<p>One possible heuristic solution would be to replace the punctuation and such with a placeholder on the translation pass, and then on the regular expression pass, replace the with a space iff there\u2019s a space (but not newline, because of line-break hyphenation) on either side. This could work, but would involve making the regex a bit more convoluted + would smush together rather than separate compound constructions that are normally hyphenated. For example, \u201cinjury-prone\u201d would become \u201cinjuryprone\u201d rather than the more sensible \u201cinjury prone.\u201d  Nonetheless, it\u2019s worth a try as an alternative implementation, perhaps under a flag like <cite>heuristic-hyphens = True</cite>.  I might implement this down the line, but I\u2019d welcome a PR to do it before I get there.</p>\n</div>\n<div id=\"contributing-license-tests-etc\">\n<h2>Contributing, license, tests, etc.</h2>\n<p>In general, this is a work in progress, pull requests very much desired.  Please add a test for any functionality you add, and please keep the default behavior the same (except for bugfixed) without any new arguments (i.e., for new functionality, just add another optional argument).</p>\n<p>Tests are bare unittest; I use nose2 for testing, but you can use whatever. Run tests from the test directory so it can find the text file in there.</p>\n<p>MIT license.</p>\n</div>\n\n          </div>"}, "last_serial": 3084486, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "cf3f3f93f97fd18ea48b615822725740", "sha256": "96dd41324ac9292e87aea0a10c01f196316bb6708f7593a850163567dd450598"}, "downloads": -1, "filename": "dirtyclean-0.1.tar.gz", "has_sig": false, "md5_digest": "cf3f3f93f97fd18ea48b615822725740", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 3511, "upload_time": "2017-08-09T17:12:40", "upload_time_iso_8601": "2017-08-09T17:12:40.221003Z", "url": "https://files.pythonhosted.org/packages/24/8f/79242bd8d989ab65b83f95d68606ece3f49d58e4b7bd837022a0d4f7c8f6/dirtyclean-0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cf3f3f93f97fd18ea48b615822725740", "sha256": "96dd41324ac9292e87aea0a10c01f196316bb6708f7593a850163567dd450598"}, "downloads": -1, "filename": "dirtyclean-0.1.tar.gz", "has_sig": false, "md5_digest": "cf3f3f93f97fd18ea48b615822725740", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 3511, "upload_time": "2017-08-09T17:12:40", "upload_time_iso_8601": "2017-08-09T17:12:40.221003Z", "url": "https://files.pythonhosted.org/packages/24/8f/79242bd8d989ab65b83f95d68606ece3f49d58e4b7bd837022a0d4f7c8f6/dirtyclean-0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:38:15 2020"}